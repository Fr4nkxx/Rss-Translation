<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最新结果</title>
    <link>http://link.springer.com</link>
    <description>Springer 提供的最新内容</description>
    <lastBuildDate>Wed, 02 Oct 2024 18:19:00 GMT</lastBuildDate>
    <item>
      <title>Deblur e-NeRF：高速或低光条件下运动模糊事件中的 NeRF</title>
      <link>http://link.springer.com/10.1007/978-3-031-73232-4_11</link>
      <description><![CDATA[摘要
事件相机的独特设计理念使其成为高速、高动态范围和低光环境的理想选择，而标准相机在这些环境中表现不佳。然而，与大多数人的想法相反，事件相机也存在运动模糊，尤其是在这些具有挑战性的条件下。这是由于事件传感器像素的带宽有限，而带宽主要与光强度成正比。因此，为了确保事件相机能够真正在这种条件下表现出色，比标准相机更具优势，必须在下游任务（尤其是重建）中考虑事件运动模糊。然而，之前没有关于从事件中重建神经辐射场 (NeRF) 的研究，也没有事件模拟器考虑过事件运动模糊的全部影响。为此，我们提出了 Deblur e-NeRF，这是一种新方法，可以直接有效地从高速或低光条件下产生的运动模糊事件中重建模糊最小的 NeRF。这项工作的核心部分是一个物理上精确的像素带宽模型，该模型考虑了事件运动模糊。我们还引入了阈值归一化总变分损失，以更好地规范大型无纹理斑块。在真实和新颖的现实模拟序列上进行的实验验证了我们的有效性。我们的代码、事件模拟器和合成事件数据集都是开源的。]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-73232-4_11</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>RS-NeRF：来自滚动快门图像的神经辐射场</title>
      <link>http://link.springer.com/10.1007/978-3-031-72952-2_10</link>
      <description><![CDATA[摘要
神经辐射场 (NeRF) 因其令人印象深刻的新颖视图合成能力而变得越来越流行。然而，它们的有效性受到大多数相机系统中常见的滚动快门 (RS) 效应的阻碍。为了解决这个问题，我们提出了 RS-NeRF，这是一种使用具有 RS 失真的输入从新颖视图合成正常图像的方法。这涉及一个物理模型，该模型复制 RS 条件下的图像形成过程并联合优化 NeRF 参数和每个图像行的相机外部参数。我们通过深入研究 RS 特性并开发算法来增强其功能，进一步解决了基本 RS-NeRF 模型的固有缺陷。首先，我们施加平滑度正则化以更好地估计轨迹并提高合成质量，与相机运动先验保持一致。我们还通过引入多采样算法来识别和解决 vanilla RS 模型中的一个根本缺陷。这种新方法通过全面利用每个中间相机姿势的不同行中的 RGB 数据来提高模型的性能。通过严格的实验，我们证明 RS-NeRF 在合成和真实场景中都超越了以前的方法，证明了它能够有效纠正与 RS 相关的失真。可用的代码和数据：https://github.com/MyNiuuu/RS-NeRF。]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-72952-2_10</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>RING-NeRF：重新思考多功能高效神经场的归纳偏差</title>
      <link>http://link.springer.com/10.1007/978-3-031-72761-0_9</link>
      <description><![CDATA[摘要
神经领域的最新进展主要依赖于开发特定于任务的监督，这通常会使模型复杂化。与开发难以组合和特定的模块不同，另一种通常被忽视的方法是将场景表示的通用先验（也称为归纳偏差）直接注入 NeRF 架构。基于这个想法，我们提出了 RING-NeRF 架构，其中包括两个归纳偏差：场景的连续多尺度表示和解码器潜在空间在空间和尺度域上的不变性。我们还设计了一个利用这些归纳偏差的单一重建过程，并通过实验证明了在多个任务（抗锯齿、少量视图重建、没有场景特定初始化的 SDF 重建）上与专用架构在质量方面的性能相当，同时效率更高。此外，RING-NeRF 具有动态提高模型分辨率的独特能力，为自适应重建开辟了道路。项目页面位于：https://cea-list.github.io/RING-NeRF]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-72761-0_9</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>URS-NeRF：神经辐射场的无序滚动快门束调整</title>
      <link>http://link.springer.com/10.1007/978-3-031-72761-0_26</link>
      <description><![CDATA[摘要
本文提出了一种新的神经辐射场（NeRF）滚动快门束调整方法，该方法利用无序滚动快门（RS）图像来获得隐式三维表示。现有的 NeRF 方法由于图像中的 RS 效应而存在图像质量低和初始相机姿势不准确的问题。此外，以前将 RS 图像合并到 NeRF 中的方法需要严格的顺序数据输入，从而限制了其广泛的适用性。相比之下，我们的方法通过估计相机姿势和速度来恢复 RS 图像的物理形态，从而消除了对顺序数据的输入限制。此外，我们采用由粗到细的训练策略，其中场景图中成对帧的 RS 极线约束用于检测落入局部最小值的相机姿势。检测到的异常姿势通过邻近姿势的插值方法进行校正。实验结果验证了我们的方法相对于最先进方法的有效性，并表明 3D 表示的重建不受视频序列输入要求的限制。]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-72761-0_26</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Structured-NeRF：具有神经表征的分层场景图</title>
      <link>http://link.springer.com/10.1007/978-3-031-72761-0_11</link>
      <description><![CDATA[摘要
我们提出了一种结构化神经辐射场 (Structured-NeRF)，用于室内场景表示，该结构基于一种新颖的分层场景图结构来组织神经辐射场。现有的以对象为中心的方法仅关注对象的固有特征，而忽略了它们之间的语义和物理关系。我们的场景图擅长管理场景内对象之间复杂的现实世界相关性，从而实现超越新颖视图合成的功能，例如场景重新排列。基于分层结构，我们引入了基于语义和物理关系的优化策略，从而简化了场景编辑所涉及的操作并确保了效率和准确性。此外，我们对对象进行阴影渲染，以进一步增强渲染图像的真实感。实验结果表明，我们的结构化表示不仅在对象级和场景级渲染中实现了最佳（SOTA）性能，而且还与LLM/VLM结合推进了下游应用，例如自动和指令/图像条件场景重新排列，从而方便且可控地将NeRF扩展到交互式编辑。]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-72761-0_11</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉 – ECCV 2024</title>
      <link>http://link.springer.com/10.1007/978-3-031-72761-0</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-72761-0</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉 – ECCV 2024</title>
      <link>http://link.springer.com/10.1007/978-3-031-72670-5</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-72670-5</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉 – ECCV 2024</title>
      <link>http://link.springer.com/10.1007/978-3-031-72933-1</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-72933-1</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉 – ECCV 2024</title>
      <link>http://link.springer.com/10.1007/978-3-031-72920-1</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-72920-1</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉 – ECCV 2024</title>
      <link>http://link.springer.com/10.1007/978-3-031-73232-4</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-73232-4</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉 – ECCV 2024</title>
      <link>http://link.springer.com/10.1007/978-3-031-72784-9</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-72784-9</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉 – ECCV 2024</title>
      <link>http://link.springer.com/10.1007/978-3-031-72952-2</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-72952-2</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉 – ECCV 2024</title>
      <link>http://link.springer.com/10.1007/978-3-031-72998-0</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-72998-0</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉 – ECCV 2024</title>
      <link>http://link.springer.com/10.1007/978-3-031-72658-3</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-72658-3</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>神经辐射场的物理合理色彩校正</title>
      <link>http://link.springer.com/10.1007/978-3-031-72784-9_10</link>
      <description><![CDATA[摘要
神经辐射场已成为许多 3D 计算机视觉和计算机图形应用的首选表示，例如新颖的视图合成和 3D 重建。多摄像头系统通常用作基于 NeRF 的多视图任务（例如动态场景采集或逼真的头像动画）中的图像捕获设置。然而，这种设置中经常被忽视的一个关键问题是多个摄像头之间的颜色响应存在明显差异，这会对 NeRF 重建性能产生不利影响。多个输入图像之间的这些颜色差异源于两个方面：1) 场景的隐式属性，例如反射和阴影，以及 2) 相机设置和光照条件的外部差异。在本文中，我们通过提出一种新颖的色彩校正模块来解决这个问题，该模块模拟相机中的物理色彩处理并嵌入到 NeRF 中，从而实现统一的色彩 NeRF 重建。除了用于外部差异的独立于视图的色彩校正模块之外，我们还预测了一个与视图相关的函数来最小化色彩残差（包括 例如 镜面反射和阴影），以消除固有属性的影响。我们进一步描述了如何使用参考图作为指导来扩展该方法，以在新颖的视图上实现美观的色彩一致性和色彩转换。实验验证了我们的方法在色彩校正和色彩一致性的定量和定性评估方面均优于基线方法。
]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-72784-9_10</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>