<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最新结果</title>
    <link>http://link.springer.com</link>
    <description>Springer 提供的最新内容</description>
    <lastBuildDate>Wed, 13 Mar 2024 06:14:40 GMT</lastBuildDate>
    <item>
      <title>基于实时距离场加速的大型运动场自由视点视频合成</title>
      <link>http://link.springer.com/10.1007/s41095-022-0323-3</link>
      <description><![CDATA[摘要
自由视点视频允许用户从任何虚拟角度观看物体，创造身临其境的视觉体验。该技术增强了多媒体表演的交互性和自由度。然而，许多自由视点视频合成方法很难满足实时、高精度的要求，特别是对于面积较大、运动物体较多的运动场。为了解决这些问题，我们提出了一种基于距离场加速的自由视点视频合成方法。其中心思想是融合多视点距离场信息并利用其自适应调整搜索步长。自适应步长搜索有两种用途：用于多目标三维表面的快速估计，以及基于全局遮挡判断的合成视图渲染。我们使用并行计算进行交互式显示、使用 CUDA 和 OpenGL 框架来实现我们的想法，并使用真实世界和模拟实验数据集进行评估。结果表明，所提出的方法可以在大型运动场上以 25 fps 渲染具有多个对象的自由视点视频。此外，我们合成的新颖视点图像的视觉质量超过了最先进的基于神经渲染的方法。
      



]]></description>
      <guid>http://link.springer.com/10.1007/s41095-022-0323-3</guid>
      <pubDate>Mon, 01 Apr 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于可微渲染的动态海洋反建模</title>
      <link>http://link.springer.com/10.1007/s41095-023-0338-4</link>
      <description><![CDATA[摘要
学习和推断捕获的 2D 场景的潜在运动模式，然后重新创建与现实世界自然现象一致的动态演化，这对图形和动画具有很高的吸引力。为了弥合虚拟和现实环境之间的技术差距，我们专注于视觉一致且属性可验证的海洋的逆向建模和重建，利用深度学习和可微物理来学习几何并以自我监督的方式构成波浪。首先，我们使用两个网络推断分层几何，这两个网络通过可微渲染器进行了优化。我们通过配备可微分海洋模型的网络从推断的几何序列中提取波浪分量。然后，可以使用重建的波浪分量来演化海洋动力学。通过大量的实验，我们验证了我们的新方法在几何重建和波浪估计方面都能产生令人满意的结果。此外，新框架具有逆向建模潜力，可促进大量图形应用，例如快速生成物理精确的场景动画以及由真实海洋场景引导的编辑。
      



]]></description>
      <guid>http://link.springer.com/10.1007/s41095-023-0338-4</guid>
      <pubDate>Mon, 01 Apr 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于实时轻量级辐射场渲染的频率重要性高斯分布</title>
      <link>http://link.springer.com/10.1007/s11042-024-18679-x</link>
      <description><![CDATA[摘要
最近，依赖辐射场的新颖视图合成领域取得了重大进展。通过结合 Splatting 技术，一种名为高斯 Splatting 的新方法实现了卓越的渲染质量和实时性能。然而，该方法的训练过程会产生很大的性能开销，并且训练得到的模型非常庞大。为了应对这些挑战，我们改进了高斯分布并提出了频率重要性高斯分布。我们的方法通过提取场景的频率特征来降低性能开销。首先，我们从采样理论的角度分析了高斯分布法空间采样策略的优点和局限性。其次，我们设计了增强高斯以更有效地表达高频信息，同时降低性能开销。第三，我们构建了频率敏感的损失函数，以增强网络感知频域的能力并优化场景的空间结构。最后，我们提出了一种基于场景背景重建程度的动态自适应密度控制策略，根据训练结果动态自适应空间样本点生成策略，防止模型中产生冗余数据。我们在几个常用的数据集上进行了实验，结果表明，我们的方法在内存开销和存储使用方面比原始方法具有显着优势，并且能够保持原始方法的图像质量。]]></description>
      <guid>http://link.springer.com/10.1007/s11042-024-18679-x</guid>
      <pubDate>Tue, 12 Mar 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用生成深度神经网络通过嘴唇运动实现音频视频同步</title>
      <link>http://link.springer.com/10.1007/s11042-024-18695-x</link>
      <description><![CDATA[摘要
随着元宇宙的展开，音频与视频的实时同步变得至关重要。许多模型（例如 Wav2Lip、Sync Net 和 Lip Gan）已被开发用于同步音频视频以呈现高影响力的内容。选择合适的损失函数直接影响音视频同步的结果和准确性。像 Wav2Lip 这样的模型，通过 Huber Loss 函数得到增强，成为该领域的领跑者。本文进行了全面的比较分析，证明 Huber Loss 在收敛效率和同步质量方面优于 L1、L2 和 SmoothL1 损失。实证结果明确主张将 Huber Loss 集成到 Wav2Lip 模型中，强调其能够使嘴唇运动与音频更加连贯和自然地集成。实验结果表明，Huber Loss 在 61,500 个步骤中实现了 0.00091 的平均训练损失和 0.00141 的评估损失，同时显着降低了 2.20669 的同步损失。这些结果代表了同步精度的显着提高，与当代损失函数相比提高了 20% 到 30%。]]></description>
      <guid>http://link.springer.com/10.1007/s11042-024-18695-x</guid>
      <pubDate>Mon, 11 Mar 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>全球激光雷达定位调查：挑战、进展和未决问题</title>
      <link>http://link.springer.com/10.1007/s11263-024-02019-5</link>
      <description><![CDATA[摘要
了解自身姿态是所有移动机器人应用的关键。因此，姿态估计是移动机器人核心功能的一部分。在过去的二十年中，激光雷达扫描仪已成为机器人定位和测绘的标准传感器。本文旨在概述基于激光雷达的全球定位的最新进展和进展。我们首先提出问题并探索应用范围。然后，我们对该方法进行回顾，包括地图、描述符提取和跨机器人定位等多个主题的最新进展。本文的内容分为三个主题。第一个主题涉及全局位置检索和局部姿态估计的结合。第二个主题是将单次测量升级为连续测量，以实现连续的全局定位。最后，第三个主题侧重于将单机器人全局定位扩展到多机器人系统中的跨机器人定位。我们在调查结束时讨论了全球激光雷达本地化的开放挑战和有希望的方向。据我们所知，这是首次针对移动机器人全球激光雷达定位的全面调查。]]></description>
      <guid>http://link.springer.com/10.1007/s11263-024-02019-5</guid>
      <pubDate>Wed, 06 Mar 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>量化鲁棒性：在噪声域中使用智能树进行 3D 树点云骨架化</title>
      <link>http://link.springer.com/10.1007/s10044-024-01238-3</link>
      <description><![CDATA[摘要
从 3D 树点云中提取树骨架受到噪声和不完整数据的挑战。虽然我们之前的工作（Dobbs 等人，在：伊比利亚模式识别和图像分析会议，施普林格，柏林，第 351-362 页，2023 年）引入了一种用于近似树枝中轴的深度学习方法，但其对各种类型的鲁棒性噪声的影响尚未得到彻底评估。本文解决了这一差距。具体来说，我们通过引入 3D Perlin 噪声（代表减性噪声）和高斯噪声（模拟加性噪声）来模拟现实世界的噪声挑战。为了促进此评估，我们引入了一个新的合成树点云数据集，可在  获取https://github.com/uc-vision/synthetic-trees-II。我们的结果表明，我们基于深度学习的骨架化方法能够容忍加性和减性噪声。]]></description>
      <guid>http://link.springer.com/10.1007/s10044-024-01238-3</guid>
      <pubDate>Tue, 05 Mar 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于 NeRF 的 GAN 的对应蒸馏</title>
      <link>http://link.springer.com/10.1007/s11263-023-01903-w</link>
      <description><![CDATA[摘要
神经辐射场 (NeRF) 在保留物体和场景的精细细节方面显示出了良好的结果。然而，与显式形状表示（例如网格）不同，在同一类别的不同 NeRF 之间建立密集对应关系仍然是一个悬而未决的问题，这在许多下游任务中至关重要。这个问题的主要困难在于 NeRF 的隐式性质和缺乏真实对应注释。在本文中，我们展示了通过利用基于 NeRF 的预训练 GAN 中封装的丰富语义和结构先验，可以绕过这些挑战。具体来说，我们从三个方面利用这些先验，即（1）将潜在代码作为全局结构指标的双变形场，（2）将生成器特征视为几何感知局部描述符的学习目标，以及（3）源无限的特定于对象的 NeRF 样本。我们的实验表明，这样的先验可以产生准确、平滑且稳健的 3D 密集对应。我们还表明，跨 NeRF 建立的密集对应关系可以有效地启用许多基于 NeRF 的下游应用，例如纹理传输。]]></description>
      <guid>http://link.springer.com/10.1007/s11263-023-01903-w</guid>
      <pubDate>Fri, 01 Mar 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>RDNeRF：用于密集自由视图合成的相对深度引导 NeRF</title>
      <link>http://link.springer.com/10.1007/s00371-023-02863-5</link>
      <description><![CDATA[摘要
在本文中，我们专注于室内场景中自由移动的密集视图合成，以实现比稀疏视图更好的用户交互。神经辐射场 (NeRF) 可以很好地处理稀疏和球形捕获的场景，但它在具有密集自由视图的场景中表现不佳。我们扩展 NeRF 来处理这些室内场景视图。我们提出了一种名为相对深度引导 NeRF (RDNeRF) 的基于学习的方法，该方法联合渲染 RGB 图像并在密集的自由视图中恢复场景几何形状。为了在没有真实深度的情况下恢复每个视图的几何形状，我们建议通过隐式函数直接学习相对深度，并将其转换为几何体边界，用于几何感知采样和 NeRF 集成。通过正确的场景几何，我们进一步对输入的隐式内部相关性进行建模，以增强 NeRF 在密集自由视图中的表示能力。我们在室内场景中进行了大量的实验，以实现密集的自由视图合成。 RDNeRF 优于当前最先进的方法，达到 24.95 PSNR 分数和 0.77 SSIM 分数。此外，它比基本模型恢复更准确的几何形状。]]></description>
      <guid>http://link.springer.com/10.1007/s00371-023-02863-5</guid>
      <pubDate>Fri, 01 Mar 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenDIBR：开放基于深度图像的 VR 光场视频实时渲染器</title>
      <link>http://link.springer.com/10.1007/s11042-023-16250-8</link>
      <description><![CDATA[摘要
在这项工作中，我们提出了一种新颖的光场渲染框架，允许观看者在从具有视觉和深度信息的多视图图像/视频数据集重建的虚拟场景中行走。考虑到沉浸式媒体应用，该框架旨在通过输入视频支持动态场景，让观看者在大范围内充分自由移动，并实现实时渲染，甚至在虚拟现实 (VR) 中也是如此。本文探讨了基于深度图像的渲染 (DIBR) 如何成为少数能够满足所有要求的最先进技术之一。因此，我们实施了 OpenDIBR（一种公开可用的 DIBR）作为该框架的概念证明。它使用 Nvidia 的视频编解码器 SDK 在 GPU 上快速解码颜色和深度视频。然后，解码后的深度图和颜色帧将变形为 OpenGL 中的输出视图。根据输入和输出相机位置，每个输入贡献通过每像素加权平均值混合在一起。视觉质量比较实验得出结论，OpenDIBR 在客观和主观上与 TMIV 相似，但优于 NeRF。在性能方面，OpenDIBR 在桌面上以 90 Hz 运行，最多可播放 4 个全高清输入视频，在 VR 中可播放 2-4 个全高清输入视频，并且可以选择通过降低视频比特率、降低深度图分辨率或动态减少视频数量来进一步提高此性能。渲染的输入视频。]]></description>
      <guid>http://link.springer.com/10.1007/s11042-023-16250-8</guid>
      <pubDate>Fri, 01 Mar 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于神经辐射场的机器人辅助微创手术动态表面重建</title>
      <link>http://link.springer.com/10.1007/s11548-023-03016-8</link>
      <description><![CDATA[摘要

目的
本研究的目的是通过解决重建高度动态手术场景的挑战来改善手术场景感知。我们提出了一种新颖的深度估计网络和结合神经辐射场的重建框架，为手术任务自动化和 AR 导航提供更准确的场景信息。


方法
我们添加了空间金字塔池化模块和 Swin-Transformer 模块来增强立体深度估计的鲁棒性。我们还通过添加最佳传输的独特匹配约束来提高深度精度。为了避免高动态场景中的变形失真，我们使用神经辐射场在时间维度上隐式表示场景，并以基于学习的方式利用深度和颜色信息对其进行优化。


结果
我们在 KITTI 和 SCARED 数据集上的实验表明，所提出的深度估计网络在自然图像上的表现接近最先进的方法，并且在医学图像上超越了 SOTA 方法3 px 误差为 1.12%，EPE 误差为 0.45 px。所提出的动态重建框架成功地在完全内窥镜冠状动脉搭桥视频上重建了动态心脏表面，实现了 SOTA 性能，PSNR 为 27.983 dB，SSIM 为 0.812，LPIPS 为 0.189。


结论
我们提出的深度估计网络和重建框架为手术场景感知领域做出了重大贡献。该框架在医学数据集上取得了比 SOTA 方法更好的结果，减少了深度图上的不匹配，并产生更准确、边缘更清晰的深度图。所提出的 ER 框架在一系列动态心脏手术图像上得到了验证。未来的工作重点将集中在提高训练速度和解决视野有限的问题上。
]]></description>
      <guid>http://link.springer.com/10.1007/s11548-023-03016-8</guid>
      <pubDate>Fri, 01 Mar 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>表示模糊聚类分析的网络集合：案例研究</title>
      <link>http://link.springer.com/10.1007/s10618-023-00977-x</link>
      <description><![CDATA[摘要
随着网络统计分析在越来越多的学科中得到应用，需要新的方法来处理这种复杂性。特别是，聚类分析是最成功、最普遍的数据探索和表征技术之一。在这项工作中，我们关注如何表示模糊聚类的网络集成。我们基于概率分布、自动编码器和联合嵌入探索三种不同的网络表示。我们比较了在合成数据上对多个网络进行聚类的事实上的标准模糊计算程序。最后，我们将这种方法应用于现实世界的案例研究。]]></description>
      <guid>http://link.springer.com/10.1007/s10618-023-00977-x</guid>
      <pubDate>Fri, 01 Mar 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用周期性特征增强的神经场建模来逼真地表示人体头部头像</title>
      <link>http://link.springer.com/10.1007/s00371-024-03299-1</link>
      <description><![CDATA[摘要
本文介绍了一种突破性的神经表示技术，称为周期性特征增强神经场建模 (PNM)，专为 3D 模型量身定制。 PNM 经过精心设计，能够熟练捕捉 4D 人体头像复杂的表面几何形状，产生跨各个领域的大量应用，包括远程会议、直播营销、短视频网络广播、VR/AR/XR 应用以及视频游戏和电影行业。虽然当前的神经建模方法在表示低频或高频表面细节方面表现出色，但在同时处理这两个方面时往往会表现不佳，从而导致整体模型质量低于标准。为了克服这一限制，PNM 利用位置编码和周期性激活，利用其傅里叶特性来增强高频数据的表示，同时保持平滑、无噪声的表面。我们的实验证实了 PNM 的优越性，在定量和定性模型重建质量以及高频几何细节的刻画方面都超越了最先进的方法。最后，我们将 PNM 应用于 4D 人体化身和 Metaverse 应用程序的数字化管道中，展示了其在动态场景中卓越的视觉性能。
      ]]></description>
      <guid>http://link.springer.com/10.1007/s00371-024-03299-1</guid>
      <pubDate>Wed, 28 Feb 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于神经辐射场的多视角内窥镜场景重建用于手术模拟</title>
      <link>http://link.springer.com/10.1007/s11548-024-03080-8</link>
      <description><![CDATA[摘要

目的
在虚拟手术中，根据CT图像构建的3D模型的外观缺乏真实感，导致住院医生可能产生误解。因此，利用内窥镜捕获的多视图图像重建真实的内窥镜场景至关重要。


方法
我们提出了一种内窥镜-NeRF 网络，用于在非固定光源下对内窥镜场景进行隐式辐射场重建，并使用体积渲染合成新颖的视图。具有多个 MLP 网络和射线变换器网络的内窥镜-NeRF 网络将内窥镜场景表示为隐式场函数，在连续 5D 向量（3D 位置和 2D 方向）处具有颜色和体积密度。最终的合成图像是通过使用体积渲染聚合目标相机每条射线上的所有采样点而获得的。我们的方法考虑了光源到采样点的距离对场景辐射亮度的影响。


结果
我们的网络在我们的设备收集的猪的肺、肝、肾和心脏上进行了验证。结果表明，我们的方法合成的内窥镜场景的新视图在 PSNR、SSIM 和 LPIPS 指标方面优于现有方法（NeRF 和 IBRNet）。


结论
我们的网络可以有效地学习具有泛化能力的辐射场函数。在新的内窥镜场景上对预训练模型进行微调，进一步优化场景的神经辐射场，可以为手术模拟提供更真实、高分辨率的渲染图像。
]]></description>
      <guid>http://link.springer.com/10.1007/s11548-024-03080-8</guid>
      <pubDate>Tue, 27 Feb 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>小儿眼眶病变：骨性和创伤性病变</title>
      <link>http://link.springer.com/10.1007/s00247-024-05882-z</link>
      <description><![CDATA[摘要
眼眶病变可大致分为眼部病变、眼外软组织病变（非肿瘤性和肿瘤性）以及骨性和创伤性病变。在本文中，我们讨论了儿科眼眶和眼球的骨性和创伤性病变的关键影像学特征和鉴别诊断，强调了 CT 和 MRI 作为主要影像学手段的作用。此外，重点介绍眼部超声检查在眼内异物诊断中的辅助作用，并讨论超声检查在外伤性视网膜脱离诊断中的主要作用。

图形摘要






]]></description>
      <guid>http://link.springer.com/10.1007/s00247-024-05882-z</guid>
      <pubDate>Tue, 27 Feb 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于饱和模糊图像的深度非盲去模糊网络</title>
      <link>http://link.springer.com/10.1007/s00521-024-09495-3</link>
      <description><![CDATA[摘要
非盲图像去模糊在低水平视觉领域引起了广泛关注。然而，现有的非盲去模糊方法无法有效处理饱和模糊图像。关键在于饱和模糊图像的退化模型不满足传统模糊图像的线性卷积模型。为了解决这个问题，本文提出了一种新颖的深度非盲去模糊方法，称为饱和图像非盲去模糊网络（SDBNet）。 SDBNet包含两个可训练的子网络，即置信估计网络（CEN）和细节增强网络（DEN）。具体来说，SDBNet使用CEN来估计饱和模糊图像的置信图，用于识别模糊图像中的饱和像素，然后使用置信图和模糊核来恢复模糊图像。最后，我们使用 DEN 来增强恢复图像的边缘和纹理。我们首先预训练 CEN 和 DEN。为了有效地预训练 CEN，我们提出了一个新的鲁棒函数，用于为 CEN 生成标签数据。实验结果表明，与现有的几种非盲去模糊方法相比，SDBNet能够有效恢复饱和模糊图像，更好地恢复模糊图像的纹理、边缘等结构信息。]]></description>
      <guid>http://link.springer.com/10.1007/s00521-024-09495-3</guid>
      <pubDate>Wed, 21 Feb 2024 00:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>