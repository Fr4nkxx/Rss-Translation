<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最新结果</title>
    <link>http://link.springer.com</link>
    <description>Springer 提供的最新内容</description>
    <lastBuildDate>Sat, 17 Feb 2024 15:10:33 GMT</lastBuildDate>
    <item>
      <title>基于实时距离场加速的大型运动场自由视点视频合成</title>
      <link>http://link.springer.com/10.1007/s41095-022-0323-3</link>
      <description><![CDATA[摘要
自由视点视频允许用户从任何虚拟角度观看物体，创造身临其境的视觉体验。该技术增强了多媒体表演的交互性和自由度。然而，许多自由视点视频合成方法很难满足实时、高精度的要求，特别是对于面积较大、运动物体较多的运动场。为了解决这些问题，我们提出了一种基于距离场加速的自由视点视频合成方法。其中心思想是融合多视点距离场信息并利用其自适应调整搜索步长。自适应步长搜索有两种用途：用于多目标三维表面的快速估计，以及基于全局遮挡判断的合成视图渲染。我们使用并行计算进行交互式显示、使用 CUDA 和 OpenGL 框架来实现我们的想法，并使用真实世界和模拟实验数据集进行评估。结果表明，所提出的方法可以在大型运动场上以 25 fps 渲染具有多个对象的自由视点视频。此外，我们合成的新颖视点图像的视觉质量超过了最先进的基于神经渲染的方法。
      



]]></description>
      <guid>http://link.springer.com/10.1007/s41095-022-0323-3</guid>
      <pubDate>Mon, 01 Apr 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于可微渲染的动态海洋反建模</title>
      <link>http://link.springer.com/10.1007/s41095-023-0338-4</link>
      <description><![CDATA[摘要
学习和推断捕获的 2D 场景的潜在运动模式，然后重新创建与现实世界自然现象一致的动态演化，这对图形和动画具有很高的吸引力。为了弥合虚拟和现实环境之间的技术差距，我们专注于视觉一致且属性可验证的海洋的逆向建模和重建，利用深度学习和可微物理来学习几何并以自我监督的方式构成波浪。首先，我们使用两个网络推断分层几何，这两个网络通过可微渲染器进行了优化。我们通过配备可微分海洋模型的网络从推断的几何序列中提取波浪分量。然后，可以使用重建的波浪分量来演化海洋动力学。通过大量的实验，我们验证了我们的新方法在几何重建和波浪估计方面都能产生令人满意的结果。此外，新框架具有逆向建模潜力，可促进大量图形应用，例如快速生成物理精确的场景动画以及由真实海洋场景引导的编辑。
      



]]></description>
      <guid>http://link.springer.com/10.1007/s41095-023-0338-4</guid>
      <pubDate>Mon, 01 Apr 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于 NeRF 的 GAN 的对应蒸馏</title>
      <link>http://link.springer.com/10.1007/s11263-023-01903-w</link>
      <description><![CDATA[摘要
神经辐射场 (NeRF) 在保留物体和场景的精细细节方面显示出了良好的结果。然而，与显式形状表示（例如网格）不同，在同一类别的不同 NeRF 之间建立密集对应关系仍然是一个悬而未决的问题，这在许多下游任务中至关重要。这个问题的主要困难在于 NeRF 的隐式性质和缺乏真实对应注释。在本文中，我们展示了通过利用基于 NeRF 的预训练 GAN 中封装的丰富语义和结构先验，可以绕过这些挑战。具体来说，我们从三个方面利用这些先验，即（1）将潜在代码作为全局结构指标的双变形场，（2）将生成器特征视为几何感知局部描述符的学习目标，以及（3）源无限的特定于对象的 NeRF 样本。我们的实验表明，这样的先验可以产生准确、平滑且稳健的 3D 密集对应。我们还表明，跨 NeRF 建立的密集对应关系可以有效地启用许多基于 NeRF 的下游应用，例如纹理传输。]]></description>
      <guid>http://link.springer.com/10.1007/s11263-023-01903-w</guid>
      <pubDate>Fri, 01 Mar 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>RDNeRF：用于密集自由视图合成的相对深度引导 NeRF</title>
      <link>http://link.springer.com/10.1007/s00371-023-02863-5</link>
      <description><![CDATA[摘要
在本文中，我们专注于室内场景中自由移动的密集视图合成，以实现比稀疏视图更好的用户交互。神经辐射场 (NeRF) 可以很好地处理稀疏和球形捕获的场景，但它在具有密集自由视图的场景中表现不佳。我们扩展 NeRF 来处理这些室内场景视图。我们提出了一种名为相对深度引导 NeRF (RDNeRF) 的基于学习的方法，该方法联合渲染 RGB 图像并在密集的自由视图中恢复场景几何形状。为了在没有真实深度的情况下恢复每个视图的几何形状，我们建议通过隐式函数直接学习相对深度，并将其转换为几何体边界，用于几何感知采样和 NeRF 集成。通过正确的场景几何，我们进一步对输入的隐式内部相关性进行建模，以增强 NeRF 在密集自由视图中的表示能力。我们在室内场景中进行了大量的实验，以实现密集的自由视图合成。 RDNeRF 优于当前最先进的方法，达到 24.95 PSNR 分数和 0.77 SSIM 分数。此外，它比基本模型恢复更准确的几何形状。]]></description>
      <guid>http://link.springer.com/10.1007/s00371-023-02863-5</guid>
      <pubDate>Fri, 01 Mar 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ReliTalk：从单个视频生成可重新点亮的说话肖像</title>
      <link>http://link.springer.com/10.1007/s11263-024-02007-9</link>
      <description><![CDATA[摘要
近年来，从单眼视频创建生动的音频驱动肖像方面取得了巨大进步。然而，如何将创建的视频头像无缝地适应不同背景和光照条件的其他场景仍然没有解决。另一方面，现有的重新照明研究大多依赖于动态照明或多视图数据，这对于创建视频肖像来说过于昂贵。为了弥补这一差距，我们提出了 ReliTalk，这是一种新颖的框架，用于从单眼视频生成可重新点亮的音频驱动的谈话肖像。我们的主要见解是从隐式学习的音频驱动的面部法线和图像中分解肖像的反射率。具体来说，我们利用从音频特征导出的 3D 面部先验，通过隐式函数来预测精细的法线贴图。然后，这些最初预测的法线通过动态估计给定视频的照明条件，在反射率分解中发挥关键作用。此外，在模拟的多个照明条件下，使用身份一致的损失来细化立体人脸表示，解决了由于单个单目视频的可用视图有限而引起的不适定问题。大量的实验验证了我们提出的框架在真实数据集和合成数据集上的优越性。我们的代码发布于 (https://github.com/arthur-qiu/ReliTalk).]]></description>
      <guid>http://link.springer.com/10.1007/s11263-024-02007-9</guid>
      <pubDate>Fri, 16 Feb 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>VI-NeRF-SLAM：具有 NeRF 映射的实时视觉惯性 SLAM</title>
      <link>http://link.springer.com/10.1007/s11554-023-01412-6</link>
      <description><![CDATA[摘要
在众多机器人和自动驾驶任务中，传统的视觉SLAM算法通过稀疏特征点估计相机在场景中的位置，并通过估计稀疏点云的深度来表达地图。然而，实际应用需要SLAM实时创建密集地图，克服点云的稀疏和遮挡问题。此外，SLAM地图的优点是具有自动完成能力，当相机仅观察到物体的80%时，地图可以自动推断并完成剩余的20%。因此，需要更密集、更智能的地图表示。在本文中，我们提出了一种具有神经辐射场重建的视觉惯性 SLAM 来解决上述挑战。我们将传统的基于规则的优化与 NeRF 相结合。这种方法可以通过快速估计相机运动和稀疏特征点深度来实时更新 NeRF 局部函数以重建 3D 场景。为了实现更好的相机姿势和全局一致的地图，我们解决了快速运动变化导致的 IMU 噪声尖峰问题，以及处理由于闭环融合导致的姿势调整。具体来说，我们采用加宽静态噪声协方差的形式来重新拟合动态噪声协方差。在闭环融合过程中，我们将闭环前和闭环后之间的位姿调整视为时空变换，将 NeRF 参数从前到后迁移，以加快 NeRF 映射中的闭环调整。此外，我们将该方法扩展到只有灰度图像的场景。通过扩展灰度图像的颜色通道并进行线性空间映射，我们可以仅用灰度图像快速重建3D场景。我们在 RGB 和灰度场景中展示了我们的方法的精度和速度优势。]]></description>
      <guid>http://link.springer.com/10.1007/s11554-023-01412-6</guid>
      <pubDate>Fri, 09 Feb 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于人体渲染的几何引导可推广 NeRF</title>
      <link>http://link.springer.com/10.1007/s11042-024-18410-w</link>
      <description><![CDATA[摘要
从稀疏的输入视图中呈现逼真的人类新颖视图是一项挑战。一方面，最近的人体渲染作品仅限于特定于个人的情况，因此不能推广到新的表演者。另一方面，可推广到新目标的算法是针对场景或对象开发的，并不直接适用于具有复杂身体姿势的新表演者。为此，我们提出了一种新的人工渲染管道，该管道仅采用从未出现在训练数据中的目标执行者的稀疏视图作为输入。然后，它会在任意视点合成高质量的捕获图像。我们框架的核心是利用几何先验来指导神经辐射场，以多视图图像作为输入进行人体渲染。这不仅可以帮助处理聚合多视图特征时由骨骼运动引起的自遮挡问题，而且有助于推理表演者的几何形状。定性和定量评估结果均表明，我们的方法比当前最先进的技术具有更强的泛化能力。]]></description>
      <guid>http://link.springer.com/10.1007/s11042-024-18410-w</guid>
      <pubDate>Thu, 08 Feb 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于 3D 数据处理的变分自动编码器</title>
      <link>http://link.springer.com/10.1007/s10462-023-10687-x</link>
      <description><![CDATA[摘要
变分自动编码器 (VAE) 在高维数据生成中发挥着重要作用，因为它们能够将随机数据表示与最新深度学习技术的强大功能相融合。这些类型的生成器的主要优点在于它们能够对信息进行编码，并且能够解码和概括新样本。这种功能在 2D 图像处理中得到了广泛的探索；然而，只有有限的研究关注用于 3D 数据处理的 VAE。在本文中，我们全面回顾了使用 VAE 进行 3D 数据处理的最新成就。这些 3D 数据类型主要是点云、网格和体素网格，它们是广泛应用的焦点，尤其是在机器人领域。首先，我们很快会介绍基本的自动编码器以及 VAE 的扩展以及与离散点云处理相关的更多子类别。然后，根据 3D 数据特定的 VAE 对空间数据的操作方式来呈现。最后，提供了一些总结方法、代码和数据集的综合表格以及引用图，以便更好地理解应用于 3D 数据的 VAE。分析的论文的结构遵循分类法，该分类法根据算法的主要数据类型和应用领域来区分算法。]]></description>
      <guid>http://link.springer.com/10.1007/s10462-023-10687-x</guid>
      <pubDate>Thu, 08 Feb 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>人工智能驱动的元宇宙：分析、挑战和未来展望</title>
      <link>http://link.springer.com/10.1007/s10462-023-10641-x</link>
      <description><![CDATA[摘要
Metaverse 是一个虚拟现实 (VR) 空间，用户可以在其中相互交互并与数字对象交互，它正在迅速成为现实。随着这个新世界的发展，人工智能 (AI) 在塑造其发展方面发挥着越来越重要的作用。将人工智能与元宇宙中的新兴技术相结合，为以前不可能实现的沉浸式体验创造了新的可能性。本文探讨了人工智能如何与物联网、区块链、自然语言处理、虚拟现实、增强现实、混合现实和扩展现实等技术相结合。在元宇宙中使用人工智能的一个潜在好处是能够根据个人用户的行为和偏好为他们创建个性化体验。在元宇宙中使用人工智能的另一个潜在好处是能够自动执行重复性任务，从而为更复杂和更具创造性的工作腾出时间和资源。然而，在元宇宙中使用人工智能也存在一些挑战，例如确保用户隐私以及解决偏见和歧视问题。通过研究在元宇宙中使用人工智能的潜在好处和挑战，包括伦理考虑，我们可以更好地为这个令人兴奋的 VR 新时代做好准备。本文对人工智能及其与元宇宙中其他新兴技术的集成进行了全面的调查，随着元宇宙的不断发展和壮大，开发人员和研究人员及时了解人工智能和新兴技术的最新发展将非常重要充分发挥他们的潜力。]]></description>
      <guid>http://link.springer.com/10.1007/s10462-023-10641-x</guid>
      <pubDate>Mon, 05 Feb 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>甲状腺切除术中是否需要识别并解剖喉外神经？一项前瞻性研究</title>
      <link>http://link.springer.com/10.1186/s12893-023-02287-x</link>
      <description><![CDATA[摘要

背景
与喉返神经相比，EBSLN（或喉外神经）的走行及其与甲状腺的关系研究较少。这是一项前瞻性术中研究，旨在确定 EBSLN 相对于 IPC、上甲状腺蒂以及神经穿过 STA 点的解剖变化。此外，该研究旨在提出一种保存其的技术程序。


方法
我们对拉巴特 Ibn Sina 医院 B 外科的 50 名患者（总共 100 根神经）进行了一项前瞻性研究，这些患者接受了甲状腺全切除术。术中，在结扎上甲状腺血管之前，目视识别并保存 EBSLN。使用已建立的分类系统对每个神经进行分类。


结果
总体汇总 EBSLN 识别率为 82%。 Cernea IIa 型（穿过 STA 的神经距离上甲状腺极上边缘不到 1 厘米）和 Friedman II 型（神经刺穿 IPC 的下部纤维）最为常见（分别为 64% 和 44%）。 Kierner IV 型（神经穿过甲状腺上极上方的 STA 分支）占 27% 的病例。


结论
更好地了解颈部的手术解剖结构，可以通过保留喉外神经和喉返神经来获得更好的甲状腺切除术效果。
]]></description>
      <guid>http://link.springer.com/10.1186/s12893-023-02287-x</guid>
      <pubDate>Sun, 04 Feb 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>建筑和工程中的人工智能从误解到改变游戏规则的前景</title>
      <link>http://link.springer.com/10.1007/s44223-023-00046-9</link>
      <description><![CDATA[摘要
人工智能以越来越快的速度和强度侵入我们的生活和职业。房地产的建筑、工程、施工和运营只是胆怯而姗姗来迟地加入了这一趋势。本文概述了迄今为止人工智能在建筑领域的基本概念、方法、一般背景和成果，讨论了成就和前景，并总结了机器学习在该领域的部署的观点。该领域最近一些“著名成就”的记录被纠正并受到挑战，该技术（真正）创造潜力的错误观念被揭穿。其根源在于对下一个工作流程的远见卓识，一方面是富有成效和创造性的建筑和工程设计、施工和房地产管理，另一方面是最先进的机器学习，这是一个雄心勃勃但现实的目标人工智能促进建筑创意、建筑设计、规划和运营的研发蓝图已提交讨论。注意力转向开源模式平台、生成模式处理、生成预设计、参数评估和优化、基于强化学习的机器学习最新成果、基于模仿的学习、从演示中学习行为策略以及自学习范式着眼于设计开发过程，而不仅仅是其结果。利用评估的客观性和简化工作流程，人工智能有望释放真正的建筑创造力，并充分利用设计、规划和运营流程的生产力和效率。]]></description>
      <guid>http://link.springer.com/10.1007/s44223-023-00046-9</guid>
      <pubDate>Fri, 02 Feb 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>生成对抗网络及其在图像合成中的应用的简要研究</title>
      <link>http://link.springer.com/10.1007/s11042-023-16175-2</link>
      <description><![CDATA[摘要
图像合成（IS）是人工智能（AI）和计算机视觉的扩展，是一种人工生成保留某些特定所需内容的图像的技术。处理 IS 问题的适当程序是使用深度生成模型来解决它。生成模型广泛应用于人工智能的众多子领域，并支持图像、文本和音乐等复杂场景的多功能演示。在本文中，一类特殊的深度生成模型，即生成对抗网络（GAN）被认为提供了一种获取从反向传播信号导出的深度插图的方法，而无需使用广泛的带注释的训练数据。 GAN 架构的设计在图像合成中起着关键作用，本文的动机是分析基于 GAN 的不同变体在图像合成方面的 GAN 架构。此外，我们还对 GAN 的紧凑分类及其主要特征、优缺点进行了研究，以确定该领域的研究挑战。]]></description>
      <guid>http://link.springer.com/10.1007/s11042-023-16175-2</guid>
      <pubDate>Thu, 01 Feb 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MusicFace：音乐驱动的富有表现力的歌脸合成</title>
      <link>http://link.springer.com/10.1007/s41095-023-0343-7</link>
      <description><![CDATA[摘要
合成由音乐驱动的生动逼真的歌声仍然是一个有趣且具有挑战性的问题。在本文中，我们提出了一种完成此任务的方法，其中包括嘴唇、面部表情、头部姿势和眼睛的自然运动。由于常见音乐音频信号中人声和背景音乐的混合信息耦合，我们设计了一种解耦和融合策略来应对这一挑战。我们首先将输入的音乐音频分解为人声流和背景音乐流。由于两个流输入信号与面部表情、头部运动和眼睛状态的动态之间存在隐式且复杂的相关性，我们用注意力方案对它们的关系进行建模，其中两个流的效果无缝融合。此外，为了提高生成结果的表现力，我们将头部运动生成分解为速度和方向，并将眼睛状态生成分解为短期眨眼和长期闭眼，并分别建模。我们还构建了一个新颖的数据集 SingingFace，以支持此任务的模型训练和评估，包括该主题的未来工作。大量的实验和用户研究表明，我们提出的方法能够合成生动的歌声面孔，在质量和数量上都优于现有技术。
      



]]></description>
      <guid>http://link.springer.com/10.1007/s41095-023-0343-7</guid>
      <pubDate>Thu, 01 Feb 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>以更细粒度解析对象：一项调查</title>
      <link>http://link.springer.com/10.1007/s11633-022-1404-6</link>
      <description><![CDATA[摘要
细粒度视觉解析，包括细粒度部分分割和细粒度对象识别，由于其在农业等许多实际应用中的重要性而引起了相当大的关注、遥感和空间技术。主要的研究工作遵循不同的范式来处理这些细粒度的子任务，而忽略了这些任务之间的内在关系。此外，鉴于大多数研究仍然碎片化，我们从学习零件关系的新角度对先进工作进行了深入研究。从这个角度来看，我们首先将最近的研究和基准综合与新的分类法相结合。在此基础上，我们重新审视细粒度零件分割和识别任务中的普遍挑战，并通过零件关系学习针对这些重要挑战提出新的解决方案。此外，我们还总结了细粒度视觉解析方面的几个有前景的研究方向，以供未来研究使用。]]></description>
      <guid>http://link.springer.com/10.1007/s11633-022-1404-6</guid>
      <pubDate>Fri, 12 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于跨模态哈希检索的无监督多视角融合语义对齐</title>
      <link>http://link.springer.com/10.1007/s11042-023-18048-0</link>
      <description><![CDATA[摘要
无监督深度跨模态哈希方法因其低计算成本、优异的存储容量和高效的检索性能而受到广泛关注。然而，现有的无监督方法仍然存在一些挑战：（1）由于缺乏标签语义，单模态和间模态实例的邻域结构信息可能无法完全集成，导致忽略深层语义相似性交互信息。 (2)无监督哈希码既不能有效解决模态实例原始特征之间的语义一致性，也不能弥合哈希码异构模态之间的差距。为了解决这些问题，我们提出了一种新的无监督深度跨模式哈希方法，称为多视角融合语义对齐哈希（MPFSAH）。主要包括两个方面。首先，为了增强跨模式通信，构建了多级语义相似性交互测量（MSSIM）。通过融合不同模态的邻域结构并增加模态内实例之间的距离，可以深度挖掘语义交互相似性，以获得有区别的语义信息。此外，我们还提出了一种新颖的多视角语义对齐机制（MPSAM）。通过最小化多视角相似度中元素的一致性量化误差，学习模态间相似度一致性。 MPSAM包括相似性一致性对齐、结构语义对齐和排序对齐。它实现了结构语义一致性，充分保证了跨模态数据相似性的有效连接，弥合了哈希码过程中的模态鸿沟。通过对三个跨模态检索数据集的实验，我们证明了我们提出的方法的有效性，该方法优于一些最先进的方法。]]></description>
      <guid>http://link.springer.com/10.1007/s11042-023-18048-0</guid>
      <pubDate>Tue, 09 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>