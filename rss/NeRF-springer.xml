<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最新结果</title>
    <link>http://link.springer.com</link>
    <description>Springer 提供的最新内容</description>
    <lastBuildDate>Tue, 10 Sep 2024 21:12:38 GMT</lastBuildDate>
    <item>
      <title>机器视觉中的合成数据增强方法综述</title>
      <link>http://link.springer.com/10.1007/s11633-022-1411-7</link>
      <description><![CDATA[摘要
解决计算机视觉问题的标准方法是使用代表目标任务的大规模图像数据集来训练深度卷积神经网络 (CNN) 模型。然而，在许多情况下，获取足够的图像数据来完成目标任务往往是一项挑战。数据增强是一种缓解这一挑战的方法。一种常见的做法是明确地以所需的方式转换现有图像，以创建实现良好泛化性能所需的训练数据量和可变性。在无法获取目标域数据的情况下，一种可行的解决方法是从头开始合成训练数据，即合成数据增强。本文对合成数据增强技术进行了广泛的回顾。它涵盖了基于逼真的 3D 图形建模、神经风格迁移 (NST)、差分神经渲染和使用生成对抗网络 (GAN) 和变分自动编码器 (VAE) 的生成建模的数据合成方法。对于每类方法，我们都将重点介绍重要的数据生成和增强技术、一般应用范围和特定用例，以及现有的限制和可能的解决方法。此外，我们还总结了用于训练计算机视觉模型的常见合成数据集，重点介绍了主要功能、应用领域和支持的任务。最后，我们讨论了合成数据增强方法的有效性。由于这是第一篇详细探讨合成数据增强方法的论文，我们希望为读者提供必要的背景信息和对现有方法及其相关问题的深入了解。]]></description>
      <guid>http://link.springer.com/10.1007/s11633-022-1411-7</guid>
      <pubDate>Tue, 01 Oct 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Ambient-NeRF：利用环境照明在低光照条件下增强神经辐射场的光列</title>
      <link>http://link.springer.com/10.1007/s11042-024-19699-3</link>
      <description><![CDATA[摘要
NeRF可以渲染照片级逼真的3D场景，广泛应用于虚拟现实、自动驾驶、游戏开发等领域，并迅速成为3D重建领域最热门的技术之一。NeRF通过从相机的空间坐标和视点发射光线，穿过场景并计算从视点看到的景色，从而生成逼真的3D场景。然而当原始输入图像亮度较低时，很难恢复场景。受计算机图形学Phong模型中环境光照的启发，假设最终渲染出的图像是场景颜色和环境光照的乘积。本文采用多层感知器 (MLP) 网络训练环境光照张量 
\(\textbf{I}\)
，并将其乘以 NeRF 预测的颜色，以渲染具有正常光照的图像。此外，我们使用 tiny-cuda-nn 作为骨干网络，简化了所提出的网络结构并大大提高了训练速度。此外，引入了新的损失函数，以在低光照条件下实现更好的图像质量。实验结果表明，与其他最先进的方法相比，所提出的方法在增强低光场景图像方面非常有效，在 LOM 数据集上的整体平均值为 PSNR：20.53、SSIM：0.785 和 LPIPS：0.258。]]></description>
      <guid>http://link.springer.com/10.1007/s11042-024-19699-3</guid>
      <pubDate>Tue, 10 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过神经元修剪实现隐式神经表征隐写术</title>
      <link>http://link.springer.com/10.1007/s00530-024-01476-9</link>
      <description><![CDATA[摘要
近年来，隐式神经表征（INR）开始应用于图像隐写术。然而，用INR表示的隐写和秘密图像的质量普遍较低。在本文中，我们提出了一种通过神经元修剪的隐式神经表示隐写术方法。首先，我们随机停用一部分神经元来训练一个INR函数来隐式表示秘密图像。随后，我们修剪那些被认为对以非结构化方式表示秘密图像不重要的神经元以获得秘密函数，同时将神经元的位置标记为密钥。最后，基于部分优化策略，我们重新激活修剪后的神经元来构建一个用于表示封面图像的隐写函数。接收者只需要共享密钥就可以从隐写函数中恢复秘密函数，从而重建秘密图像。实验结果表明，该方法不仅可以实现秘密图像的无损恢复，而且在容量、保真度和不可检测性方面也表现良好。对不同分辨率图像的实验验证了该方法在图像质量上比现有的隐式表示隐写方法具有明显的优势。]]></description>
      <guid>http://link.springer.com/10.1007/s00530-024-01476-9</guid>
      <pubDate>Tue, 10 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用深度学习从 X 射线衍射数据中实现端到端结构确定</title>
      <link>http://link.springer.com/10.1038/s41524-024-01401-8</link>
      <description><![CDATA[摘要
粉末晶体学是通过分析晶体粉末中的分子的 X 射线衍射 (XRD) 图案来确定分子结构的实验科学。由于许多材料都可以作为晶体粉末获得，因此粉末晶体学在许多领域越来越有用。然而，粉末晶体学没有已知的分析解决方案，因此结构推断通常涉及一个繁琐的迭代设计、结构细化和熟练专家的领域知识的过程。完全自动化计算推理过程的一个关键障碍是将问题以适合机器学习的端到端定量形式公式化，同时捕捉分子取向、对称性和重建分辨率的模糊性。在这里，我们提出了一种从粉末衍射数据中确定结构的 ML 方法。它的工作原理是使用基于变分坐标的深度神经网络估计晶胞中的电子密度。我们展示了该方法在计算粉末 X 射线衍射 (PXRD) 上的表现，同时输入了部分化学成分信息。当对立方和三方晶体系统的理论模拟数据进行评估时，该系统与未知材料的真实数据的平均相似度高达 93.4%（以结构相似性指数衡量），无论是已知还是部分已知的化学成分信息，都表明即使输入数据质量低下且不完整，也有望成功解决结构问题。该方法不预设晶体结构，并且可轻松扩展到其他情况，例如纳米材料和纹理样品，为重建尚未解决的纳米结构铺平了道路。]]></description>
      <guid>http://link.springer.com/10.1038/s41524-024-01401-8</guid>
      <pubDate>Sat, 07 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>智能航空航天感知的计算机视觉任务：概述</title>
      <link>http://link.springer.com/10.1007/s11431-024-2714-4</link>
      <description><![CDATA[摘要
计算机视觉任务对于航空航天任务至关重要，因为它们可以帮助航天器理解和解释太空环境，例如估计位置和方向、重建 3D 模型和识别物体，这些任务已经得到广泛研究，以成功执行任务。然而，卡尔曼滤波、运动结构和多视角立体成像等传统方法不够稳健，无法处理恶劣条件，导致结果不可靠。近年来，基于深度学习 (DL) 的感知技术显示出巨大的潜力，并且优于传统方法，特别是在对不断变化的环境的稳健性方面。为了进一步推进基于 DL 的航空航天感知，已经提出了各种框架、数据集和策略，表明未来应用具有巨大的潜力。在本次调查中，我们旨在探索感知任务中使用的有前景的技术，并强调基于 DL 的航空航天感知的重要性。我们首先概述航天感知，包括近年来开发的经典航天计划、常用的传感器和传统的感知方法。随后，我们深入研究航天任务中的三个基本感知任务：姿态估计、三维重建和识别，因为它们对于后续的决策和控制至关重要。最后，我们讨论了当前研究的局限性和可能性，并对未来的发展进行了展望，包括使用有限数据集的挑战、改进算法的必要性以及多源信息融合的潜在好处。]]></description>
      <guid>http://link.springer.com/10.1007/s11431-024-2714-4</guid>
      <pubDate>Sun, 01 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>特邀社论：2022 年英国机器视觉会议特刊</title>
      <link>http://link.springer.com/10.1007/s11263-024-02038-2</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/s11263-024-02038-2</guid>
      <pubDate>Sun, 01 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一种基于注意力机制的轻量级图像去雾网络</title>
      <link>http://link.springer.com/10.1007/s11760-024-03392-x</link>
      <description><![CDATA[摘要
在当前基于卷积的图像去雾网络中，增加卷积层的深度和宽度是提高网络性能的常用策略。然而，这种方法显著增加了去雾网络的复杂性和计算成本。为了解决这个问题，本文提出了一种U形多尺度自适应选择网络（UMA-Net）。在不引入额外参数和计算成本的情况下，该网络利用不同尺度的卷积核对感受野的影响。它将标准卷积和扩张卷积结合到前馈网络（FFN）中，提出了一个多尺度自适应（MA）去雾模块，进一步扩展了感受野并关注FFN内重要的空间和通道信息。为了充分利用MA模块的多尺度特性，提出了一个轻量级的通道注意引导融合（CAGF）模块，实现了从雾蒙蒙图像中恢复高质量的去雾图像。大量实验证明了所提模块的有效性，在 Reside SOTS 数据集上，仅用 0.816M 参数和 8.794G FLOPs 就取得了最佳性能。]]></description>
      <guid>http://link.springer.com/10.1007/s11760-024-03392-x</guid>
      <pubDate>Sun, 01 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CLIP-Flow：解码 CLIP 空间中编码的图像</title>
      <link>http://link.springer.com/10.1007/s41095-023-0375-z</link>
      <description><![CDATA[摘要
本研究介绍了 CLIP-Flow，这是一种用于从给定图像或文本生成图像的新型网络。为了有效利用两种模态中包含的丰富语义，我们设计了一种语义引导的图像和文本到图像合成方法。具体而言，我们采用对比语言-图像预训练 (CLIP) 作为编码器来提取语义，采用 StyleGAN 作为解码器从此类信息生成图像。此外，为了连接 CLIP 的嵌入空间和 StyleGAN 的潜在空间，采用了真实的 NVP，并通过激活规范化和可逆卷积对其进行了修改。由于 CLIP 中的图像和文本共享相同的表示空间，因此可以将文本提示直接输入到 CLIP-Flow 中以实现文本到图像的合成。我们在多个数据集上进行了广泛的实验，以验证所提出的图像到图像合成方法的有效性。此外，我们在公共数据集 Multi-Modal CelebA-HQ 上进行了文本到图像合成测试。实验验证了我们的方法可以生成高质量的文本匹配图像，并且在定性和定量方面与最先进的方法相当。




]]></description>
      <guid>http://link.springer.com/10.1007/s41095-023-0375-z</guid>
      <pubDate>Wed, 28 Aug 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>动态环境下基于空间结构比较的RGB-D SLAM</title>
      <link>http://link.springer.com/10.1007/s11042-024-20128-8</link>
      <description><![CDATA[摘要
基于RGB-D的同步定位与地图构建（RGB-D SLAM）是机器人领域的研究热点，如何消除动态物体对RGB-D SLAM的影响仍是一大难题。针对这一问题，本文提出了一种基于空间结构比较的改进型RGB-D SLAM。在提出的RGB-D SLAM定位阶段，基于深度信息将特征点匹配对分为两组，即深度有效和深度未对齐。通过比较两个三维点集的空间结构矩阵，识别出静态特征点匹配对。此外，利用获得的姿态去除深度未对齐的动态特征点匹配对，以提高后端优化的性能。在地图处理阶段，采用了一种结合距离检测和处理坐标误差的新型地图点方法，并提出了一种重复点删除策略来消除参考帧中的冗余地图点。最后，在公开的动态TUM RGB-D数据集上对提出的RGB-D SLAM进行了测试。实验结果表明，与动态环境下最先进的RGB-D SLAM方法相比，提出的RGB-D SLAM具有更优的性能。]]></description>
      <guid>http://link.springer.com/10.1007/s11042-024-20128-8</guid>
      <pubDate>Wed, 28 Aug 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>森林应用中处理地面点云的软件解决方案综述</title>
      <link>http://link.springer.com/10.1007/s40725-024-00228-2</link>
      <description><![CDATA[摘要

综述目的
近年来，人们对 3D 点云在林业和森林生态学中的应用兴趣大增。随着激光扫描等新型 3D 捕获技术的发展，越来越多的算法被并行开发出来，用于将 3D 点云数据处理成林业应用的更切实的结果。从这些可用的算法中，用户很难决定应用哪种算法来最好地实现他们的目标。在这里，我们全面概述了点云采集和处理工具及其精准林业的输出。然后，我们提供了一个包含 24 种算法的综合数据库，用于处理使用近距离技术（特别是地面平台）获得的森林点云。


最新发现
在确定的 24 种解决方案中，有 20 种是开源的，两种是免费软件，其余两种是商业产品。作为 COST Action 3DForEcoTech 的一部分，可以在基于 Web 的平台上访问编译的解决方案数据库以及相应的安装和一般使用技术指南。该数据库可以作为社区的单一信息来源，帮助社区选择适合其需求的特定软件/算法。


摘要
我们得出结论，用于处理点云的各种算法的开发提供了强大的工具，可以对未来的森林资源清查产生重大影响，尽管我们注意到创建标准化范例的必要性。
]]></description>
      <guid>http://link.springer.com/10.1007/s40725-024-00228-2</guid>
      <pubDate>Sat, 17 Aug 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于局部图像风格化的音频引导隐式神经表征</title>
      <link>http://link.springer.com/10.1007/s41095-024-0413-5</link>
      <description><![CDATA[摘要
我们提出了一种音频引导局部图像风格化的新框架。声音通常提供有关场景特定背景的信息，并且与场景或对象的某个部分密切相关。然而，现有的图像风格化工作专注于使用图像或文本输入对整个图像进行风格化。基于音频输入对图像的特定部分进行风格化是自然但具有挑战性的。这项工作提出了一个框架，其中用户提供音频输入以在输入图像中定位目标，并提供另一个音频输入以局部风格化目标对象或场景。我们首先使用利用 CLIP 嵌入空间的视听定位网络生成精细定位图。然后，我们利用隐式神经表征 (INR) 以及预测的定位图根据声音信息对目标进行风格化。INR 操纵局部像素值以与提供的音频输入在语义上一致。我们的实验表明，所提出的框架优于其他音频引导风格化方法。此外，我们观察到我们的方法构建了简洁的定位图，并根据给定的音频输入自然地操纵目标对象或场景。




]]></description>
      <guid>http://link.springer.com/10.1007/s41095-024-0413-5</guid>
      <pubDate>Wed, 14 Aug 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FMGS：基础模型嵌入 3D 高斯分层，用于整体 3D 场景理解</title>
      <link>http://link.springer.com/10.1007/s11263-024-02183-8</link>
      <description><![CDATA[摘要
准确感知现实世界 3D 对象的几何和语义属性对于增强现实和机器人应用的持续发展至关重要。为此，我们提出了基础模型嵌入高斯分层 (FMGS)，它将基础模型的视觉语言嵌入合并到 3D 高斯分层 (GS) 中。这项工作的主要贡献是一种重建和表示 3D 视觉语言模型的有效方法。这是通过将基于图像的基础模型生成的特征图提炼为从我们的 3D 模型渲染的特征图来实现的。为了确保高质量的渲染和快速的训练，我们通过整合 GS 和多分辨率哈希编码 (MHE) 的优势引入了一种新颖的场景表示。我们有效的训练过程还引入了像素对齐损失，使得相同语义实体的渲染特征距离接近，遵循像素级语义边界。我们的结果表明，多视图语义一致性非常出色，有助于完成各种下游任务，以 
\({10.2}\)
 的速度超越了最先进的方法，尽管我们的推理速度 
\({851\times }\)
 更快。​​这项研究探索了视觉、语言和 3D 场景表示的交集，为在不受控制的现实环境中增强场景理解铺平了道路。我们计划在[项目页面]上发布代码。]]></description>
      <guid>http://link.springer.com/10.1007/s11263-024-02183-8</guid>
      <pubDate>Mon, 12 Aug 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于对抗模仿学习的网络进行类别级 6D 物体姿态估计</title>
      <link>http://link.springer.com/10.1007/s00138-024-01592-6</link>
      <description><![CDATA[摘要
类别级6D物体姿态估计是计算机视觉领域中一项非常基础和关键的研究。为了摆脱对物体3D模型的依赖，分析综合型物体姿态估计方法近年来得到了广泛的研究。虽然这些方法在泛化方面有一定的提升，但类别级物体姿态估计的准确性仍有待提高。本文提出了一种基于对抗模仿学习的类别级6D物体姿态估计网络AIL-Net。AIL-Net采用状态-动作分布匹配准则，能够执行数据集中未出现过的专家动作，防止物体姿态估计陷入不良状态。我们进一步设计了一个通过生成对抗模仿学习估计物体姿态的框架，该方法能够区分AIL-Net中的专家策略和模仿策略。实验结果表明，我们的方法在REAL275数据集和Cars数据集上取得了具有竞争力的类别级物体姿态估计性能。]]></description>
      <guid>http://link.springer.com/10.1007/s00138-024-01592-6</guid>
      <pubDate>Mon, 12 Aug 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>EPSM 2023，医学工程与物理科学</title>
      <link>http://link.springer.com/10.1007/s13246-024-01460-7</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/s13246-024-01460-7</guid>
      <pubDate>Mon, 12 Aug 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一种多任务有效性度量和自适应协同训练方法，用于通过少量样本提高学习效果</title>
      <link>http://link.springer.com/10.1007/s10845-024-02475-3</link>
      <description><![CDATA[摘要
将深度学习 (DL) 集成到视觉检测方法中，越来越多地被认为是一种可大幅提高适应性和鲁棒性的宝贵方法。然而，众所周知，高性能神经网络通常需要具有高质量手动注释的大量训练数据集，而这在许多制造过程中很难获得。为了提高 DL 方法在样本较少的情况下执行视觉任务的性能，本文提出了一种称为辅助任务有效性 (EAT) 的新指标，并提出了一种多任务学习方法，利用该指标来选择有效的辅助任务分支并自适应地将它们与主任务一起训练。在两个少量样本的视觉任务上进行的实验表明，所提方法有效地消除了无效的任务分支，并增强了所选任务对主任务的贡献：在姿势关键点检测中将平均归一化像素误差从 0.0613 降低到 0.0143，在表面缺陷分割中将交并比 (IoU) 从 0.6383 提升到 0.6921。值得注意的是，这些增强是在不需要额外的手动标记工作的情况下实现的。]]></description>
      <guid>http://link.springer.com/10.1007/s10845-024-02475-3</guid>
      <pubDate>Mon, 12 Aug 2024 00:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>