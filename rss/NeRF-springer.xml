<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最新结果</title>
    <link>http://link.springer.com</link>
    <description>Springer 提供的最新内容</description>
    <lastBuildDate>Fri, 31 Jan 2025 15:13:12 GMT</lastBuildDate>
    <item>
      <title>FMGS：基础模型嵌入 3D 高斯分层，用于整体 3D 场景理解</title>
      <link>http://link.springer.com/10.1007/s11263-024-02183-8</link>
      <description><![CDATA[摘要
准确感知现实世界 3D 对象的几何和语义属性对于增强现实和机器人应用的持续发展至关重要。为此，我们提出了基础模型嵌入高斯分层 (FMGS)，它将基础模型的视觉语言嵌入合并到 3D 高斯分层 (GS) 中。这项工作的主要贡献是一种重建和表示 3D 视觉语言模型的有效方法。这是通过将基于图像的基础模型生成的特征图提炼为从我们的 3D 模型渲染的特征图来实现的。为了确保高质量的渲染和快速的训练，我们通过整合 GS 和多分辨率哈希编码 (MHE) 的优势引入了一种新颖的场景表示。我们有效的训练过程还引入了像素对齐损失，使得相同语义实体的渲染特征距离接近，遵循像素级语义边界。我们的结果表明，多视图语义一致性非常出色，有助于完成各种下游任务，以 
\({10.2}\)
 的速度超越了最先进的方法，尽管我们的推理速度 
\({851\times }\)
 更快。​​这项研究探索了视觉、语言和 3D 场景表示的交集，为在不受控制的现实世界环境中增强场景理解铺平了道路。我们计划在[项目页面]上发布代码。]]></description>
      <guid>http://link.springer.com/10.1007/s11263-024-02183-8</guid>
      <pubDate>Sat, 01 Feb 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DE-NAF：用于稀疏视图 CBCT 重建的解耦神经衰减场</title>
      <link>http://link.springer.com/10.1007/s10044-025-01416-x</link>
      <description><![CDATA[摘要
锥形束计算机断层扫描 (CBCT) 可获取三维内部图像，尤其适用于骨骼等高矿物质密度结构。然而，尤其是在稀疏视图场景中，其可视化低密度软组织的能力有限，限制了其临床应用。为了解决这个问题，本研究提出了一种称为解耦神经衰减场 (DE-NAF) 的方法。具体而言，DE-NAF 使用自适应混合编码器，其中包括哈希编码和 3D 特征网格编码方法。这种方法将 CBCT 重建分解为两个部分。哈希编码用于高矿物质密度结构，例如骨骼，因为它具有卓越的编码质量，并且能够在哈希冲突解决期间保留高矿物质密度结构的特征。对于低密度软组织，如肌肉，采用三维特征网格，有效保留低密度软组织的特征信息，自适应混合编码器提取这些特征，再由多层感知器（MLP）解码器解码，预测X射线衰减值，实现精确重建。此外，引入结构感知的损失，增强组织对比度和细节，进一步辅助CBCT重建。大量实验表明，DE-NAF有效解决了CBCT在低密度软组织成像方面的局限性，保持了完整的结构完整性，重建质量优于其他方法。]]></description>
      <guid>http://link.springer.com/10.1007/s10044-025-01416-x</guid>
      <pubDate>Fri, 31 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过事件进行自监督快门展开</title>
      <link>http://link.springer.com/10.1007/s11263-025-02364-z</link>
      <description><![CDATA[摘要
连续时间全局快门视频恢复 (CGVR) 在从失真的滚动快门 (RS) 图像中恢复未失真的高帧率全局快门 (GS) 视频时面临巨大挑战。由于 RS 帧内扫描线和帧间曝光中缺乏时间动态信息，这个问题严重不适定，尤其是在没有关于相机/物体运动的先验知识的情况下。在现实场景中，对场景/运动和数据特定特征的常用人为假设容易产生次优解决方案。为了应对这一挑战，我们提出了一个基于事件的 CGVR 网络，该网络采用自监督学习范式，即 SelfUnroll，并利用事件相机的极高时间分辨率来提供准确的帧间/帧内动态信息。具体来说，提出了一种基于事件的帧间/帧内补偿器 (E-IC)，用于预测任意时间间隔之间的每像素动态，包括时间转换和空间平移。通过探索 RS-RS、RS-GS 和 GS-RS 方面的联系，我们明确地用所提出的 E-IC 制定了相互约束，从而实现了没有地面真实 GS 图像的监督。对合成和真实数据集的广泛评估表明，所提出的方法实现了最先进的方法，并在现实场景中表现出基于事件的 RS2GS 反演的卓越性能。数据集和代码可在 https://w3un.github.io/selfunroll/ 上找到。]]></description>
      <guid>http://link.springer.com/10.1007/s11263-025-02364-z</guid>
      <pubDate>Wed, 29 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CAD-NeRF：通过 CAD 模型检索从未校准的少视图图像中学习 NeRF</title>
      <link>http://link.springer.com/10.1007/s11704-024-40417-7</link>
      <description><![CDATA[摘要
从多视图图像重建是 3D 视觉中长期存在的问题，其中神经辐射场 (NeRF) 显示出巨大的潜力，并能获得新视图的逼真渲染图像。目前，大多数 NeRF 方法要么需要精确的相机姿势，要么需要大量的输入图像，甚至两者兼而有之。从没有姿势的少视图图像重建 NeRF 具有挑战性且高度不适定。为了解决这个问题，我们提出了 CAD-NeRF，这是一种从少于 10 张没有任何已知姿势的图像重建的方法。具体来说，我们从 ShapeNet 构建了一个包含几个 CAD 模型的迷你库，并从许多随机视图渲染它们。给定稀疏视图输入图像，我们从库中运行模型和姿势检索，以获得具有相似形状的模型，用作密度监督和姿势初始化。这里我们提出了一种多视图姿态检索方法来避免视图之间的姿态冲突，这是未校准的 NeRF 方法中一个新的、从未见过的问题。然后，通过 CAD 指导训练对象的几何形状。联合优化密度场的变形和相机姿态。然后对纹理和密度进行训练和微调。所有训练阶段均以自监督的方式进行。对合成图像和真实图像的综合评估表明，CAD-NeRF 成功地从检索到的 CAD 模型中学习到具有大变形的准确密度，显示出泛化能力。]]></description>
      <guid>http://link.springer.com/10.1007/s11704-024-40417-7</guid>
      <pubDate>Tue, 28 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过解开槽位注意力机制来学习以对象为中心的全局表征</title>
      <link>http://link.springer.com/10.1007/s10994-024-06687-9</link>
      <description><![CDATA[摘要
人类能够辨别不同环境中物体的场景独立特征，从而能够在光照、视角、大小和位置等变化因素下快速识别物体，并想象同一物体在不同场景下的完整图像。现有的以物体为中心的学习方法仅提取场景相关的以物体为中心的表征，缺乏像人类一样跨场景识别同一物体的能力。此外，一些现有方法放弃了单个物体的生成能力来处理复杂场景。本文介绍了一种新颖的以物体为中心的学习方法，通过学习一组全局以物体为中心的表征，使人工智能系统具有类似人类的跨场景识别物体的能力，并生成包含特定物体的不同场景。为了学习全局以对象为中心的表示，以封装对象的全局不变属性（即完整的外观和形状），本文设计了一个解缠槽注意力模块，将场景特征转换为与场景相关的属性（例如比例、位置和方向）和与场景无关的表示（即外观和形状）。实验结果证实了所提方法的有效性，在全局以对象为中心的表示学习、对象识别、具有特定对象的场景生成和场景分解方面表现出色。]]></description>
      <guid>http://link.springer.com/10.1007/s10994-024-06687-9</guid>
      <pubDate>Mon, 27 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>关节囊周围神经组冷冻神经松解术作为终末期合并症患者髋部骨折姑息非手术治疗的一种选择</title>
      <link>http://link.springer.com/10.1007/s12630-024-02902-2</link>
      <description><![CDATA[摘要

目的
我们报告使用囊周神经组 (PENG) 冷冻神经松解术对髋部骨折和严重合并症患者进行长期镇痛，作为髋部骨折手术的替代方案。


临床特征
一名来自辅助生活设施的虚弱但清醒且完全自主的 97 岁女性在地面跌倒后遭受了右股骨近端头下骨折。她患有严重的合并症，包括终末期呼吸道疾病。尝试进行半关节置换术；然而，在用 2 毫升 0.5% 等比重布比卡因进行脊柱麻醉后，她出现严重的呼吸功能障碍，导致手术中止。多学科共同决定进行姑息非手术治疗 (P-NOM)，并成功实施 PENG 冷冻神经松解术。这显著减轻了患者运动时的疼痛，大大增强和方便了患者的护理。在剩余的住院时间内，她不再需要任何阿片类镇痛药，残留疼痛通过常规对乙酰氨基酚进行控制。在理疗师或护理人员的帮助下，她能够站起来，他们对她冷冻神经松解术后的康复给予了积极评价。她的情况持续良好，并在七天后出院，住进了辅助生活设施。


结论
我们的病例报告显示，PENG 冷冻神经溶解术可作为髋部骨折手术中 P-NOM 的一种选择，为那些可能不适合麻醉和手术的患者提供显着且持久的疼痛缓解。
]]></description>
      <guid>http://link.springer.com/10.1007/s12630-024-02902-2</guid>
      <pubDate>Mon, 27 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>智能电影摄影：电影制作人工智能研究回顾</title>
      <link>http://link.springer.com/10.1007/s10462-024-11089-3</link>
      <description><![CDATA[摘要
本文首次全面回顾了娱乐目的真实摄像机内容采集背景下的人工智能 (AI) 研究，面向研究人员和电影摄影师。针对智能电影摄影 (IC) 领域缺乏综述论文以及相关计算机视觉研究范围广的问题，我们展示了 IC 领域的整体视图，同时提供了技术见解，这对跨学科专家来说很重要。我们提供有关生成 AI、物体检测、自动摄像机校准和 3-D 内容采集的技术背景，并提供参考资料以帮助非技术读者。应用部分将工作分为四种制作类型：一般制作、虚拟制作、现场制作和空中制作。在每个应用部分中，我们 (1) 根据研究主题对工作进行细分，并 (2) 描述与每种制作类型相关的趋势和挑战。在最后一章中，我们讨论了 IC 研究的更大范围，并总结了该领域对创意产业部门产生影响的巨大潜力。我们认为，与虚拟制作相关的工作最有可能影响其他制作媒介，这是由于人们对用于机内虚拟效果 (ICVFX) 的 LED 体积/舞台以及用于虚拟建模现实世界场景和演员的自动 3-D 捕捉的兴趣日益浓厚。我们还讨论了有关使用创意 AI 的道德和法律问题，这些问题会影响艺术家、演员、技术人员和普通公众。]]></description>
      <guid>http://link.springer.com/10.1007/s10462-024-11089-3</guid>
      <pubDate>Sat, 25 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>VC-GS：通过交替分支优化实现视图一致的去模糊高斯溅射</title>
      <link>http://link.springer.com/10.1007/s00371-025-03806-y</link>
      <description><![CDATA[摘要
最近对 3D 高斯溅射的研究表明，使用多张摆拍图像进行新视图合成具有良好的应用前景。这些在现实场景中拍摄的图像可能会很模糊，导致合成质量不理想。在 3D 高斯溅射之前利用 2D 图像去模糊模型来锐化模糊图像是一种自然的灵感。然而，直接将 2D 图像去模糊模型应用于新视图合成会导致性能不佳，因为不可避免地会出现不一致性。针对上述问题，本文介绍了一种新方法，即通过交替分支优化策略实现视图一致高斯溅射 (VC-GS)。VC-GS 由两个分支组成：前分支和后分支。前向分支在进行 3D 高斯分层之前使用 2D 去模糊模型恢复多视图模糊输入，而后向分支直接对模糊输入进行 3D 高斯分层并使用相同模型恢复合成视图。通过前向分支和后向分支的交替优化，前向分支中的 3D 高斯模型可以在模糊输入下实现高质量重建并保持视图一致性。为了验证该方法的有效性，我们在合成数据集和真实数据集上进行了定性和定量实验，结果表明所提出的方法取得了更好的结果。我们的项目页面位于https://github.com/DTG777/View-Consistent-Gaussian-Splatting-from-Blurry-Images-for-Novel-View-Synthesis。]]></description>
      <guid>http://link.springer.com/10.1007/s00371-025-03806-y</guid>
      <pubDate>Thu, 23 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于建模动态人体的可动画 3D 高斯</title>
      <link>http://link.springer.com/10.1007/s11704-024-40497-5</link>
      <description><![CDATA[摘要
我们提出了一种可动画化的 3D 高斯表示，用于实时合成新视角和姿势下的高保真人体视频。给定人体的多视角视频，我们在静止姿势的标准空间中学习一组 3D 高斯。每个高斯都与一些基本属性（即位置、不透明度、比例、旋转、球面谐波系数）相关联，这些属性表示所有视频帧中人体的平均外观，以及一个潜在代码和一组混合权重，用于动态外观校正和姿势变换。潜在代码被馈送到具有目标姿势的多层感知器 (MLP)，以校正标准空间中的高斯，从而捕捉目标姿势下的外观变化。然后使用线性混合蒙皮 (LBS) 及其混合权重将校正后的高斯转换为目标姿势。通过高斯分层，可以实时渲染新视角和姿势下的高保真人体图像。与最先进的基于 NeRF 的方法相比，我们的可动画高斯表示可以产生更引人注目的结果，捕捉细节更准确，并实现卓越的渲染性能。]]></description>
      <guid>http://link.springer.com/10.1007/s11704-024-40497-5</guid>
      <pubDate>Wed, 22 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>上颌结节与牙槽结节的鉴别——批判性图片评论</title>
      <link>http://link.springer.com/10.1007/s00276-025-03569-0</link>
      <description><![CDATA[摘要

目的
上颌结节是牙科和颌面外科中的一个重要解剖标志，但术语混乱。这种不一致妨碍了临床实践和跨学科交流。


方法
使用不同的资源来论证标准化上颌结节相关术语的必要性，以提高诊断精度并最终改善患者预后。


结果
大多数涉及上颌远端牙槽骨的临床和外科研究错误地将其表示为“上颌结节”。通过认识到这种结构的不同定义，可以减少研究的错误和误解，并改善跨学科合作。建议将术语“牙槽结节”专门指上颌骨牙槽突的远端。从解剖学角度来看，上颌结节属于上颌骨体，构成上颌窦后壁的一部分；因此，它不应位于牙槽突内。


结论
遵守解剖术语将明确关键的临床和手术标志，并加强临床和学术环境中的沟通。
]]></description>
      <guid>http://link.springer.com/10.1007/s00276-025-03569-0</guid>
      <pubDate>Wed, 22 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于深度学习的档案馆和图书馆装订保护评估流程</title>
      <link>http://link.springer.com/10.1007/s11042-025-20615-6</link>
      <description><![CDATA[摘要
图书馆是书面遗产的保管者，这些遗产既庞大又脆弱，由于书籍材料的敏感性，需要定期监控。对于大多数图书馆来说，人为地不可能单独监控每本书。为了应对这一挑战，我们探索了深度学习的潜力，以自动收集书脊上出现危险结构变化的书籍的精确数据。我们的研究重点是“巴黎议会”档案，并捕获了两个装订照片数据库。执行注释以区分装订与背景、标记文本并创建与不同类型的更改相对应的二进制掩码。检测管道涉及使用 Mask R-CNN 进行实例分割，以将书架图像分离为单独的装订照片。使用在 ImageNet 上预先训练的 Vision Transformer (ViT) 执行更改分类，并使用针对多标签分类进行微调的模型。虽然检测流程在标签检测和文本识别方面表现出很高的性能，但挑战包括低估结合表面积和识别某些改变。]]></description>
      <guid>http://link.springer.com/10.1007/s11042-025-20615-6</guid>
      <pubDate>Tue, 21 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>复杂背景和障碍物视频中的对象跟踪框架</title>
      <link>http://link.springer.com/10.1007/s00521-024-10853-4</link>
      <description><![CDATA[摘要
视频中的对象跟踪研究利用计算机视觉和机器学习技术来识别和跟踪视频连续图像帧中的对象。研究中使用的流行算法是 YOLO、DeepSORT、StrongSORT 和 ByteTrack。YOLO 因其硬件效率和在对象检测任务中的出色性能而获得认可。它有效地检测视频连续图像帧中的对象。另一方面，DeepSORT、StrongSORT 和 ByteTrack 利用预测功能来估计下一图像帧中对象的位置。这些算法在增强对象跟踪系统的功能方面发挥着重要作用。最近，许多研究人员提出了提高对象跟踪任务准确性的方法。然而，将视频划分为连续图像帧以跟踪对象可能会带来挑战，尤其是当视频具有复杂的背景和图像帧中的许多障碍物时。事实上，图像帧中复杂的背景和许多障碍物会降低应用程序中跟踪对象的准确性。在本研究中，我们提出了一个框架TraObs，用于在视频图像帧中存在Obs的复杂背景下进行对象跟踪。在TraObs中，我们为检测器和跟踪器提出了两种新颖的机制CFM和M-controller，以提高对象跟踪性能的准确性。CFM处理视频某些图像帧中缺失的跟踪对象，而M-controller确保系统中跟踪的对象具有正确的对象id。我们分别设计了一些使用四个测试数据集和基准数据集的实验，以分析TraObs的性能。此外，我们还使用HOTA和MOTA指标来评估我们提出的方法。实验结果也证明了我们的框架TraObs中CFM和M控制器的有效性。]]></description>
      <guid>http://link.springer.com/10.1007/s00521-024-10853-4</guid>
      <pubDate>Fri, 17 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>STNeRF：对称三平面神经辐射场，用于从单视图车辆图像进行新视图合成</title>
      <link>http://link.springer.com/10.1007/s10489-024-06005-9</link>
      <description><![CDATA[摘要
本文介绍了 STNeRF，这是一种从单视图 2D 图像合成车辆新视图的方法，无需 3D 地面真实数据（例如点云、深度图、CAD 模型等）作为先验知识。这项任务的一个重大挑战来自 CNN 的特性，当使用单一视角的图像进行训练和验证时，局部特征的使用会导致合成图像的平面表示。许多当前方法倾向于忽略局部特征并在整个重建过程中依赖全局特征，这可能会导致合成图像中细粒度细节的丢失。为了解决这个问题，我们引入了对称三平面神经辐射场 (STNeRF)。STNeRF 采用具有空间感知卷积的三平面特征提取器将 2D 图像特征扩展到 3D。这将包含局部特征的外观组件和由全局特征组成的形状组件分离，并利用它们构建神经辐射场。然后使用这些神经先验来渲染新视图。此外，STNeRF 利用车辆的对称特性将外观组件从对原始视点的依赖中解放出来，并将其与目标空间的对称性对齐，从而增强神经辐射场网络表示不可见区域的能力。定性和定量评估表明，STNeRF 在几何和外观重建方面均优于现有解决方案。更多补充材料和实现代码可在以下链接中访问：https://github.com/ll594282475/STNeRF。]]></description>
      <guid>http://link.springer.com/10.1007/s10489-024-06005-9</guid>
      <pubDate>Tue, 14 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 $$\mu $$ 多普勒超声成像进行皮质血管的大规模体内采集、分割和 3D 重建</title>
      <link>http://link.springer.com/10.1007/s12021-024-09706-1</link>
      <description><![CDATA[摘要
大脑由密集且分支的血管网络组成，这些血管网络由各种大小的动脉、静脉和毛细血管组成。评估脑血管病变风险的一种方法是使用计算模型来预测血液供应减少的生理影响，并将这些反应与脑损伤的观察结果联系起来。因此，建立大脑血管系统的详细 3D 组织至关重要，这可用于开发更准确的计算机模型。为此，我们调整了之前设计用于记录大规模活动的功能性超声成像平台，以实现对皮质血管系统的快速且可重复的采集、分割和重建。它首次使我们能够数字化皮质
\(\sim 100\)
-
\(\mu \)
m3 空间分辨率。与大多数可用策略不同，我们的方法可以在几分钟内体内完成。此外，它易于实施，因为它既不需要外源造影剂，也不需要较长的后处理时间。因此，我们对整个皮层血管进行了重建并进行了定量分析，包括 i) 对每只动物 1500 多条血管中的下行动脉和上行静脉进行分类，以及 ii) 快速估计它们的长度。重要的是，我们在皮层中风模型中证实了我们的方法的相关性，该模型可以快速可视化缺血性病变。这一发展有助于扩展超声神经成像的能力，以更好地了解脑血管病变，例如中风、血管性认知障碍和脑肿瘤，并且对于临床具有很高的可扩展性。]]></description>
      <guid>http://link.springer.com/10.1007/s12021-024-09706-1</guid>
      <pubDate>Tue, 14 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>对抗性学习用于室内场景的无引导单深度图补全</title>
      <link>http://link.springer.com/10.1007/s00138-024-01652-x</link>
      <description><![CDATA[摘要
在没有任何彩色图像指导的情况下，单深度图补全是计算机视觉中具有挑战性的不适定问题。大多数传统的深度图补全方法依赖于从相应彩色图像中提取的信息，需要大量计算和基于优化的后处理功能，无法实时产生结果。生成对抗网络的成功应用已在包括彩色图像修复在内的多个计算机视觉问题中取得了重大进展。然而，与彩色图像相比，深度图的局部和非局部特征的对比阻碍了为彩色图像修复设计的深度学习模型直接应用于深度图补全。受这些挑战的启发，在这项工作中，我们建议使用深度对抗学习来得出单个退化观察中缺失深度信息的合理估计，而无需来自相应 RGB 帧的任何指导和任何后处理。不同类型的深度图退化，例如模拟随机和文本缺失像素以及 Kinect 深度图中发现的连续大洞，都得到了有效处理，以重建干净的深度图。我们还进行了一项消融研究，以调查我们的对抗网络架构对恢复缺失场景深度信息的贡献。我们对 NYU-Depth V2 数据集进行了说明性实验分析，并对 Middlebury 和 Matterport3D 数据集进行了零样本泛化，将我们提出的方法与几种最先进的算法进行了比较。实验结果证明了所提方法的稳健性和有效性。]]></description>
      <guid>http://link.springer.com/10.1007/s00138-024-01652-x</guid>
      <pubDate>Tue, 07 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>