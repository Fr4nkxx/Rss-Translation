<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最新结果</title>
    <link>http://link.springer.com</link>
    <description>Springer 提供的最新内容</description>
    <lastBuildDate>Fri, 27 Sep 2024 18:18:13 GMT</lastBuildDate>
    <item>
      <title>机器视觉中的合成数据增强方法综述</title>
      <link>http://link.springer.com/10.1007/s11633-022-1411-7</link>
      <description><![CDATA[摘要
解决计算机视觉问题的标准方法是使用代表目标任务的大规模图像数据集来训练深度卷积神经网络 (CNN) 模型。然而，在许多情况下，获取足够的图像数据来完成目标任务往往是一项挑战。数据增强是一种缓解这一挑战的方法。一种常见的做法是明确地以所需的方式转换现有图像，以创建实现良好泛化性能所需的训练数据量和可变性。在无法获取目标域数据的情况下，一种可行的解决方法是从头开始合成训练数据，即合成数据增强。本文对合成数据增强技术进行了广泛的回顾。它涵盖了基于逼真的 3D 图形建模、神经风格迁移 (NST)、差分神经渲染和使用生成对抗网络 (GAN) 和变分自动编码器 (VAE) 的生成建模的数据合成方法。对于每类方法，我们都将重点介绍重要的数据生成和增强技术、一般应用范围和特定用例，以及现有的限制和可能的解决方法。此外，我们还总结了用于训练计算机视觉模型的常见合成数据集，重点介绍了主要功能、应用领域和支持的任务。最后，我们讨论了合成数据增强方法的有效性。由于这是第一篇详细探讨合成数据增强方法的论文，我们希望为读者提供必要的背景信息和对现有方法及其相关问题的深入了解。]]></description>
      <guid>http://link.springer.com/10.1007/s11633-022-1411-7</guid>
      <pubDate>Tue, 01 Oct 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于多视角 2D 环绕图像的自监督 3D 语义占用预测</title>
      <link>http://link.springer.com/10.1007/s41064-024-00308-9</link>
      <description><![CDATA[摘要
对环境几何和语义的精确 3D 表示为各种下游任务奠定了基础，对于路径规划和避障等自动驾驶相关任务至关重要。这项工作的重点是 3D 语义占用预测，即将场景重建为体素网格，其中每个体素都分配有一个占用和语义标签。我们提出了一种基于卷积神经网络的方法，该方法利用来自重叠最小的环视设置的多张彩色图像，以及相关的内部和外部摄像头参数作为输入，将观察到的环境重建为 3D 语义占用图。为了解释从单目 2D 图像重建 3D 表示的不适定性，图像信息会随时间进行整合：假设摄像机设置正在移动，来自连续时间步骤的图像用于形成多视图立体设置。在详尽的实验中，我们研究了动态物体所带来的挑战以及使用 3D 或 2D 参考数据训练所提出方法的可能性。后者受到生成和注释 3D 地面真实数据相对较高的成本的激励。此外，我们提出并研究了一种新颖的自监督训练方案，该方案不需要任何几何参考数据，而仅依赖于稀疏语义地面真实值。对 Occ3D 数据集的评估，包括与文献中当前最先进的自监督方法的比较，证明了我们的自监督变体的潜力。]]></description>
      <guid>http://link.springer.com/10.1007/s41064-024-00308-9</guid>
      <pubDate>Tue, 01 Oct 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>UGINR：通过隐式神经表征实现大规模非结构化网格缩减</title>
      <link>http://link.springer.com/10.1007/s12650-024-01003-y</link>
      <description><![CDATA[
摘要
最近，隐式神经表征 (INR) 在处理 3D 体积数据方面表现出了显著的能力，尤其是在数据压缩方面。然而，大多数研究主要集中在结构化网格上，而结构化网格在科学领域，尤其是物理学领域并不常见。为了解决这一限制，我们提出了一种通过隐式神经表征 (UGINR) 进行非结构化网格缩减的方法。UGINR 采用分而治之的方法；具体来说，我们根据值将大规模数据分割成块。随后，我们对每个部分使用 INR 网络来学习其独特的特征。最后，我们整合这些单独的网络以实现压缩目标。为了确保与既定的研究方法兼容，我们仅对非结构化网格中每个单元的顶点进行采样。通过权重量化，我们的模型可以实现高压缩率。为了说明所提方法的有效性，我们在各种数据集上进行了实验，证明了我们的方法在科学可视化和大规模数据压缩方面的稳健性。


图形摘要





]]></description>
      <guid>http://link.springer.com/10.1007/s12650-024-01003-y</guid>
      <pubDate>Tue, 01 Oct 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于实时渲染神经辐射场的多项式</title>
      <link>http://link.springer.com/10.1007/s00371-024-03660-4</link>
      <description><![CDATA[摘要
在神经辐射场（NeRF）中，生成高度逼真的渲染结果需要大量光线采样和多层感知器的在线查询。然而，这会导致渲染速度变慢。先前的研究通过设计更快的神经场景表示评估或预计算场景属性来减少渲染时间来解决此问题。在本文中，我们提出了一种实时渲染方法PNeRF。PNeRF利用连续多项式函数来近似空间体积密度和颜色信息。此外，我们将视线方向信息从渲染方程中分离出来，从而为体积渲染方程提供了新的表达式。通过将观察视点的起始坐标和观察方向向量作为神经网络的输入，我们得到了相应观察光线的渲染结果。因此，每条光线的渲染只需要神经网络的一次前向推理。为了进一步提高渲染速度，我们设计了一种六轴球面方法来存储与观察视点的起始坐标和观察方向向量相对应的渲染结果。这使得我们能够显著提高渲染速度并保持渲染质量，同时将存储空间要求降至最低。在 LLFF 数据集上的实验验证表明，我们的方法在保持渲染质量的同时提高了渲染速度，并且占用的存储空间最小。这些结果表明了我们的方法在实时渲染领域的潜力，为更高效的渲染提供了有效的解决方案。]]></description>
      <guid>http://link.springer.com/10.1007/s00371-024-03660-4</guid>
      <pubDate>Fri, 27 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LEO：用于人体视频合成的生成潜像动画器</title>
      <link>http://link.springer.com/10.1007/s11263-024-02231-3</link>
      <description><![CDATA[摘要
时空连贯性是合成高质量视频的主要挑战，尤其是在合成包含丰富全局和局部变形的人体视频时。为了解决这一挑战，以前的方法在生成过程中采用了不同的特征来表示外观和运动。然而，由于缺乏严格的机制来保证这种分离，将运动与外观分离仍然具有挑战性，导致空间扭曲和时间抖动破坏了时空连贯性。受此启发，我们在此提出了 LEO，一种用于人体视频合成的新型框架，重点关注时空连贯性。我们的主要思想是在生成过程中将运动表示为一系列流图，这本质上将运动与外观隔离开来。我们通过基于流的图像动画器和潜在运动扩散模型 (LMDM) 实现了这个想法。前者将运动代码空间与流图空间联系起来，以扭曲和修复的方式合成视频帧。LMDM 通过合成运动代码序列来学习在训练数据中预先捕捉运动。广泛的定量和定性分析表明，与 TaichiHD、FaceForensics 和 CelebV-HQ 数据集上的先前方法相比，LEO 显著提高了人体视频的连贯合成。此外，LEO 中外观和运动的有效分离允许执行另外两个任务，即无限长度的人体视频合成以及保留内容的视频编辑。项目页面：https://wyhsirius.github.io/LEO-project/。

]]></description>
      <guid>http://link.springer.com/10.1007/s11263-024-02231-3</guid>
      <pubDate>Fri, 27 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从单目图像或视频中重建逼真的 3D 着装人体的创新 AI 技术：一项调查</title>
      <link>http://link.springer.com/10.1007/s00371-024-03641-7</link>
      <description><![CDATA[摘要
近年来，从单目图像或视频重建高质量 3D 着装人体因其重要的实际应用而越来越受欢迎。虽然有几项调查已经解决了从图像或视频重建全身参数化人体模型的问题，但本调查专门深入研究了重建 3D 着装人体的挑战和方法。它涵盖了着装人体重建的姿势相关方法和动态方法。关于从单目图像重建姿势相关的着装人体，我们研究了使用在高质量 3D 扫描上训练的回归模型来估计着装人体几何形状的方法。此外，我们还探索了利用大规模扩散模型中的纹理先验来增强对被遮挡或看不见的区域中人体外观推断的研究。在从单目和稀疏多视角视频重建动态着装人体方面，我们分析了利用神经辐射场和 3D 高斯表示的人体建模技术，这些技术使用变形场来捕捉跨帧的人体运动。此外，我们概述了这些研究中的数据集和常用的定量评估指标。最后，我们总结了未解决的问题，并提出了现实重建着装人体的未来研究方向，强调了值得进一步研究的领域。
]]></description>
      <guid>http://link.springer.com/10.1007/s00371-024-03641-7</guid>
      <pubDate>Thu, 26 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于人工智能的超声引导腰方肌阻滞自动分割</title>
      <link>http://link.springer.com/10.1007/s10278-024-01267-8</link>
      <description><![CDATA[摘要
超声引导腰方肌阻滞（QLB）技术已成为腹部和盆腔手术中广泛使用的围手术期镇痛方法。由于超声图像上腰方肌（QLM）的解剖复杂性和个体差异性，神经阻滞严重依赖麻醉师的经验。因此，利用人工智能（AI）识别超声图像中的不同组织区域至关重要。在我们的研究中，我们回顾性地收集了112名患者（3162张图像），并开发了一个名为Q-VUM的深度学习模型，它是一个基于视觉几何组16（VGG16）网络的U形网络。Q-VUM精确分割各种组织，包括QLM、腹外斜肌、腹内斜肌、腹横肌（统称为EIT）和骨骼。此外，我们对 Q-VUM 进行了评估。我们的模型表现出了稳健的性能，平均交并比 (mIoU)、平均像素准确率、骰子系数和准确率分别达到 0.734、0.829、0.841 和 0.944。QLM 实现的 IoU、召回率、准确率和骰子系数分别为 0.711、0.813、0.850 和 0.831。此外，Q-VUM 预测显示，阻塞区域中 85% 的像素落在实际阻塞区域内。最后，我们的模型表现出比常见的深度学习分割网络更强的分割性能（分别为 0.734 vs. 0.720 和 0.720）。总之，我们提出了一个名为 Q-VUM 的模型，可以实时准确识别腰方肌的解剖结构。该模型可帮助麻醉师精确定位神经阻滞部位，从而减少潜在的并发症并提高神经阻滞手术的有效性。]]></description>
      <guid>http://link.springer.com/10.1007/s10278-024-01267-8</guid>
      <pubDate>Wed, 25 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过视图特定查询的光场角度超分辨率</title>
      <link>http://link.springer.com/10.1007/s00371-024-03620-y</link>
      <description><![CDATA[摘要
光场角度超分辨率 (LFASR) 旨在从稀疏采样的输入中重建密集采样的光场。最近，基于卷积神经网络的方法取得了令人鼓舞的结果。然而，这些方法中的大多数分别使用目标密集光的视图特定特征（即内容和视图位置），目标光场的几何结构信息尚未充分探索。为此，我们提出了视图特定查询，将密集光场的视图位置信息集成到 LFASR 的 Transformer（称为 ViewFormer）中。具体而言，我们首先利用 Transformer 编码器来处理输入的稀疏采样光场。然后，使用视图插值操作沿目标光场的水平和垂直方向处理提取的子孔径特征，生成新的子孔径表示，称为视图特定查询。接下来，特定视图查询包含目标光场的视图坐标信息，并通过 Transformer 解码器逐层动态增强。增强的特定视图查询被馈送到重建模块进行最终的光场合成。此外，为了进一步挖掘有关输入稀疏采样光场的更多信息，我们对 Transformer 的构建块采用了通道注意方案。在常用的 LFASR 基准上进行了广泛的实验。与其他方法相比，ViewFormer 在流行的 LFASR 基准（包括真实世界和合成数据）上取得了新的最佳结果。]]></description>
      <guid>http://link.springer.com/10.1007/s00371-024-03620-y</guid>
      <pubDate>Sun, 22 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>文本驱动的着装人体图像合成，结合 3D 人体模型估计，帮助购物</title>
      <link>http://link.springer.com/10.1007/s11042-024-20187-x</link>
      <description><![CDATA[摘要
网上购物已成为现代消费文化不可或缺的一部分。然而，它面临着基于文本描述可视化服装项目和估计其适合个人体型的挑战。在这项工作中，我们提出了一种创新的解决方案来解决这些挑战，即通过文本驱动的着装人体图像合成和 3D 人体模型估计，利用矢量量化变分自动编码器 (VQ-VAE) 的强大功能。在视觉和图形领域，创建多样化和高质量的人体图像是一项至关重要但又困难的任务。由于服装设计和纹理种类繁多，现有的生成模型通常不足以满足最终用户的需求。在这项提议的工作中，我们引入了一种解决方案，该解决方案由通过多个模型的各种数据集提供，因此可以提供优化的解决方案以及具有各种姿势的高质量图像。我们使用两种不同的程序从预定的人体姿势开始创建全身 2D 人体照片。 1) 首先将提供的人体姿势转换为人体解析图，其中包含一些描述衣服形状的句子。 2) 然后向开发的模型提供有关衣服纹理的更多信息作为输入，以生成最终的人体图像。 该模型分为两个不同的部分，第一个是粗略级别的代码本，用于处理总体结果，另一个是精细级别的代码本，用于处理细节。 如前所述，精细级别的代码本专注于纹理的细节，而粗略级别的代码本涵盖了结构中纹理的描述。 与分层代码本一起训练的解码器将各个级别的预期索引转换为人体图像。 由于使用了专家组合，因此创建的图像可以依赖于细粒度的文本输入。 通过预测更精细级别的指标来改善服装纹理的质量。 根据大量定量和定性评估，实施这些策略可以产生比最先进的程序更多样化和高质量的人体图像。这些生成的照片将转换为 3D 模型，从而产生多种姿势和结果，或者您可能只是从产生各种姿势的数据集中制作 3D 模型。PIFu 方法的应用分别使用 Marching cube 算法和 Stacked Hourglass 方法生成 3D 模型和逼真的图像。这导致基于文本描述生成高分辨率图像并将生成的图像重建为 3D 模型。实现的初始分数和 Fréchet 截距、SSIM 和 PSNR 分别为 1.64 ± 0.20 和 24.64527782349843、0.642919520 和 32.87157744102002。与其他技术相比，实施的方法得分很高。这项技术对于重塑电子商务格局具有巨大的希望，可提供更具沉浸感和信息量的探索服装选择的方式。]]></description>
      <guid>http://link.springer.com/10.1007/s11042-024-20187-x</guid>
      <pubDate>Thu, 19 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>NeRF-OR：从稀疏视图 RGB-D 视频中重建手术室场景的神经辐射场</title>
      <link>http://link.springer.com/10.1007/s11548-024-03261-5</link>
      <description><![CDATA[摘要

目的
手术室 (OR) 中的 RGB-D 摄像机提供复杂手术场景的同步视图。将这些多视图数据同化为统一表示可以实现下游任务，例如对象检测和跟踪、姿势估计和动作识别。神经辐射场 (NeRF) 可以在有限的内存占用下提供复杂场景的连续表示。然而，现有的 NeRF 方法在现实世界的 OR 设置中表现不佳，因为其中一小部分摄像机从完全不同的有利位置捕捉房间。在本研究中，我们提出了 NeRF-OR，一种用于在手术室中对动态手术场景进行 3D 重建的方法。


方法
其他稀疏视图数据集方法使用飞行时间传感器深度或从彩色图像估计的密集深度，而 NeRF-OR 则使用两者的组合。深度估计可以减轻由于反射材料和物体边界而导致传感器深度图像中出现的缺失值。我们建议使用根据估计深度计算的表面法线进行监督，因为这些法线在很大程度上是尺度不变的。


结果
我们将 NeRF-OR 拟合到 4D-OR 数据集中的静态手术场景，并表明其表示在几何上是准确的，而最先进的方法会崩溃为次优解决方案。与早期的工作相比，NeRF-OR 在训练时掌握精细的场景细节，速度提高了 30
\(\times \)
。此外，NeRF-OR 可以捕获整个手术视频，同时合成中间时间值的视图，平均 PSNR 为 24.86 dB。最后，通过对包含少至三个训练视图的 NVS-RGBD 数据集进行基准测试，我们发现我们的方法在手术室以外的稀疏视图设置中具有优势。NeRF-OR 合成的图像的 PSNR 为 26.72 dB，比最先进的方法提高了 1.7%。


结论
我们的结果表明，NeRF-OR 允许使用少量具有完全不同有利位置的摄像机拍摄的视频进行新颖的视图合成，这是手术室中的典型摄像机设置。代码可通过以下方式获取：github.com/Beerend/NeRF-OR。
]]></description>
      <guid>http://link.springer.com/10.1007/s11548-024-03261-5</guid>
      <pubDate>Fri, 13 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Ambient-NeRF：利用环境照明在低光照条件下增强神经辐射场的光列</title>
      <link>http://link.springer.com/10.1007/s11042-024-19699-3</link>
      <description><![CDATA[摘要
NeRF可以渲染照片级逼真的3D场景，广泛应用于虚拟现实、自动驾驶、游戏开发等领域，并迅速成为3D重建领域最热门的技术之一。NeRF通过从相机的空间坐标和视点发射光线，穿过场景并计算从视点看到的景色，从而生成逼真的3D场景。然而当原始输入图像亮度较低时，很难恢复场景。受计算机图形学Phong模型中环境光照的启发，假设最终渲染出的图像是场景颜色和环境光照的乘积。本文采用多层感知器 (MLP) 网络训练环境光照张量 
\(\textbf{I}\)
，并将其乘以 NeRF 预测的颜色，以渲染具有正常光照的图像。此外，我们使用 tiny-cuda-nn 作为骨干网络，简化了所提出的网络结构并大大提高了训练速度。此外，引入了新的损失函数，以在低光照条件下实现更好的图像质量。实验结果表明，与其他最先进的方法相比，所提出的方法在增强低光场景图像方面非常有效，在 LOM 数据集上的整体平均值为 PSNR：20.53、SSIM：0.785 和 LPIPS：0.258。]]></description>
      <guid>http://link.springer.com/10.1007/s11042-024-19699-3</guid>
      <pubDate>Tue, 10 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过神经元修剪实现隐式神经表征隐写术</title>
      <link>http://link.springer.com/10.1007/s00530-024-01476-9</link>
      <description><![CDATA[摘要
近年来，隐式神经表征（INR）开始应用于图像隐写术。然而，用INR表示的隐写和秘密图像的质量普遍较低。在本文中，我们提出了一种通过神经元修剪的隐式神经表示隐写术方法。首先，我们随机停用一部分神经元来训练一个INR函数来隐式表示秘密图像。随后，我们修剪那些被认为对以非结构化方式表示秘密图像不重要的神经元以获得秘密函数，同时将神经元的位置标记为密钥。最后，基于部分优化策略，我们重新激活修剪后的神经元来构建一个用于表示封面图像的隐写函数。接收者只需要共享密钥就可以从隐写函数中恢复秘密函数，从而重建秘密图像。实验结果表明，该方法不仅可以实现秘密图像的无损恢复，而且在容量、保真度和不可检测性方面也表现良好。对不同分辨率图像的实验验证了该方法在图像质量上比现有的隐式表示隐写方法具有明显的优势。]]></description>
      <guid>http://link.springer.com/10.1007/s00530-024-01476-9</guid>
      <pubDate>Tue, 10 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用深度学习从 X 射线衍射数据中实现端到端结构测定</title>
      <link>http://link.springer.com/10.1038/s41524-024-01401-8</link>
      <description><![CDATA[摘要
粉末晶体学是通过分析晶体粉末中的分子的 X 射线衍射 (XRD) 图案来确定分子结构的实验科学。由于许多材料都可以作为晶体粉末获得，因此粉末晶体学在许多领域越来越有用。然而，粉末晶体学没有已知的分析解决方案，因此结构推断通常涉及一个繁琐的迭代设计、结构细化和熟练专家的领域知识的过程。完全自动化计算推理过程的一个关键障碍是将问题以适合机器学习的端到端定量形式公式化，同时捕捉分子取向、对称性和重建分辨率的模糊性。在这里，我们提出了一种从粉末衍射数据中确定结构的 ML 方法。它的工作原理是使用基于变分坐标的深度神经网络估计晶胞中的电子密度。我们展示了该方法在计算粉末 X 射线衍射 (PXRD) 上的表现，同时输入了部分化学成分信息。当对立方和三方晶体系统的理论模拟数据进行评估时，该系统与未知材料的真实数据的平均相似度高达 93.4%（以结构相似性指数衡量），无论是已知还是部分已知的化学成分信息，都表明即使输入数据质量低下且不完整，也有望成功解决结构问题。该方法不预设晶体结构，并且可轻松扩展到其他情况，例如纳米材料和纹理样品，为重建尚未解决的纳米结构铺平了道路。]]></description>
      <guid>http://link.springer.com/10.1038/s41524-024-01401-8</guid>
      <pubDate>Sat, 07 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>uSF：学习具有不确定性的神经语义场</title>
      <link>http://link.springer.com/10.3103/S1060992X24700176</link>
      <description><![CDATA[
摘要
最近，人们对 NeRF 方法的兴趣日益浓厚，该方法可以重建三维场景的可微分表示。此类方法的主要局限性之一是无法评估模型对其预测的置信度。在本文中，我们提出了一种用于形成扩展向量表示的新神经网络模型，称为 uSF，该模型不仅可以预测每个点的颜色和语义标签，还可以估计相应的不确定性值。我们表明，在少量可用于训练的图像的情况下，量化不确定性的模型比没有这种功能的模型表现更好。uSF 方法的代码可在 https://github.com/sevashasla/usf/ 上公开获取。
]]></description>
      <guid>http://link.springer.com/10.3103/S1060992X24700176</guid>
      <pubDate>Sun, 01 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>智能航空航天感知的计算机视觉任务：概述</title>
      <link>http://link.springer.com/10.1007/s11431-024-2714-4</link>
      <description><![CDATA[摘要
计算机视觉任务对于航空航天任务至关重要，因为它们可以帮助航天器理解和解释太空环境，例如估计位置和方向、重建 3D 模型和识别物体，这些任务已经得到广泛研究，可以成功执行任务。然而，卡尔曼滤波、运动结构和多视角立体成像等传统方法不够稳健，无法处理恶劣条件，导致结果不可靠。近年来，基于深度学习 (DL) 的感知技术显示出巨大的潜力，并且优于传统方法，特别是在对不断变化的环境的稳健性方面。为了进一步推进基于 DL 的航空航天感知，已经提出了各种框架、数据集和策略，表明未来应用具有巨大的潜力。在本次调查中，我们旨在探索感知任务中使用的有前景的技术，并强调基于 DL 的航空航天感知的重要性。我们首先概述航天感知，包括近年来开发的经典航天计划、常用的传感器和传统的感知方法。随后，我们深入研究航天任务中的三个基本感知任务：姿态估计、三维重建和识别，因为它们对于后续的决策和控制至关重要。最后，我们讨论了当前研究的局限性和可能性，并对未来的发展进行了展望，包括使用有限数据集的挑战、改进算法的必要性以及多源信息融合的潜在好处。]]></description>
      <guid>http://link.springer.com/10.1007/s11431-024-2714-4</guid>
      <pubDate>Sun, 01 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>