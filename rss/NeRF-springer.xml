<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最新结果</title>
    <link>http://link.springer.com</link>
    <description>Springer 提供的最新内容</description>
    <lastBuildDate>Thu, 28 Mar 2024 12:20:26 GMT</lastBuildDate>
    <item>
      <title>从未曝光图像中学习神经辐射场的鲁棒多尺度表示</title>
      <link>http://link.springer.com/10.1007/s11263-023-01936-1</link>
      <description><![CDATA[摘要
我们针对计算机视觉中基于神经图像的渲染问题引入了一种改进的解决方案。给定在火车时从自由移动的摄像机拍摄的一组图像，所提出的方法可以在测试时从新的视角合成场景的真实图像。本文提出的关键思想是（i）通过强大的管道从未摆出的日常图像中恢复准确的相机参数在神经新视图合成问题中同样重要； (ii) 在不同分辨率下对对象的内容进行建模更为实用，因为在日常的未摆姿势的图像中很可能出现戏剧性的相机运动。为了整合关键思想，我们利用了场景刚性、多尺度神经场景表示和单图像深度预测的基础知识。具体来说，所提出的方法使得相机参数在基于神经场的建模框架中是可学习的。通过假设每个视图的深度预测按比例给出，我们限制了连续帧之间的相对姿势。根据相对姿势，绝对相机姿势估计是通过多尺度神经场网络内基于图神经网络的多个运动平均来建模的，从而产生单个损失函数。优化引入的损失函数可以从未摆出的图像中提供相机内在、外在和图像渲染。我们通过示例证明，对于从日常获取的未摆姿势的多视图图像中准确建模多尺度神经场景表示的统一框架，在场景表示框架内进行精确的相机姿态估计同样重要。如果不考虑相机姿态估计管道中的鲁棒性措施，多尺度混叠伪影的建模可能会适得其反。我们在几个基准数据集上进行了广泛的实验，以证明我们的方法的适用性。]]></description>
      <guid>http://link.springer.com/10.1007/s11263-023-01936-1</guid>
      <pubDate>Mon, 01 Apr 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于实时距离场加速的大型运动场自由视点视频合成</title>
      <link>http://link.springer.com/10.1007/s41095-022-0323-3</link>
      <description><![CDATA[摘要
自由视点视频允许用户从任何虚拟角度观看物体，创造身临其境的视觉体验。该技术增强了多媒体表演的交互性和自由度。然而，许多自由视点视频合成方法很难满足实时、高精度的要求，特别是对于面积较大、运动物体较多的运动场。为了解决这些问题，我们提出了一种基于距离场加速的自由视点视频合成方法。其中心思想是融合多视点距离场信息并利用其自适应调整搜索步长。自适应步长搜索有两种用途：用于多目标三维表面的快速估计，以及基于全局遮挡判断的合成视图渲染。我们使用并行计算进行交互式显示、使用 CUDA 和 OpenGL 框架来实现我们的想法，并使用真实世界和模拟实验数据集进行评估。结果表明，所提出的方法可以在大型运动场上以 25 fps 渲染具有多个对象的自由视点视频。此外，我们合成的新颖视点图像的视觉质量超过了最先进的基于神经渲染的方法。
      



]]></description>
      <guid>http://link.springer.com/10.1007/s41095-022-0323-3</guid>
      <pubDate>Mon, 01 Apr 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>体积可重新照明面的更深入分析</title>
      <link>http://link.springer.com/10.1007/s11263-023-01899-3</link>
      <description><![CDATA[摘要
人像视点和照明编辑是 VR/AR、电影和摄影等多种应用中的一个重要问题。几何和照明的全面知识对于获得逼真的结果至关重要。当前的方法无法在 3D 中显式建模，同时处理单个图像的视点和照明编辑。在本文中，我们提出了 VoRF，这是一种新颖的方法，甚至可以将单个肖像图像作为输入，并在可以从任意视点观看的新颖照明下重新照亮人头。 VoRF 将人体头部表示为连续的体积场，并使用基于坐标的 MLP 以及用于身份和照明的单独潜在空间来学习人体头部的先验模型。先前的模型是通过自动解码器的方式在不同类别的头部形状和外观上学习的，从而允许 VoRF 从单个输入图像泛化到新的测试身份。此外，VoRF 具有反射 MLP，它使用先前模型的中间特征在新颖的视图下渲染一次一光 (OLAT) 图像。我们通过将这些 OLAT 图像与目标环境图相结合来合成新颖的照明。定性和定量评估证明了 VoRF 在重新照明和新颖视图合成方面的有效性，即使应用于不受控制的照明下看不见的物体也是如此。这项工作是 Rao 等人的延伸。 （VoRF：体积可重新照明面孔 2022）。我们对我们的模型进行了广泛的评估和烧蚀研究，并提供了一个应用程序，可以使用文本输入重新照亮任何面孔。]]></description>
      <guid>http://link.springer.com/10.1007/s11263-023-01899-3</guid>
      <pubDate>Mon, 01 Apr 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FFEINR：时空超分辨率的流特征增强隐式神经表示</title>
      <link>http://link.springer.com/10.1007/s12650-024-00959-1</link>
      <description><![CDATA[摘要
大规模数值模拟能够生成高达 TB 甚至 PB 的数据。作为一种有前途的数据缩减方法，超分辨率（SR）已在科学可视化界得到广泛研究。然而，它们大多数都是基于深度卷积神经网络或生成对抗网络，需要在构建网络之前确定比例因子。导致单次训练仅支持固定因子，泛化能力较差。为了解决这些问题，本文提出了一种用于流场数据时空超分辨率的流特征增强隐式神经表示（FFEINR）。它可以在模型结构和采样分辨率方面充分利用隐式神经表示。神经表示基于具有周期性激活函数的全连接网络，这使我们能够获得轻量级模型。学习到的连续表示可以将低分辨率流场输入数据解码为任意空间和时间分辨率，从而允许灵活的上采样。通过引入输入层的特征增强来促进 FFEINR 的训练过程，这补充了流场的上下文信息。为了证明该方法的有效性，通过设置不同的超参数在不同的数据集上进行了一系列实验。结果表明，FFEINR取得了明显优于三线性插值方法的结果。

图形摘要






]]></description>
      <guid>http://link.springer.com/10.1007/s12650-024-00959-1</guid>
      <pubDate>Mon, 01 Apr 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于可微渲染的动态海洋反建模</title>
      <link>http://link.springer.com/10.1007/s41095-023-0338-4</link>
      <description><![CDATA[摘要
学习和推断捕获的 2D 场景的潜在运动模式，然后重新创建与现实世界自然现象一致的动态演化，这对图形和动画具有很高的吸引力。为了弥合虚拟和现实环境之间的技术差距，我们专注于视觉一致且属性可验证的海洋的逆向建模和重建，利用深度学习和可微物理来学习几何并以自我监督的方式构成波浪。首先，我们使用两个网络推断分层几何，这两个网络通过可微渲染器进行了优化。我们通过配备可微分海洋模型的网络从推断的几何序列中提取波浪分量。然后，可以使用重建的波浪分量来演化海洋动力学。通过大量的实验，我们验证了我们的新方法在几何重建和波浪估计方面都能产生令人满意的结果。此外，新框架具有逆向建模潜力，可促进大量图形应用，例如快速生成物理精确的场景动画以及由真实海洋场景引导的编辑。
      



]]></description>
      <guid>http://link.springer.com/10.1007/s41095-023-0338-4</guid>
      <pubDate>Mon, 01 Apr 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于城市场景新颖视图合成的多视图立体调节 NeRF</title>
      <link>http://link.springer.com/10.1007/s00371-024-03321-6</link>
      <description><![CDATA[摘要
神经辐射场 (NeRF) 将场景编码为神经表示，在单个对象和小空间区域上展示了令人印象深刻的新颖视图合成质量。然而，当面对城市室外环境时，NeRF 受到单个 MLP 容量和输入视图不足的限制，导致几何形状不正确，阻碍了真实渲染的制作。在本文中，我们提出了 MVSRegNeRF，这是专注于大规模自动驾驶场景的神经辐射场的扩展。我们采用传统的基于补丁匹配的多视图立体（MVS）方法来生成密集的深度图，我们利用它来调节 NeRF 的几何优化。我们还将多分辨率哈希编码集成到我们的神经场景表示中，以加速训练过程。由于我们的方法相对精确的几何约束，我们在现实世界的大规模街道场景上实现了高质量的新颖视图合成。我们在 KITTI-360 数据集上的实验表明，MVSRegNeRF 在新颖视图外观合成任务中优于最先进的方法。
      ]]></description>
      <guid>http://link.springer.com/10.1007/s00371-024-03321-6</guid>
      <pubDate>Wed, 27 Mar 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于隐式表示的 3D 形状生成的最新进展</title>
      <link>http://link.springer.com/10.1007/s44267-024-00042-1</link>
      <description><![CDATA[摘要
各种技术已经被开发和引入，以满足为虚拟现实和增强现实等高级应用创建三维 (3D) 内容的迫切需求。然而，与标准二维 (2D) 图像数据相比，3D 形状的复杂性对其表示和生成提出了更大的挑战。文献中已经提出了不同类型的表示，包括网格、体素和隐函数。由于辐射场表示的出现，隐式表示引起了研究人员的极大兴趣，它允许同时重建几何形状和外观。随后的工作成功地将传统的带符号距离场与隐式表示联系起来，最近，三平面提供了使用 2D 内容生成器生成辐射场的可能性。已经发表了许多专注于这些特定研究领域的文章。本文对基于隐式表示的 3D 形状生成的最新研究进行了全面分析，并根据所采用的表示和生成架构对这些研究进行了分类。详细检查每个表示的属性。还提出了该领域未来研究的潜在途径。]]></description>
      <guid>http://link.springer.com/10.1007/s44267-024-00042-1</guid>
      <pubDate>Mon, 25 Mar 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于多尺度哈希编码的神经几何表示</title>
      <link>http://link.springer.com/10.1007/s41095-023-0340-x</link>
      <description><![CDATA[摘要
最近，基于神经隐式函数的表示引起了越来越多的关注，并被广泛用于使用可微神经网络来表示表面。然而，使用现有的神经几何表示从点云或多视图图像进行表面重建仍然存在计算速度慢和精度差的问题。为了缓解这些问题，我们提出了一种基于多尺度哈希编码的神经几何表示，它有效且高效地将表面表示为带符号的距离场。我们新颖的神经网络结构仔细地将低频傅里叶位置编码与多尺度哈希编码结合起来。相应地重新设计了几何网络的初始化和渲染模块的几何特征。我们的实验表明，所提出的表示对于重建具有数百万个点的点云来说至少快 10 倍。它还显着提高了多视图重建的速度和准确性。我们的代码和模型可以在 https://github.com/Dengzhi-中国科学技术大学/神经几何重建。
      



]]></description>
      <guid>http://link.springer.com/10.1007/s41095-023-0340-x</guid>
      <pubDate>Fri, 22 Mar 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大学生的自闭症、性别和身份</title>
      <link>http://link.springer.com/10.1007/s44202-024-00116-7</link>
      <description><![CDATA[摘要

目的
本研究旨在调查出生时指定女性 (AFAB) 自闭症患者如何看待自己的自闭症和性别认同及其关系。


方法
这是一项基于访谈的定性研究，对象为 10 名自闭症大学生，出生时被指定为女性。参与者分享了他们在自闭症诊断、性别认同和自我意识方面的经验。


结果
该研究发现了四个主题： (i) 存在多种类型的诊断障碍，以及其中一些障碍的缓解因素； (ii) 各种动机导致参与者进行伪装，但负面影响促使许多人减少伪装行为； (iii) 自闭症与性别观念和行为有关； (iv) 自闭症以性别以外的多种方式影响身份。


结论
这项研究对自闭症与性别和跨性别身份的关系进行了观察并提出了问题。这项研究在很大程度上同意并建立在现有文献中发现的模式的基础上，同时将自闭症患者的声音添加到文献中。
]]></description>
      <guid>http://link.springer.com/10.1007/s44202-024-00116-7</guid>
      <pubDate>Thu, 21 Mar 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>机器视觉中的综合数据增强方法综述</title>
      <link>http://link.springer.com/10.1007/s11633-022-1411-7</link>
      <description><![CDATA[摘要
解决计算机视觉问题的标准方法是使用代表目标任务的大规模图像数据集来训练深度卷积神经网络 (CNN) 模型。然而，在许多场景中，为目标任务获取足够的图像数据通常具有挑战性。数据增强是缓解这一挑战的一种方法。常见的做法是以所需的方式显式地转换现有图像，以创建实现良好泛化性能所需的训练数据量和可变性。在无法访问目标域数据的情况下，一个可行的解决方法是从头开始合成训练数据，即合成数据增强。本文对合成数据增强技术进行了广泛的回顾。它涵盖了基于真实 3D 图形建模、神经风格迁移 (NST)、差分神经渲染以及使用生成对抗网络 (GAN) 和变分自动编码器 (VAE) 的生成建模的数据合成方法。对于每一类方法，我们重点关注重要的数据生成和增强技术、一般应用范围和特定用例，以及现有的限制和可能的解决方法。此外，我们还提供了用于训练计算机视觉模型的常见合成数据集的摘要，重点介绍了主要功能、应用领域和支持的任务。最后，我们讨论合成数据增强方法的有效性。由于这是第一篇详细探讨合成数据增强方法的论文，我们希望为读者提供必要的背景信息以及对现有方法及其随之而来的问题的深入了解。]]></description>
      <guid>http://link.springer.com/10.1007/s11633-022-1411-7</guid>
      <pubDate>Wed, 20 Mar 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过学习自适应多模态先验来生成形状</title>
      <link>http://link.springer.com/10.1007/s00371-024-03303-8</link>
      <description><![CDATA[摘要
使用深度生成模型进行图像创建的最新进展引起了人们的极大兴趣和进展，但自动三维形状创建领域在很大程度上尚未开发，并激发了巨大的兴趣跨多个学科的研究活动。我们使用被描述为体积网格的几何数据将一种先前命名的后验变分混合添加到对抗网络中。我们的主要贡献是将一种称为后验变分混合先验的新型先验引入到对抗性网络中，称为VampPrior-3DGAN，以数学原理的方式。具体来说，我们利用编码器作为正则化器来惩罚缺失模式，同时引入后验先验的变分混合作为 GAN 的潜变量分布，以动态自适应地更新其先验分布。这种架构背后的关键直觉是，潜在变量应该保留有关数据的信息，以尽量减少先前假设的不当影响。这种对 GAN 框架看似简单的修改却出人意料地有效，尽管使用有限的数据进行训练，但所产生的模型仍可实现生成样本的多样性。通过对 VampPrior-3DGAN 的潜在概率流形进行采样，可以轻松生成逼真的 3D 对象。为了验证，我们将我们的方法应用于三维体积生成、单个 RGB 图像的重建和单个透视图的部分形状完成等领域的任务，并表明它与状态相同或优于状态 -定量和定性方面的最先进方法。]]></description>
      <guid>http://link.springer.com/10.1007/s00371-024-03303-8</guid>
      <pubDate>Wed, 20 Mar 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>门诊全髋关节和全膝关节置换术：文献综述和围手术期注意事项</title>
      <link>http://link.springer.com/10.1007/s12630-024-02699-0</link>
      <description><![CDATA[摘要

目的
全关节置换术 (TJA)，特别是髋关节和膝关节置换术，是最常进行的外科手术之一。手术和麻醉技术的进步/发展使得 TJA 能够在门诊/当天出院的基础上进行。在这个持续专业发展模块中，我们综合了围手术期证据，这些证据可能有助于开发成功的门诊 TJA 路径。


来源
我们搜索了 MEDLINE、Embase、CENTRAL 和 Cochrane 系统评价数据库中的动态或快速通道 TJA 文章。在缺乏流动环境的直接证据的情况下，我们从住院 TJA 文献中推断出证据。


主要发现
涵盖患者、医疗和社会因素的患者选择是 TJA 患者当天成功出院的基础。术中麻醉类型的证据有利于椎管内技术，以达到当天出院标准并减少围手术期并发症。用于椎管内麻醉的短效局部麻醉剂的可用性将影响麻醉剂的选择。尽管如此，可以考虑在精心挑选的人群中采用多模式镇痛和抗血栓药物的现代全身麻醉。区域镇痛是多模式镇痛方案的一个组成部分，可减少阿片类药物的消耗，促进当天出院，减少再入院。对于门诊全膝关节置换术，收肌管阻滞与关节周围局麻药浸润相结合是一种合适的区域镇痛方案。


结论
TJA 麻醉已经发展到当天出院将成为特定患者的常态。必须建立早期出院途径，以防止该人群产生不良影响和再次入院。随着越来越多的动态 TJA 产生更多数据，将会出现更强有力的证据来证明理想的麻醉成分可以优化结果。
]]></description>
      <guid>http://link.springer.com/10.1007/s12630-024-02699-0</guid>
      <pubDate>Tue, 19 Mar 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于实时轻量级辐射场渲染的频率重要性高斯分布</title>
      <link>http://link.springer.com/10.1007/s11042-024-18679-x</link>
      <description><![CDATA[摘要
最近，依赖辐射场的新颖视图合成领域取得了重大进展。通过结合 Splatting 技术，一种名为高斯 Splatting 的新方法实现了卓越的渲染质量和实时性能。然而，该方法的训练过程会产生很大的性能开销，并且训练得到的模型非常庞大。为了应对这些挑战，我们改进了高斯分布并提出了频率重要性高斯分布。我们的方法通过提取场景的频率特征来降低性能开销。首先，我们从采样理论的角度分析了高斯分布法空间采样策略的优点和局限性。其次，我们设计了增强高斯以更有效地表达高频信息，同时降低性能开销。第三，我们构建了频率敏感损失函数，以增强网络感知频域的能力并优化场景的空间结构。最后，我们提出了一种基于场景背景重建程度的动态自适应密度控制策略，根据训练结果动态自适应空间样本点生成策略，防止模型中产生冗余数据。我们在几个常用的数据集上进行了实验，结果表明，我们的方法在内存开销和存储使用方面比原始方法具有显着优势，并且能够保持原始方法的图像质量。]]></description>
      <guid>http://link.springer.com/10.1007/s11042-024-18679-x</guid>
      <pubDate>Tue, 12 Mar 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用生成深度神经网络与嘴唇运动进行音频视频同步</title>
      <link>http://link.springer.com/10.1007/s11042-024-18695-x</link>
      <description><![CDATA[摘要
随着元宇宙的展开，音频与视频的实时同步变得至关重要。许多模型（例如 Wav2Lip、Sync Net 和 Lip Gan）已被开发用于同步音频视频以呈现高影响力的内容。选择合适的损失函数直接影响音视频同步的结果和准确性。像 Wav2Lip 这样的模型，通过 Huber Loss 函数得到增强，成为该领域的领跑者。本文进行了全面的比较分析，证明 Huber Loss 在收敛效率和同步质量方面优于 L1、L2 和 SmoothL1 损失。实证结果明确主张将 Huber Loss 集成到 Wav2Lip 模型中，强调其能够使嘴唇运动与音频更加连贯和自然地集成。实验结果表明，Huber Loss 在 61,500 个步骤中实现了 0.00091 的平均训练损失和 0.00141 的评估损失，同时显着降低了 2.20669 的同步损失。这些结果代表了同步精度的显着提高，与当代损失函数相比提高了 20% 到 30%。]]></description>
      <guid>http://link.springer.com/10.1007/s11042-024-18695-x</guid>
      <pubDate>Mon, 11 Mar 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>全球激光雷达定位调查：挑战、进展和未决问题</title>
      <link>http://link.springer.com/10.1007/s11263-024-02019-5</link>
      <description><![CDATA[摘要
了解自身姿态是所有移动机器人应用的关键。因此，姿态估计是移动机器人核心功能的一部分。在过去的二十年中，激光雷达扫描仪已成为机器人定位和测绘的标准传感器。本文旨在概述基于激光雷达的全球定位的最新进展和进展。我们首先提出问题并探索应用范围。然后，我们对该方法进行回顾，包括地图、描述符提取和跨机器人定位等多个主题的最新进展。本文的内容分为三个主题。第一个主题涉及全局位置检索和局部姿态估计的结合。第二个主题是将单次测量升级为连续测量，以实现连续的全局定位。最后，第三个主题侧重于将单机器人全局定位扩展到多机器人系统中的跨机器人定位。我们在调查结束时讨论了全球激光雷达本地化的开放挑战和有希望的方向。据我们所知，这是首次针对移动机器人全球激光雷达定位的全面调查。]]></description>
      <guid>http://link.springer.com/10.1007/s11263-024-02019-5</guid>
      <pubDate>Wed, 06 Mar 2024 00:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>