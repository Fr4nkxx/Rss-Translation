<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最新结果</title>
    <link>http://link.springer.com</link>
    <description>Springer 提供的最新内容</description>
    <lastBuildDate>Wed, 24 Apr 2024 21:11:14 GMT</lastBuildDate>
    <item>
      <title>用于饱和模糊图像的深度非盲去模糊网络</title>
      <link>http://link.springer.com/10.1007/s00521-024-09495-3</link>
      <description><![CDATA[摘要
非盲图像去模糊在低水平视觉领域引起了广泛关注。然而，现有的非盲去模糊方法无法有效处理饱和模糊图像。关键在于饱和模糊图像的退化模型不满足传统模糊图像的线性卷积模型。为了解决这个问题，本文提出了一种新颖的深度非盲去模糊方法，称为饱和图像非盲去模糊网络（SDBNet）。 SDBNet包含两个可训练的子网络，即置信估计网络（CEN）和细节增强网络（DEN）。具体来说，SDBNet使用CEN来估计饱和模糊图像的置信图，用于识别模糊图像中的饱和像素，然后使用置信图和模糊核来恢复模糊图像。最后，我们使用 DEN 来增强恢复图像的边缘和纹理。我们首先预训练 CEN 和 DEN。为了有效地预训练 CEN，我们提出了一个新的鲁棒函数，用于为 CEN 生成标签数据。实验结果表明，与现有的几种非盲去模糊方法相比，SDBNet能够有效恢复饱和模糊图像，更好地恢复模糊图像的纹理、边缘等结构信息。]]></description>
      <guid>http://link.springer.com/10.1007/s00521-024-09495-3</guid>
      <pubDate>Wed, 01 May 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>客座社论：2022 年英国机器视觉会议特刊</title>
      <link>http://link.springer.com/10.1007/s11263-024-02038-2</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/s11263-024-02038-2</guid>
      <pubDate>Wed, 24 Apr 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>高效的 EndoNeRF 重建及其在数据驱动手术模拟中的应用</title>
      <link>http://link.springer.com/10.1007/s11548-024-03114-1</link>
      <description><![CDATA[摘要


目的

医疗保健行业对手术场景的真实建模和高效模拟的需求日益增长。借助有效的可变形手术场景模型，临床医生能够在接近真实病例的场景下进行手术规划和手术训练。然而，实现这一目标的一个重大挑战是缺乏具有精确形状和纹理的高质量软组织模型。为了解决这一差距，我们提出了一个数据驱动的框架，该框架利用新兴的神经辐射场技术来实现高质量的手术重建，并探索其在手术模拟中的应用。



方法

我们首先专注于开发一种基于 NeRF 的快速手术场景 3D 重建方法，以实现最先进的性能。该方法可以显着优于传统的 3D 重建方法，传统的 3D 重建方法无法捕捉大变形并产生细粒度的形状和纹理。然后，我们提出通过闭合网格提取算法自动创建交互式手术模拟环境的管道。
      



结果

我们的实验验证了我们提出的手术场景 3D 重建方法的卓越性能和效率。我们进一步利用重建的软组织进行 FEM 和 MPM 模拟，展示我们的方法在数据驱动的手术模拟中的实际应用。
      



结论

我们提出了一种新颖的基于 NeRF 的重建框架，重点是模拟目的。我们的重建框架有助于高效创建高质量的手术软组织 3D 模型。通过多次软组织模拟的验证，我们表明我们的工作有可能有益于下游临床任务，例如外科教育。
]]></description>
      <guid>http://link.springer.com/10.1007/s11548-024-03114-1</guid>
      <pubDate>Wed, 24 Apr 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>社论</title>
      <link>http://link.springer.com/10.1007/s41870-024-01838-6</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/s41870-024-01838-6</guid>
      <pubDate>Tue, 23 Apr 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习引导方法检测突尼斯方言自发语音中的言语障碍</title>
      <link>http://link.springer.com/10.1007/s42979-024-02775-8</link>
      <description><![CDATA[摘要
这项工作研究了自然口语理解领域内的不流畅处理任务。我们提出了一种基于转录的方法，具有纯粹的语言特征，用于检测突尼斯方言转录中的不流畅性。不流畅处理的任务是检测口语记录中的自发障碍，区分流畅和不流畅的单词。该方法的独创性在于自动处理突尼斯方言自发口语中的多种不流利类型。同样，它包含各种语言特征，例如形态句法标签和单词同义词。音节延长、语音单词、单词片段和简单重复是根据基于规则的方法进行的，而复杂的重复、插入、替换和删除是通过机器学习方法使用基于转换的模型来检测的。我们将基于转换的模型与之前工作中提出的基于序列标记的模型进行比较。实验表明，两个模型均适用于突尼斯方言的不流畅检测任务，F-Measure率分别为79.81%和78.97%。]]></description>
      <guid>http://link.springer.com/10.1007/s42979-024-02775-8</guid>
      <pubDate>Wed, 17 Apr 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于新颖视图合成的协作神经辐射场</title>
      <link>http://link.springer.com/10.1007/s00371-024-03379-2</link>
      <description><![CDATA[摘要
神经辐射场 (NeRF) 通过估计点属性（密度和颜色），然后采用体积渲染方法来合成逼真的新颖视图。然而，准确预测任意点属性对基于 NeRF 的单一模型提出了挑战。这种限制直接影响新颖视图合成的质量。为了解决这个问题，提出了一种基于多个 NeRF 模型的协作策略。该策略首次将多模型级联架构引入NeRF，以实现高质量的新颖视图合成。其目的是利用空间中的级联架构来逐步增强点属性的准确性。级联架构包括点调整和快照融合。具体来说，点调整利用基于 NeRF 的预训练模型来预测空间中每个点的初始密度和颜色。此步骤提供目标场景的初始渲染。然后，这些点的初始密度和颜色直接转移到后续基于 NeRF 的模型中。这一过程指导后续基于 NeRF 的模型专注于初始点属性的细化并合成更真实的新颖视图。最后，快照融合融合多个并行后续基于 NeRF 的模型的输出（称为快照），以合成最终的高质量新颖视图。所提出的策略使用一系列已建立的基于 NeRF 的方法进行了测试，例如 NeRF、Instant-NGP 和 TensoRF。本研究的实验数据来源于真实的360度合成数据集和LLFF数据集。结果表明，所提出的协作策略与已建立的基于 NeRF 的方法可以提高新颖视图合成的质量，超越相应的单一模型。我们的项目页面位于 https:/ /github.com/ZhenyangLiu/Collaborative-Neural-Radiance-Fields-for-Novel-View-Synthesis。]]></description>
      <guid>http://link.springer.com/10.1007/s00371-024-03379-2</guid>
      <pubDate>Fri, 12 Apr 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Twinnet：通过卷积编码器-解码器和多层感知器合成体积渲染图像的耦合功能</title>
      <link>http://link.springer.com/10.1007/s00371-024-03368-5</link>
      <description><![CDATA[摘要
体积可视化在学术界和工业界都发挥着至关重要的作用，因为体积数据广泛应用于医学、地球科学和工程等领域。为了解决体渲染的复杂性，神经渲染已成为一种潜在的解决方案，有助于生成高质量的体渲染图像。在本文中，我们提出了 TwineNet，一种专门为体渲染设计的神经网络架构。 TwineNet 通过利用跨多个特征层的缠绕跳跃连接来组合从体数据、传递函数和视点中提取的特征。在 TwineNet 架构的基础上，我们引入了两个神经网络 VolTFNet 和 PosTFNet，它们利用卷积编码器解码器和多层感知器来合成具有新颖传递函数和视点的体积渲染图像。我们的实验结果证明，与最先进的方法相比，我们的模型在生成具有新颖的传递函数和视点的高质量体积渲染图像方面具有优越性。这项研究有助于推动体积渲染领域的发展，并展示了神经渲染技术在科学可视化中的潜力。]]></description>
      <guid>http://link.springer.com/10.1007/s00371-024-03368-5</guid>
      <pubDate>Fri, 12 Apr 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Multi3D：3D 感知多模态图像合成</title>
      <link>http://link.springer.com/10.1007/s41095-024-0422-4</link>
      <description><![CDATA[摘要
3D 感知图像合成已实现高质量和强大的 3D 一致性。现有的 3D 可控生成模型旨在通过单一模态（例如 2D 分割或草图）合成 3D 感知图像，但缺乏精细控制生成内容（例如纹理和年龄）的能力。为了增强用户引导的可控性，我们提出了 Multi3D，一种支持多模态输入的 3D 感知可控图像合成模型。我们的模型可以使用二维标签图（例如分割图或草图图）来控制生成图像的几何形状，同时通过文本描述来调节生成图像的外观。为了证明我们方法的有效性，我们在多个数据集上进行了实验，包括 CelebAMask-HQ、AFHQ-cat 和 shapenet-car。定性和定量评估表明，我们的方法优于现有的最先进方法。
      



]]></description>
      <guid>http://link.springer.com/10.1007/s41095-024-0422-4</guid>
      <pubDate>Wed, 03 Apr 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>不成对语义神经人图像合成</title>
      <link>http://link.springer.com/10.1007/s00371-024-03331-4</link>
      <description><![CDATA[摘要
姿势引导的人物图像合成是一项具有挑战性的任务，旨在生成外观与源图像相同但姿势与目标图像相同的逼真人物图像。由于多视图信息的遗漏，现有的方法经常会出现明显的伪影，并且某些方法在训练过程中对源-目标图像配对的要求进一步限制了模型的应用。为了解决这些问题，我们提出了一种名为 SNPIS 的语义神经人图像合成框架，该框架利用神经辐射场（NeRF）从多视图源图像和目标语义图合成任意姿势的高保真人体图像。首先，我们引入语义镜像方向调整，强制采样点聚焦于人体，有效抑制背景干扰并增强人体细节。然后，我们设计了一种基于 NeRF 的外观形状解耦生成对抗网络，该网络将多视图源图像和相应语义图生成的共享体积的外观和形状分开。最后，我们使用获得的解耦生成器来合成由目标语义图引导的人类图像，采用外观反转，并在语义一致性约束下优化姿势重建。实验结果表明，我们的方法不仅优于现有的不配对姿势引导人体图像合成方法，而且还可以与许多配对方法竞争。]]></description>
      <guid>http://link.springer.com/10.1007/s00371-024-03331-4</guid>
      <pubDate>Tue, 02 Apr 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从未曝光图像中学习神经辐射场的鲁棒多尺度表示</title>
      <link>http://link.springer.com/10.1007/s11263-023-01936-1</link>
      <description><![CDATA[摘要
我们针对计算机视觉中基于神经图像的渲染问题引入了一种改进的解决方案。给定在火车时从自由移动的摄像机拍摄的一组图像，所提出的方法可以在测试时从新的视角合成场景的真实图像。本文提出的关键思想是（i）通过强大的管道从未摆出的日常图像中恢复准确的相机参数在神经新视图合成问题中同样重要； (ii) 在不同分辨率下对对象的内容进行建模更为实用，因为在日常的未摆姿势的图像中很可能出现戏剧性的相机运动。为了整合关键思想，我们利用了场景刚性、多尺度神经场景表示和单图像深度预测的基础知识。具体来说，所提出的方法使得相机参数在基于神经场的建模框架中是可学习的。通过假设每个视图的深度预测按比例给出，我们限制了连续帧之间的相对姿势。根据相对姿势，绝对相机姿势估计是通过多尺度神经场网络内基于图神经网络的多个运动平均来建模的，从而产生单个损失函数。优化引入的损失函数可以从未摆出的图像中提供相机内在、外在和图像渲染。我们通过示例证明，对于一个统一的框架来根据日常获取的未摆姿势的多视图图像准确地建模多尺度神经场景表示，在场景表示框架内进行精确的相机姿态估计同样重要。如果不考虑相机姿态估计管道中的鲁棒性措施，多尺度混叠伪影的建模可能会适得其反。我们在几个基准数据集上进行了广泛的实验，以证明我们的方法的适用性。]]></description>
      <guid>http://link.springer.com/10.1007/s11263-023-01936-1</guid>
      <pubDate>Mon, 01 Apr 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于实时距离场加速的大型运动场自由视点视频合成</title>
      <link>http://link.springer.com/10.1007/s41095-022-0323-3</link>
      <description><![CDATA[摘要
自由视点视频允许用户从任何虚拟角度观看物体，创造身临其境的视觉体验。该技术增强了多媒体表演的交互性和自由度。然而，许多自由视点视频合成方法很难满足实时、高精度的要求，特别是对于面积较大、运动物体较多的运动场。为了解决这些问题，我们提出了一种基于距离场加速的自由视点视频合成方法。其中心思想是融合多视点距离场信息并利用其自适应调整搜索步长。自适应步长搜索有两种用途：用于多目标三维表面的快速估计，以及基于全局遮挡判断的合成视图渲染。我们使用并行计算进行交互式显示、使用 CUDA 和 OpenGL 框架来实现我们的想法，并使用真实世界和模拟实验数据集进行评估。结果表明，所提出的方法可以在大型运动场上以 25 fps 渲染具有多个对象的自由视点视频。此外，我们合成的新颖视点图像的视觉质量超过了最先进的基于神经渲染的方法。
      



]]></description>
      <guid>http://link.springer.com/10.1007/s41095-022-0323-3</guid>
      <pubDate>Mon, 01 Apr 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>体积可重新照明面的更深入分析</title>
      <link>http://link.springer.com/10.1007/s11263-023-01899-3</link>
      <description><![CDATA[摘要
人像视点和照明编辑是 VR/AR、电影和摄影等多种应用中的一个重要问题。几何和照明的全面知识对于获得逼真的结果至关重要。当前的方法无法在 3D 中显式建模，同时处理单个图像的视点和照明编辑。在本文中，我们提出了 VoRF，这是一种新颖的方法，甚至可以将单个肖像图像作为输入，并在可以从任意视点观看的新颖照明下重新照亮人头。 VoRF 将人体头部表示为连续的体积场，并使用基于坐标的 MLP 以及用于身份和照明的单独潜在空间来学习人体头部的先验模型。先前的模型是通过自动解码器的方式在不同类别的头部形状和外观上学习的，从而允许 VoRF 从单个输入图像泛化到新的测试身份。此外，VoRF 具有反射 MLP，它使用先前模型的中间特征在新颖的视图下渲染一次一光 (OLAT) 图像。我们通过将这些 OLAT 图像与目标环境图相结合来合成新颖的照明。定性和定量评估证明了 VoRF 在重新照明和新颖视图合成方面的有效性，即使应用于不受控制的照明下看不见的对象也是如此。这项工作是 Rao 等人的延伸。 （VoRF：体积可重新照明面孔 2022）。我们对我们的模型进行了广泛的评估和烧蚀研究，并提供了一个应用程序，可以使用文本输入重新照亮任何面孔。]]></description>
      <guid>http://link.springer.com/10.1007/s11263-023-01899-3</guid>
      <pubDate>Mon, 01 Apr 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FFEINR：时空超分辨率的流特征增强隐式神经表示</title>
      <link>http://link.springer.com/10.1007/s12650-024-00959-1</link>
      <description><![CDATA[摘要
大规模数值模拟能够生成高达 TB 甚至 PB 的数据。作为一种有前途的数据缩减方法，超分辨率（SR）已在科学可视化界得到广泛研究。然而，它们大多数都是基于深度卷积神经网络或生成对抗网络，需要在构建网络之前确定比例因子。导致单次训练仅支持固定因子，泛化能力较差。为了解决这些问题，本文提出了一种用于流场数据时空超分辨率的流特征增强隐式神经表示（FFEINR）。它可以在模型结构和采样分辨率方面充分利用隐式神经表示。神经表示基于具有周期性激活函数的全连接网络，这使我们能够获得轻量级模型。学习到的连续表示可以将低分辨率流场输入数据解码为任意空间和时间分辨率，从而允许灵活的上采样。通过引入输入层的特征增强来促进 FFEINR 的训练过程，这补充了流场的上下文信息。为了证明该方法的有效性，通过设置不同的超参数在不同的数据集上进行了一系列实验。结果表明，FFEINR取得了明显优于三线性插值方法的结果。

图形摘要






]]></description>
      <guid>http://link.springer.com/10.1007/s12650-024-00959-1</guid>
      <pubDate>Mon, 01 Apr 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过 SfM–MVS 从图像序列顺序生成局部 3D 模型之间基于估计相机轨迹的集成</title>
      <link>http://link.springer.com/10.1007/s10015-024-00949-4</link>
      <description><![CDATA[摘要
本文描述了一种三维 (3D) 建模方法，用于从相机获取的图像序列中顺序和空间地了解未知环境中的情况。该方法按时间顺序将图像序列按图像数量划分为子图像序列，通过运动和多视图立体结构（SfM-MVS）从子图像序列生成局部3D模型，并集成模型。每个子图像序列中的图像与先前和后续子图像序列部分重叠。使用根据 SfM-MVS 估计的摄像机轨迹计算出的变换参数，将局部 3D 模型集成到 3D 模型中。在我们的实验中，我们定量比较了集成模型的质量与从批次中的所有图像生成的 3D 模型，以及使用从相机获取的三个真实数据集获得这些模型的计算时间。因此，该方法可以生成高质量的集成模型，并通过 SfM-MVS 与使用批次中所有图像的 3D 模型进行比较，并减少计算时间。]]></description>
      <guid>http://link.springer.com/10.1007/s10015-024-00949-4</guid>
      <pubDate>Mon, 01 Apr 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>报告</title>
      <link>http://link.springer.com/10.1007/s41064-024-00289-9</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/s41064-024-00289-9</guid>
      <pubDate>Mon, 01 Apr 2024 00:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>