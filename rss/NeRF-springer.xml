<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最新结果</title>
    <link>http://link.springer.com</link>
    <description>Springer可用的最新内容</description>
    <lastBuildDate>Wed, 02 Apr 2025 12:30:23 GMT</lastBuildDate>
    <item>
      <title>扩散模型和生成人工智能：框架，应用和挑战</title>
      <link>https://link.springer.com/article/10.1007/s11831-025-10266-z</link>
      <description><![CDATA[扩散模型（DMS）最近已成为深层生成模型的高效类别，在各种领域（包括图像合成，视频生成和分子设计）中取得了非凡的结果。这项调查对有关该主题的研究扩展进行了全面分析。这项研究的主要目的是研究生成人工智能系统的结构和要求。最初，对生成AI系统的实施的先决条件和前沿思想进行了分析。为了阐明该方法的运行机制，对DMS的设计选择进行了彻底检查，涵盖了诸如细化，平行生成，编辑，贴上，镶嵌和跨域产生等方面。这项研究广泛回顾了基本的DMS及其在诸如计算机视觉（CV），自然语言处理（NLP），图像合成和跨学科应用（场景产生，3D视觉，视频建模，医学图像诊断，时间表诊断，时间序列分析，音频分析，3D分子产生等）等领域的不同应用。对所有使用生成AI方法用于每个域中各种下游任务的所有作品进行了比较研究。还对数据集进行了全面研究。最后，它讨论了当前方法的局限性，以及对其他技术和未来方向的需求，以便在这方面取得有意义的进步。]]></description>
      <guid>https://link.springer.com/article/10.1007/s11831-025-10266-z</guid>
      <pubDate>Wed, 02 Apr 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过使用改进的Orb-Slam和深度学习视觉大满贯算法，改善医院物流机器人在复杂照明变化下的导航优化</title>
      <link>https://link.springer.com/article/10.1007/s42452-025-06775-y</link>
      <description><![CDATA[在复杂的照明条件下，医院物流机器人在定位和导航方面面临严重的挑战。传统的球（定向快速和旋转的简短简短）算法通常存在诸如不稳定的特征点提取，位置准确性差，定位准确性差，并且长期导航路径计划在环境中的长期照明变化，这会极大地影响机器人的导航效率和准确性。在本文中，使用了改进的ORB-SLAM3算法来改善机器人在具有复杂照明的医院环境中的导航性能。改进的ORB-SLAM3算法提取物通过构造图像金字塔来确保在不同的照明条件下可以稳定获得有效的视觉信息，从而具有不同尺度的点；结合自适应阈值和双阈值方法的策略用于优化特征点提取的准确性，并且通过四树模型有效地管理特征点，以确保特征点的均匀分布。这些措施显着提高了特征点提取的质量和匹配精度；该算法还通过结合惯性测量单元数据，进一步提高定位准确性和系统稳定性，从而紧密结合视觉和惯性信息。实验结果表明，快速光明的环境变化，平均定位误差，导航图构建完整性和改进的Orb-Slam 3 algorithm的效率为0.26 m，80％和7.26 ms，不相差。医院物流机器人的效率，但也提高了其在剧烈照明变化的环境中的稳定性和可靠性。提高医院内部物流系统的智能水平是非常重要的。]]></description>
      <guid>https://link.springer.com/article/10.1007/s42452-025-06775-y</guid>
      <pubDate>Wed, 02 Apr 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于单个RGB-LIDAR视图的结构化3D高斯裂口，用于新型视图合成</title>
      <link>https://link.springer.com/article/10.1007/s10489-025-06494-2</link>
      <description><![CDATA[ 3D场景重建是计算机视觉和图形中的一项关键任务，最近在3D高斯碎片（3DG）中取得了进步，展示了令人印象深刻的新型视图综合（NVS）结果。但是，大多数3DGS方法都依赖于多视图图像，这些图像并非总是可用的，尤其是在室外环境中。在本文中，我们仅使用单视数据探索3D场景重建，包括RGB图像和LIDAR传感器的稀疏点云。为了解决有限的参考和激光雷达传感器不足点云所带来的挑战，我们提出了一个基于体素的结构化3DGS框架，并随着深度预测增强了。我们引入了一个新的深度，先前的向导Voxel生长和修剪算法，该算法利用了深度图来完善场景结构并提高渲染质量。此外，我们设计了一种具有自适应体素尺寸的虚拟背景拟合方法，以适应室外场景中LIDAR数据的稀疏分布。我们的方法超过了现有的方法，包括脚手架，高斯式PRO，3DGS，MIPSPLATTING和UNIDEPTH，就PSNR，SSIM，LPIPS和FID指标而言，在KITTI和Waymo数据集上，在单个ViewPoint 3D Recoluction和NVS和NVS上的有效性展示了其有效性。]]></description>
      <guid>https://link.springer.com/article/10.1007/s10489-025-06494-2</guid>
      <pubDate>Mon, 31 Mar 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AAGS：外观感知3D高斯裂开，无约束的照片集</title>
      <link>https://link.springer.com/article/10.1007/s00530-025-01742-4</link>
      <description><![CDATA[从无限制的野外照片收藏中重建3D场景一直是一个挑战性的问题。主要难度在于不同的外观条件和不受控制的图像样本的瞬态遮挡器。随着神经辐射领域（NERF）的发展，以前的作品制定了一些有效的策略来解决此问题。但是，受深网和体积渲染技术的限制，这些方法通常需要大量的时间成本。最近，3D高斯脱落（3DGS）的出现大大加快了3D重建任务的训练和渲染速度。然而，香草3DGS难以区分野外照片集的不同外观。为了解决上述问题，我们提出了出现感知的3D高斯裂片（AAGS），这是3DGS的新型扩展到无约束的照片集。具体而言，我们采用一个外观提取器来捕获图像样本的全局特征，从而使视觉条件的区别，例如，例如，照明和天气，跨不同的观测值。此外，为了减轻瞬态遮挡器的影响，我们设计了一个瞬态驱动模块，该模块可自适应地学习2D可见性图，从而从复杂的现实世界场景中分解静态目标。进行了广泛的实验，以验证我们的AAG的有效性和优势。与以前的作品相比，我们的方法不仅可以更好地重建和渲染质量，而且大大降低了培训和渲染开销。代码将在&lt;a href =“ https://github.com/zhang-wencong/aags”上发布。]]></description>
      <guid>https://link.springer.com/article/10.1007/s00530-025-01742-4</guid>
      <pubDate>Fri, 28 Mar 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>纹理驱动的姿势引导的人类图像合成</title>
      <link>https://link.springer.com/article/10.1007/s10044-025-01452-7</link>
      <description><![CDATA[随着计算机视觉和人工智能的快速发展，在角色图像合成领域已经取得了重大突破。尽管现有方法可以综合目标姿势图像，但处理复杂的纹理和姿势比对仍存在局限性，例如纹理失真，姿势错位和缺失的信息。为了解决这个问题，本文提出了一种姿势引导的人类图像合成方法，称为人姿势转移生成对抗网络（HPT-GAN）。该模型通过引入Resblocks模块，设计纹理传输模块（TTM）和TORGB模块来显着提高合成图像的质量和效率。具体而言，重新建筑增强了梯度稳定性，同时保留上下文信息，TTM通过多头注意机制有效地对齐纹理，并且TORGB模块优化了多分辨率特征的融合。与类似方法相比，HPT-GAN具有少量参数，同时达到更快的处理速度。此外，它在DeepFashion和Market-1501数据集上取得了良好的成果。]]></description>
      <guid>https://link.springer.com/article/10.1007/s10044-025-01452-7</guid>
      <pubDate>Fri, 28 Mar 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>3D神经风格化的进步：一项调查</title>
      <link>https://link.springer.com/article/10.1007/s11263-025-02403-9</link>
      <description><![CDATA[现代人工智能提供了一种新颖而变革的方法，可以跨越图像，视频和3D数据（释放创造力的力量）跨越各种样式和方式创建数字艺术，并彻底改变了我们感知并与视觉内容互动的方式。本文报道了使用神经网络的表达能力进行风格化的3D资产创造和操纵的最新进展。我们考虑了关键的设计选择，例如场景表示，指导数据，优化策略和输出样式，以建立神经风格风格化的分类学。在此类分类法的基础上，我们的调查首先重新审视了2D图像上神经风化的背景，然后对3D数据的最新神经风格化方法进行了深入的讨论，并伴随着评估选定的网格和神经场风格化方法的基准测试。根据调查中获得的见解，我们强调了实际意义，开放挑战，未来的研究以及神经风格的潜在影响，这促进了研究人员和从业人员使用现代人工智能的3D内容创造的迅速发展的景观。 。]]></description>
      <guid>https://link.springer.com/article/10.1007/s11263-025-02403-9</guid>
      <pubDate>Fri, 28 Mar 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>来自多视图特征立体匹配先验的健壮的小说视图综合</title>
      <link>https://link.springer.com/article/10.1007/s00530-025-01757-x</link>
      <description><![CDATA[ nerf（神经辐射场）表现出从未知视图中合成图像的能力。但是，它面临着诸如闭塞，非斜体表面，稀疏输入以及多视图图像中通常遇到的弱质地等因素所面临的挑战。这些复杂性通常会导致拟合错误的场景几何形状，从而导致次优的新型视图综合质量。本文通过利用基于功能的多视立体匹配（MVS）先验的潜力来解决这一挑战。该方法的区别是其基于MVS估算的深度值，不确定性和深度间隔的自适应构建高斯函数的区别，从而为任意缩放场景提供了灵活性和适应性。在此基础上，通过比较该分布之间的差异与量渲染射线上采样点的重量分布之间的差异来实现NERF训练过程的优化。此外，我们提出了一种有效的Riemann和近似策略，以进一步提高深度损失的性能。适用于三个实际场景数据集的定量指标，即LLFF，IBRNET和DTU，表明本文提供的方法显着提高了与当前的先进方法相比，新型视图合成的质量，可实现3.8％至26.9％的增强。可视化实验揭示了强大的优化结果，尤其是在常规NERF遇到困难的挑战区域中。
                图形摘要
                
]]></description>
      <guid>https://link.springer.com/article/10.1007/s00530-025-01757-x</guid>
      <pubDate>Tue, 25 Mar 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>传输图引导的深层展开网络，用于增强水下图像</title>
      <link>https://link.springer.com/article/10.1007/s11227-025-07155-4</link>
      <description><![CDATA[近年来，随着海洋行业的发展，水下图像增强和恢复的重要性变得越来越突出。然而，由于介质在不同波长下的光吸收和光吸收，水下图像通常会遭受颜色失真和低对比度的影响。但是，大多数现有网络都采用了一种端到端的映射方法，该方法忽略了图像增强过程中的先前信息，从而导致缺乏可解释性。为了应对这些挑战，我们提出了一个传输图引导的深层展开的网络，以增强水下图像。我们的方法由三个核心组成部分组成：自适应面膜照明动态先验（AMIDP），传输引导的多尺度卷积词典（TGMCD）和恒定的空间聚合模块（CSAM）。 AMIDP通过掩码自动编码器和动态卷积提取图像的照明特性，从而使照明和反射信息的单独建模分别为共享和独特的特征。然后将这些功能输入到TGMCD模块中，该模块由传输图指导以进行迭代优化。在此过程中，我们用可学习的多尺度残差模块替换传统的近端运算符，并结合了先前的信息和约束以增强模型性能。此外，CSAM旨在加强跨功能的信息融合，确保最终增强的图像在保留关键细节的同时纠正扭曲。在多个水下数据集上进行的广泛实验表明，我们的方法可实现最先进的性能，从而验证其有效性和优势。我们的代码可在 https://github.com/makabala/tgdu-master。]]></description>
      <guid>https://link.springer.com/article/10.1007/s11227-025-07155-4</guid>
      <pubDate>Sun, 23 Mar 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>对基于社区的夏季干预措施进行营养辅助组件的范围审查，旨在改善儿童与体重相关的结果</title>
      <link>https://link.springer.com/article/10.1186/s12889-025-22241-1</link>
      <description><![CDATA[背景
                儿童，尤其是来自低收入家庭的孩子，在夏季，体重增加和粮食不安全。夏季干预措施包括通过免费或补贴食品或食物的钱提供营养援助，可以很好地解决粮食不安全和肥胖。但是，尚无对夏季干预措施的特征和发现的全面审查，旨在改善儿童与体重相关的结果，包括粮食安全，饮食摄入，体育锻炼和体重。这项研究旨在用包括与儿童体重相关的结果在内的营养成分来描述夏季干预的特征和发现。
              
                方法
                使用该术语“夏季”，“夏季”，“食物”，“营养”，“用餐”，“午餐”，“午餐”，“午餐”或“不安全感”，搜索了此范围评论，Cinahl，Eric，Ovid Medline和Scopus数据库。三名独立审阅者筛选了手稿以获得资格。
              
                结果
                确定了13个手稿。除营养援助外，大多数夏季干预措施的大多数（ n  = 10，77％）提供了营养教育和/或体育活动参与或教育的活动。大多数干预措施（69％）是通过夏令营或学校提供的，而60％的干预措施通过夏季食品服务计划提供了营养援助，以免费餐或小吃的形式提供营养援助。粮食不安全是研究最少的结果。这些夏季干预措施与儿童与体重相关的结果之间的关联是使用各种措施和研究设计研究的，只有三项随机对照研究，其中两项没有足够的动力样本。一些准实验研究记录了干预参与与果实和蔬菜摄入，中度至剧烈的体育锻炼与BMI Z分数或百分位数之间的积极关联，但发现不一致。 
              
                结论
                需要进行更严格的设计和足够动力的样品进行进一步的研究，以评估具有营养辅助的多组分夏季干预措施的影响，以最大程度地利用与儿童体重相关的健康和平等的干预益处。
              
                临床试验号
                不适用。]]></description>
      <guid>https://link.springer.com/article/10.1186/s12889-025-22241-1</guid>
      <pubDate>Sat, 22 Mar 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LIDAR，IMU和相机融合用于同时本地化和映射：系统评价</title>
      <link>https://link.springer.com/article/10.1007/s10462-025-11187-w</link>
      <description><![CDATA[同时本地化和映射（SLAM）是智能无名系统估算其运动和重建未知环境的至关重要技术。但是，由于传感器本身的缺陷，具有一个传感器的SLAM系统仅具有稳健性和稳定性。最近的研究表明，由于不同传感器的相互补偿，具有多个传感器（主要由LIDAR，摄像机和IMU）的SLAM系统具有更好的性能。本文研究了多传感器融合大满贯的最新进展。该综述包括对不同传感器的优势和缺点的系统分析以及多传感器解决方案的命令。它通过融合传感器将多传感器融合大满贯系统分为四种主要类型：LIDAR-IMU SLAM，Visual-Imu Slam，Lidar-Visual-Visual-Visual-visual Slam和Lidar-Imu-Visual-Visual Sllam，并进行了详细的分析以及对管道和原理的讨论。同时，该论文调查常用数据集并引入评估指标。最后，总结了多传感器融合大满贯的现有挑战和未来的机会。。]]></description>
      <guid>https://link.springer.com/article/10.1007/s10462-025-11187-w</guid>
      <pubDate>Wed, 19 Mar 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>胺碘酮治疗引起的罕见神经病毒性：病例报告</title>
      <link>https://link.springer.com/article/10.1186/s43044-025-00631-5</link>
      <description><![CDATA[背景
                胺碘酮是一种以其潜在副作用而闻名的抗心律失常药物，其中一种是神经肌病，尽管它仍然相对较少。这种情况可能带有肌肉无力，疼痛和震颤，可能导致功能障碍。尚不完全了解胺碘酮诱导的神经瘤性的确切机制，但可能涉及直接肌肉毒性和对神经传导的影响。
              
                案例表现
                我们介绍了一个68岁的男性患有症状心律失常的右心室发育不良，患有长期胺碘酮，经历了双侧腿部疼痛和与胺碘酮使用相关的弱点。在临床检查中，下肢的运动强度为2/5，触觉降低。生物学评估显示肌酸激酶和C反应蛋白的正常水平。脊柱MRI正常。肌电图的“ EMG”揭示了非长度依赖性感觉运动脱髓鞘性多神经病。停用胺碘酮后，迁移率和功能都显示出显着改善。
              
                结论
                这些观察结果强调了在接受胺碘酮治疗的患者中进行神经系统检查的重要性，以鉴定罕见的并发症，例如神经瘤病。重要的是，在停用药物后，神经瘤通常是可逆的。]]></description>
      <guid>https://link.springer.com/article/10.1186/s43044-025-00631-5</guid>
      <pubDate>Wed, 19 Mar 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Easyvis：用于腹腔镜手术盒培训师的实时3D可视化软件系统</title>
      <link>https://link.springer.com/article/10.1007/s13304-025-02153-w</link>
      <description><![CDATA[ easyvis是一种新兴的沉浸式3D腹腔镜可视化系统，可提高腹腔镜手术的效率。它将多个微型摄像头和光源与手术端口整合在一起，以在所需的角度提供手术内手术的视觉。在这项工作中，我们在腹腔镜手术训练盒环境中使用Easyvis微型摄像机组装来开发一种可视化算法，并具有简化的训练任务，以验证这种新技术的可行性。由于大多数腹腔镜手术工具是刚体的对象，因此可以离线获取它们的3D形状。我们开发了2D对象检测和跟踪算法，以获取每个对象的2D姿势和3D融合算法，以使用估计的2D姿势来估计和跟踪每个对象的3D姿势。然后，与每个对象的获得的3D模型一起，我们能够使用3D表面模型（离线获取）和从单个微型胶片中获取的图像在所需视图处渲染每个对象。除前景刚性对象外，背景3D模型是使用结构化的灯光和运动结构来获取的。与前景物体的快速运动相比，假定背景正在缓慢变化。因此，背景3D模型仅需要偶尔更新。我们的渲染算法能够整合前景和背景3D模型，以促进从理想的视角渲染基于图像的渲染。我们进行了实验以验证渲染图像的准确性和质量。]]></description>
      <guid>https://link.springer.com/article/10.1007/s13304-025-02153-w</guid>
      <pubDate>Sun, 16 Mar 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GRPOSENET：使用稀疏RGB视图的可推广且可靠的6D对象姿势估计网络</title>
      <link>https://link.springer.com/article/10.1007/s00371-025-03852-6</link>
      <description><![CDATA[六度的自由对象姿势估计在各种计算机视觉和机器人技术任务中起着至关重要的作用。现有方法通常很大程度上依赖CAD模型和实质性的先验信息，从而将其概括限制为在开放场景中看不见的对象。为了解决此限制，我们提出了GRPOSENET，这是一个可推广且可靠的6D对象姿势估计网络，可以仅使用带有参考姿势的稀疏RGB图像来预测看不见的对象的姿势。 GRPOSENET包括一个开放世界检测器，一个观点选择器和自适应多尺度炼油厂。开放世界检测器利用预先训练的大型模型进行零拍分段和特征提取，克服检测和与看不见的对象的匹配错误。视点选择器使用我们设计的相似性网络选择最初的参考视图以进行初始姿势估计。自适应多尺度炼油厂通过迭代更新旋转和基于多尺度特征和自适应重量的翻译残差而进一步完善了姿势。基准数据集和我们可靠的测试数据集RBMOP的广泛实验表明，GRPOSENET实现了最新的性能，显示出极好的概括和鲁棒性，可对看不见的对象和稀疏视图。这些代码和数据集可在： https://github.com/kiers.com/kiersas/grposenet 。]]></description>
      <guid>https://link.springer.com/article/10.1007/s00371-025-03852-6</guid>
      <pubDate>Tue, 11 Mar 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LFVGS：轻巧的高斯拆卸方法，用于几个镜头综合</title>
      <link>https://link.springer.com/article/10.1007/s11227-025-07114-z</link>
      <description><![CDATA[尽管通过3D高斯拆卸实现了新型视图的显着进步，但其依赖于致密的输入视图和高存储需求限制了其适用性。为了应对这一挑战，这项研究提出了LFVGS，这是一个基于3DGS技术的轻量级小说综合框架。 LFVGS可以实现低成本，高质量的新型视图综合，并限制了有限的输入视图（只有三种），同时大大降低了存储要求。为了通过稀疏输入来实现高性能，我们引入了两种关键策略。首先，为了减轻初始输入不足的问题，我们利用距离得分和不透明度阈值在动态而密集地将新高斯人插入最初的高斯原始人中，从而避免了在稀疏视图约束下表示不足表示的问题。其次，为了加强因稀疏输入视图所施加的几何约束，我们将深度先验纳入了监督，利用全球到本地的深度正规化，以减少渲染深度和深度先验之间的差异，从而纠正场景几何形状。最后，通过使用简单的MLP表示依赖视图的颜色，复杂反射区域和局部对象轮廓的合成质量得到显着改善，而模型存储开销显着降低。实验结果表明，LFVGS在LLFF，MIPNERF360和SHINY数据集上实现了出色的总体性能，从而实现了完全端到端的轻质优化过程。该代码可在： https://github.com/leexiaotong1/lfvgs   ]]></description>
      <guid>https://link.springer.com/article/10.1007/s11227-025-07114-z</guid>
      <pubDate>Wed, 05 Mar 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Phydiisp：弱光机构视觉的物理学导致管道</title>
      <link>https://link.springer.com/article/10.1007/s11760-025-03918-x</link>
      <description><![CDATA[在弱光环境中，机器视觉任务通常会遭受性能退化，因为传统的图像信号处理（ISP）管道主要针对图像质量指标进行优化，例如峰值信噪比和结构相似性指数，这些指数无法充分满足这些应用程序的特定需求。在挑战性照明条件下，现有方法在增强计算机视觉任务所需的关键图像特征方面缺乏。为了解决这个问题，我们介绍了Phydiisp，这是一条物理引导的，可区分的ISP管道，旨在在弱光场景中提高机器视觉性能。 Phydiisp将传统的ISP设计原理与物理见解相结合，包括用于原始RGB转换的表演，全球音调映射以调整整体亮度以及基于多尺度视网膜的增强功能，以应对低光挑战。实验结果表明，通过有效增强关键图像特征，Phydiisp在对象检测准确性中的现有ISP方法优于现有的ISP方法。此外，当经过L1损失训练并与黑光环境的数据集和真正的原始型RGB转换相符时，它表明了竞争性的图像质量。这些结果证实了Phydiisp是现实世界中低光机器视觉应用的可行有效解决方案。]]></description>
      <guid>https://link.springer.com/article/10.1007/s11760-025-03918-x</guid>
      <pubDate>Wed, 05 Mar 2025 00:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>