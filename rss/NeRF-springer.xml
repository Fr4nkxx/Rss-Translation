<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最新结果</title>
    <link>http://link.springer.com</link>
    <description>Springer 提供的最新内容</description>
    <lastBuildDate>Wed, 28 Feb 2024 06:14:08 GMT</lastBuildDate>
    <item>
      <title>基于实时距离场加速的大型运动场自由视点视频合成</title>
      <link>http://link.springer.com/10.1007/s41095-022-0323-3</link>
      <description><![CDATA[摘要
自由视点视频允许用户从任何虚拟角度观看物体，创造身临其境的视觉体验。该技术增强了多媒体表演的交互性和自由度。然而，许多自由视点视频合成方法很难满足实时、高精度的要求，特别是对于面积较大、运动物体较多的运动场。为了解决这些问题，我们提出了一种基于距离场加速的自由视点视频合成方法。其中心思想是融合多视点距离场信息并利用其自适应调整搜索步长。自适应步长搜索有两种用途：用于多目标三维表面的快速估计，以及基于全局遮挡判断的合成视图渲染。我们使用并行计算进行交互式显示、使用 CUDA 和 OpenGL 框架来实现我们的想法，并使用真实世界和模拟实验数据集进行评估。结果表明，所提出的方法可以在大型运动场上以 25 fps 渲染具有多个对象的自由视点视频。此外，我们合成的新颖视点图像的视觉质量超过了最先进的基于神经渲染的方法。
      



]]></description>
      <guid>http://link.springer.com/10.1007/s41095-022-0323-3</guid>
      <pubDate>Mon, 01 Apr 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于可微渲染的动态海洋反建模</title>
      <link>http://link.springer.com/10.1007/s41095-023-0338-4</link>
      <description><![CDATA[摘要
学习和推断捕获的 2D 场景的潜在运动模式，然后重新创建与现实世界自然现象一致的动态演化，这对图形和动画具有很高的吸引力。为了弥合虚拟和现实环境之间的技术差距，我们专注于视觉一致且属性可验证的海洋的逆向建模和重建，利用深度学习和可微物理来学习几何并以自我监督的方式构成波浪。首先，我们使用两个网络推断分层几何，这两个网络通过可微渲染器进行了优化。我们通过配备可微分海洋模型的网络从推断的几何序列中提取波浪分量。然后，可以使用重建的波浪分量来演化海洋动力学。通过大量的实验，我们验证了我们的新方法在几何重建和波浪估计方面都能产生令人满意的结果。此外，新框架具有逆向建模潜力，可促进大量图形应用，例如快速生成物理精确的场景动画以及由真实海洋场景引导的编辑。
      



]]></description>
      <guid>http://link.springer.com/10.1007/s41095-023-0338-4</guid>
      <pubDate>Mon, 01 Apr 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于 NeRF 的 GAN 的对应蒸馏</title>
      <link>http://link.springer.com/10.1007/s11263-023-01903-w</link>
      <description><![CDATA[摘要
神经辐射场 (NeRF) 在保留物体和场景的精细细节方面显示出了有希望的结果。然而，与显式形状表示（例如网格）不同，在同一类别的不同 NeRF 之间建立密集对应关系仍然是一个悬而未决的问题，这在许多下游任务中至关重要。这个问题的主要困难在于 NeRF 的隐式性质和缺乏真实对应注释。在本文中，我们展示了通过利用基于 NeRF 的预训练 GAN 中封装的丰富语义和结构先验，可以绕过这些挑战。具体来说，我们从三个方面利用这些先验，即（1）将潜在代码作为全局结构指标的双变形场，（2）将生成器特征视为几何感知局部描述符的学习目标，以及（3）源无限的特定于对象的 NeRF 样本。我们的实验表明，这样的先验可以产生准确、平滑且稳健的 3D 密集对应。我们还表明，跨 NeRF 建立的密集对应关系可以有效地启用许多基于 NeRF 的下游应用，例如纹理传输。]]></description>
      <guid>http://link.springer.com/10.1007/s11263-023-01903-w</guid>
      <pubDate>Fri, 01 Mar 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>RDNeRF：用于密集自由视图合成的相对深度引导 NeRF</title>
      <link>http://link.springer.com/10.1007/s00371-023-02863-5</link>
      <description><![CDATA[摘要
在本文中，我们专注于室内场景中自由移动的密集视图合成，以实现比稀疏视图更好的用户交互。神经辐射场 (NeRF) 可以很好地处理稀疏和球形捕获的场景，但它在具有密集自由视图的场景中表现不佳。我们扩展 NeRF 来处理这些室内场景视图。我们提出了一种名为相对深度引导 NeRF (RDNeRF) 的基于学习的方法，该方法联合渲染 RGB 图像并在密集的自由视图中恢复场景几何形状。为了在没有真实深度的情况下恢复每个视图的几何形状，我们建议通过隐式函数直接学习相对深度，并将其转换为几何体边界，用于几何感知采样和 NeRF 集成。通过正确的场景几何，我们进一步对输入的隐式内部相关性进行建模，以增强 NeRF 在密集自由视图中的表示能力。我们在室内场景中进行了大量的实验，以实现密集的自由视图合成。 RDNeRF 优于当前最先进的方法，达到 24.95 PSNR 分数和 0.77 SSIM 分数。此外，它比基本模型恢复更准确的几何形状。]]></description>
      <guid>http://link.springer.com/10.1007/s00371-023-02863-5</guid>
      <pubDate>Fri, 01 Mar 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于神经辐射场的机器人辅助微创手术动态表面重建</title>
      <link>http://link.springer.com/10.1007/s11548-023-03016-8</link>
      <description><![CDATA[摘要

目的
本研究的目的是通过解决重建高度动态手术场景的挑战来改善手术场景感知。我们提出了一种新颖的深度估计网络和结合神经辐射场的重建框架，为手术任务自动化和 AR 导航提供更准确的场景信息。


方法
我们添加了空间金字塔池化模块和 Swin-Transformer 模块来增强立体深度估计的鲁棒性。我们还通过添加最佳传输的独特匹配约束来提高深度精度。为了避免高动态场景中的变形失真，我们使用神经辐射场在时间维度上隐式表示场景，并以基于学习的方式利用深度和颜色信息对其进行优化。


结果
我们在 KITTI 和 SCARED 数据集上的实验表明，所提出的深度估计网络在自然图像上的表现接近最先进的方法，并且在医学图像上超越了 SOTA 方法3 px 误差为 1.12%，EPE 误差为 0.45 px。所提出的动态重建框架成功地在完全内窥镜冠状动脉搭桥视频上重建了动态心脏表面，实现了 SOTA 性能，PSNR 为 27.983 dB，SSIM 为 0.812，LPIPS 为 0.189。


结论
我们提出的深度估计网络和重建框架为手术场景感知领域做出了重大贡献。该框架在医学数据集上取得了比 SOTA 方法更好的结果，减少了深度图上的不匹配，并产生更准确、边缘更清晰的深度图。所提出的 ER 框架在一系列动态心脏手术图像上得到了验证。未来的工作重点将集中在提高训练速度和解决视野有限的问题上。
]]></description>
      <guid>http://link.springer.com/10.1007/s11548-023-03016-8</guid>
      <pubDate>Fri, 01 Mar 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>表示模糊聚类分析的网络集合：案例研究</title>
      <link>http://link.springer.com/10.1007/s10618-023-00977-x</link>
      <description><![CDATA[摘要
随着网络统计分析在越来越多的学科中得到应用，需要新的方法来处理这种复杂性。特别是，聚类分析是最成功、最普遍的数据探索和表征技术之一。在这项工作中，我们关注如何表示模糊聚类的网络集成。我们基于概率分布、自动编码器和联合嵌入探索三种不同的网络表示。我们比较了在合成数据上对多个网络进行聚类的事实上的标准模糊计算程序。最后，我们将这种方法应用于现实世界的案例研究。]]></description>
      <guid>http://link.springer.com/10.1007/s10618-023-00977-x</guid>
      <pubDate>Fri, 01 Mar 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于神经辐射场的多视角内窥镜场景重建用于手术模拟</title>
      <link>http://link.springer.com/10.1007/s11548-024-03080-8</link>
      <description><![CDATA[摘要

目的
在虚拟手术中，根据CT图像构建的3D模型的外观缺乏真实感，导致住院医生可能产生误解。因此，利用内窥镜捕获的多视图图像重建真实的内窥镜场景至关重要。


方法
我们提出了一种内窥镜-NeRF 网络，用于在非固定光源下对内窥镜场景进行隐式辐射场重建，并使用体积渲染合成新颖的视图。具有多个 MLP 网络和射线变换器网络的内窥镜-NeRF 网络将内窥镜场景表示为隐式场函数，在连续 5D 向量（3D 位置和 2D 方向）上具有颜色和体积密度。最终的合成图像是通过使用体积渲染聚合目标相机每条射线上的所有采样点而获得的。我们的方法考虑了光源到采样点的距离对场景辐射亮度的影响。


结果
我们的网络在我们的设备收集的猪的肺、肝、肾和心脏上进行了验证。结果表明，我们的方法合成的内窥镜场景的新视图在 PSNR、SSIM 和 LPIPS 指标方面优于现有方法（NeRF 和 IBRNet）。


结论
我们的网络可以有效地学习具有泛化能力的辐射场函数。在新的内窥镜场景上对预训练模型进行微调，进一步优化场景的神经辐射场，可以为手术模拟提供更真实、高分辨率的渲染图像。
]]></description>
      <guid>http://link.springer.com/10.1007/s11548-024-03080-8</guid>
      <pubDate>Tue, 27 Feb 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>小儿眼眶病变：骨性和创伤性病变</title>
      <link>http://link.springer.com/10.1007/s00247-024-05882-z</link>
      <description><![CDATA[摘要
眼眶病变可大致分为眼部病变、眼外软组织病变（非肿瘤性和肿瘤性）以及骨性和创伤性病变。在本文中，我们讨论了儿科眼眶和眼球的骨性和创伤性病变的关键影像学特征和鉴别诊断，强调了 CT 和 MRI 作为主要影像学手段的作用。此外，重点介绍眼部超声检查在眼内异物诊断中的辅助作用，并讨论超声检查在外伤性视网膜脱离诊断中的主要作用。

图形摘要






]]></description>
      <guid>http://link.springer.com/10.1007/s00247-024-05882-z</guid>
      <pubDate>Tue, 27 Feb 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于饱和模糊图像的深度非盲去模糊网络</title>
      <link>http://link.springer.com/10.1007/s00521-024-09495-3</link>
      <description><![CDATA[摘要
非盲图像去模糊在低水平视觉领域引起了广泛关注。然而，现有的非盲去模糊方法无法有效处理饱和模糊图像。关键在于饱和模糊图像的退化模型不满足传统模糊图像的线性卷积模型。为了解决这个问题，本文提出了一种新颖的深度非盲去模糊方法，称为饱和图像非盲去模糊网络（SDBNet）。 SDBNet包含两个可训练的子网络，即置信估计网络（CEN）和细节增强网络（DEN）。具体来说，SDBNet使用CEN来估计饱和模糊图像的置信图，用于识别模糊图像中的饱和像素，然后使用置信图和模糊核来恢复模糊图像。最后，我们使用 DEN 来增强恢复图像的边缘和纹理。我们首先预训练 CEN 和 DEN。为了有效地预训练 CEN，我们提出了一个新的鲁棒函数，用于为 CEN 生成标签数据。实验结果表明，与现有的几种非盲去模糊方法相比，SDBNet能够有效恢复饱和模糊图像，更好地恢复模糊图像的纹理、边缘等结构信息。]]></description>
      <guid>http://link.springer.com/10.1007/s00521-024-09495-3</guid>
      <pubDate>Wed, 21 Feb 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大会报告：第36届ICMART大会（2023年9月29日至10月1日，荷兰阿姆斯特丹）</title>
      <link>http://link.springer.com/10.1007/s00052-024-00108-9</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/s00052-024-00108-9</guid>
      <pubDate>Tue, 20 Feb 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FFEINR：时空超分辨率的流特征增强隐式神经表示</title>
      <link>http://link.springer.com/10.1007/s12650-024-00959-1</link>
      <description><![CDATA[摘要
大规模数值模拟能够生成高达 TB 甚至 PB 的数据。作为一种有前途的数据缩减方法，超分辨率（SR）已在科学可视化界得到广泛研究。然而，它们大多数都是基于深度卷积神经网络或生成对抗网络，需要在构建网络之前确定比例因子。导致单次训练仅支持固定因子，泛化能力较差。为了解决这些问题，本文提出了一种用于流场数据时空超分辨率的流特征增强隐式神经表示（FFEINR）。它可以在模型结构和采样分辨率方面充分利用隐式神经表示。神经表示基于具有周期性激活函数的全连接网络，这使我们能够获得轻量级模型。学习到的连续表示可以将低分辨率流场输入数据解码为任意空间和时间分辨率，从而允许灵活的上采样。通过引入输入层的特征增强来促进 FFEINR 的训练过程，这补充了流场的上下文信息。为了证明该方法的有效性，通过设置不同的超参数在不同的数据集上进行了一系列实验。结果表明，FFEINR取得了明显优于三线性插值方法的结果。

图形摘要






]]></description>
      <guid>http://link.springer.com/10.1007/s12650-024-00959-1</guid>
      <pubDate>Mon, 19 Feb 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ReliTalk：从单个视频生成可重新点亮的说话肖像</title>
      <link>http://link.springer.com/10.1007/s11263-024-02007-9</link>
      <description><![CDATA[摘要
近年来，从单眼视频创建生动的音频驱动肖像方面取得了巨大进步。然而，如何将创建的视频头像无缝地适应不同背景和光照条件的其他场景仍然没有解决。另一方面，现有的重新照明研究大多依赖于动态照明或多视图数据，这对于创建视频肖像来说过于昂贵。为了弥补这一差距，我们提出了 ReliTalk，这是一种新颖的框架，用于从单眼视频生成可重新点亮的音频驱动的谈话肖像。我们的主要见解是从隐式学习的音频驱动的面部法线和图像中分解肖像的反射率。具体来说，我们利用从音频特征导出的 3D 面部先验，通过隐式函数来预测精细的法线贴图。然后，这些最初预测的法线通过动态估计给定视频的照明条件，在反射率分解中发挥关键作用。此外，在模拟的多个照明条件下，使用身份一致的损失来细化立体人脸表示，解决了由于单个单目视频的可用视图有限而引起的不适定问题。大量的实验验证了我们提出的框架在真实数据集和合成数据集上的优越性。我们的代码发布于 (https://github.com/arthur-qiu/ReliTalk).]]></description>
      <guid>http://link.springer.com/10.1007/s11263-024-02007-9</guid>
      <pubDate>Fri, 16 Feb 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>VI-NeRF-SLAM：具有 NeRF 映射的实时视觉惯性 SLAM</title>
      <link>http://link.springer.com/10.1007/s11554-023-01412-6</link>
      <description><![CDATA[摘要
在众多机器人和自动驾驶任务中，传统的视觉SLAM算法通过稀疏特征点估计相机在场景中的位置，并通过估计稀疏点云的深度来表达地图。然而，实际应用需要SLAM实时创建密集地图，克服点云的稀疏和遮挡问题。此外，SLAM地图的优点是具有自动完成能力，当相机仅观察到物体的80%时，地图可以自动推断并完成剩余的20%。因此，需要更密集、更智能的地图表示。在本文中，我们提出了一种具有神经辐射场重建的视觉惯性 SLAM 来解决上述挑战。我们将传统的基于规则的优化与 NeRF 相结合。这种方法可以通过快速估计相机运动和稀疏特征点深度来实时更新 NeRF 局部函数以重建 3D 场景。为了实现更好的相机姿势和全局一致的地图，我们解决了快速运动变化导致的 IMU 噪声尖峰问题，以及处理由于闭环融合导致的姿势调整。具体来说，我们采用加宽静态噪声协方差的形式来重新拟合动态噪声协方差。在闭环融合过程中，我们将闭环前和闭环后之间的位姿调整视为时空变换，将 NeRF 参数从前到后迁移，以加快 NeRF 映射中的闭环调整。此外，我们将该方法扩展到只有灰度图像的场景。通过扩展灰度图像的颜色通道并进行线性空间映射，我们可以仅用灰度图像快速重建3D场景。我们在 RGB 和灰度场景中展示了我们的方法的精度和速度优势。]]></description>
      <guid>http://link.springer.com/10.1007/s11554-023-01412-6</guid>
      <pubDate>Fri, 09 Feb 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于人体渲染的几何引导可推广 NeRF</title>
      <link>http://link.springer.com/10.1007/s11042-024-18410-w</link>
      <description><![CDATA[摘要
从稀疏的输入视图中呈现逼真的人类新颖视图具有挑战性。一方面，最近的人体渲染作品仅限于特定于个人的情况，因此不能推广到新的表演者。另一方面，可推广到新目标的算法是针对场景或对象开发的，并不直接适用于具有复杂身体姿势的新表演者。为此，我们提出了一种新的人工渲染管道，该管道仅采用从未出现在训练数据中的目标执行者的稀疏视图作为输入。然后，它会在任意视点合成高质量的捕获图像。我们框架的核心是利用几何先验来指导神经辐射场，以多视图图像作为输入进行人体渲染。这不仅可以帮助处理聚合多视图特征时由骨骼运动引起的自遮挡问题，而且有助于推理表演者的几何形状。定性和定量评估结果均表明，我们的方法比当前最先进的技术具有更强的泛化能力。]]></description>
      <guid>http://link.springer.com/10.1007/s11042-024-18410-w</guid>
      <pubDate>Thu, 08 Feb 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于 3D 数据处理的变分自动编码器</title>
      <link>http://link.springer.com/10.1007/s10462-023-10687-x</link>
      <description><![CDATA[摘要
变分自动编码器 (VAE) 在高维数据生成中发挥着重要作用，因为它们能够将随机数据表示与最新深度学习技术的强大功能相融合。这些类型的生成器的主要优点在于它们能够对信息进行编码，并且能够解码和概括新样本。这种功能在 2D 图像处理中得到了广泛的探索；然而，只有有限的研究关注用于 3D 数据处理的 VAE。在本文中，我们全面回顾了使用 VAE 进行 3D 数据处理的最新成就。这些 3D 数据类型主要是点云、网格和体素网格，它们是广泛应用的焦点，尤其是在机器人领域。首先，我们很快会介绍基本的自动编码器以及 VAE 的扩展以及与离散点云处理相关的更多子类别。然后，根据 3D 数据特定的 VAE 对空间数据的操作方式来呈现。最后，提供了一些总结方法、代码和数据集的综合表格以及引用图，以便更好地理解应用于 3D 数据的 VAE。分析的论文的结构遵循分类法，该分类法根据算法的主要数据类型和应用领域来区分算法。]]></description>
      <guid>http://link.springer.com/10.1007/s10462-023-10687-x</guid>
      <pubDate>Thu, 08 Feb 2024 00:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>