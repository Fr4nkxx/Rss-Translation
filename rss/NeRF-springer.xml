<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最新结果</title>
    <link>http://link.springer.com</link>
    <description>Springer 提供的最新内容</description>
    <lastBuildDate>Sat, 28 Sep 2024 21:12:40 GMT</lastBuildDate>
    <item>
      <title>计算机视觉 – ECCV 2024</title>
      <link>http://link.springer.com/10.1007/978-3-031-72667-5</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-72667-5</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>HAC：用于 3D 高斯溅射压缩的哈希网格辅助上下文</title>
      <link>http://link.springer.com/10.1007/978-3-031-72667-5_24</link>
      <description><![CDATA[摘要
3D 高斯分层 (3DGS) 已成为一种有前途的新型视图合成框架，具有高保真度和快速渲染速度。然而，大量的高斯及其相关属性需要有效的压缩技术。然而，高斯点云（或我们论文中的锚点）的稀疏和无序性质给压缩带来了挑战。为了解决这个问题，我们利用无组织锚点和结构化哈希网格之间的关系，利用它们的相互信息进行上下文建模，并提出了一种哈希网格辅助上下文 (HAC) 框架，用于高度紧凑的 3DGS 表示。我们的方法引入了一个二进制哈希网格来建立连续的空间一致性，使我们能够通过精心设计的上下文模型揭示锚点的固有空间关系。为了便于熵编码，我们利用高斯分布来准确估计每个量化属性的概率，其中提出了一个自适应量化模块，以实现这些属性的高精度量化，从而提高保真度恢复。此外，我们采用了自适应掩蔽策略来消除无效的高斯和锚点。重要的是，我们的工作是探索基于上下文的 3DGS 表示压缩的先驱，与原始 3DGS 相比，其尺寸显著减小了 
\(75\times \)
，同时提高了保真度，并且与 SoTA 3DGS 压缩方法 Scaffold-GS 相比，其尺寸减小了 
\(11\times \)
。我们的代码可在此处获得。]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-72667-5_24</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ScaleDreamer：具有异步乐谱提炼的可扩展文本到 3D 合成</title>
      <link>http://link.springer.com/10.1007/978-3-031-72667-5_1</link>
      <description><![CDATA[摘要
通过利用文本到图像的扩散先验，分数蒸馏可以在没有成对的文本-3D 训练数据的情况下合成 3D 内容。最近的研究不再花费数小时对每个文本提示进行在线优化，而是专注于学习一个文本到 3D 的生成网络来摊销多个文本-3D 关系，从而可以在几秒钟内合成 3D 内容。然而，现有的分数蒸馏方法很难扩展到大量的文本提示，因为很难将预训练的扩散先验与来自各种文本提示的渲染图像的分布对齐。目前最先进的技术，如变分分数蒸馏，对预训练的扩散模型进行微调，以最小化噪声预测误差，从而对齐分布，然而，这些方法训练起来不稳定，会损害模型对大量文本提示的理解能力。基于扩散模型在较早的时间步长中往往具有较低噪声预测误差的观察结果，我们提出了异步分数蒸馏 (ASD)，通过将扩散时间步长移至较早的时间步长来最小化噪声预测误差。ASD 训练稳定，可扩展至 100k 个提示。它在不改变预训练扩散模型权重的情况下降低了噪声预测误差，从而保持了其对提示的强大理解能力。我们使用不同的文本到 3D 架构进行了广泛的实验，包括 Hyper-iNGP 和 3DConv-Net。结果证明了 ASD 在稳定的 3D 生成器训练、高质量 3D 内容合成和出色的提示一致性方面的有效性，尤其是在大型提示语料库下。代码可在 https://github.com/theEricMa/ScaleDreamer 上找到。]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-72667-5_1</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>内部成像：电磁逆散射问题的隐式解</title>
      <link>http://link.springer.com/10.1007/978-3-031-72667-5_20</link>
      <description><![CDATA[摘要
电磁逆散射问题 (EISP) 在计算成像中得到了广泛的应用。通过求解 EISP，可以根据散射的电磁场非侵入式地确定散射体的内部相对介电常数。尽管之前已经努力解决 EISP，但由于反演和离散化带来的挑战，仍然难以找到更好的解决方案。本文通过隐式方法解决了 EISP 中的这些挑战。通过将散射体的相对介电常数表示为连续隐式表示，我们的方法能够解决离散化引起的低分辨率问题。此外，在正向框架内优化这种隐式表示使我们能够方便地规避逆估计带来的挑战。我们的方法在标准基准数据集上优于现有方法。项目页面：https://luo-ziyuan.github.io/Imaging-Interiors.]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-72667-5_20</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>机器视觉中的合成数据增强方法综述</title>
      <link>http://link.springer.com/10.1007/s11633-022-1411-7</link>
      <description><![CDATA[摘要
解决计算机视觉问题的标准方法是使用代表目标任务的大规模图像数据集来训练深度卷积神经网络 (CNN) 模型。然而，在许多情况下，获取足够的图像数据来完成目标任务往往是一项挑战。数据增强是一种缓解这一挑战的方法。一种常见的做法是明确地以所需的方式转换现有图像，以创建实现良好泛化性能所需的训练数据量和可变性。在无法获取目标域数据的情况下，一种可行的解决方法是从头开始合成训练数据，即合成数据增强。本文对合成数据增强技术进行了广泛的回顾。它涵盖了基于逼真的 3D 图形建模、神经风格迁移 (NST)、差分神经渲染和使用生成对抗网络 (GAN) 和变分自动编码器 (VAE) 的生成建模的数据合成方法。对于每类方法，我们都将重点介绍重要的数据生成和增强技术、一般应用范围和特定用例，以及现有的限制和可能的解决方法。此外，我们还总结了用于训练计算机视觉模型的常见合成数据集，重点介绍了主要功能、应用领域和支持的任务。最后，我们讨论了合成数据增强方法的有效性。由于这是第一篇详细探讨合成数据增强方法的论文，我们希望为读者提供必要的背景信息和对现有方法及其相关问题的深入了解。]]></description>
      <guid>http://link.springer.com/10.1007/s11633-022-1411-7</guid>
      <pubDate>Tue, 01 Oct 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于多视角 2D 环绕图像的自监督 3D 语义占用预测</title>
      <link>http://link.springer.com/10.1007/s41064-024-00308-9</link>
      <description><![CDATA[摘要
对环境几何和语义的精确 3D 表示为各种下游任务奠定了基础，对于路径规划和避障等自动驾驶相关任务至关重要。这项工作的重点是 3D 语义占用预测，即将场景重建为体素网格，其中每个体素都分配有一个占用和语义标签。我们提出了一种基于卷积神经网络的方法，该方法利用来自重叠最小的环视设置的多张彩色图像，以及相关的内部和外部摄像头参数作为输入，将观察到的环境重建为 3D 语义占用图。为了解释从单目 2D 图像重建 3D 表示的不适定性，图像信息会随时间进行整合：假设摄像机设置正在移动，来自连续时间步骤的图像用于形成多视图立体设置。在详尽的实验中，我们研究了动态物体所带来的挑战以及使用 3D 或 2D 参考数据训练所提出方法的可能性。后者受到生成和注释 3D 地面真实数据相对较高的成本的激励。此外，我们提出并研究了一种新颖的自监督训练方案，该方案不需要任何几何参考数据，而仅依赖于稀疏语义地面真实值。对 Occ3D 数据集的评估，包括与文献中当前最先进的自监督方法的比较，证明了我们的自监督变体的潜力。]]></description>
      <guid>http://link.springer.com/10.1007/s41064-024-00308-9</guid>
      <pubDate>Tue, 01 Oct 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>UGINR：通过隐式神经表征实现大规模非结构化网格缩减</title>
      <link>http://link.springer.com/10.1007/s12650-024-01003-y</link>
      <description><![CDATA[
摘要
最近，隐式神经表征 (INR) 在处理 3D 体积数据方面表现出了显著的能力，尤其是在数据压缩方面。然而，大多数研究主要集中在结构化网格上，而结构化网格在科学领域，尤其是物理学领域并不常见。为了解决这一限制，我们提出了一种通过隐式神经表征 (UGINR) 进行非结构化网格缩减的方法。UGINR 采用分而治之的方法；具体来说，我们根据值将大规模数据分割成块。随后，我们对每个部分使用 INR 网络来学习其独特的特征。最后，我们整合这些单独的网络以实现压缩目标。为了确保与既定的研究方法兼容，我们仅对非结构化网格中每个单元的顶点进行采样。通过权重量化，我们的模型可以实现高压缩率。为了说明所提方法的有效性，我们在各种数据集上进行了实验，证明了我们的方法在科学可视化和大规模数据压缩方面的稳健性。


图形摘要





]]></description>
      <guid>http://link.springer.com/10.1007/s12650-024-01003-y</guid>
      <pubDate>Tue, 01 Oct 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于实时渲染神经辐射场的多项式</title>
      <link>http://link.springer.com/10.1007/s00371-024-03660-4</link>
      <description><![CDATA[摘要
在神经辐射场（NeRF）中，生成高度逼真的渲染结果需要大量光线采样和多层感知器的在线查询。然而，这会导致渲染速度变慢。先前的研究通过设计更快的神经场景表示评估或预计算场景属性来减少渲染时间来解决此问题。在本文中，我们提出了一种实时渲染方法PNeRF。PNeRF利用连续多项式函数来近似空间体积密度和颜色信息。此外，我们将视线方向信息从渲染方程中分离出来，从而为体积渲染方程提供了新的表达式。通过将观察视点的起始坐标和观察方向向量作为神经网络的输入，我们得到了相应观察光线的渲染结果。因此，每条光线的渲染只需要神经网络的一次前向推理。为了进一步提高渲染速度，我们设计了一种六轴球面方法来存储与观察视点的起始坐标和观察方向向量相对应的渲染结果。这使得我们能够显著提高渲染速度并保持渲染质量，同时将存储空间要求降至最低。在 LLFF 数据集上的实验验证表明，我们的方法在保持渲染质量的同时提高了渲染速度，并且占用的存储空间最小。这些结果表明了我们的方法在实时渲染领域的潜力，为更高效的渲染提供了有效的解决方案。]]></description>
      <guid>http://link.springer.com/10.1007/s00371-024-03660-4</guid>
      <pubDate>Fri, 27 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LEO：用于人体视频合成的生成潜像动画器</title>
      <link>http://link.springer.com/10.1007/s11263-024-02231-3</link>
      <description><![CDATA[摘要
时空连贯性是合成高质量视频的主要挑战，尤其是在合成包含丰富全局和局部变形的人体视频时。为了解决这一挑战，以前的方法在生成过程中采用了不同的特征来表示外观和运动。然而，由于缺乏严格的机制来保证这种分离，将运动与外观分离仍然具有挑战性，导致空间扭曲和时间抖动破坏了时空连贯性。受此启发，我们在此提出了 LEO，一种用于人体视频合成的新型框架，重点关注时空连贯性。我们的主要思想是在生成过程中将运动表示为一系列流图，这本质上将运动与外观隔离开来。我们通过基于流的图像动画器和潜在运动扩散模型 (LMDM) 实现了这个想法。前者将运动代码空间与流图空间联系起来，以扭曲和修复的方式合成视频帧。LMDM 通过合成运动代码序列来学习在训练数据中预先捕捉运动。广泛的定量和定性分析表明，与 TaichiHD、FaceForensics 和 CelebV-HQ 数据集上的先前方法相比，LEO 显著提高了人体视频的连贯合成。此外，LEO 中外观和运动的有效分离允许执行另外两个任务，即无限长度的人体视频合成以及保留内容的视频编辑。项目页面：https://wyhsirius.github.io/LEO-project/。

]]></description>
      <guid>http://link.springer.com/10.1007/s11263-024-02231-3</guid>
      <pubDate>Fri, 27 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从单目图像或视频中重建逼真的 3D 着装人体的创新 AI 技术：一项调查</title>
      <link>http://link.springer.com/10.1007/s00371-024-03641-7</link>
      <description><![CDATA[摘要
近年来，从单目图像或视频重建高质量 3D 着装人体因其重要的实际应用而越来越受欢迎。虽然有几项调查已经解决了从图像或视频重建全身参数化人体模型的问题，但本调查专门深入研究了重建 3D 着装人体的挑战和方法。它涵盖了着装人体重建的姿势相关方法和动态方法。关于从单目图像重建姿势相关的着装人体，我们研究了使用在高质量 3D 扫描上训练的回归模型来估计着装人体几何形状的方法。此外，我们还探索了利用大规模扩散模型中的纹理先验来增强对被遮挡或看不见的区域中人体外观推断的研究。在从单目和稀疏多视角视频重建动态着装人体方面，我们分析了利用神经辐射场和 3D 高斯表示的人体建模技术，这些技术使用变形场来捕捉跨帧的人体运动。此外，我们概述了这些研究中的数据集和常用的定量评估指标。最后，我们总结了未解决的问题，并提出了现实重建着装人体的未来研究方向，强调了值得进一步研究的领域。
]]></description>
      <guid>http://link.springer.com/10.1007/s00371-024-03641-7</guid>
      <pubDate>Thu, 26 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于人工智能的超声引导腰方肌阻滞自动分割</title>
      <link>http://link.springer.com/10.1007/s10278-024-01267-8</link>
      <description><![CDATA[摘要
超声引导腰方肌阻滞（QLB）技术已成为腹部和盆腔手术中广泛使用的围手术期镇痛方法。由于超声图像上腰方肌（QLM）的解剖复杂性和个体差异性，神经阻滞严重依赖麻醉师的经验。因此，利用人工智能（AI）识别超声图像中的不同组织区域至关重要。在我们的研究中，我们回顾性地收集了112名患者（3162张图像），并开发了一个名为Q-VUM的深度学习模型，它是一个基于视觉几何组16（VGG16）网络的U形网络。Q-VUM精确分割各种组织，包括QLM、腹外斜肌、腹内斜肌、腹横肌（统称为EIT）和骨骼。此外，我们对 Q-VUM 进行了评估。我们的模型表现出了稳健的性能，平均交并比 (mIoU)、平均像素准确率、骰子系数和准确率分别达到 0.734、0.829、0.841 和 0.944。QLM 实现的 IoU、召回率、准确率和骰子系数分别为 0.711、0.813、0.850 和 0.831。此外，Q-VUM 预测显示，阻塞区域中 85% 的像素落在实际阻塞区域内。最后，我们的模型表现出比常见的深度学习分割网络更强的分割性能（分别为 0.734 vs. 0.720 和 0.720）。总之，我们提出了一个名为 Q-VUM 的模型，可以实时准确识别腰方肌的解剖结构。该模型可帮助麻醉师精确定位神经阻滞部位，从而减少潜在的并发症并提高神经阻滞手术的有效性。]]></description>
      <guid>http://link.springer.com/10.1007/s10278-024-01267-8</guid>
      <pubDate>Wed, 25 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过视图特定查询的光场角度超分辨率</title>
      <link>http://link.springer.com/10.1007/s00371-024-03620-y</link>
      <description><![CDATA[摘要
光场角度超分辨率 (LFASR) 旨在从稀疏采样的输入中重建密集采样的光场。最近，基于卷积神经网络的方法取得了令人鼓舞的结果。然而，这些方法中的大多数分别使用目标密集光的视图特定特征（即内容和视图位置），目标光场的几何结构信息尚未充分探索。为此，我们提出了视图特定查询，将密集光场的视图位置信息集成到 LFASR 的 Transformer（称为 ViewFormer）中。具体而言，我们首先利用 Transformer 编码器来处理输入的稀疏采样光场。然后，使用视图插值操作沿目标光场的水平和垂直方向处理提取的子孔径特征，生成新的子孔径表示，称为视图特定查询。接下来，特定视图查询包含目标光场的视图坐标信息，并通过 Transformer 解码器逐层动态增强。增强的特定视图查询被馈送到重建模块进行最终的光场合成。此外，为了进一步挖掘有关输入稀疏采样光场的更多信息，我们对 Transformer 的构建块采用了通道注意方案。在常用的 LFASR 基准上进行了广泛的实验。与其他方法相比，ViewFormer 在流行的 LFASR 基准（包括真实世界和合成数据）上取得了新的最佳结果。]]></description>
      <guid>http://link.springer.com/10.1007/s00371-024-03620-y</guid>
      <pubDate>Sun, 22 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>文本驱动的着装人体图像合成，结合 3D 人体模型估计，帮助购物</title>
      <link>http://link.springer.com/10.1007/s11042-024-20187-x</link>
      <description><![CDATA[摘要
网上购物已成为现代消费文化不可或缺的一部分。然而，它面临着基于文本描述可视化服装项目和估计其适合个人体型的挑战。在这项工作中，我们提出了一种创新的解决方案来解决这些挑战，即通过文本驱动的着装人体图像合成和 3D 人体模型估计，利用矢量量化变分自动编码器 (VQ-VAE) 的强大功能。在视觉和图形领域，创建多样化和高质量的人体图像是一项至关重要但又困难的任务。由于服装设计和纹理种类繁多，现有的生成模型通常不足以满足最终用户的需求。在这项提议的工作中，我们引入了一种解决方案，该解决方案由通过多个模型的各种数据集提供，因此可以提供优化的解决方案以及具有各种姿势的高质量图像。我们使用两种不同的程序从预定的人体姿势开始创建全身 2D 人体照片。 1) 首先将提供的人体姿势转换为人体解析图，其中包含一些描述衣服形状的句子。 2) 然后向开发的模型提供有关衣服纹理的更多信息作为输入，以生成最终的人体图像。 该模型分为两个不同的部分，第一个是粗略级别的代码本，用于处理总体结果，另一个是精细级别的代码本，用于处理细节。 如前所述，精细级别的代码本专注于纹理的细节，而粗略级别的代码本涵盖了结构中纹理的描述。 与分层代码本一起训练的解码器将各个级别的预期索引转换为人体图像。 由于使用了专家组合，因此创建的图像可以依赖于细粒度的文本输入。 通过预测更精细级别的指标来改善服装纹理的质量。 根据大量定量和定性评估，实施这些策略可以产生比最先进的程序更多样化和高质量的人体图像。这些生成的照片将转换为 3D 模型，从而产生多种姿势和结果，或者您可能只是从产生各种姿势的数据集中制作 3D 模型。PIFu 方法的应用分别使用 Marching cube 算法和 Stacked Hourglass 方法生成 3D 模型和逼真的图像。这导致基于文本描述生成高分辨率图像并将生成的图像重建为 3D 模型。实现的初始分数和 Fréchet 截距、SSIM 和 PSNR 分别为 1.64 ± 0.20 和 24.64527782349843、0.642919520 和 32.87157744102002。与其他技术相比，实施的方法得分很高。这项技术对于重塑电子商务格局具有巨大的希望，可提供更具沉浸感和信息量的探索服装选择的方式。]]></description>
      <guid>http://link.springer.com/10.1007/s11042-024-20187-x</guid>
      <pubDate>Thu, 19 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>NeRF-OR：从稀疏视图 RGB-D 视频中重建手术室场景的神经辐射场</title>
      <link>http://link.springer.com/10.1007/s11548-024-03261-5</link>
      <description><![CDATA[摘要

目的
手术室 (OR) 中的 RGB-D 摄像机提供复杂手术场景的同步视图。将这些多视图数据同化为统一表示可以实现下游任务，例如对象检测和跟踪、姿势估计和动作识别。神经辐射场 (NeRF) 可以在有限的内存占用下提供复杂场景的连续表示。然而，现有的 NeRF 方法在现实世界的 OR 设置中表现不佳，因为其中一小部分摄像机从完全不同的有利位置捕捉房间。在本研究中，我们提出了 NeRF-OR，一种用于在手术室中对动态手术场景进行 3D 重建的方法。


方法
其他稀疏视图数据集方法使用飞行时间传感器深度或从彩色图像估计的密集深度，而 NeRF-OR 则使用两者的组合。深度估计可以减轻由于反射材料和物体边界而导致传感器深度图像中出现的缺失值。我们建议使用根据估计深度计算的表面法线进行监督，因为这些法线在很大程度上是尺度不变的。


结果
我们将 NeRF-OR 拟合到 4D-OR 数据集中的静态手术场景，并表明其表示在几何上是准确的，而最先进的方法会崩溃为次优解决方案。与早期的工作相比，NeRF-OR 在训练时掌握精细的场景细节，速度提高了 30
\(\times \)
。此外，NeRF-OR 可以捕获整个手术视频，同时合成中间时间值的视图，平均 PSNR 为 24.86 dB。最后，通过对包含少至三个训练视图的 NVS-RGBD 数据集进行基准测试，我们发现我们的方法在手术室以外的稀疏视图设置中具有优势。NeRF-OR 合成的图像的 PSNR 为 26.72 dB，比最先进的方法提高了 1.7%。


结论
我们的结果表明，NeRF-OR 允许使用少量具有完全不同有利位置的摄像机拍摄的视频进行新颖的视图合成，这是手术室中的典型摄像机设置。代码可通过以下方式获取：github.com/Beerend/NeRF-OR。
]]></description>
      <guid>http://link.springer.com/10.1007/s11548-024-03261-5</guid>
      <pubDate>Fri, 13 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Ambient-NeRF：利用环境照明在低光照条件下增强神经辐射场的光列</title>
      <link>http://link.springer.com/10.1007/s11042-024-19699-3</link>
      <description><![CDATA[摘要
NeRF可以渲染照片级逼真的3D场景，广泛应用于虚拟现实、自动驾驶、游戏开发等领域，并迅速成为3D重建领域最热门的技术之一。NeRF通过从相机的空间坐标和视点发射光线，穿过场景并计算从视点看到的景色，从而生成逼真的3D场景。然而当原始输入图像亮度较低时，很难恢复场景。受计算机图形学Phong模型中环境光照的启发，假设最终渲染出的图像是场景颜色和环境光照的乘积。本文采用多层感知器 (MLP) 网络训练环境光照张量 
\(\textbf{I}\)
，并将其乘以 NeRF 预测的颜色，以渲染具有正常光照的图像。此外，我们使用 tiny-cuda-nn 作为骨干网络，简化了所提出的网络结构并大大提高了训练速度。此外，引入了新的损失函数，以在低光照条件下实现更好的图像质量。实验结果表明，与其他最先进的方法相比，所提出的方法在增强低光场景图像方面非常有效，在 LOM 数据集上的整体平均值为 PSNR：20.53、SSIM：0.785 和 LPIPS：0.258。]]></description>
      <guid>http://link.springer.com/10.1007/s11042-024-19699-3</guid>
      <pubDate>Tue, 10 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>