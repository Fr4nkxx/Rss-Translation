<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最新结果</title>
    <link>http://link.springer.com</link>
    <description>Springer 提供的最新内容</description>
    <lastBuildDate>Thu, 29 Aug 2024 01:00:19 GMT</lastBuildDate>
    <item>
      <title>特邀社论：2022 年英国机器视觉会议特刊</title>
      <link>http://link.springer.com/10.1007/s11263-024-02038-2</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/s11263-024-02038-2</guid>
      <pubDate>Sun, 01 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一种基于注意力机制的轻量级图像去雾网络</title>
      <link>http://link.springer.com/10.1007/s11760-024-03392-x</link>
      <description><![CDATA[摘要
在当前基于卷积的图像去雾网络中，增加卷积层的深度和宽度是提高网络性能的常用策略。然而，这种方法显著增加了去雾网络的复杂性和计算成本。为了解决这个问题，本文提出了一种U形多尺度自适应选择网络（UMA-Net）。在不引入额外参数和计算成本的情况下，该网络利用不同尺度的卷积核对感受野的影响。它将标准卷积和扩张卷积结合到前馈网络（FFN）中，提出了一个多尺度自适应（MA）去雾模块，进一步扩展了感受野并关注FFN内重要的空间和通道信息。为了充分利用MA模块的多尺度特性，提出了一个轻量级的通道注意引导融合（CAGF）模块，实现了从雾蒙蒙图像中恢复高质量的去雾图像。大量实验证明了所提模块的有效性，在 Reside SOTS 数据集上，仅用 0.816M 参数和 8.794G FLOPs 就取得了最佳性能。]]></description>
      <guid>http://link.springer.com/10.1007/s11760-024-03392-x</guid>
      <pubDate>Sun, 01 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CLIP-Flow：解码 CLIP 空间中编码的图像</title>
      <link>http://link.springer.com/10.1007/s41095-023-0375-z</link>
      <description><![CDATA[摘要
本研究介绍了 CLIP-Flow，这是一种用于从给定图像或文本生成图像的新型网络。为了有效利用两种模态中包含的丰富语义，我们设计了一种语义引导的图像和文本到图像合成方法。具体而言，我们采用对比语言-图像预训练 (CLIP) 作为编码器来提取语义，采用 StyleGAN 作为解码器从此类信息生成图像。此外，为了连接 CLIP 的嵌入空间和 StyleGAN 的潜在空间，采用了真实的 NVP，并通过激活规范化和可逆卷积对其进行了修改。由于 CLIP 中的图像和文本共享相同的表示空间，因此可以将文本提示直接输入到 CLIP-Flow 中以实现文本到图像的合成。我们在多个数据集上进行了广泛的实验，以验证所提出的图像到图像合成方法的有效性。此外，我们在公共数据集 Multi-Modal CelebA-HQ 上进行了文本到图像合成测试。实验验证了我们的方法可以生成高质量的文本匹配图像，并且在定性和定量方面与最先进的方法相当。




]]></description>
      <guid>http://link.springer.com/10.1007/s41095-023-0375-z</guid>
      <pubDate>Wed, 28 Aug 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>动态环境下基于空间结构比较的RGB-D SLAM</title>
      <link>http://link.springer.com/10.1007/s11042-024-20128-8</link>
      <description><![CDATA[摘要
基于RGB-D的同步定位与地图构建（RGB-D SLAM）是机器人领域的研究热点，如何消除动态物体对RGB-D SLAM的影响仍是一大难题。针对这一问题，本文提出了一种基于空间结构比较的改进型RGB-D SLAM。在提出的RGB-D SLAM定位阶段，基于深度信息将特征点匹配对分为两组，即深度有效和深度未对齐。通过比较两个三维点集的空间结构矩阵，识别出静态特征点匹配对。此外，利用获得的姿态去除深度未对齐的动态特征点匹配对，以提高后端优化的性能。在地图处理阶段，采用了一种结合距离检测和处理坐标误差的新型地图点方法，并提出了一种重复点去除策略来消除参考帧中的冗余地图点。最后，在公开的动态TUM RGB-D数据集上对提出的RGB-D SLAM进行了测试。实验结果表明，与动态环境下最先进的RGB-D SLAM方法相比，提出的RGB-D SLAM具有更优的性能。]]></description>
      <guid>http://link.springer.com/10.1007/s11042-024-20128-8</guid>
      <pubDate>Wed, 28 Aug 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>智能航空航天感知的计算机视觉任务：概述</title>
      <link>http://link.springer.com/10.1007/s11431-024-2714-4</link>
      <description><![CDATA[摘要
计算机视觉任务对于航空航天任务至关重要，因为它们可以帮助航天器理解和解释太空环境，例如估计位置和方向、重建 3D 模型和识别物体，这些任务已经得到广泛研究，可以成功执行任务。然而，卡尔曼滤波、运动结构和多视角立体成像等传统方法不够稳健，无法处理恶劣条件，导致结果不可靠。近年来，基于深度学习 (DL) 的感知技术显示出巨大的潜力，并且优于传统方法，特别是在对不断变化的环境的稳健性方面。为了进一步推进基于 DL 的航空航天感知，已经提出了各种框架、数据集和策略，表明未来应用具有巨大的潜力。在本次调查中，我们旨在探索感知任务中使用的有前景的技术，并强调基于 DL 的航空航天感知的重要性。我们首先概述航天感知，包括近年来开发的经典航天计划、常用的传感器和传统的感知方法。随后，我们深入研究航天任务中的三个基本感知任务：姿态估计、三维重建和识别，因为它们对于后续的决策和控制至关重要。最后，我们讨论了当前研究的局限性和可能性，并对未来的发展进行了展望，包括使用有限数据集的挑战、改进算法的必要性以及多源信息融合的潜在好处。]]></description>
      <guid>http://link.springer.com/10.1007/s11431-024-2714-4</guid>
      <pubDate>Wed, 21 Aug 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>森林应用中处理地面点云的软件解决方案综述</title>
      <link>http://link.springer.com/10.1007/s40725-024-00228-2</link>
      <description><![CDATA[摘要

综述目的
近年来，人们对 3D 点云在林业和森林生态学中的应用兴趣大增。随着激光扫描等新型 3D 捕获技术的发展，越来越多的算法被并行开发出来，用于将 3D 点云数据处理成林业应用的更切实的结果。从这些可用的算法中，用户很难决定应用哪种算法来最好地实现他们的目标。在这里，我们全面概述了点云采集和处理工具及其精准林业的输出。然后，我们提供了一个包含 24 种算法的综合数据库，用于处理使用近距离技术（特别是地面平台）获得的森林点云。


最新发现
在确定的 24 种解决方案中，有 20 种是开源的，两种是免费软件，其余两种是商业产品。作为 COST Action 3DForEcoTech 的一部分，可以在基于 Web 的平台上访问编译的解决方案数据库以及相应的安装和一般使用技术指南。该数据库可以作为社区的单一信息来源，帮助社区选择适合其需求的特定软件/算法。


摘要
我们得出结论，用于处理点云的各种算法的开发提供了强大的工具，可以对未来的森林资源清查产生重大影响，尽管我们注意到创建标准化范例的必要性。
]]></description>
      <guid>http://link.springer.com/10.1007/s40725-024-00228-2</guid>
      <pubDate>Sat, 17 Aug 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于局部图像风格化的音频引导隐式神经表征</title>
      <link>http://link.springer.com/10.1007/s41095-024-0413-5</link>
      <description><![CDATA[摘要
我们提出了一种音频引导局部图像风格化的新框架。声音通常提供有关场景特定背景的信息，并且与场景或对象的某个部分密切相关。然而，现有的图像风格化工作专注于使用图像或文本输入对整个图像进行风格化。基于音频输入对图像的特定部分进行风格化是自然但具有挑战性的。这项工作提出了一个框架，其中用户提供音频输入以在输入图像中定位目标，并提供另一个音频输入以局部风格化目标对象或场景。我们首先使用利用 CLIP 嵌入空间的视听定位网络生成精细定位图。然后，我们利用隐式神经表征 (INR) 以及预测的定位图根据声音信息对目标进行风格化。INR 操纵局部像素值以与提供的音频输入在语义上一致。我们的实验表明，所提出的框架优于其他音频引导风格化方法。此外，我们观察到我们的方法构建了简洁的定位图，并根据给定的音频输入自然地操纵目标对象或场景。




]]></description>
      <guid>http://link.springer.com/10.1007/s41095-024-0413-5</guid>
      <pubDate>Wed, 14 Aug 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FMGS：基础模型嵌入 3D 高斯分层，用于整体 3D 场景理解</title>
      <link>http://link.springer.com/10.1007/s11263-024-02183-8</link>
      <description><![CDATA[摘要
准确感知现实世界 3D 对象的几何和语义属性对于增强现实和机器人应用的持续发展至关重要。为此，我们提出了基础模型嵌入高斯分层 (FMGS)，它将基础模型的视觉语言嵌入合并到 3D 高斯分层 (GS) 中。这项工作的主要贡献是一种重建和表示 3D 视觉语言模型的有效方法。这是通过将基于图像的基础模型生成的特征图提炼为从我们的 3D 模型渲染的特征图来实现的。为了确保高质量的渲染和快速的训练，我们通过整合 GS 和多分辨率哈希编码 (MHE) 的优势引入了一种新颖的场景表示。我们有效的训练过程还引入了像素对齐损失，使得相同语义实体的渲染特征距离接近，遵循像素级语义边界。我们的结果表明，多视图语义一致性非常出色，有助于完成各种下游任务，以 
\({10.2}\)
 的速度超越了最先进的方法，尽管我们的推理速度 
\({851\times }\)
 更快。​​这项研究探索了视觉、语言和 3D 场景表示的交集，为在不受控制的现实环境中增强场景理解铺平了道路。我们计划在[项目页面]上发布代码。]]></description>
      <guid>http://link.springer.com/10.1007/s11263-024-02183-8</guid>
      <pubDate>Mon, 12 Aug 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于对抗模仿学习的网络进行类别级 6D 物体姿态估计</title>
      <link>http://link.springer.com/10.1007/s00138-024-01592-6</link>
      <description><![CDATA[摘要
类别级6D物体姿态估计是计算机视觉领域中一项非常基础和关键的研究。为了摆脱对物体3D模型的依赖，分析综合型物体姿态估计方法近年来得到了广泛的研究。虽然这些方法在泛化方面有一定的提升，但类别级物体姿态估计的准确性仍有待提高。本文提出了一种基于对抗模仿学习的类别级6D物体姿态估计网络AIL-Net。AIL-Net采用状态-动作分布匹配准则，能够执行数据集中未出现过的专家动作，防止物体姿态估计陷入不良状态。我们进一步设计了一个通过生成对抗模仿学习估计物体姿态的框架，该方法能够区分AIL-Net中的专家策略和模仿策略。实验结果表明，我们的方法在REAL275数据集和Cars数据集上取得了具有竞争力的类别级物体姿态估计性能。]]></description>
      <guid>http://link.springer.com/10.1007/s00138-024-01592-6</guid>
      <pubDate>Mon, 12 Aug 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>EPSM 2023，医学工程与物理科学</title>
      <link>http://link.springer.com/10.1007/s13246-024-01460-7</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/s13246-024-01460-7</guid>
      <pubDate>Mon, 12 Aug 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一种多任务有效性度量和自适应协同训练方法，用于通过少量样本提高学习效果</title>
      <link>http://link.springer.com/10.1007/s10845-024-02475-3</link>
      <description><![CDATA[摘要
将深度学习 (DL) 集成到视觉检测方法中，越来越多地被认为是一种可大幅提高适应性和鲁棒性的宝贵方法。然而，众所周知，高性能神经网络通常需要具有高质量手动注释的大量训练数据集，而这在许多制造过程中很难获得。为了提高 DL 方法在样本较少的情况下执行视觉任务的性能，本文提出了一种称为辅助任务有效性 (EAT) 的新指标，并提出了一种多任务学习方法，利用该指标来选择有效的辅助任务分支并自适应地将它们与主任务一起训练。在两个少量样本的视觉任务上进行的实验表明，所提方法有效地消除了无效的任务分支，并增强了所选任务对主任务的贡献：在姿势关键点检测中将平均归一化像素误差从 0.0613 降低到 0.0143，在表面缺陷分割中将交并比 (IoU) 从 0.6383 提升到 0.6921。值得注意的是，这些增强是在不需要额外的手动标记工作的情况下实现的。]]></description>
      <guid>http://link.springer.com/10.1007/s10845-024-02475-3</guid>
      <pubDate>Mon, 12 Aug 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于神经辐射场的支气管镜导航方法</title>
      <link>http://link.springer.com/10.1007/s11548-024-03243-7</link>
      <description><![CDATA[摘要

目的
我们介绍了一种新颖的支气管镜导航方法，该方法利用神经辐射场 (NeRF) 仅从支气管镜图像中被动定位内窥镜。该方法旨在克服当前依赖外部基础设施或需要主动调整支气管镜的支气管镜导航工具的局限性和挑战。


方法
为了应对挑战，我们利用 NeRF 进行支气管镜导航，从而实现从支气管镜图像中进行被动内窥镜定位。我们开发了一个两阶段流程：使用术前数据进行离线训练和手术期间进行在线被动姿势估计。为了提高性能，我们采用 Anderson 加速并结合语义外观迁移来处理训练和推理阶段之间的模拟到现实差距。


结果
我们通过对虚拟支气管镜图像和物理模型进行测试来评估我们方法的可行性，以对抗基于 SLAM 的方法。我们的虚拟数据集中的平均旋转误差约为 3.18
\(^\circ \)
，平移误差约为 4.95 毫米。在物理幻影测试中，平均旋转和平移误差约为 5.14
\(^\circ \)
 和 13.12 毫米。


结论
我们基于 NeRF 的支气管镜导航方法消除了对外部基础设施和主动调整的依赖，为支气管镜导航带来了有希望的进步。在模拟和真实世界幻影模型上的实验验证证明了其在应对低纹理和具有挑战性的照明条件等挑战方面的有效性。
]]></description>
      <guid>http://link.springer.com/10.1007/s11548-024-03243-7</guid>
      <pubDate>Wed, 07 Aug 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DialogueNeRF：实现逼真的虚拟面对面对话视频生成</title>
      <link>http://link.springer.com/10.1007/s44267-024-00057-8</link>
      <description><![CDATA[摘要
对话是虚拟角色在元宇宙中活动的重要组成部分。随着自然语言处理的发展，文本和语音对话生成取得了重大突破。然而，面对面的对话占日常对话的绝大部分，而现有的大多数方法都集中在单人说话头像的生成上。在这项工作中，我们更进一步，考虑生成逼真的面对面对话视频。对话生成比单人说话头像生成更具挑战性，因为它不仅需要生成照片般逼真的单个说话头像，还需要听众对说话者的回应。在本文中，我们提出了一种基于神经辐射场 (NeRF) 的新型统一框架来应对这些挑战。具体而言，我们在不同条件下使用 NeRF 框架对说话者和听众进行建模，以控制个人表情。说话者由音频信号驱动，而听众的响应取决于视觉和听觉信息。通过这种方式，人类虚拟形象之间可以生成面对面的对话视频，所有对话者都在同一网络中建模。此外，为了方便今后对该任务的研究，我们还收集了一个包含 34 个视频片段的新人类对话数据集。定量和定性实验从不同方面评估了我们的方法，例如图像质量、姿势序列趋势和生成的视频中场景的自然渲染。实验结果表明，生成的视频中的虚拟形象能够进行逼真的对话，并保持个人风格。]]></description>
      <guid>http://link.springer.com/10.1007/s44267-024-00057-8</guid>
      <pubDate>Wed, 07 Aug 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PIDSNeRF：姿势插值深度监督神经辐射场，用于从具有挑战性的输入中进行视图合成</title>
      <link>http://link.springer.com/10.1007/s11042-024-19978-z</link>
      <description><![CDATA[摘要
最近，神经辐射场（NeRF）在通过多视图进行新视图合成的任务中表现出色。本研究引入了一种先进的优化框架，称为姿势插值深度监督神经辐射场（PIDSNeRF），旨在解决 NeRF 在新视图合成中遇到的挑战。这些挑战表现为伪影、纹理细节丢失和几何不一致，特别是在以不同的光照条件、无纹理区域和稀疏图像为输入的场景中。PIDSNeRF 的原理涉及基于已知相机在 3D 空间域中插值虚拟相机位置，随后，将相机姿势估计期间形成的稀疏点云重新投影到这些虚拟姿势上，采用反角度加权策略，从而生成深度监督射线。此外，我们提出了一种深度扩散方法，将深度监督的射线沿像素平面扩散，根据扩散距离形成具有不同方差的高斯函数。最后，通过优化每条射线沿线采样点的权重分布与相应高斯函数沿该射线的概率密度函数之间的 Kullback-Leibler（KL）散度来实现体绘制时的深度监督。上述过程可以更全面地优化与射线沿线采样点对齐的多分辨率网格特征。在DTU、LLFF和DL3DV-10K数据集上的实验表明，PIDSNeRF可以在几分钟内合成完整的新颖视图，并且合成图像的各项指标都达到了最优性能。]]></description>
      <guid>http://link.springer.com/10.1007/s11042-024-19978-z</guid>
      <pubDate>Tue, 06 Aug 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于野外细粒度单目 3D 人脸重建的自监督学习</title>
      <link>http://link.springer.com/10.1007/s00530-024-01436-3</link>
      <description><![CDATA[摘要
由于传统 3DMM（3D 可变形模型）的局限性以及缺乏高保真 3D 面部扫描数据，从单目图像重建 3D 人脸是一项具有挑战性的计算机视觉任务。为了解决这个问题，我们提出了一种新颖的从粗到细的自监督学习框架，用于从野外单目图像重建细粒度 3D 人脸。在粗阶段，从单个图像中提取的人脸参数用于通过 3DMM 重建粗 3D 人脸。在细化阶段，我们设计了一个小波变换感知模型，从输入图像中提取不同频域中的面部细节。此外，我们提出了一个基于小波变换感知模型的深度位移模块，从输入图像和渲染的粗面部的展开 UV 纹理生成细化的位移图，可用于合成详细的 3D 人脸几何形状。此外，我们提出了一种基于小波变换感知模型的新型反照率图模块，以捕获高频纹理信息并生成与面部照明一致的详细反照率图。详细的面部几何形状和反照率图用于重建细粒度的 3D 面部，而无需任何标记数据。我们已经进行了大量实验，证明了我们的方法优于现有的最先进的 3D 面部重建方法，这些方法在四个公共数据集（包括 CelebA、LS3D、LFW 和 NoW 基准）上进行。实验结果表明，我们的方法实现了更高的准确性和鲁棒性，特别是在遮挡、大姿势和变化的照明等具有挑战性的条件下。]]></description>
      <guid>http://link.springer.com/10.1007/s00530-024-01436-3</guid>
      <pubDate>Mon, 05 Aug 2024 00:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>