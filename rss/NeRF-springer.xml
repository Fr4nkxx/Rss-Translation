<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最新结果</title>
    <link>http://link.springer.com</link>
    <description>Springer 提供的最新内容</description>
    <lastBuildDate>Wed, 03 Jan 2024 18:14:04 GMT</lastBuildDate>
    <item>
      <title>基于实时距离场加速的大型运动场自由视点视频合成</title>
      <link>http://link.springer.com/10.1007/s41095-022-0323-3</link>
      <description><![CDATA[摘要
自由视点视频允许用户从任何虚拟角度观看物体，创造身临其境的视觉体验。该技术增强了多媒体表演的交互性和自由度。然而，许多自由视点视频合成方法很难满足实时、高精度的要求，特别是对于面积较大、运动物体较多的运动场。为了解决这些问题，我们提出了一种基于距离场加速的自由视点视频合成方法。其中心思想是融合多视点距离场信息并利用其自适应调整搜索步长。自适应步长搜索有两种用途：用于多目标三维表面的快速估计，以及基于全局遮挡判断的合成视图渲染。我们使用并行计算进行交互式显示、使用 CUDA 和 OpenGL 框架来实现我们的想法，并使用真实世界和模拟实验数据集进行评估。结果表明，所提出的方法可以在大型运动场上以 25 fps 渲染具有多个对象的自由视点视频。此外，我们合成的新颖视点图像的视觉质量超过了最先进的基于神经渲染的方法。
      



]]></description>
      <guid>http://link.springer.com/10.1007/s41095-022-0323-3</guid>
      <pubDate>Mon, 01 Apr 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于可微渲染的动态海洋反建模</title>
      <link>http://link.springer.com/10.1007/s41095-023-0338-4</link>
      <description><![CDATA[摘要
学习和推断捕获的 2D 场景的潜在运动模式，然后重新创建与现实世界自然现象一致的动态演化，这对图形和动画具有很高的吸引力。为了弥合虚拟和现实环境之间的技术差距，我们专注于视觉一致和属性可验证的海洋的逆向建模和重建，利用深度学习和可微物理来学习几何并以自我监督的方式构成波浪。首先，我们使用两个网络推断分层几何，这两个网络通过可微渲染器进行了优化。我们通过配备可微分海洋模型的网络从推断的几何序列中提取波浪分量。然后，可以使用重建的波浪分量来演化海洋动力学。通过大量的实验，我们验证了我们的新方法在几何重建和波浪估计方面都能产生令人满意的结果。此外，新框架具有逆向建模潜力，可促进大量图形应用，例如快速生成物理精确的场景动画以及由真实海洋场景引导的编辑。
      



]]></description>
      <guid>http://link.springer.com/10.1007/s41095-023-0338-4</guid>
      <pubDate>Mon, 01 Apr 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MusicFace：音乐驱动的富有表现力的歌脸合成</title>
      <link>http://link.springer.com/10.1007/s41095-023-0343-7</link>
      <description><![CDATA[摘要
合成由音乐驱动的生动逼真的歌声仍然是一个有趣且具有挑战性的问题。在本文中，我们提出了一种完成此任务的方法，其中包括嘴唇、面部表情、头部姿势和眼睛的自然运动。由于常见音乐音频信号中人声和背景音乐的混合信息耦合，我们设计了一种解耦和融合策略来应对这一挑战。我们首先将输入的音乐音频分解为人声流和背景音乐流。由于两个流输入信号与面部表情、头部运动和眼睛状态的动态之间存在隐式且复杂的相关性，我们用注意力方案对它们的关系进行建模，其中两个流的效果无缝融合。此外，为了提高生成结果的表现力，我们将头部运动生成分解为速度和方向，并将眼睛状态生成分解为短期眨眼和长期闭眼，并分别建模。我们还构建了一个新颖的数据集 SingingFace，以支持该任务的模型训练和评估，包括该主题的未来工作。大量的实验和用户研究表明，我们提出的方法能够合成生动的歌声面孔，在质量和数量上都优于现有技术。
      



]]></description>
      <guid>http://link.springer.com/10.1007/s41095-023-0343-7</guid>
      <pubDate>Thu, 01 Feb 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过深度学习和组织学肿瘤映射到手术部位对基底细胞癌进行术中切缘评估</title>
      <link>http://link.springer.com/10.1038/s41698-023-00477-7</link>
      <description><![CDATA[摘要
实体癌的成功治疗依赖于肿瘤的完全手术切除，无论是用于明确治疗还是在辅助治疗之前。术中和术后径向切片是最常见的切缘评估形式，可能导致切除不完全并增加复发和重复手术的风险。莫氏显微手术通过对 100% 的外周和深层边缘进行实时边缘评估来完全切除基底细胞癌和鳞状细胞癌。许多肿瘤类型的实时评估受到组织大小、复杂性和全身麻醉期间标本处理/评估时间的限制。我们开发了一个人工智能平台，通过自动粗略建议、肿瘤到手术标本的映射和定位来减少组织预处理和组织学评估时间。使用基底细胞癌作为模型系统，结果表明该方法可以解决手术实验室效率瓶颈，实现快速、完整的术中切缘评估。]]></description>
      <guid>http://link.springer.com/10.1038/s41698-023-00477-7</guid>
      <pubDate>Wed, 03 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>非视距成像技术研究进展</title>
      <link>http://link.springer.com/10.1007/s12204-023-2686-8</link>
      <description><![CDATA[摘要
非视距成像通过分析携带隐藏场景信息的中继表面上的漫反射光来恢复拐角处的隐藏物体。非视距成像由于其在自动驾驶、国防、医学成像、灾后救援等领域的巨大应用潜力，特别是近年来引起了国内外研究人员的广泛关注。非视距成像的研究主要集中在成像系统、正演模型和重建算法上。本文系统总结了现有主动和被动场景下的非视距成像技术，分析了非视距成像技术面临的挑战和未来方向。]]></description>
      <guid>http://link.springer.com/10.1007/s12204-023-2686-8</guid>
      <pubDate>Tue, 02 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多重多色抗病毒检测适合高通量研究</title>
      <link>http://link.springer.com/10.1038/s41467-023-44339-z</link>
      <description><![CDATA[摘要
为了遏制病毒流行和大流行，需要具有针对整个病毒属或病毒科的抗病毒药物。在这里，我们开发了一种基于细胞的多重抗病毒测定，可同时对多种病毒进行高通量筛选，正如使用三种远缘相关的正黄病毒（登革热、日本脑炎和黄热病病毒）所证明的那样。每种病毒都标有独特的荧光蛋白，从而能够通过高内涵成像对细胞培养物进行单独监测。采用特定的抗血清和小分子抑制剂来验证多重方法产生与单病毒感染测定相当的抑制谱。为了便于下游分析，开发了一个内核来对多维定量数据进行解卷积并将其减少到三个笛卡尔坐标。该方法适用于不同科的病毒，例如基孔肯雅病毒、副流感病毒和布尼亚姆维拉病毒的共同感染。正如对大约 1200 种药物样小分子的初步筛选所示，多重方法有望促进更广谱抗病毒药物的发现。]]></description>
      <guid>http://link.springer.com/10.1038/s41467-023-44339-z</guid>
      <pubDate>Tue, 02 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过多平面 NeRF 进行点云渲染</title>
      <link>http://link.springer.com/10.1007/978-3-031-50072-5_16</link>
      <description><![CDATA[摘要
我们结合点云多平面投影和 NeRF 提出了一种新的神经点云渲染方法[https://github.com/Mayxmu/PCMP-NeRF .]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-50072-5_16</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>神经网络作为摄影测量的替代方案。使用即时 NeRF 和体积渲染</title>
      <link>http://link.springer.com/10.1007/978-3-031-36155-5_30</link>
      <description><![CDATA[摘要
人工智能 (AI) 和机器学习算法的使用最近给许多领域带来了革命性的变化，包括通过数字孪生增强遗产及其在 VR 维度中的使用。本文讨论了各种人工智能技术（生成对抗网络（GAN））在开发交互式和沉浸式 VR 严肃游戏以增强遗产方面的潜在用途。 GAN 可用于生成空间、物体和人脸的真实且完全人工的图像，可应用于构建交互式环境的设计和概念过程。此外，机器学习算法可以提高游戏对用户的适应性，例如根据用户的动作自动构建游戏的难度级别或情节。然而，近年来，新的、更高性能的人工智能过程已经出现，它们已经超越了 GAN 算法的生成能力，后者主要用于生成不存在元素的 2D 图像。 NeRF（神经辐射场）是 NVIDIA 开发的一项技术，由最近由名为 Instant NeRF 的专有代码优化的复杂神经网络提供支持。与不直接支持人工智能的替代照片建模技术相比，该技术能够使用更少的摄影数据快速准确地生成物理环境的详细 3D 模型。未来，该技术可用于生成与现实无区别的虚拟环境，也可积极用于遗产保护。]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-36155-5_30</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>T(G)V-NeRF：规则化神经辐射场的强大基线，训练视图很少</title>
      <link>http://link.springer.com/10.1007/978-3-031-47765-2_12</link>
      <description><![CDATA[摘要
诸如神经辐射场 (NeRF) 之类的隐式表示已成为新颖视图领域的事实上标准3D 场景合成。然而，他们令人惊叹的结果通常意味着使用数十张训练图像，并将相应的摄像机很好地定位在场景中。本文研究了一种新的、基于全变分的正则化方法，用于在很少（少于 10 个）训练图像的情况下训练 NeRF。它利用 NeRF 反向传播算法来评估推断深度图上的一阶和二阶导数项，以增强场景底层表面的平滑度。通过标准真实图像基准上最先进的性能，我们表明所提出的方法（称为 TV-NeRF 和 TGV-NeRF）在很少的训练视图的新颖视图合成中建立了强有力的基线。]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-47765-2_12</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CaSE-NeRF：神经辐射场的相机设置编辑</title>
      <link>http://link.springer.com/10.1007/978-3-031-50072-5_8</link>
      <description><![CDATA[摘要
神经辐射场 (NeRF) 通过从多视图图像合成新颖的视图，在三维 (3D) 重建方面表现出了卓越的质量。然而，之前基于 NeRF 的方法不允许用户在场景中执行用户控制的摄像机设置编辑。虽然现有的工作提出了修改辐射场的方法，但这些修改仅限于训练集中的相机设置。因此，我们提出了神经辐射场的相机设置编辑（CaSE-NeRF），以从具有不同相机设置的一组视图中恢复辐射场。在我们的方法中，我们允许用户在场景上执行受控的相机设置编辑，并合成编辑场景的新颖视图图像，而无需重新训练网络。我们方法的关键在于分别对每个相机参数进行建模，并根据薄镜头成像原理渲染各种 3D 散焦效果。通过遵循真实相机的图像处理，我们对其进行隐式建模并学习在潜在空间中连续且独立于图像的增益。色温和曝光的控制是即插即用的，并且可以轻松集成到基于 NeRF 的框架中。因此，我们的方法允许对 3D 场景的视点和相机设置进行手动和自由的捕获后控制。通过对两个真实场景数据集的广泛实验，我们证明了我们的方法在重建具有一致的 3D 几何和外观的正常 NeRF 方面的成功。我们的相关代码和数据可以在 https://github.com/CPREgroup/CaSE-NeRF 获取.]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-50072-5_8</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计算机图形学的进展</title>
      <link>http://link.springer.com/10.1007/978-3-031-50072-5</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-50072-5</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>模式识别和计算机视觉</title>
      <link>http://link.springer.com/10.1007/978-981-99-8432-9</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8432-9</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计算智能的进展</title>
      <link>http://link.springer.com/10.1007/978-3-031-47765-2</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-47765-2</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ACFNeRF：通过基于点云的距离场加速且无缓存的神经渲染</title>
      <link>http://link.springer.com/10.1007/978-981-99-8432-9_27</link>
      <description><![CDATA[摘要
神经辐射场为逼真的场景渲染和新颖的视图合成提供了非凡的途径。然而，诸如训练时间缓慢、推理持续时间过长以及处理大规模场景的限制等挑战仍然存在。为了解决 NeRF 推理速度慢的瓶颈，我们提出 ACFNeRF 利用点云训练距离场，改进 NeRF 的采样策略，大幅提升其推理速度。我们的方法实现了每秒 150 帧的令人印象深刻的推理速率，从而实现了房间规模场景中的实时渲染。全面的实验验证了我们方法的优越性，在无缓存条件下，其速度比现有 NeRF 加速技术显着提高 10-20 倍。
      ]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8432-9_27</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>超越数字表示</title>
      <link>http://link.springer.com/10.1007/978-3-031-36155-5</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-36155-5</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>