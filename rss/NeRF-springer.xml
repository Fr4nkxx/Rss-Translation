<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最新结果</title>
    <link>http://link.springer.com</link>
    <description>Springer 提供的最新内容</description>
    <lastBuildDate>Thu, 19 Sep 2024 09:14:26 GMT</lastBuildDate>
    <item>
      <title>机器视觉中的合成数据增强方法综述</title>
      <link>http://link.springer.com/10.1007/s11633-022-1411-7</link>
      <description><![CDATA[摘要
解决计算机视觉问题的标准方法是使用代表目标任务的大规模图像数据集来训练深度卷积神经网络 (CNN) 模型。然而，在许多情况下，获取足够的图像数据来完成目标任务往往是一项挑战。数据增强是一种缓解这一挑战的方法。一种常见的做法是明确地以所需的方式转换现有图像，以创建实现良好泛化性能所需的训练数据量和可变性。在无法获取目标域数据的情况下，一种可行的解决方法是从头开始合成训练数据，即合成数据增强。本文对合成数据增强技术进行了广泛的回顾。它涵盖了基于逼真的 3D 图形建模、神经风格迁移 (NST)、差分神经渲染和使用生成对抗网络 (GAN) 和变分自动编码器 (VAE) 的生成建模的数据合成方法。对于每类方法，我们都将重点介绍重要的数据生成和增强技术、一般应用范围和特定用例，以及现有的限制和可能的解决方法。此外，我们还总结了用于训练计算机视觉模型的常见合成数据集，重点介绍了主要功能、应用领域和支持的任务。最后，我们讨论了合成数据增强方法的有效性。由于这是第一篇详细探讨合成数据增强方法的论文，我们希望为读者提供必要的背景信息和对现有方法及其相关问题的深入了解。]]></description>
      <guid>http://link.springer.com/10.1007/s11633-022-1411-7</guid>
      <pubDate>Tue, 01 Oct 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>UGINR：通过隐式神经表征实现大规模非结构化网格缩减</title>
      <link>http://link.springer.com/10.1007/s12650-024-01003-y</link>
      <description><![CDATA[
摘要
最近，隐式神经表征 (INR) 在处理 3D 体积数据方面表现出了显著的能力，尤其是在数据压缩方面。然而，大多数研究主要集中在结构化网格上，而结构化网格在科学领域，尤其是物理学领域并不常见。为了解决这一限制，我们提出了一种通过隐式神经表征 (UGINR) 进行非结构化网格缩减的方法。UGINR 采用分而治之的方法；具体来说，我们根据值将大规模数据分割成块。随后，我们对每个部分使用 INR 网络来学习其独特的特征。最后，我们整合这些单独的网络以实现压缩目标。为了确保与既定的研究方法兼容，我们仅对非结构化网格中每个单元的顶点进行采样。通过权重量化，我们的模型可以实现高压缩率。为了说明所提方法的有效性，我们在各种数据集上进行了实验，证明了我们的方法在科学可视化和大规模数据压缩方面的稳健性。


图形摘要





]]></description>
      <guid>http://link.springer.com/10.1007/s12650-024-01003-y</guid>
      <pubDate>Tue, 01 Oct 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>文本驱动的着装人体图像合成，结合 3D 人体模型估计，帮助购物</title>
      <link>http://link.springer.com/10.1007/s11042-024-20187-x</link>
      <description><![CDATA[摘要
网上购物已成为现代消费文化不可或缺的一部分。然而，它面临着基于文本描述可视化服装项目和估计其适合个人体型的挑战。在这项工作中，我们提出了一种创新的解决方案来解决这些挑战，即通过文本驱动的着装人体图像合成和 3D 人体模型估计，利用矢量量化变分自动编码器 (VQ-VAE) 的强大功能。在视觉和图形领域，创建多样化和高质量的人体图像是一项至关重要但又困难的任务。由于服装设计和纹理种类繁多，现有的生成模型通常不足以满足最终用户的需求。在这项提议的工作中，我们引入了一种解决方案，该解决方案由通过多个模型的各种数据集提供，因此可以提供优化的解决方案以及具有各种姿势的高质量图像。我们使用两种不同的程序从预定的人体姿势开始创建全身 2D 人体照片。 1) 首先将提供的人体姿势转换为人体解析图，其中包含一些描述衣服形状的句子。 2) 然后向开发的模型提供有关衣服纹理的更多信息作为输入，以生成最终的人体图像。 该模型分为两个不同的部分，第一个是粗略级别的代码本，用于处理总体结果，另一个是精细级别的代码本，用于处理细节。 如前所述，精细级别的代码本专注于纹理的细节，而粗略级别的代码本涵盖了结构中纹理的描述。 与分层代码本一起训练的解码器将各个级别的预期索引转换为人体图像。 由于使用了专家组合，因此创建的图像可以依赖于细粒度的文本输入。 通过预测更精细级别的指标来改善服装纹理的质量。 根据大量定量和定性评估，实施这些策略可以产生比最先进的程序更多样化和高质量的人体图像。这些生成的照片将转换为 3D 模型，从而产生多种姿势和结果，或者您可能只是从产生各种姿势的数据集中制作 3D 模型。PIFu 方法的应用分别使用 Marching cube 算法和 Stacked Hourglass 方法生成 3D 模型和逼真的图像。这导致基于文本描述生成高分辨率图像并将生成的图像重建为 3D 模型。实现的初始分数和 Fréchet 截距、SSIM 和 PSNR 分别为 1.64 ± 0.20 和 24.64527782349843、0.642919520 和 32.87157744102002。与其他技术相比，实施的方法得分很高。这项技术对于重塑电子商务格局具有巨大的希望，可提供更具沉浸感和信息量的探索服装选择的方式。]]></description>
      <guid>http://link.springer.com/10.1007/s11042-024-20187-x</guid>
      <pubDate>Thu, 19 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于多视角 2D 环绕图像的自监督 3D 语义占用预测</title>
      <link>http://link.springer.com/10.1007/s41064-024-00308-9</link>
      <description><![CDATA[摘要
对环境几何和语义的精确 3D 表示为各种下游任务奠定了基础，对于路径规划和避障等自动驾驶相关任务至关重要。这项工作的重点是 3D 语义占用预测，即将场景重建为体素网格，其中每个体素都分配有一个占用和语义标签。我们提出了一种基于卷积神经网络的方法，该方法利用来自重叠最小的环视设置的多张彩色图像，以及相关的内部和外部摄像头参数作为输入，将观察到的环境重建为 3D 语义占用图。为了解释从单目 2D 图像重建 3D 表示的不适定性，图像信息会随时间进行整合：假设摄像机设置正在移动，来自连续时间步骤的图像用于形成多视图立体设置。在详尽的实验中，我们研究了动态物体所带来的挑战以及使用 3D 或 2D 参考数据训练所提出方法的可能性。后者受到生成和注释 3D 地面真实数据相对较高的成本的激励。此外，我们提出并研究了一种新颖的自监督训练方案，该方案不需要任何几何参考数据，而仅依赖于稀疏语义地面真实值。对 Occ3D 数据集的评估，包括与文献中当前最先进的自监督方法的比较，证明了我们的自监督变体的潜力。]]></description>
      <guid>http://link.springer.com/10.1007/s41064-024-00308-9</guid>
      <pubDate>Wed, 18 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>NeRF-OR：从稀疏视图 RGB-D 视频中重建手术室场景的神经辐射场</title>
      <link>http://link.springer.com/10.1007/s11548-024-03261-5</link>
      <description><![CDATA[摘要

目的
手术室 (OR) 中的 RGB-D 摄像机提供复杂手术场景的同步视图。将这些多视图数据同化为统一表示可以实现下游任务，例如对象检测和跟踪、姿势估计和动作识别。神经辐射场 (NeRF) 可以在有限的内存占用下提供复杂场景的连续表示。然而，现有的 NeRF 方法在现实世界的 OR 设置中表现不佳，因为其中一小部分摄像机从完全不同的有利位置捕捉房间。在本研究中，我们提出了 NeRF-OR，一种用于在手术室中对动态手术场景进行 3D 重建的方法。


方法
其他稀疏视图数据集方法使用飞行时间传感器深度或从彩色图像估计的密集深度，而 NeRF-OR 则使用两者的组合。深度估计可以减轻由于反射材料和物体边界而导致传感器深度图像中出现的缺失值。我们建议使用根据估计深度计算的表面法线进行监督，因为这些法线在很大程度上是尺度不变的。


结果
我们将 NeRF-OR 拟合到 4D-OR 数据集中的静态手术场景，并表明其表示在几何上是准确的，而最先进的方法会崩溃为次优解决方案。与早期的工作相比，NeRF-OR 在训练 30
\(\times \)
 时可以掌握精细的场景细节。此外，NeRF-OR 可以捕获整个手术视频，同时合成中间时间值的视图，平均 PSNR 为 24.86 dB。最后，通过对包含少至三个训练视图的 NVS-RGBD 数据集进行基准测试，我们发现我们的方法在手术室以外的稀疏视图设置中具有优势。NeRF-OR 合成的图像的 PSNR 为 26.72 dB，比最先进的方法提高了 1.7%。


结论
我们的结果表明，NeRF-OR 允许使用少量具有完全不同有利位置的摄像机拍摄的视频进行新颖的视图合成，这是手术室中的典型摄像机设置。代码可通过以下方式获取：github.com/Beerend/NeRF-OR。
]]></description>
      <guid>http://link.springer.com/10.1007/s11548-024-03261-5</guid>
      <pubDate>Fri, 13 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Ambient-NeRF：利用环境照明在低光照条件下增强神经辐射场的光列</title>
      <link>http://link.springer.com/10.1007/s11042-024-19699-3</link>
      <description><![CDATA[摘要
NeRF可以渲染照片级逼真的3D场景，广泛应用于虚拟现实、自动驾驶、游戏开发等领域，并迅速成为3D重建领域最热门的技术之一。NeRF通过从相机的空间坐标和视点发射光线，穿过场景并计算从视点看到的视野，从而生成逼真的3D场景。然而当原始输入图像亮度较低时，很难恢复场景。受计算机图形学Phong模型中环境光照的启发，假设最终渲染出的图像是场景颜色和环境光照的乘积。本文采用多层感知器 (MLP) 网络训练环境光照张量 
\(\textbf{I}\)
，并将其乘以 NeRF 预测的颜色，以渲染具有正常光照的图像。此外，我们使用 tiny-cuda-nn 作为骨干网络，简化了所提出的网络结构并大大提高了训练速度。此外，引入了新的损失函数，以在低光照条件下实现更好的图像质量。实验结果表明，与其他最先进的方法相比，所提出的方法在增强低光场景图像方面非常有效，在 LOM 数据集上的整体平均值为 PSNR：20.53、SSIM：0.785 和 LPIPS：0.258。]]></description>
      <guid>http://link.springer.com/10.1007/s11042-024-19699-3</guid>
      <pubDate>Tue, 10 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过神经元修剪实现隐式神经表征隐写术</title>
      <link>http://link.springer.com/10.1007/s00530-024-01476-9</link>
      <description><![CDATA[摘要
近年来，隐式神经表征（INR）开始应用于图像隐写术。然而，用INR表示的隐写和秘密图像的质量普遍较低。在本文中，我们提出了一种通过神经元修剪的隐式神经表示隐写术方法。首先，我们随机停用一部分神经元来训练一个INR函数来隐式表示秘密图像。随后，我们修剪那些被认为对以非结构化方式表示秘密图像不重要的神经元以获得秘密函数，同时将神经元的位置标记为密钥。最后，基于部分优化策略，我们重新激活修剪后的神经元来构建一个用于表示封面图像的隐写函数。接收者只需要共享密钥就可以从隐写函数中恢复秘密函数，从而重建秘密图像。实验结果表明，该方法不仅可以实现秘密图像的无损恢复，而且在容量、保真度和不可检测性方面也表现良好。对不同分辨率图像的实验验证了该方法在图像质量上比现有的隐式表示隐写方法具有明显的优势。]]></description>
      <guid>http://link.springer.com/10.1007/s00530-024-01476-9</guid>
      <pubDate>Tue, 10 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用深度学习从 X 射线衍射数据中实现端到端结构确定</title>
      <link>http://link.springer.com/10.1038/s41524-024-01401-8</link>
      <description><![CDATA[摘要
粉末晶体学是通过分析晶体粉末中的分子的 X 射线衍射 (XRD) 图案来确定分子结构的实验科学。由于许多材料都可以作为晶体粉末获得，因此粉末晶体学在许多领域越来越有用。然而，粉末晶体学没有已知的分析解决方案，因此结构推断通常涉及一个繁琐的迭代设计、结构细化和熟练专家的领域知识的过程。完全自动化计算推理过程的一个关键障碍是将问题以适合机器学习的端到端定量形式公式化，同时捕捉分子取向、对称性和重建分辨率的模糊性。在这里，我们提出了一种从粉末衍射数据中确定结构的 ML 方法。它的工作原理是使用基于变分坐标的深度神经网络估计晶胞中的电子密度。我们展示了该方法在计算粉末 X 射线衍射 (PXRD) 上的表现，同时输入了部分化学成分信息。当对立方和三方晶体系统的理论模拟数据进行评估时，该系统与未知材料的真实数据的平均相似度高达 93.4%（以结构相似性指数衡量），无论是已知还是部分已知的化学成分信息，都表明即使输入数据质量低下且不完整，也有望成功解决结构问题。该方法不预设晶体结构，并且可轻松扩展到其他情况，例如纳米材料和纹理样品，为重建尚未解决的纳米结构铺平了道路。]]></description>
      <guid>http://link.springer.com/10.1038/s41524-024-01401-8</guid>
      <pubDate>Sat, 07 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>智能航空航天感知的计算机视觉任务：概述</title>
      <link>http://link.springer.com/10.1007/s11431-024-2714-4</link>
      <description><![CDATA[摘要
计算机视觉任务对于航空航天任务至关重要，因为它们可以帮助航天器理解和解释太空环境，例如估计位置和方向、重建 3D 模型和识别物体，这些任务已经得到广泛研究，可以成功执行任务。然而，卡尔曼滤波、运动结构和多视角立体成像等传统方法不够稳健，无法处理恶劣条件，导致结果不可靠。近年来，基于深度学习 (DL) 的感知技术显示出巨大的潜力，并且优于传统方法，特别是在对不断变化的环境的稳健性方面。为了进一步推进基于 DL 的航空航天感知，已经提出了各种框架、数据集和策略，表明未来应用具有巨大的潜力。在本次调查中，我们旨在探索感知任务中使用的有前景的技术，并强调基于 DL 的航空航天感知的重要性。我们首先概述航天感知，包括近年来开发的经典航天计划、常用的传感器和传统的感知方法。随后，我们深入研究航天任务中的三个基本感知任务：姿态估计、三维重建和识别，因为它们对于后续的决策和控制至关重要。最后，我们讨论了当前研究的局限性和可能性，并对未来的发展进行了展望，包括使用有限数据集的挑战、改进算法的必要性以及多源信息融合的潜在好处。]]></description>
      <guid>http://link.springer.com/10.1007/s11431-024-2714-4</guid>
      <pubDate>Sun, 01 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>特邀社论：2022 年英国机器视觉会议特刊</title>
      <link>http://link.springer.com/10.1007/s11263-024-02038-2</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/s11263-024-02038-2</guid>
      <pubDate>Sun, 01 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一种基于注意力机制的轻量级图像去雾网络</title>
      <link>http://link.springer.com/10.1007/s11760-024-03392-x</link>
      <description><![CDATA[摘要
在当前基于卷积的图像去雾网络中，增加卷积层的深度和宽度是提高网络性能的常用策略。然而，这种方法显著增加了去雾网络的复杂性和计算成本。为了解决这个问题，本文提出了一种U形多尺度自适应选择网络（UMA-Net）。在不引入额外参数和计算成本的情况下，该网络利用不同尺度的卷积核对感受野的影响。它将标准卷积和扩张卷积结合到前馈网络（FFN）中，提出了一个多尺度自适应（MA）去雾模块，进一步扩展了感受野并关注FFN内重要的空间和通道信息。为了充分利用MA模块的多尺度特性，提出了一个轻量级的通道注意引导融合（CAGF）模块，实现了从雾蒙蒙图像中恢复高质量的去雾图像。大量实验证明了所提模块的有效性，在 Reside SOTS 数据集上，仅用 0.816M 参数和 8.794G FLOPs 就取得了最佳性能。]]></description>
      <guid>http://link.springer.com/10.1007/s11760-024-03392-x</guid>
      <pubDate>Sun, 01 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CLIP-Flow：解码 CLIP 空间中编码的图像</title>
      <link>http://link.springer.com/10.1007/s41095-023-0375-z</link>
      <description><![CDATA[摘要
本研究介绍了 CLIP-Flow，这是一种用于从给定图像或文本生成图像的新型网络。为了有效利用两种模态中包含的丰富语义，我们设计了一种语义引导的图像和文本到图像合成方法。具体而言，我们采用对比语言-图像预训练 (CLIP) 作为编码器来提取语义，采用 StyleGAN 作为解码器从此类信息生成图像。此外，为了连接 CLIP 的嵌入空间和 StyleGAN 的潜在空间，采用了真实的 NVP，并通过激活规范化和可逆卷积对其进行了修改。由于 CLIP 中的图像和文本共享相同的表示空间，因此可以将文本提示直接输入到 CLIP-Flow 中以实现文本到图像的合成。我们在多个数据集上进行了广泛的实验，以验证所提出的图像到图像合成方法的有效性。此外，我们在公共数据集 Multi-Modal CelebA-HQ 上进行了文本到图像合成测试。实验验证了我们的方法可以生成高质量的文本匹配图像，并且在定性和定量方面与最先进的方法相当。




]]></description>
      <guid>http://link.springer.com/10.1007/s41095-023-0375-z</guid>
      <pubDate>Wed, 28 Aug 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>动态环境下基于空间结构比较的RGB-D SLAM</title>
      <link>http://link.springer.com/10.1007/s11042-024-20128-8</link>
      <description><![CDATA[摘要
基于RGB-D的同步定位与地图构建（RGB-D SLAM）是机器人领域的研究热点，如何消除动态物体对RGB-D SLAM的影响仍是一大难题。针对这一问题，本文提出了一种基于空间结构比较的改进型RGB-D SLAM。在提出的RGB-D SLAM定位阶段，基于深度信息将特征点匹配对分为两组，即深度有效和深度未对齐。通过比较两个三维点集的空间结构矩阵，识别出静态特征点匹配对。此外，利用获得的姿态去除深度未对齐的动态特征点匹配对，以提高后端优化的性能。在地图处理阶段，采用了一种结合距离检测和处理坐标误差的新型地图点方法，并提出了一种重复点删除策略来消除参考帧中的冗余地图点。最后，在公开的动态TUM RGB-D数据集上对提出的RGB-D SLAM进行了测试。实验结果表明，与动态环境下最先进的RGB-D SLAM方法相比，提出的RGB-D SLAM具有更优的性能。]]></description>
      <guid>http://link.springer.com/10.1007/s11042-024-20128-8</guid>
      <pubDate>Wed, 28 Aug 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>森林应用中处理地面点云的软件解决方案综述</title>
      <link>http://link.springer.com/10.1007/s40725-024-00228-2</link>
      <description><![CDATA[摘要

综述目的
近年来，人们对 3D 点云在林业和森林生态学中的应用兴趣大增。随着激光扫描等新型 3D 捕获技术的发展，越来越多的算法被并行开发出来，用于将 3D 点云数据处理成林业应用的更切实的结果。从这些可用的算法中，用户很难决定应用哪种算法来最好地实现他们的目标。在这里，我们全面概述了点云采集和处理工具及其精准林业的输出。然后，我们提供了一个包含 24 种算法的综合数据库，用于处理使用近距离技术（特别是地面平台）获得的森林点云。


最新发现
在确定的 24 种解决方案中，有 20 种是开源的，两种是免费软件，其余两种是商业产品。作为 COST Action 3DForEcoTech 的一部分，可以在基于 Web 的平台上访问编译的解决方案数据库以及相应的安装和一般使用技术指南。该数据库可以作为社区的单一信息来源，帮助社区选择适合其需求的特定软件/算法。


摘要
我们得出结论，用于处理点云的各种算法的开发提供了强大的工具，可以对未来的森林资源清查产生重大影响，尽管我们注意到创建标准化范例的必要性。
]]></description>
      <guid>http://link.springer.com/10.1007/s40725-024-00228-2</guid>
      <pubDate>Sat, 17 Aug 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于局部图像风格化的音频引导隐式神经表征</title>
      <link>http://link.springer.com/10.1007/s41095-024-0413-5</link>
      <description><![CDATA[摘要
我们提出了一种音频引导局部图像风格化的新框架。声音通常提供有关场景特定背景的信息，并且与场景或对象的某个部分密切相关。然而，现有的图像风格化工作专注于使用图像或文本输入对整个图像进行风格化。基于音频输入对图像的特定部分进行风格化是自然但具有挑战性的。这项工作提出了一个框架，其中用户提供音频输入以在输入图像中定位目标，并提供另一个音频输入以局部风格化目标对象或场景。我们首先使用利用 CLIP 嵌入空间的视听定位网络生成精细定位图。然后，我们利用隐式神经表征 (INR) 以及预测的定位图根据声音信息对目标进行风格化。INR 操纵局部像素值以与提供的音频输入在语义上一致。我们的实验表明，所提出的框架优于其他音频引导风格化方法。此外，我们观察到我们的方法构建了简洁的定位图，并根据给定的音频输入自然地操纵目标对象或场景。




]]></description>
      <guid>http://link.springer.com/10.1007/s41095-024-0413-5</guid>
      <pubDate>Wed, 14 Aug 2024 00:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>