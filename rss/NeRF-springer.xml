<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最新结果</title>
    <link>http://link.springer.com</link>
    <description>Springer 提供的最新内容</description>
    <lastBuildDate>Thu, 30 May 2024 18:16:51 GMT</lastBuildDate>
    <item>
      <title>CARS 2024 — 第 38 届计算机辅助放射学和外科手术国际大会暨展览会论文集 西班牙巴塞罗那，2024 年 6 月 18 日至 21 日</title>
      <link>http://link.springer.com/10.1007/s11548-024-03128-9</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/s11548-024-03128-9</guid>
      <pubDate>Sat, 01 Jun 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于多尺度哈希编码的神经几何表示</title>
      <link>http://link.springer.com/10.1007/s41095-023-0340-x</link>
      <description><![CDATA[摘要
最近，基于神经隐函数的表示方法受到越来越多的关注，并被广泛用于使用可微分神经网络表示表面。然而，使用现有的神经几何表示方法从点云或多视图图像进行表面重建仍然受到计算速度慢和精度差的影响。为了缓解这些问题，我们提出了一种基于多尺度哈希编码的神经几何表示方法，该方法可以有效、高效地将表面表示为有符号距离场。我们新颖的神经网络结构将低频傅里叶位置编码与多尺度哈希编码巧妙地结合起来。相应地重新设计了几何网络的初始化和渲染模块的几何特征。我们的实验表明，对于重建具有数百万点的点云，所提出的表示方法至少快 10 倍。它还显著提高了多视图重建的速度和准确性。我们的代码和模型可在 https://github.com/Dengzhi-USTC/Neural-Geometry-Reconstruction 上找到。




]]></description>
      <guid>http://link.springer.com/10.1007/s41095-023-0340-x</guid>
      <pubDate>Sat, 01 Jun 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>以更细的粒度解析对象：一项调查</title>
      <link>http://link.springer.com/10.1007/s11633-022-1404-6</link>
      <description><![CDATA[摘要
细粒度视觉解析，包括细粒度部件分割和细粒度对象识别，因其在许多实际应用中的重要性而引起了广泛关注，例如农业、遥感和空间技术。主要研究工作按照不同的范式解决这些细粒度子任务，而忽略了这些任务之间的内在关系。此外，鉴于大多数研究仍然零散，我们从学习部件关系的新角度对高级工作进行了深入研究。从这个角度来看，我们首先整合最近的研究并使用新的分类法对综合进行基准测试。在此基础上，我们重新审视了细粒度部件分割和识别任务中的普遍挑战，并针对这些重要挑战提出了通过部件关系学习的新解决方案。此外，我们总结了细粒度视觉解析中的几条有希望的研究方向，以供未来研究。]]></description>
      <guid>http://link.springer.com/10.1007/s11633-022-1404-6</guid>
      <pubDate>Sat, 01 Jun 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>新兴人工智能应用：元宇宙、物联网、网络安全、医疗保健——概述</title>
      <link>http://link.springer.com/10.1007/s11042-023-17890-6</link>
      <description><![CDATA[摘要
“人工智能”(AI) 一词指的是能够关注周围环境并从周围环境中学习的“智能”高科技。它是人类有史以来创造的最具革命性的技术。涉及机器学习和深度学习技术的常见人工智能方法可以有效地应用于解决当今的各种网络安全问题。此外，元宇宙是关于人们如何通过技术相互交流和互动。本调查探讨了人工智能及其新兴应用及其各种技术的作用，例如元宇宙、医疗保健、物联网、游戏等等。为了确定人工智能技术固有的优势、缺陷、机会和风险，本调查报告通过广泛的文献调查，进行了 SWOT（优势、劣势、机会和威胁）评估。最后，调查报告总结了人工智能应用的当前知识状态，并讨论了近期研究的发现，以确保人工智能进步和应用的有利变化。还讨论了一些技术人工智能挑战，如高速、高性能硬件和减少训练数据量等，并展望了未来前景。]]></description>
      <guid>http://link.springer.com/10.1007/s11042-023-17890-6</guid>
      <pubDate>Sat, 01 Jun 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于学习的光场成像：概述</title>
      <link>http://link.springer.com/10.1186/s13640-024-00628-1</link>
      <description><![CDATA[摘要
传统摄影只能提供场景的二维图像，而新兴的成像模式（如光场）则能够通过捕捉来自不同方向的光线来表示更高维度的视觉信息。光场提供身临其境的体验、场景中的存在感，并可以增强不同的视觉任务。因此，对光场处理方法的研究越来越受欢迎。然而，这确实以更高的数据量和计算复杂性为代价。随着机器学习和深度架构在图像处理应用中的日益普及，在光场处理方法的设计中也观察到了向基于学习的方法的范式转变。开发了各种基于学习的方法来高效处理不同视觉任务的大量光场数据，同时提高性能。考虑到光场视觉任务的多样性和部署的基于学习的框架，有必要调查该领域中分散的基于学习的工作，以深入了解当前的趋势和挑战。本文旨在回顾现有的基于学习的光场成像解决方案，并总结最有前景的框架。此外，还重点介绍了评估方法和可用的光场数据集。最后，本文对未来的研究方向进行了简要展望。]]></description>
      <guid>http://link.springer.com/10.1186/s13640-024-00628-1</guid>
      <pubDate>Thu, 30 May 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于边界裂隙校正的泡沫尺度测量分割方法</title>
      <link>http://link.springer.com/10.1007/s10489-024-05552-5</link>
      <description><![CDATA[摘要
矿物浮选过程中，浮选表面泡沫的尺寸特征对浮选工艺的生产指标有显著影响。然而，泡沫图像中的信息错综复杂，存在泡沫堆积和气泡粘附等现象，导致边界信息不清晰。现有的方法难以准确全面地分割气泡边界。本文旨在设计一种精确描绘气泡边界的分割方法，从而能够测量气泡尺寸特征并评估平均气泡尺寸和气泡数量状态。设计了一种结合全维卷积点渲染的级联解码分支，利用渲染概念对错误分类的像素进行重新分析和分类。该重新校准过程通过解决这些不准确性来提高分割精度。使用现场泡沫工业数据集进行了实验，证明了所提出的方法在分割和测量浮选泡沫场景方面的显著优势。其中，分割准确率达到92.86%，平均气泡尺寸和气泡数量的测量误差仅为10.3%和8.6%。]]></description>
      <guid>http://link.springer.com/10.1007/s10489-024-05552-5</guid>
      <pubDate>Wed, 29 May 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>自我中心视野的未来展望</title>
      <link>http://link.springer.com/10.1007/s11263-024-02095-7</link>
      <description><![CDATA[摘要
未来会怎样？我们想知道！ 在本次调查中，我们探讨了当前自我中心视觉研究与人们期待的未来之间的差距，未来可穿戴计算设备（带有外置摄像头和数字覆盖）有望融入我们的日常生活。为了理解这一差距，本文首先通过基于人物的故事设想未来，并通过示例展示当前技术的局限性。然后，我们提供了未来与先前定义的研究任务之间的映射。对于每项任务，我们都会调查其开创性的工作、当前最先进的方法和可用的数据集，然后反思限制其适用于未来研究的缺点。请注意，本次调查侧重于自我中心视觉的软件模型，独立于任何特定硬件。本文最后提出了立即探索领域的建议，以开启我们通往未来永远在线、个性化和改善生活的自我中心视觉的道路。]]></description>
      <guid>http://link.springer.com/10.1007/s11263-024-02095-7</guid>
      <pubDate>Tue, 28 May 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FusionDeformer：使用扩散模型的文本引导网格变形</title>
      <link>http://link.springer.com/10.1007/s00371-024-03463-7</link>
      <description><![CDATA[摘要
网格变形具有广泛的应用，包括角色创建、几何建模、变形动画和变形。最近，基于 CLIP 模型的网格变形方法展示了执行自动文本引导网格变形的能力。然而，使用 2D 引导来变形 3D 网格试图解决不适定问题并导致扭曲和不平滑，而基于 CLIP 的方法无法消除这些问题，因为它们专注于语义感知特征并且无法识别这些伪影。为此，我们提出了 FusionDeformer，这是一种利用扩散模型的新型自动文本引导网格变形方法。变形是通过分数蒸馏采样实现的，它最小化了渲染变形网格的分布与文本条件分布之间的 KL 散度。为了缓解内在的不适定问题，我们将两种方法纳入我们的框架。第一种方法涉及将多个正交视图组合成单个图像，提供稳健的变形，同时避免对额外内存的需求。第二种方法采用了一种新的正则化方法来解决不平滑的伪影问题。我们的实验结果表明，该方法可以生成高质量、平滑变形的网格，这些网格与输入的文本描述精确对齐，同时保留拓扑关系。此外，我们的方法为动画设计提供了一种文本变形方法，使普通用户能够制作特效动画。
]]></description>
      <guid>http://link.springer.com/10.1007/s00371-024-03463-7</guid>
      <pubDate>Sat, 25 May 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>WATCHER：用于 Deepfake 检测的小波引导纹理内容层次关系学习</title>
      <link>http://link.springer.com/10.1007/s11263-024-02116-5</link>
      <description><![CDATA[摘要
人脸伪造技术的惊人进步产生了视觉上无法追踪的深度伪造视频，因此这些技术的潜在恶意滥用引发了极大的担忧。现有的深度伪造检测器主要关注特定的伪造模式，并使用 CNN 主干提取全局特征进行伪造检测。由于对内容和​​纹理特征的探索不足，它们经常受到特定于方法的伪造区域的过度拟合的影响，因此对越来越逼真的伪造表现出有限的泛化能力。在本文中，我们提出了一个 Wavelet 引导的 T纹理-内容Hi层次化 关系 (WATCHER) 学习框架，以更深入地研究关系感知的纹理内容特征。具体而言，我们提出了一种小波引导的自动编码器方案来检索一般的视觉表征，该方案能够感知高频细节以理解伪造品。为了进一步挖掘细粒度的伪造线索，提出了一个纹理内容注意图学习模块，通过分层学习协议中的多级注意图来丰富内容和纹理特征的上下文信息。最后，我们提出了一个渐进式多域特征交互模块，旨在对关系增强的纹理内容伪造特征进行语义推理。在流行的基准数据集上进行的大量实验证实了我们的 WATCHER 模型的优越性，始终以显著的优势胜过最先进的方法。]]></description>
      <guid>http://link.springer.com/10.1007/s11263-024-02116-5</guid>
      <pubDate>Thu, 23 May 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>色彩配置文件如何影响光场渲染和新颖视图合成中的视觉质量</title>
      <link>http://link.springer.com/10.1007/s11042-024-19396-1</link>
      <description><![CDATA[摘要
本文研究了图像颜色配置文件对光场渲染中视觉质量的影响。光场渲染方法需要多个输入视图，从不同的相机位置捕获相同的场景。这些视图可以在现实生活中由相机捕获，也可以从合成的 3D 场景中渲染。可以从输入视图合成一个新视图，从虚拟相机位置捕获场景。现代电影摄影使用各种颜色配置文件，这些颜色配置文件会影响相机传感器上测量的光线如何转换为数字图像的颜色值。不同的配置文件可用于不同的目的，无论是艺术目的还是实用目的，以揭示必要的细节。本文的主要科学问题是某些颜色配置文件是否可以提高光场渲染中新视图合成的质量。本文表明，对数配置文件可以显著提高视觉质量。使用来自不同类别的三种光场渲染方法来比较 17 种广泛使用的颜色配置文件。额外的测量表明，需要在渲染管道的最后应用后期处理和颜色分级以确保最佳质量。此外，还测量了去噪算法对光场渲染的影响。]]></description>
      <guid>http://link.springer.com/10.1007/s11042-024-19396-1</guid>
      <pubDate>Mon, 20 May 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Instant3D：即时文本到 3D 生成</title>
      <link>http://link.springer.com/10.1007/s11263-024-02097-5</link>
      <description><![CDATA[摘要
文本到 3D 生成引起了计算机视觉社区的广泛关注。现有方法主要针对每个文本提示从头开始优化神经场，依赖于繁重且重复的训练成本，这阻碍了它们的实际部署。在本文中，我们提出了一种用于快速文本到 3D 生成的新框架，称为 Instant3D。经过训练后，Instant3D 能够在不到 1 秒的时间内通过一次前馈网络为看不见的文本提示创建 3D 对象。我们通过设计一个直接从文本提示构建 3D 三平面的新网络来实现这一惊人的速度。我们的 Instant3D 的核心创新在于我们探索了将文本条件有效注入网络的策略。具体来说，我们建议结合三种关键机制：交叉注意、风格注入和 token-to-plane 转换，共同确保输出与输入文本的精确对齐。此外，我们提出了一个简单而有效的激活函数 scaled-sigmoid 来替代原来的 sigmoid 函数，这将使训练收敛速度提高十倍以上。最后，针对 3D 生成中的 Janus（多头）问题，我们提出了一种自适应的 Perp-Neg 算法，该算法可以在训练过程中根据 Janus 问题的严重程度动态调整概念否定尺度，有效降低多头效应。在各种基准数据集上进行的大量实验表明，所提出的算法在定性和定量方面均优于最新方法，同时实现了显著更高的效率。代码、数据和模型可在https://ming1993li.github.io/Instant3DProj/获取。]]></description>
      <guid>http://link.springer.com/10.1007/s11263-024-02097-5</guid>
      <pubDate>Thu, 16 May 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>机器学习辅助探索仿射德利涅-卢斯蒂格簇</title>
      <link>http://link.springer.com/10.1007/s42543-024-00086-8</link>
      <description><![CDATA[摘要
本文介绍了一项新颖的跨学科研究，该研究利用机器学习 (ML) 辅助框架来探索仿射德利涅-卢斯蒂格簇 (ADLV) 的几何形状。主要目标是研究 ADLV 的非空模式、维度和不可约组件的枚举。我们提出的框架展示了数据生成、模型训练、模式分析和人工检查的递归流程，展示了 ML 与纯数学研究之间的复杂相互作用。值得注意的是，我们的数据生成过程非常细致入微，强调选择有意义的子集和适当的特征集。我们证明该框架有潜力加速纯数学研究，从而发现新的猜想和有前途的研究方向，否则可能需要大量时间才能发现。我们重新发现了虚拟维度公式，并为有关某个维度下限的新发现问题提供了完整的数学证明。此外，我们向读者发出公开邀请，提供计算 ADLV 和 ML 模型的源代码，促进进一步的探索。本文最后分享了宝贵的经验，并强调了从这次合作中吸取的教训。]]></description>
      <guid>http://link.springer.com/10.1007/s42543-024-00086-8</guid>
      <pubDate>Wed, 15 May 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>理解 GAN：基础知识、变体、训练挑战、应用和未解决的问题</title>
      <link>http://link.springer.com/10.1007/s11042-024-19361-y</link>
      <description><![CDATA[摘要
生成对抗网络 (GAN) 是一种在对抗设置中训练生成模型的新框架，近年来引起了广泛关注。GAN 框架的两个对立神经网络，即生成器和鉴别器，在零和游戏中同时进行训练，其中生成器生成图像来欺骗经过训练以区分真实图像和合成图像的鉴别器。在本文中，我们全面回顾了 GAN 的最新发展。首先，我们介绍了各种深度生成模型、GAN 的基本理论和训练机制以及潜在空间。我们进一步讨论了 GAN 的几种代表性变体。尽管 GAN 已成功用于各种应用，但众所周知，它们的训练非常不稳定。一般来说，人们对 GAN 如何收敛缺乏了解。我们从统计学、博弈论和控制论的角度简要讨论了 GAN 不稳定性和收敛问题的根源，并描述了几种稳定训练 GAN 的技术。评估 GAN 一直是一项具有挑战性的任务，因为目前还没有就哪种方法更适合模型比较达成共识。因此，我们简要讨论了 GAN 的定量和定性评估方法。然后，我们进行了几次实验，根据这些评估指标比较了具有代表性的 GAN 变体。此外，我们还简要讨论了 GAN 的应用领域。最后，我们概述了 GAN 中几个重要的未解决的问题和未来的研究趋势。]]></description>
      <guid>http://link.springer.com/10.1007/s11042-024-19361-y</guid>
      <pubDate>Tue, 14 May 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>探索使用预训练特征进行立体匹配</title>
      <link>http://link.springer.com/10.1007/s11263-024-02090-y</link>
      <description><![CDATA[摘要
对于许多视觉任务，利用预训练特征可以提高性能，并始终受益于预训练技术的快速发展。然而，在立体匹配领域，预训练特征的使用尚未得到广泛研究。在本文中，我们首次系统地探索了预训练特征在立体匹配中的应用。为了灵活地使用任何预训练主干和立体匹配网络的组合，我们开发了可变形颈部 (DN)，将这两个组件的网络架构解耦。DN 的核心思想是利用可变形注意机制迭代地从浅层到深层融合预训练特征。从经验上讲，我们的探索揭示了影响使用预训练特征进行立体匹配的关键因素。我们进一步研究了预训练特征的实例级信息的作用，证明它有利于立体匹配，但在基于卷积的特征融合过程中可以受到抑制。基于注意力机制，提出的 DN 模块有效地利用了预训练特征中的实例级信息。此外，我们还提供了对效率-准确度权衡的理解，得出结论，从效率角度考虑，使用预训练特征也是一种不错的选择。]]></description>
      <guid>http://link.springer.com/10.1007/s11263-024-02090-y</guid>
      <pubDate>Sat, 11 May 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>BTD-RF：使用块项张量分解进行 3D 场景重建</title>
      <link>http://link.springer.com/10.1007/s10489-024-05476-0</link>
      <description><![CDATA[摘要
神经辐射场 (NeRF) 在视图合成任务中表现出色，但在三维 (3D) 场景重建期间需要大量内存和模型参数。本文提出了一种块项张量分解辐射场 (BTD-RF)，这是一种在保持重建质量的同时实现显着模型压缩的新方法。BTD-RF 将高维辐射场分解为低维张量块，得到的值比基线方法小 2.21 倍。将模型分解为低维张量块允许用轻量级多线性注意机制取代 Transformer 的标准多头注意，采用逐元素乘积和共享参数。这显着降低了模型复杂性而不会影响性能。对各种数据集的广泛评估表明，与以前的方法相比，BTD-RF 实现了卓越的图像重建质量。定量指标和定性评估证实，BTD-RF 生成的图像在结构和感知上接近地面实况，尽管设计轻巧，但仍表现出色。BTD-RF 在三维 (3D) 场景重建中提供了模型大小和重建质量之间的令人信服的权衡。其高效的设计使其适用于资源受限的应用程序，同时提供高保真度的结果，为更广泛的 NeRF 利用铺平了道路。代码可在 https://github.com/seonbin-kim/BTDRF 获得]]></description>
      <guid>http://link.springer.com/10.1007/s10489-024-05476-0</guid>
      <pubDate>Thu, 09 May 2024 00:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>