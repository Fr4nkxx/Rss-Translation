<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最新结果</title>
    <link>http://link.springer.com</link>
    <description>Springer 提供的最新内容</description>
    <lastBuildDate>Sun, 28 Apr 2024 00:54:55 GMT</lastBuildDate>
    <item>
      <title>DepthGAN：从语义布局生成基于 GAN 的深度</title>
      <link>http://link.springer.com/10.1007/s41095-023-0350-8</link>
      <description><![CDATA[摘要
现有的基于 GAN 的生成方法通常用于语义图像合成。我们提出了基于 GAN 的架构是否可以生成可信深度图的问题，并发现现有方法由于缺乏全局几何相关性而难以生成合理表示 3D 场景结构的深度图。因此，我们提出了 DepthGAN，这是一种使用语义布局作为输入来生成深度图的新方法，以辅助构建和操作结构良好的 3D 场景点云。具体而言，我们首先构建一个具有级联语义感知转换器块的特征生成模型，以获得具有全局结构信息的深度特征。对于我们的语义感知转换器块，我们提出了一个混合注意力模块和一个语义感知层规范化模块，以更好地利用语义一致性来生成深度特征。此外，我们提出了一种新颖的语义加权深度合成模块，它为当前场景生成自适应深度间隔。我们通过使用不同深度范围的语义感知深度权重的加权组合来生成最终的深度图。通过这种方式，我们获得了更准确的深度图。在室内和室外数据集上进行的大量实验表明，DepthGAN 在深度生成任务中在数量和视觉上都取得了优异的结果。




]]></description>
      <guid>http://link.springer.com/10.1007/s41095-023-0350-8</guid>
      <pubDate>Sat, 27 Apr 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CT-MVSNet：带有变压器的曲率引导多视图立体</title>
      <link>http://link.springer.com/10.1007/s11042-024-19227-3</link>
      <description><![CDATA[摘要
多视图立体（MVS）可以从多视图图像集合中实现密集的三维重建。虽然基于深度学习的MVS显着提高了重建性能，但重建的准确性和完整性仍需要改进，以满足三维内容生成的实际应用的需要。因此，提出了一种新的 MVS 方法，即带有变压器的曲率引导多视图立体。通过探索视图间关系并使用表面曲率测量图像表面的感受野大小和特征信息，该方法适应各种候选曲率尺度，自适应地提取更详细的特征以进行精确的成本计算。此外，提出了一种基于变压器的特征匹配网络，以更好地识别视图间相似性并提高特征匹配精度。此外，基于特征匹配的相似度测量模块将曲率和视图间相似度测量紧密结合，进一步提高重建精度。在DTU数据集和Tanks and Temples数据集上的实验验证了所提方法的有效性。]]></description>
      <guid>http://link.springer.com/10.1007/s11042-024-19227-3</guid>
      <pubDate>Tue, 23 Apr 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多视点立体特征分布归一化网络</title>
      <link>http://link.springer.com/10.1007/s00371-024-03334-1</link>
      <description><![CDATA[摘要
作为3D重建的关键技术，多视立体（MVS）研究随着深度学习的发展取得了重大进展。然而，由于不同视点之间的特征分布不一致，MVS 面临着挑战。这种现象已被认为是 MVS 的一个重要瓶颈。为了克服这一限制，我们提出了一种称为特征分布归一化网络（FDN-MVS）的创新方法。为了减轻因相对姿势不一致而产生的误差，我们利用多个视图之间的单应性变换，并提出了分布残差细化（DRR）模块。扩大尺度、残差连接和坐标系固定会带来更有意义的特征分布学习。为了进一步缩小不同观点之间的差异，我们建议进行特征归一化，将特征的计算限制在共同分布内。实验结果证明了我们方法的有效性。与常用的基准算法 CasMVSNet 相比，我们在 DTU 数据集上的误差降低了 13.52%，在 Tanks 和 Temples 上的 Fscore 提高了 18.77%数据集。代码位于 https://github.com/ZYangChen/FDN-MVS .]]></description>
      <guid>http://link.springer.com/10.1007/s00371-024-03334-1</guid>
      <pubDate>Wed, 17 Apr 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于城市场景新颖视图合成的多视图立体调节 NeRF</title>
      <link>http://link.springer.com/10.1007/s00371-024-03321-6</link>
      <description><![CDATA[摘要
神经辐射场 (NeRF) 将场景编码为神经表示，在单个对象和小空间区域上展示了令人印象深刻的新颖视图合成质量。然而，当面对城市室外环境时，NeRF 受到单个 MLP 容量和输入视图不足的限制，导致几何形状不正确，阻碍了真实渲染的制作。在本文中，我们提出了 MVSRegNeRF，这是专注于大规模自动驾驶场景的神经辐射场的扩展。我们采用传统的基于补丁匹配的多视图立体（MVS）方法来生成密集的深度图，我们利用它来调节 NeRF 的几何优化。我们还将多分辨率哈希编码集成到我们的神经场景表示中，以加速训练过程。由于我们的方法相对精确的几何约束，我们在现实世界的大规模街道场景上实现了高质量的新颖视图合成。我们在 KITTI-360 数据集上的实验表明，MVSRegNeRF 在新颖视图外观合成任务中优于最先进的方法。
      ]]></description>
      <guid>http://link.springer.com/10.1007/s00371-024-03321-6</guid>
      <pubDate>Wed, 27 Mar 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于多尺度哈希编码的神经几何表示</title>
      <link>http://link.springer.com/10.1007/s41095-023-0340-x</link>
      <description><![CDATA[摘要
最近，基于神经隐函数的表示引起了越来越多的关注，并被广泛用于使用可微神经网络来表示表面。然而，使用现有的神经几何表示从点云或多视图图像进行表面重建仍然存在计算速度慢和精度差的问题。为了缓解这些问题，我们提出了一种基于多尺度哈希编码的神经几何表示，它有效且高效地将表面表示为带符号的距离场。我们新颖的神经网络结构仔细地将低频傅立叶位置编码与多尺度哈希编码结合起来。相应地重新设计了几何网络的初始化和渲染模块的几何特征。我们的实验表明，所提出的表示对于重建具有数百万个点的点云来说至少快 10 倍。它还显着提高了多视图重建的速度和准确性。我们的代码和模型可以在 https://github.com/Dengzhi-中国科学技术大学/神经几何重建。
      



]]></description>
      <guid>http://link.springer.com/10.1007/s41095-023-0340-x</guid>
      <pubDate>Fri, 22 Mar 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>BSI-MVS：具有双向语义信息的多视图立体网络</title>
      <link>http://link.springer.com/10.1038/s41598-024-55612-6</link>
      <description><![CDATA[摘要
多视角立体视觉（MVS）的基本原理是通过从多个视角提取深度信息来进行三维重建。目前大多数 SOTA MVS 网络都是基于 Vision Transformer 的，这通常意味着昂贵的计算复杂度。为了降低计算复杂度并提高深度图精度，我们提出了一种具有双向语义信息的 MVS 网络（BSI-MVS）。首先，我们设计了一个多级空间金字塔模块来生成多层特征图以提取多尺度信息。然后我们提出了一个 2D 双向-LSTM 模块来捕获水平和垂直方向上不同时间步骤的双向语义信息，其中包含丰富的深度信息。最后，基于各级特征图构建成本量以优化最终的深度图。我们在 DTU 和 BlendedMVS 数据集上进行了实验。结果表明，我们的网络在整体指标上分别超越TransMVSNet、CasMVSNet、CVP-MVSNet和AACVP-MVSNet 17.84%、36.42%、14.96%和4.86%，在客观指标和可视化方面也显示出明显的性能提升。]]></description>
      <guid>http://link.springer.com/10.1038/s41598-024-55612-6</guid>
      <pubDate>Thu, 21 Mar 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于神经辐射场的多视角内窥镜场景重建用于手术模拟</title>
      <link>http://link.springer.com/10.1007/s11548-024-03080-8</link>
      <description><![CDATA[摘要

目的
在虚拟手术中，根据CT图像构建的3D模型的外观缺乏真实感，导致住院医生可能产生误解。因此，利用内窥镜捕获的多视角图像重建真实的内窥镜场景至关重要。


方法
我们提出了一种内窥镜-NeRF 网络，用于在非固定光源下对内窥镜场景进行隐式辐射场重建，并使用体积渲染合成新颖的视图。具有多个 MLP 网络和射线变换器网络的内窥镜-NeRF 网络将内窥镜场景表示为隐式场函数，在连续 5D 向量（3D 位置和 2D 方向）上具有颜色和体积密度。最终的合成图像是通过使用体积渲染聚合目标相机每条射线上的所有采样点而获得的。我们的方法考虑了光源到采样点的距离对场景辐射亮度的影响。


结果
我们的网络在我们的设备收集的猪的肺、肝、肾和心脏上进行了验证。结果表明，我们的方法合成的内窥镜场景的新视图在 PSNR、SSIM 和 LPIPS 指标方面优于现有方法（NeRF 和 IBRNet）。


结论
我们的网络可以有效地学习具有泛化能力的辐射场函数。在新的内窥镜场景上对预训练模型进行微调，进一步优化场景的神经辐射场，可以为手术模拟提供更真实、高分辨率的渲染图像。
]]></description>
      <guid>http://link.springer.com/10.1007/s11548-024-03080-8</guid>
      <pubDate>Tue, 27 Feb 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PLKA-MVSNet：具有大型内核卷积注意力的并行多视图立体</title>
      <link>http://link.springer.com/10.1007/978-981-99-8145-8_10</link>
      <description><![CDATA[摘要
在本文中，我们提出 PLKA-MVSNet 来解决基于学习的多视图立体（MVS）方法的深度估计中剩余的挑战，特别是不准确的深度估计在具有挑战性的区域，例如低纹理区域、弱照明条件和非朗伯表面。我们将此问题归因于特征提取器性能不足以及MVS管道传输造成的信息丢失，并给出了我们的优化方案。具体来说，我们通过使用多个小卷积而不是单个大卷积引入并行大核注意（PLKA），以增强纹理和结构信息的感知，这使我们能够捕获更大的感受野和长距离信息。为了适应从粗到细的 MVS 流程，我们采用 PLKA 构建多级特征提取器。此外，我们提出并行成本量聚合（PCVA）来增强聚合成本量的鲁棒性。它在2D维度引入了两个决策注意力，以弥补3D卷积压缩中的信息损失和像素遗漏。特别是，我们的方法在 DTU 数据集上显示出超越基于 Transformer 的方法的最佳整体性能，并在具有挑战性的 Tanks 和 Temples 高级数据集上取得了最佳结果。
      ]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8145-8_10</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用增强级联进行三维植物重建-MVSNet</title>
      <link>http://link.springer.com/10.1007/978-981-99-8432-9_23</link>
      <description><![CDATA[摘要
三维重建是恢复植物形态结构的重要方法，完整、准确的植物3D点云可以更好地反映植物的表型参数，例如植物植物的高度、体积和叶面积。为了获得更加完整、准确的植物点云，本文在Cascade-MVSNet网络的基础上提出了一系列的增强。为了改善植物弱纹理区域的重建，在特征提取阶段引入了轻量级注意机制。为了增强植物点云的有效点并抑制无效点，采用焦点损失作为损失函数，将深度估计问题视为分类任务以获得更准确的深度信息。此外，为了提高模型的效率，我们用深度可分离卷积代替标准卷积，在保持性能的同时减少参数数量和计算复杂度。上述工作应用于植物数据集，重建显示无效点显着减少，重建点云更清晰。除了对植物数据集进行研究外，我们还在公开的 DTU 数据集上评估了我们的方法。实验结果表明，DTU 数据集的重建完整性显着提高，整体表现具有竞争力。]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8432-9_23</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CT-MVSNet：具有跨尺度变压器的高效多视图立体</title>
      <link>http://link.springer.com/10.1007/978-3-031-53308-2_29</link>
      <description><![CDATA[摘要
最近的深度多视图立体（MVS）方法已广泛将变压器纳入级联网络中以进行高分辨率深度估计，取得了令人印象深刻的结果。然而，现有的基于变压器的方法受到计算成本的限制，阻碍了它们扩展到更精细的阶段。在本文中，我们提出了一种新颖的跨尺度变换器（CT），它可以在不同阶段处理特征表示，而无需额外的计算。具体来说，我们引入了一种自适应匹配感知变压器（AMT），它在多个尺度上采用不同的交互式注意力组合。这种组合策略使我们的网络能够捕获图像内上下文信息并增强图像间特征关系。此外，我们提出了一种双特征引导聚合（DFGA），将粗略的全局语义信息嵌入到更精细的成本卷构造中，以进一步加强全局和局部特征感知。同时，我们设计了一个特征度量损失（FM Loss）来评估变换前后的特征偏差，以减少特征不匹配对深度估计的影响。对 DTU 数据集和 Tanks and Temples (T &amp;T) 基准的大量实验表明，我们的方法取得了最先进的结果。代码可在 https://github.com/wscstrive/CT-MVSNet 获取。]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-53308-2_29</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>模式识别和计算机视觉</title>
      <link>http://link.springer.com/10.1007/978-981-99-8432-9</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8432-9</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>神经信息处理</title>
      <link>http://link.springer.com/10.1007/978-981-99-8145-8</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8145-8</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多媒体建模</title>
      <link>http://link.springer.com/10.1007/978-3-031-53308-2</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-53308-2</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于多视图立体和并行优化的自监督边缘结构学习</title>
      <link>http://link.springer.com/10.1007/978-3-031-53311-2_33</link>
      <description><![CDATA[摘要
最近的研究表明，许多自监督方法在多视图立体（MVS）方面取得了明显的进展。然而，现有方法忽略了重建目标的边缘结构信息，包括外部轮廓和内部结构的边缘信息。这可能导致重建结果的边缘和完整性不太令人满意。为了解决这个问题，我们提出了一种提取边缘结构图的提取器，并创新地设计了边缘结构Loss来约束网络更多地关注参考视图的边缘结构特征，以改善重建结果的纹理细节。特别地，我们利用多视图立体中构建成本体积的思想，并将源视图的边缘结构图扭曲到参考视图，以提供可靠的自监督。此外，我们设计了一种结合局部和全局属性的掩蔽机制，保证了鲁棒性并提高了模型对复杂样本的重建完整性。此外，我们采用有效的并行加速方法来提高训练速度和重建效率。对 DTU 和 Tanks &amp;Temples 基准的大量实验表明，与其他无监督工作相比，我们的方法提高了准确性和完整性。此外，我们的并行方法在保证准确性的同时提高了效率。代码将被发布。]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-53311-2_33</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有分层几何约束的可推广神经辐射场</title>
      <link>http://link.springer.com/10.1007/978-981-99-8432-9_26</link>
      <description><![CDATA[摘要
广义神经辐射场模型的出现极大地扩展了新颖的视图合成任务的适用性。然而，现有的方法主要依赖从相邻视图获得的 2D 特征来辅助渲染，这通常会导致看不见的视点的退化。一些方法在模型中引入成本量来提供几何先验，但这种粗略的几何信息无法有效地约束模型，导致渲染图像具有丰富的伪影。在这项工作中，我们建议为模型提供分层几何约束，以实现更好的渲染结果。我们引入级联 MVSNet 来提供分层场景结构特征，以推断底层场景几何形状。它限制了模型在看不见的视点下的渲染。此外，分层特征提供了有助于细节重建的精细表示。此外，我们利用级联 MVSNet 生成的分层深度图来约束采样过程，确保采样点集中在场景表面附近。这种采样策略过滤掉了大量无用的采样点，提高了采样效率和渲染质量。与之前的广义方法不同，我们采用 Neus 提出的新权重函数来消除密度场中的固有偏差，从而将颜色和密度的预测与带符号的距离场分开。大量实验表明，我们提出的方法显着提高了渲染质量，并且优于以前的方法。]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8432-9_26</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>