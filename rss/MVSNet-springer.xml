<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最新结果</title>
    <link>http://link.springer.com</link>
    <description>Springer 提供的最新内容</description>
    <lastBuildDate>Fri, 22 Mar 2024 12:19:32 GMT</lastBuildDate>
    <item>
      <title>基于多尺度哈希编码的神经几何表示</title>
      <link>http://link.springer.com/10.1007/s41095-023-0340-x</link>
      <description><![CDATA[摘要
最近，基于神经隐函数的表示引起了越来越多的关注，并被广泛用于使用可微神经网络来表示表面。然而，使用现有的神经几何表示从点云或多视图图像进行表面重建仍然存在计算速度慢和精度差的问题。为了缓解这些问题，我们提出了一种基于多尺度哈希编码的神经几何表示，它有效且高效地将表面表示为带符号的距离场。我们新颖的神经网络结构仔细地将低频傅立叶位置编码与多尺度哈希编码结合起来。相应地重新设计了几何网络的初始化和渲染模块的几何特征。我们的实验表明，所提出的表示对于重建具有数百万个点的点云来说至少快 10 倍。它还显着提高了多视图重建的速度和准确性。我们的代码和模型可以在 https://github.com/Dengzhi-中国科学技术大学/神经几何重建。
      



]]></description>
      <guid>http://link.springer.com/10.1007/s41095-023-0340-x</guid>
      <pubDate>Fri, 22 Mar 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>BSI-MVS：具有双向语义信息的多视图立体网络</title>
      <link>http://link.springer.com/10.1038/s41598-024-55612-6</link>
      <description><![CDATA[摘要
多视图立体（MVS）的基本原理是通过从多个视图中提取深度信息来进行3D重建。当前大多数 SOTA MVS 网络都基于 Vision Transformer，这通常意味着昂贵的计算复杂性。为了降低计算复杂度并提高深度图精度，我们提出了一种具有双向语义信息的 MVS 网络（BSI-MVS）。首先，我们设计了一个多级空间金字塔模块来生成多层特征图以提取多尺度信息。然后，我们提出了一个2D双向LSTM模块来捕获水平和垂直方向不同时间步长的双向语义信息，其中包含丰富的深度信息。最后，基于不同级别的特征图构建成本量以优化最终的深度图。我们在 DTU 和 BlendedMVS 数据集上进行实验。结果表明，我们的网络在总体指标方面分别超过了 TransMVSNet、CasMVSNet、CVP-MVSNet 和 AACVP-MVSNet 17.84%、36.42%、14.96% 和 4.86%，这也显示了目标性能的显着提升。指标和可视化。]]></description>
      <guid>http://link.springer.com/10.1038/s41598-024-55612-6</guid>
      <pubDate>Thu, 21 Mar 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于神经辐射场的多视角内窥镜场景重建用于手术模拟</title>
      <link>http://link.springer.com/10.1007/s11548-024-03080-8</link>
      <description><![CDATA[摘要

目的
在虚拟手术中，根据CT图像构建的3D模型的外观缺乏真实感，导致住院医生可能产生误解。因此，利用内窥镜捕获的多视角图像重建真实的内窥镜场景至关重要。


方法
我们提出了一种内窥镜-NeRF 网络，用于在非固定光源下对内窥镜场景进行隐式辐射场重建，并使用体积渲染合成新颖的视图。具有多个 MLP 网络和射线变换器网络的内窥镜-NeRF 网络将内窥镜场景表示为隐式场函数，在连续 5D 向量（3D 位置和 2D 方向）上具有颜色和体积密度。最终的合成图像是通过使用体积渲染聚合目标相机每条射线上的所有采样点而获得的。我们的方法考虑了光源到采样点的距离对场景辐射亮度的影响。


结果
我们的网络在我们的设备收集的猪的肺、肝、肾和心脏上进行了验证。结果表明，我们的方法合成的内窥镜场景的新视图在 PSNR、SSIM 和 LPIPS 指标方面优于现有方法（NeRF 和 IBRNet）。


结论
我们的网络可以有效地学习具有泛化能力的辐射场函数。在新的内窥镜场景上对预训练模型进行微调，进一步优化场景的神经辐射场，可以为手术模拟提供更真实、高分辨率的渲染图像。
]]></description>
      <guid>http://link.springer.com/10.1007/s11548-024-03080-8</guid>
      <pubDate>Tue, 27 Feb 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PLKA-MVSNet：具有大型内核卷积注意力的并行多视图立体</title>
      <link>http://link.springer.com/10.1007/978-981-99-8145-8_10</link>
      <description><![CDATA[摘要
在本文中，我们提出 PLKA-MVSNet 来解决基于学习的多视图立体（MVS）方法的深度估计中剩余的挑战，特别是不准确的深度估计在具有挑战性的区域，例如低纹理区域、弱照明条件和非朗伯表面。我们将此问题归因于特征提取器性能不足以及MVS管道传输造成的信息丢失，并给出了我们的优化方案。具体来说，我们通过使用多个小卷积而不是单个大卷积引入并行大核注意（PLKA），以增强纹理和结构信息的感知，这使我们能够捕获更大的感受野和长距离信息。为了适应从粗到细的 MVS 流程，我们采用 PLKA 构建多级特征提取器。此外，我们提出并行成本量聚合（PCVA）来增强聚合成本量的鲁棒性。它在2D维度上引入了两个决策注意力，以弥补3D卷积压缩中的信息损失和像素遗漏。特别是，我们的方法在 DTU 数据集上显示出超越基于 Transformer 的方法的最佳整体性能，并在具有挑战性的 Tanks 和 Temples 高级数据集上取得了最佳结果。
      ]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8145-8_10</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用增强级联进行三维植物重建-MVSNet</title>
      <link>http://link.springer.com/10.1007/978-981-99-8432-9_23</link>
      <description><![CDATA[摘要
三维重建是恢复植物形态结构的重要方法，完整、准确的植物3D点云可以更好地反映植物的表型参数，例如植物植物的高度、体积和叶面积。为了获得更加完整、准确的植物点云，本文在Cascade-MVSNet网络的基础上提出了一系列的增强。为了改善植物弱纹理区域的重建，在特征提取阶段引入了轻量级注意机制。为了增强植物点云的有效点并抑制无效点，采用焦点损失作为损失函数，将深度估计问题视为分类任务以获得更准确的深度信息。此外，为了提高模型的效率，我们用深度可分离卷积代替标准卷积，在保持性能的同时减少参数数量和计算复杂度。上述工作应用于植物数据集，重建显示无效点显着减少，重建点云更清晰。除了对植物数据集进行研究外，我们还在公开的 DTU 数据集上评估了我们的方法。实验结果表明，DTU 数据集的重建完整性显着提高，整体表现具有竞争力。]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8432-9_23</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CT-MVSNet：具有跨尺度变压器的高效多视图立体</title>
      <link>http://link.springer.com/10.1007/978-3-031-53308-2_29</link>
      <description><![CDATA[摘要
最近的深度多视图立体（MVS）方法已广泛将变压器纳入级联网络中以进行高分辨率深度估计，取得了令人印象深刻的结果。然而，现有的基于变压器的方法受到计算成本的限制，阻碍了它们扩展到更精细的阶段。在本文中，我们提出了一种新颖的跨尺度变换器（CT），它可以在不同阶段处理特征表示，而无需额外的计算。具体来说，我们引入了一种自适应匹配感知变压器（AMT），它在多个尺度上采用不同的交互式注意力组合。这种组合策略使我们的网络能够捕获图像内上下文信息并增强图像间特征关系。此外，我们提出了一种双特征引导聚合（DFGA），将粗略的全局语义信息嵌入到更精细的成本卷构造中，以进一步加强全局和局部特征感知。同时，我们设计了一个特征度量损失（FM Loss）来评估变换前后的特征偏差，以减少特征不匹配对深度估计的影响。对 DTU 数据集和 Tanks and Temples (T &amp;T) 基准的大量实验表明，我们的方法取得了最先进的结果。代码可在 https://github.com/wscstrive/CT-MVSNet 获取。]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-53308-2_29</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>模式识别和计算机视觉</title>
      <link>http://link.springer.com/10.1007/978-981-99-8432-9</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8432-9</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>神经信息处理</title>
      <link>http://link.springer.com/10.1007/978-981-99-8145-8</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8145-8</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多媒体建模</title>
      <link>http://link.springer.com/10.1007/978-3-031-53308-2</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-53308-2</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于多视图立体和并行优化的自监督边缘结构学习</title>
      <link>http://link.springer.com/10.1007/978-3-031-53311-2_33</link>
      <description><![CDATA[摘要
最近的研究表明，许多自监督方法在多视图立体（MVS）方面取得了明显的进展。然而，现有方法忽略了重建目标的边缘结构信息，包括外部轮廓和内部结构的边缘信息。这可能导致重建结果的边缘和完整性不太令人满意。为了解决这个问题，我们提出了一种提取边缘结构图的提取器，并创新地设计了边缘结构Loss来约束网络更多地关注参考视图的边缘结构特征，以改善重建结果的纹理细节。特别地，我们利用多视图立体中构造成本体积的思想，并将源视图的边缘结构图扭曲到参考视图，以提供可靠的自监督。此外，我们设计了一种结合局部和全局属性的掩蔽机制，保证了鲁棒性并提高了模型对复杂样本的重建完整性。此外，我们采用有效的并行加速方法来提高训练速度和重建效率。对 DTU 和 Tanks &amp;Temples 基准的大量实验表明，与其他无监督工作相比，我们的方法提高了准确性和完整性。此外，我们的并行方法在保证准确性的同时提高了效率。代码将被发布。]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-53311-2_33</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有分层几何约束的可推广神经辐射场</title>
      <link>http://link.springer.com/10.1007/978-981-99-8432-9_26</link>
      <description><![CDATA[摘要
广义神经辐射场模型的出现极大地扩展了新颖的视图合成任务的适用性。然而，现有的方法主要依靠从相邻视图获得的 2D 特征来辅助渲染，这通常会导致看不见的视点的退化。一些方法在模型中引入成本量来提供几何先验，但这种粗略的几何信息无法有效地约束模型，导致渲染图像具有丰富的伪影。在这项工作中，我们建议为模型提供分层几何约束，以实现更好的渲染结果。我们引入级联 MVSNet 来提供分层场景结构特征，以推断底层场景几何形状。它限制了模型在看不见的视点下的渲染。此外，分层特征提供了有助于细节重建的精细表示。此外，我们利用级联 MVSNet 生成的分层深度图来约束采样过程，确保采样点集中在场景表面附近。这种采样策略过滤掉了大量无用的采样点，提高了采样效率和渲染质量。与之前的广义方法不同，我们采用 Neus 提出的新权重函数来消除密度场中的固有偏差，从而将颜色和密度的预测与带符号的距离场分开。大量实验表明，我们提出的方法显着提高了渲染质量，并且优于以前的方法。]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8432-9_26</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多媒体建模</title>
      <link>http://link.springer.com/10.1007/978-3-031-53311-2</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-53311-2</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有模型压缩的神经网络</title>
      <link>http://link.springer.com/10.1007/978-981-99-5068-3</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-5068-3</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>网络修剪</title>
      <link>http://link.springer.com/10.1007/978-981-99-5068-3_5</link>
      <description><![CDATA[摘要
网络剪枝是深度学习中使用的一种技术，通过消除不必要的连接或参数来减少神经网络的大小和复杂性。修剪的目的是创建更高效​​、更精简的模型，以保持或提高性能，同时减少计算要求和内存占用。]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-5068-3_5</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>神经信息处理</title>
      <link>http://link.springer.com/10.1007/978-981-99-8070-3</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8070-3</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>