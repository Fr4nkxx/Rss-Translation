<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最新结果</title>
    <link>http://link.springer.com</link>
    <description>Springer可用的最新内容</description>
    <lastBuildDate>Sat, 08 Feb 2025 01:03:10 GMT</lastBuildDate>
    <item>
      <title>单杆超级分辨的边缘投影概要仪（SSSR-FPP）：每秒3D成像100,000帧，深度学习</title>
      <link>http://link.springer.com/10.1038/s41377-024-01721-w</link>
      <description><![CDATA[摘要
揭示了在机械，物理学和生物学中各种短暂事件背后隐藏的基本方面，这是获得具有超快时间分辨率的三维（3D）图像的高度期望的能力长期以来一直在寻求。作为最常用的3D传感技术之一，条纹投影概要仪（FPP）从具有依次结构化照明拍摄的立体声图像中重建了场景的深度。但是，当前FPP方法的成像速度通常被限制在几个kHz上，该kHz受投影仪相机硬件的限制以及相位检索和解开所需的附带模式的数量。在这里，我们报告了一种新型的基于学习的超快3D成像技术，称为单杆超级分辨FPP（SSSR-FPP），可在100,000 Hz处实现超快3D成像。 SSSR-FPP仅使用一对低信噪比（SNR），低分辨率和像素的边缘模式作为输入，而高分辨率的未包装相和边缘订单可以用特定的训练有素的深神经网络解密。我们的方法通过减少传统高速相机的成像窗口来利用显着的速度增长，同时通过深度学习“再生”丢失的空间分辨率。为了展示SSSR-FPP的高时空分辨率，我们介绍了几个瞬态场景的3D摄影，包括旋转的涡轮叶叶片，爆炸的构建块以及蒸汽机的往复运动等，这些动作以前是不可能的，甚至是不可能的，甚至是不可能的使用常规方法捕获。实验结果将SSSR-FPP建立为3D光学传感领域的重要一步，为各个科学学科的各种动态过程提供了新的见解。]]></description>
      <guid>http://link.springer.com/10.1038/s41377-024-01721-w</guid>
      <pubDate>Fri, 07 Feb 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>重新思考多视图立体声的概率量：概率分析方法</title>
      <link>http://link.springer.com/10.1007/s10489-025-06284-w</link>
      <description><![CDATA[摘要
现有的基于学习的多视图立体声（MVS）模型主要集中于通过级联结构预测深度图，以实现更强大的重建结果。但是，他们经常强调提高立体声匹配的质量，同时忽略深度假设的重要性。在本文中，我们从概率体积分析的角度提出了一种新型的MVS模型。首先，考虑深度细化的概率量的指导效应。理想情况下，沿概率体积深度维度的概率分布遵循单峰模式。我们设计了一个单峰曲线以适合这种模式。然后，根据预定义的概率阈值，为每个像素位置自适应选择合理的深度细化范围。此外，考虑到匹配噪声可能导致概率量显示为模糊的单峰峰，我们设计了概率体积分式 - 工程模块（PVS-PVM）。该模块基于条件约束执行峰值搜索，将概率体积分为主概率和子概率量，然后计算出它们的两组深度假设。最后，根据这些深度假设计算新的主概率和子概率量，并合并以预测深度。这种方法可以更全面地考虑具有更高概率的区域，从而提高了深度假设的鲁棒性。实验结果表明，我们的方法有效地利用了概率信息信息来指导深度图的细化，并产生增强的DTU和TAMP上的重建结果。寺庙数据集。我们的代码将在 https://github.com/zongh5a/probmvsnet 。 /p&gt;]]></description>
      <guid>http://link.springer.com/10.1007/s10489-025-06284-w</guid>
      <pubDate>Mon, 03 Feb 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>上下文感知的多视图立体声网络，用于有效的边缘保护深度估计</title>
      <link>http://link.springer.com/10.1007/s11263-024-02337-8</link>
      <description><![CDATA[摘要
基于学习的多视图立体方法近年来通过采用粗糙至细深度估计框架取得了巨大进步。但是，现有方法仍然遇到困难在无特征区域，物体边界和薄结构中恢复深度方面的困难，这主要是由于低纹理区域中匹配线索的区分性差，这是3D卷积神经网络的固有平滑属性，用于成本量正规化成本量以及最粗糙的规模特征的信息丢失。为了解决这些问题，我们提出了一个上下文感知的多视图立体网络（CANET），该网络（CANET）利用图像中的上下文提示来实现有效的边缘延伸深度估计。参考视图中的结构自相似性信息由引入的自相似性的成本聚合模块利用，以在成本量中执行远程依赖关系建模，这可以提高无特征区域的对位。随后，参考视图中的上下文信息通过提出的层次层次呈现剩余学习模块逐渐完善多尺度的深度估计，从而在边缘进行了微妙的深度估计。为了通过使其更加专注于精致的区域来丰富最佳量表，提出了一个焦点选择模块，可以通过更细节（例如薄结构）来增强初始深度的恢复。通过将上述策略整合到设计精良的轻质级联框架中，Canet可以实现卓越的性能和效率权衡。广泛的实验表明，所提出的方法以快速推理速度和低内存使用量实现最先进的性能。值得注意的是，在所有已发表的基于学习的方法中，CANET在具有挑战性的坦克和寺庙中排名第一。]]></description>
      <guid>http://link.springer.com/10.1007/s11263-024-02337-8</guid>
      <pubDate>Tue, 07 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SS-BEV：基于多尺度空间结构理解的多相机BEV对象检测</title>
      <link>http://link.springer.com/10.1007/s11760-024-03762-5</link>
      <description><![CDATA[摘要
 bev（鸟类视图）基于多个相机的对象检测已成为自动驾驶领域的主流范式。但是，我们已经确定在检测到极大或小物体时，许多现有方法的性能较差。为了解决此问题，本文提出了一个新的称为SS-BEV的多相机BEV检测的框架。首先，我们设计了一个更具表现力的特征提取模块A-WBFP，该模块通过级联的方法集成了平行的非常弯曲的卷积和加权双向特征金字塔块进入骨干网络。这有助于防止更深的网络层中小型对象信息的丢失并增强接受场，从而产生充满上下文信息的特征图。然后，我们介绍MORD模块，该模块利用雷达点云的准确深度信息来改善模型对大对象和小物体的空间结构理解。通过学习对象的内部结构和选定的参考点之间的相对深度，构建了相应的损耗函数以监督最终检测性能。 SS-BEV在具有挑战性的Nuscenes验证设置上胜过基线模型，NDS检测分数有2.1分。在Nuscenes测试集中，它的检测准确度分别为66.1％和22.3％的障碍物和建筑车辆地图，基于多型相机和激光雷达融合，超过了一些方法。。]]></description>
      <guid>http://link.springer.com/10.1007/s11760-024-03762-5</guid>
      <pubDate>Thu, 02 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>元素成像和处理</title>
      <link>http://link.springer.com/10.1007/978-981-97-6915-5</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-981-97-6915-5</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>朝向大规模的元素重建</title>
      <link>http://link.springer.com/10.1007/978-981-97-6915-5_5</link>
      <description><![CDATA[摘要
以无与伦比的现实主义和细节级别重建现实世界的场景已成为计算机视觉和图形领域的长期目标。实现这一目标需要在感应技术和元素重建算法中进行协调的努力。]]></description>
      <guid>http://link.springer.com/10.1007/978-981-97-6915-5_5</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉 -  ECCV 2024</title>
      <link>http://link.springer.com/10.1007/978-3-031-73036-8</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-73036-8</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多视图立体声的功能分布归一化网络</title>
      <link>http://link.springer.com/10.1007/s00371-024-03334-1</link>
      <description><![CDATA[摘要
作为3D重建的关键技术，多视图立体声（MVS）的研究在深度学习的发展中取得了重大进展。但是，由于不同观点的特征分布不一致，MVS面临挑战。这种现象被认为是MV中的重要瓶颈。为了克服这一限制，我们提出了一种创新的方法，称为特征分布归一化网络（FDN – MV）。为了减轻不一致的相对姿势引起的错误，我们利用多个视图之间的本术转换，并提出分布残差改进（DRR）模块。扩大规模，残留连接和坐标系固定会导致更有意义的特征分布学习。为了进一步缩小不同观点之间的差异，我们建议特征归一化，从而限制共同分布中特征的计算。实验结果证明了我们方法的有效性。与常用的基准算法CASMVSNET相比，DTU数据集的误差降低了13.52％，并将 f  -Score提高了18.77％的储罐和坦普斯。数据集。代码可在 https://github.com/zyangchen/fdn-mvs  。]]></description>
      <guid>http://link.springer.com/10.1007/s00371-024-03334-1</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>模式识别</title>
      <link>http://link.springer.com/10.1007/978-3-031-78456-9</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-78456-9</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉 -  ECCV 2024</title>
      <link>http://link.springer.com/10.1007/978-3-031-73030-6</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-73030-6</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>平滑度，综合和抽样：重新考虑无监督的多视图立体声，DIV损失</title>
      <link>http://link.springer.com/10.1007/978-3-031-73036-8_22</link>
      <description><![CDATA[摘要
尽管无监督的多视角立体声（MV）取得了重大进展，但自引入以来，核心损失公式在很大程度上保持不变。但是，我们确定了对该核心损失的基本局限性，并提出了三个主要变化，以改善深度先验，遮挡和观察依赖性效果的建模。首先，我们使用夹紧的深度平滑度约束在预测的深度图中消除了突出的楼梯式和边缘伪像。其次，我们提出了一种学习的视图合成方法，以生成用于光度损失的图像，避免使用手工编码的启发式方法来处理观点依赖性效果。最后，我们将其他视图示例视图以超出用作MVS输入的视图，从而挑战网络以预测与看不见的视图相匹配的深度。这些贡献共同构成了我们称为 div 损失的改进的监督策略。我们DIV损失的主要优点是，它可以轻松地放入现有的无监督MVS培训管道中，从而显着改善了竞争性重建基准测试基准，并在对象界面围绕最小培训成本的对象界限进行了极大的定性性能。  ]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-73036-8_22</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>模式识别和计算机视觉</title>
      <link>http://link.springer.com/10.1007/978-981-97-8508-7</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-981-97-8508-7</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>可见性 - 感知的pixelwise视图选择多视图立体声匹配</title>
      <link>http://link.springer.com/10.1007/978-3-031-78456-9_9</link>
      <description><![CDATA[摘要
基于贴片的多视图立体算法的性能受到用于匹配成本计算的所选源视图的很大影响。现有方法通常以相当临时的方式检测闭塞，这可能会对计算产生负面影响。相比之下，我们的论文介绍了一种创新的方法，该方法故意模拟可见度。我们提出了一种新颖的可见性引导的Pixelwise选择方案，该方案使用验证的解决方案中的可见性信息逐步完善参考视图中每个像素的源视图集。此外，人工多BEE菌落（AMBC）算法被利用为不同像素的最佳搜索解决方案。为了确保相邻像素的平稳性并更好地管理无纹理区域，将奖励分配给来自经过验证的来源的解决方案。通过在两个数据集上的实验验证的我们的方法改善了闭塞和低纹理区域中的详细恢复，表明在苛刻的场景上表现出值得注意的性能。我们的实施可在 https://github.com/ ricky-s/visibility-ware pixelwise-view-selection 。]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-78456-9_9</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉 -  ECCV 2024</title>
      <link>http://link.springer.com/10.1007/978-3-031-73220-1</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-73220-1</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有自适应特征提取和区域感知深度预测的多视图深度估计</title>
      <link>http://link.springer.com/10.1007/978-981-97-8508-7_3</link>
      <description><![CDATA[摘要
多视图深度估计是3D重建的重要任务，该任务旨在通过多视图立体声（MVS）技术从多视图图像中获取深度图。但是，当输入图像包含遮挡和低纹理的具有挑战性的区域时，现有的MVS方法可能无法表现良好。为了解决这些问题，本文提出了一个多视图深度估计框架，该框架由自适应特征提取和区域感知的深度预测组成。为了获得更好的像素特征匹配，使用基于CNN的自适应局部特征提取器（ALFE）和基于变压器的全局特征提取器（GFE）构建自适应特征提取器（GFE），以捕获具有挑战性地区的代表性和强大特征。为了获得更好的深度图输出，使用区域感知的深度细化模块（RA-DRM）构建了区域感知的深度预测，该模块（RA-DRM）迭代地完善了由不同区域中提取的特征引导的深度图。定性和定量实验在DTU和BlendenDMVS数据集上进行。结果表明，ALFE，GFE和RA-DRM的拟议模块的有效性，比较表明我们基于学习的MVS方法优于相关方法。]]></description>
      <guid>http://link.springer.com/10.1007/978-981-97-8508-7_3</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>