<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最新结果</title>
    <link>http://link.springer.com</link>
    <description>Springer 提供的最新内容</description>
    <lastBuildDate>Mon, 03 Feb 2025 18:18:25 GMT</lastBuildDate>
    <item>
      <title>重新思考多视角立体的概率体积：一种概率分析方法</title>
      <link>http://link.springer.com/10.1007/s10489-025-06284-w</link>
      <description><![CDATA[摘要
现有的基于学习的多视角立体（MVS）模型主要侧重于通过级联结构预测深度图，以实现更为稳健的重建结果。然而，它们往往强调提高立体匹配的质量，而忽略了深度假设的重要性。本文从概率体积分析的角度提出了一种新的 MVS 模型。首先，考虑概率体积对深度细化的引导作用。理想情况下，概率体积沿深度维度的概率分布遵循单峰模式。我们设计了一条单峰曲线来拟合这种模式。然后，基于预定义的概率阈值，自适应地为每个像素位置选择合理的深度细化范围。此外，考虑到匹配噪声可能导致概率体积呈现为模糊的单峰峰，我们设计了概率体积分裂合并模块（PVS-PVM）。该模块根据条件约束进行峰值搜索，将概率体积分为主概率体积和子概率体积，然后从中计算两组深度假设。最后，根据这些深度假设计算新的主概率体积和子概率体积，并合并以预测深度。这种方法可以更全面地考虑概率较高的区域，从而提高深度假设的稳健性。实验结果表明，我们的方法有效地利用了概率体积信息来指导深度图细化，并在 DTU 和 Tanks &amp; Temples 数据集上获得了增强的重建结果。我们的代码将在 https://github.com/zongh5a/ProbMVSNet 发布。]]></description>
      <guid>http://link.springer.com/10.1007/s10489-025-06284-w</guid>
      <pubDate>Mon, 03 Feb 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于高效边缘保留深度估计的上下文感知多视图立体网络</title>
      <link>http://link.springer.com/10.1007/s11263-024-02337-8</link>
      <description><![CDATA[摘要
近年来，基于学习的多视角立体方法通过采用由粗到细的深度估计框架取得了长足的进步。然而，现有的方法在恢复无特征区域、物体边界和薄结构的深度方面仍然遇到困难，这主要是由于低纹理区域中匹配线索的可区分性差、用于成本体积正则化的 3D 卷积神经网络固有的平滑特性以及最粗尺度特征的信息丢失。为了解决这些问题，我们提出了一种上下文感知的多视角立体网络 (CANet)，利用图像中的上下文线索来实现有效的边缘保留深度估计。引入的自相似性参与成本聚合模块利用参考视图中的结构自相似性信息在成本体积中执行远程依赖关系建模，从而提高无特征区域的可匹配性。随后利用参考视图中的上下文信息通过提出的分层边缘保留残差学习模块逐步细化多尺度深度估计，从而实现边缘处的精细深度估计。为了丰富最粗尺度的特征，使其更加关注精细区域，提出了一个焦点选择模块，该模块可以增强初始深度的恢复，并具有更精细的细节，例如薄结构。通过将上述策略集成到精心设计的轻量级级联框架中，CANet 实现了卓越的性能和效率权衡。大量实验表明，所提出的方法具有快速的推理速度和较低的内存使用率，实现了最先进的性能。值得注意的是，CANet 在具有挑战性的 Tanks and Temples 高级数据集和 ETH3D 高分辨率基准上排名第一，在所有已发表的基于学习的方法中。]]></description>
      <guid>http://link.springer.com/10.1007/s11263-024-02337-8</guid>
      <pubDate>Tue, 07 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SS-BEV：基于多尺度空间结构理解的多摄像头 BEV 物体检测</title>
      <link>http://link.springer.com/10.1007/s11760-024-03762-5</link>
      <description><![CDATA[摘要
基于多摄像头的鸟瞰 (BEV) 物体检测已成为自动驾驶领域的主流范式。然而，我们发现许多现有方法在检测极大或极小的物体时表现不佳。为了解决这个问题，本文提出了一种称为 SS-BEV 的多摄像头 BEV 检测新框架。首先，我们设计了一个更具表现力的特征提取模块 A-WBFP，它使用级联方法将并行空洞卷积和加权双向特征金字塔块集成到主干网络中。这有助于防止更深的网络层中小物体信息的丢失并增强感受野，从而生成富含上下文信息的特征图。然后我们引入了 MORD 模块，它利用来自雷达点云的精确深度信息来提高模型对大物体和小物体的空间结构理解。通过学习物体内部结构与所选参考点之间的相对深度，构建相应的损失函数来监督最终的检测性能。SS-BEV 在具有挑战性的 nuScenes 验证集上的表现优于基线模型，NDS 检测分数提高了 2.1 分。在 nuScenes 测试集上，它对障碍物和工程车辆的检测准确率分别达到 66.1% 和 22.3% mAP，超越了一些基于多摄像头和 LiDAR 融合的方法。]]></description>
      <guid>http://link.springer.com/10.1007/s11760-024-03762-5</guid>
      <pubDate>Thu, 02 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>全光成像和处理</title>
      <link>http://link.springer.com/10.1007/978-981-97-6915-5</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-981-97-6915-5</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>迈向大规模全光重建</title>
      <link>http://link.springer.com/10.1007/978-981-97-6915-5_5</link>
      <description><![CDATA[摘要
重建具有无与伦比的真实感和细节水平的真实世界场景一直是计算机视觉和图形领域的长期目标。实现这一目标需要在传感技术和全光重建算法方面进行协调努力。]]></description>
      <guid>http://link.springer.com/10.1007/978-981-97-6915-5_5</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉 – ECCV 2024</title>
      <link>http://link.springer.com/10.1007/978-3-031-73036-8</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-73036-8</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多视角立体视觉的特征分布归一化网络</title>
      <link>http://link.springer.com/10.1007/s00371-024-03334-1</link>
      <description><![CDATA[摘要
作为三维重建的关键技术，多视角立体视觉（MVS）的研究随着深度学习的发展取得了重大进展。然而，由于不同视角之间的特征分布不一致，MVS 面临着挑战。这一现象被认为是 MVS 的一个重要瓶颈。为了克服这一限制，我们提出了一种称为特征分布归一化网络（FDN-MVS）的创新方法。为了减轻由不一致的相对姿势引起的误差，我们利用多个视图之间的单应性变换并提出分布残差细化（DRR）模块。扩大尺度、残差连接和坐标系固定可以实现更有意义的特征分布学习。为了进一步缩小不同视图之间的差异，我们建议进行特征归一化，将特征的计算限制在一个共同的分布内。实验结果证明了我们方法的有效性。与常用的基准算法 CasMVSNet 相比，我们在 DTU 数据集上将误差降低了 13.52%，在 Tanks and Temples 数据集上将 F 得分提高了 18.77%。代码可在 https://github.com/ZYangChen/FDN-MVS 获取。]]></description>
      <guid>http://link.springer.com/10.1007/s00371-024-03334-1</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>模式识别</title>
      <link>http://link.springer.com/10.1007/978-3-031-78456-9</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-78456-9</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉 – ECCV 2024</title>
      <link>http://link.springer.com/10.1007/978-3-031-73030-6</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-73030-6</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>平滑度、合成和采样：重新思考带有 DIV 损失的无监督多视角立体视觉</title>
      <link>http://link.springer.com/10.1007/978-3-031-73036-8_22</link>
      <description><![CDATA[摘要
尽管无监督多视图立体 (MVS) 取得了重大进展，但核心损失公式自推出以来基本保持不变。然而，我们确定了这种核心损失的基本限制，并提出了三项重大变化，以改进深度先验、遮挡和视图相关效应的建模。首先，我们使用限制深度平滑度约束消除了预测深度图中突出的阶梯状和边缘伪影。其次，我们提出了一种学习视图合成方法来生成光度损失的图像，避免使用手工编码的启发式方法来处理视图相关效应。最后，除了用作 MVS 输入的视图之外，我们还对其他视图进行采样以进行监督，挑战网络预测与未见视图相匹配的深度。这些贡献共同形成了一种改进的监督策略，我们称之为 DIV 损失。我们的 DIV 损失的主要优势在于它可以轻松地放入现有的无监督 MVS 训练流程中，从而显著提高竞争性重建基准，并以最小的训练成本显著提高对象边界周围的定性性能。]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-73036-8_22</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>模式识别与计算机视觉</title>
      <link>http://link.springer.com/10.1007/978-981-97-8508-7</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-981-97-8508-7</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于多视图立体匹配的可视性感知像素视图选择</title>
      <link>http://link.springer.com/10.1007/978-3-031-78456-9_9</link>
      <description><![CDATA[摘要
基于 PatchMatch 的多视图立体算法的性能在很大程度上受到用于匹配成本计算的所选源视图的影响。现有方法通常以相当临时的方式检测遮挡，这可能会对计算产生负面影响。相比之下，我们的论文介绍了一种刻意模拟视图可见性的创新方法。我们提出了一种新颖的可见性引导像素视图选择方案，该方案使用来自经过验证的解决方案的可见性信息逐步细化参考视图中每个像素的源视图集。此外，利用人工多蜂群 (AMBC) 算法并行搜索不同像素的最优解决方案。为了确保相邻像素的平滑度并更好地管理无纹理区域，将奖励分配给来自经过验证的来源的解决方案。我们的方法通过对两个数据集的实验进行了验证，提高了遮挡和低纹理区域的细节恢复能力，在要求苛刻的场景中表现出了显著的性能。我们的实现可在https://github.com/Ricky-S/Visibility-Aware-Pixelwise-View-Selection上找到。]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-78456-9_9</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉 – ECCV 2024</title>
      <link>http://link.springer.com/10.1007/978-3-031-73220-1</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-73220-1</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有自适应特征提取和区域感知深度预测的多视图深度估计</title>
      <link>http://link.springer.com/10.1007/978-981-97-8508-7_3</link>
      <description><![CDATA[摘要
多视角深度估计是三维重建的一项重要任务，旨在通过多视角立体 (MVS) 技术从多视角图像中获取深度图。然而，当输入图像包含具有遮挡和低纹理的具有挑战性的区域时，现有的 MVS 方法可能无法获得良好的效果。为了解决这些问题，本文提出了一种多视角深度估计框架，该框架包括自适应特征提取和区域感知深度预测。为了获得更好的像素特征匹配，使用基于 CNN 的自适应局部特征提取器 (ALFE) 和基于 Transformer 的全局特征提取器 (GFE) 构建自适应特征提取，以捕获具有挑战性区域中的代表性和稳健特征。为了获得更好的深度图输出，使用区域感知深度细化模块 (RA-DRM) 构建区域感知深度预测，该模块在不同区域提取的特征的指导下迭代细化深度图。在DTU和BlendedMVS数据集上进行了定性和定量实验，结果表明了所提出的ALFE、GFE和RA-DRM模块的有效性，比较结果表明基于学习的MVS方法优于相关方法。]]></description>
      <guid>http://link.springer.com/10.1007/978-981-97-8508-7_3</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉 – ECCV 2024</title>
      <link>http://link.springer.com/10.1007/978-3-031-72630-9</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-72630-9</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>