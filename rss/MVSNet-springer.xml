<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最新结果</title>
    <link>http://link.springer.com</link>
    <description>Springer 提供的最新内容</description>
    <lastBuildDate>Wed, 02 Oct 2024 15:14:43 GMT</lastBuildDate>
    <item>
      <title>计算机视觉 – ECCV 2024</title>
      <link>http://link.springer.com/10.1007/978-3-031-72649-1</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-72649-1</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉 – ECCV 2024</title>
      <link>http://link.springer.com/10.1007/978-3-031-72670-5</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-72670-5</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉 – ECCV 2024</title>
      <link>http://link.springer.com/10.1007/978-3-031-72658-3</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-72658-3</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉 – ECCV 2024</title>
      <link>http://link.springer.com/10.1007/978-3-031-72998-0</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-72998-0</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉 – ECCV 2024</title>
      <link>http://link.springer.com/10.1007/978-3-031-72761-0</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-72761-0</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉 – ECCV 2024</title>
      <link>http://link.springer.com/10.1007/978-3-031-72933-1</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-72933-1</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MVSGaussian：基于多视角立体图像的快速通用高斯溅射重建</title>
      <link>http://link.springer.com/10.1007/978-3-031-72649-1_3</link>
      <description><![CDATA[摘要
我们提出了 MVSSagaussian，这是一种源自多视图立体 (MVS) 的新型可泛化 3D 高斯表示方法，可以有效地重建未见过的场景。具体而言，1) 我们利用 MVS 对几何感知的高斯表示进行编码并将其解码为高斯参数。2) 为了进一步提高性能，我们提出了一种混合高斯渲染，该渲染集成了高效的体积渲染设计以实现新颖的视图合成。3) 为了支持针对特定场景的快速微调，我们引入了一种多视图几何一致聚合策略来有效聚合由可泛化模型生成的点云，作为每个场景优化的初始化。与以前基于 NeRF 的可泛化方法相比，MVSSagaussian 可以实现实时渲染，并且每个场景的合成质量更好。以前的基于 NeRF 的可泛化方法通常需要几分钟的微调和几秒的每张图像渲染时间。与 vanilla 3D-GS 相比，MVSGaussian 以更少的训练计算成本实现了更好的视图合成。在 DTU、Real Forward-facing、NeRF Synthetic 和 Tanks and Temples 数据集上进行的大量实验验证了 MVSGaussian 以令人信服的通用性、实时渲染速度和快速的场景优化实现了最先进的性能。
]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-72649-1_3</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CaesarNeRF：针对小样本通用神经渲染的校准语义表征</title>
      <link>http://link.springer.com/10.1007/978-3-031-72658-3_5</link>
      <description><![CDATA[摘要
通用性和小样本学习是神经辐射场 (NeRF) 中的关键挑战，这通常是由于缺乏对像素级渲染的整体理解。我们引入了 CaesarNeRF，这是一种端到端方法，它利用场景级CAlibratEd SemAntic R表示以及像素级表示来推进小样本、可通用的神经渲染，在不损害高质量细节的情况下促进整体理解。CaesarNeRF 明确地模拟了参考视图的差异以结合场景级语义表示，从而提供经过校准的整体理解。此校准过程将各种视点与精确位置对齐，并通过连续细化来进一步增强以捕捉不同的细节。在公共数据集（包括 LLFF、Shiny、mip-NeRF 360 和 MVImgNet）上进行的大量实验表明，CaesarNeRF 在不同数量的参考视图中均能提供最先进的性能，即使使用单个参考图像也能证明其有效性。]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-72658-3_5</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于深度学习的高效精准三维实景重建框架</title>
      <link>http://link.springer.com/10.1007/s41870-024-02066-8</link>
      <description><![CDATA[摘要
通常，可以借助用于 3D 地理可视化和场景分析的几种照片成像技术来重建 3D 场景。然而，重建场景所涉及的方法并不准确且耗时。因此，本文提出了一种基于深度学习的多视图立体 (MVS) 图像 3D 重建框架。所提出的多视图聚合匹配网络 (MVAMN) 包括数据集的预处理、使用卷积神经网络 (CNN) 进行多尺度特征提取、成本体积正则化和深度图推理的细化，以处理更具挑战性的情况，包括相互遮挡、倾斜照片中的大深度变化和实质性的视点变化。细化阶段涉及从初始深度图和细化深度图计算概率分布。精修完成后，计算损失函数来量化初始深度图和精修深度图之间的误差，从而优化和提高所提框架的性能。最后，通过考虑开源图像数据集，验证了该模型的性能并与现有的深度学习 MVS 方法进行了比较。结果表明，较低的内存利用率和较高的效率使得所提模型非常适合大规模高分辨率航空影像的三维表面重建任务。]]></description>
      <guid>http://link.springer.com/10.1007/s41870-024-02066-8</guid>
      <pubDate>Tue, 01 Oct 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLR-MVSNet：用于低纹理场景重建的轻量级网络</title>
      <link>http://link.springer.com/10.1007/s00530-024-01464-z</link>
      <description><![CDATA[摘要
近年来，基于学习的MVS方法相对于传统方法取得了优异的性能，但这些方法仍然存在明显的不足，如传统卷积网络效率低、特征融合简单导致重建不完整等。本研究提出了一种用于低纹理场景重建的轻量级网络（LLR-MVSNet）。为了提高精度和效率，提出了一种轻量级网络，包括多尺度特征提取模块和加权特征融合模块。多尺度特征提取模块采用深度可分离卷积和逐点卷积替代传统卷积，可以减少网络参数，提高模型效率。为了提高融合精度，提出了加权特征融合模块，可以选择性地强调特征，抑制无用信息，提高融合精度。凭借快速的计算速度和高性能，我们的方法超越了最先进的基准，并在 DTU 和 Tanks &amp; Temples 数据集上表现良好。我们方法的代码将在 https://github.com/wln19/LLR-MVSNet 上提供。]]></description>
      <guid>http://link.springer.com/10.1007/s00530-024-01464-z</guid>
      <pubDate>Mon, 09 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DAR-MVSNet：一种用于多视角立体视觉的新型双重注意残差网络</title>
      <link>http://link.springer.com/10.1007/s11760-024-03276-0</link>
      <description><![CDATA[摘要
基于学习的多视角立体视觉（MVS）在三维重建领域展现出巨大的应用前景。然而，现有的MVS方法在特征学习过程中，存在感受野大小固定的问题，导致信息丢失，影响对场景几何结构的理解，对具有复杂几何结构和光照条件的区域的重建质量构成挑战。因此，我们提出了DAR-MVSNet，它由双注意引导的特征金字塔网络（DA-FPN）和3D残差U-net模块（3D-RUM）组成。DA-FPN包括两个模块：基于注意的上下文提取模块（ACEM）和基于自注意的模块（SAM）。ACEM通过多个扩张卷积、空间和通道注意来扩张感受野并初步过滤深度特征。为了进一步消除冗余特征，提出SAM来增强深度特征的表示能力。此外，3D-RUM旨在增强特征传递和信息流动，从而解决严重的全局特征信息丢失问题。本文通过大量实验证明了DAR-MVSNet的有效性。在DTU数据集和Tanks and Temples基准上的结果与最先进的MVS方法的比较证明了其优越的性能。
]]></description>
      <guid>http://link.springer.com/10.1007/s11760-024-03276-0</guid>
      <pubDate>Sun, 01 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有自适应相关成本体积的多视角立体增强特征金字塔</title>
      <link>http://link.springer.com/10.1007/s10489-024-05574-z</link>
      <description><![CDATA[
摘要
多级特征通常用于级联网络，这是目前多视角立体视觉 (MVS) 中的主导框架。然而，最近流行的多级特征提取器网络存在一个潜在问题，即忽略了细粒度​​结构特征对于 MVS 任务中粗深度推断的重要性。判别性结构特征在匹配中起着重要作用，有助于提高深度推断的性能。在本文中，我们提出了一种有效的级联结构 MVS 模型 FANet，其中构建了一个增强特征金字塔，旨在预测可靠的初始深度值。具体而言，通过自下而上的特征增强路径，使用浅层中丰富的空间结构信息增强深层特征。对于增强的最顶层特征，还采用了注意机制来抑制冗余信息并选择重要特征进行后续匹配。为了保证整个模型的轻量级和最优性能，构建了一个高效的模块，通过利用平均相似度度量来计算参考视图和源视图之间的特征相关性，然后自适应地将它们聚合成统一的相关性成本量，构建一个轻量级有效的成本量，可靠地表示视点对应关系。在 DTU 和 Tanks &amp;Temple 基准上进行的大量定量和定性比较表明，所提出的模型比最先进的 MVS 方法表现出更好的重建质量。


图形摘要





]]></description>
      <guid>http://link.springer.com/10.1007/s10489-024-05574-z</guid>
      <pubDate>Sun, 01 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>智能航空航天感知的计算机视觉任务：概述</title>
      <link>http://link.springer.com/10.1007/s11431-024-2714-4</link>
      <description><![CDATA[摘要
计算机视觉任务对于航空航天任务至关重要，因为它们可以帮助航天器理解和解释太空环境，例如估计位置和方向、重建 3D 模型和识别物体，这些任务已经得到广泛研究，可以成功执行任务。然而，卡尔曼滤波、运动结构和多视角立体成像等传统方法不够稳健，无法处理恶劣条件，导致结果不可靠。近年来，基于深度学习 (DL) 的感知技术显示出巨大的潜力，并且优于传统方法，特别是在对不断变化的环境的稳健性方面。为了进一步推进基于 DL 的航空航天感知，已经提出了各种框架、数据集和策略，表明未来应用具有巨大的潜力。在本次调查中，我们旨在探索感知任务中使用的有前景的技术，并强调基于 DL 的航空航天感知的重要性。我们首先概述航天感知，包括近年来开发的经典航天计划、常用的传感器和传统的感知方法。随后，我们深入研究航天任务中的三个基本感知任务：姿态估计、三维重建和识别，因为它们对于后续的决策和控制至关重要。最后，我们讨论了当前研究的局限性和可能性，并对未来的发展进行了展望，包括使用有限数据集的挑战、改进算法的必要性以及多源信息融合的潜在好处。]]></description>
      <guid>http://link.springer.com/10.1007/s11431-024-2714-4</guid>
      <pubDate>Sun, 01 Sep 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CNN 在 VR/AR/MR/XR 中的应用：系统文献综述</title>
      <link>http://link.springer.com/10.1007/s10055-024-01044-6</link>
      <description><![CDATA[摘要
本研究对卷积神经网络在虚拟现实、增强现实、混合现实和扩展现实技术中的应用进行了系统的文献综述。我们将这些应用分为三个主要类别：交互，其中网络通过虚拟和增强设置扩大用户参与度；创作，展示网络协助生成高质量视觉表现的能力；执行，强调应用程序在不同设备和情况下的优化和适应性。本研究为沉浸式技术领域的学者、研究人员和专业人士提供了全面的指南，为这些现实中跨学科的网络应用领域提供了深刻见解。此外，我们强调了有关这些现实及其与神经网络交集的显著贡献。]]></description>
      <guid>http://link.springer.com/10.1007/s10055-024-01044-6</guid>
      <pubDate>Fri, 30 Aug 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PIDSNeRF：姿势插值深度监督神经辐射场，用于从具有挑战性的输入中进行视图合成</title>
      <link>http://link.springer.com/10.1007/s11042-024-19978-z</link>
      <description><![CDATA[摘要
最近，神经辐射场（NeRF）在通过多视图进行新视图合成的任务中表现出色。本研究引入了一种先进的优化框架，称为姿势插值深度监督神经辐射场（PIDSNeRF），旨在解决 NeRF 在新视图合成中遇到的挑战。这些挑战表现为伪影、纹理细节丢失和几何不一致，特别是在以不同的光照条件、无纹理区域和稀疏图像为输入的场景中。PIDSNeRF 的原理涉及基于已知相机在 3D 空间域中插值虚拟相机位置，随后，将相机姿势估计期间形成的稀疏点云重新投影到这些虚拟姿势上，采用反角度加权策略，从而生成深度监督射线。此外，我们提出了一种深度扩散方法，将深度监督的射线沿像素平面扩散，根据扩散距离形成具有不同方差的高斯函数。最后，通过优化每条射线沿线采样点的权重分布与相应高斯函数沿该射线的概率密度函数之间的 Kullback-Leibler（KL）散度来实现体绘制时的深度监督。上述过程可以更全面地优化与射线沿线采样点对齐的多分辨率网格特征。在DTU、LLFF和DL3DV-10K数据集上的实验表明，PIDSNeRF可以在几分钟内合成完整的新颖视图，并且合成图像的各项指标都达到了最优性能。]]></description>
      <guid>http://link.springer.com/10.1007/s11042-024-19978-z</guid>
      <pubDate>Tue, 06 Aug 2024 00:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>