<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最新结果</title>
    <link>http://link.springer.com</link>
    <description>Springer 提供的最新内容</description>
    <lastBuildDate>Fri, 19 Jul 2024 09:13:03 GMT</lastBuildDate>
    <item>
      <title>基于深度学习的高效精准三维实景重建框架</title>
      <link>http://link.springer.com/10.1007/s41870-024-02066-8</link>
      <description><![CDATA[摘要
通常，可以借助用于 3D 地理可视化和场景分析的几种照片成像技术来重建 3D 场景。然而，重建场景所涉及的方法并不准确且耗时。因此，本文提出了一种基于深度学习的多视图立体 (MVS) 图像 3D 重建框架。所提出的多视图聚合匹配网络 (MVAMN) 包括数据集的预处理、使用卷积神经网络 (CNN) 进行多尺度特征提取、成本体积正则化和深度图推理的细化，以处理更具挑战性的情况，包括相互遮挡、倾斜照片中的大深度变化和实质性的视点变化。细化阶段涉及从初始深度图和细化深度图计算概率分布。精修完成后，计算损失函数来量化初始深度图和精修深度图之间的误差，从而优化和提高所提框架的性能。最后，通过考虑开源图像数据集，验证了该模型的性能并与现有的深度学习 MVS 方法进行了比较。结果表明，较低的内存利用率和较高的效率使得所提模型非常适合大规模高分辨率航空影像的三维表面重建任务。]]></description>
      <guid>http://link.springer.com/10.1007/s41870-024-02066-8</guid>
      <pubDate>Thu, 18 Jul 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SG-NeRF：用于新视图合成的稀疏输入广义神经辐射场</title>
      <link>http://link.springer.com/10.1007/s11390-024-4157-6</link>
      <description><![CDATA[摘要
用于渲染新视图的传统神经辐射场需要密集的输入图像和场景前优化，这限制了它们的实际应用。我们提出了一种从输入图像推断场景并进行高质量渲染的泛化方法，称为SG-NeRF（稀疏输入广义神经辐射场），无需场景前优化。首先，我们基于卷积注意和多级融合机制构建改进的多视图立体结构，以从稀疏输入图像中获得场景的几何特征和外观特征，然后通过多头注意将这些特征聚合为神经辐射场的输入。这种利用神经辐射场解码场景特征而不是映射位置和方向的策略使我们的方法能够进行跨场景训练和推理，从而使神经辐射场能够泛化用于在看不见的场景上进行新视图合成。在DTU数据集上测试了泛化能力，在同等输入条件下，我们的PSNR（峰值信噪比）比基线方法提高了3.14。此外，如果场景中有密集的输入视图，通过进一步的细化训练，可以在短时间内将平均PSNR提高1.04，获得更高质量的渲染效果。]]></description>
      <guid>http://link.springer.com/10.1007/s11390-024-4157-6</guid>
      <pubDate>Wed, 26 Jun 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过改进的 MVSNet 增强多视图 3D 重建</title>
      <link>http://link.springer.com/10.1038/s41598-024-64805-y</link>
      <description><![CDATA[摘要
三维重建作为环境感知的关键组成部分，已广泛应用于多个领域，但现有技术在三维场景重建方面仍有进一步改进的潜力。我们提出了一种基于MVSNet网络架构的改进重建算法。为了从图像中获取更丰富的像素细节，我们建议部署一个集成残差框架的DE模块，以取代现行的特征提取机制。DE模块使用ECA-Net和空洞卷积来扩大感受野范围，通过残差结构进行特征拼接和融合，以保留原始图像的全局信息。此外，利用注意力机制细化了3D成本体积的正则化过程，增强了跨多尺度特征体积的信息整合，从而提高了深度估计精度。使用 DTU 数据集评估我们的模型时，结果显示网络的 3D 重建的完整性 (comp) 为 0.411 毫米，总体质量为 0.418 毫米。这一性能高于传统方法和其他基于深度学习的方法。此外，点云模型的视觉表示表现出显着的进步。在 Blended MVS 数据集上的试验表明我们的网络表现出值得称赞的泛化能力。]]></description>
      <guid>http://link.springer.com/10.1038/s41598-024-64805-y</guid>
      <pubDate>Wed, 19 Jun 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有自适应相关成本体积的多视角立体增强特征金字塔</title>
      <link>http://link.springer.com/10.1007/s10489-024-05574-z</link>
      <description><![CDATA[
摘要
多级特征通常用于级联网络，这是目前多视角立体视觉 (MVS) 中的主导框架。然而，最近流行的多级特征提取器网络存在一个潜在问题，即忽略了细粒度​​结构特征对于 MVS 任务中粗深度推断的重要性。判别性结构特征在匹配中起着重要作用，有助于提高深度推断的性能。在本文中，我们提出了一种有效的级联结构 MVS 模型 FANet，其中构建了一个增强特征金字塔，旨在预测可靠的初始深度值。具体而言，通过自下而上的特征增强路径，使用浅层中丰富的空间结构信息增强深层特征。对于增强的最顶层特征，还采用了注意机制来抑制冗余信息并选择重要特征进行后续匹配。为了保证整个模型的轻量级和最优性能，构建了一个高效的模块，通过利用平均相似度度量来计算参考视图和源视图之间的特征相关性，然后自适应地将它们聚合成统一的相关性成本量，构建一个轻量级有效的成本量，可靠地表示视点对应关系。在 DTU 和 Tanks &amp;Temple 基准上进行的大量定量和定性比较表明，所提出的模型比最先进的 MVS 方法表现出更好的重建质量。


图形摘要





]]></description>
      <guid>http://link.springer.com/10.1007/s10489-024-05574-z</guid>
      <pubDate>Sat, 15 Jun 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DepthGAN：基于 GAN 的语义布局深度生成</title>
      <link>http://link.springer.com/10.1007/s41095-023-0350-8</link>
      <description><![CDATA[摘要
现有的基于 GAN 的生成方法通常用于语义图像合成。我们提出了基于 GAN 的架构是否可以生成可信深度图的问题，并发现现有方法由于缺乏全局几何相关性而难以生成合理表示 3D 场景结构的深度图。因此，我们提出了 DepthGAN，这是一种使用语义布局作为输入来生成深度图的新方法，以辅助构建和操作结构良好的 3D 场景点云。具体而言，我们首先构建一个具有级联语义感知转换器块的特征生成模型，以获得具有全局结构信息的深度特征。对于我们的语义感知转换器块，我们提出了一个混合注意力模块和一个语义感知层规范化模块，以更好地利用语义一致性来生成深度特征。此外，我们提出了一种新颖的语义加权深度合成模块，它为当前场景生成自适应深度间隔。我们通过使用不同深度范围的语义感知深度权重的加权组合来生成最终的深度图。通过这种方式，我们获得了更准确的深度图。在室内和室外数据集上进行的大量实验表明，DepthGAN 在深度生成任务中在数量和视觉上都取得了优异的结果。




]]></description>
      <guid>http://link.springer.com/10.1007/s41095-023-0350-8</guid>
      <pubDate>Sat, 01 Jun 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于多尺度哈希编码的神经几何表示</title>
      <link>http://link.springer.com/10.1007/s41095-023-0340-x</link>
      <description><![CDATA[摘要
最近，基于神经隐函数的表示方法受到越来越多的关注，并被广泛用于使用可微分神经网络表示曲面。然而，使用现有的神经几何表示方法从点云或多视图图像进行曲面重建仍然面临计算速度慢、精度差的问题。为了缓解这些问题，我们提出了一种基于多尺度哈希编码的神经几何表示方法，该方法可以有效、高效地将曲面表示为有符号距离场。我们新颖的神经网络结构将低频傅里叶位置编码与多尺度哈希编码巧妙地结合起来。相应地重新设计了几何网络的初始化和渲染模块的几何特征。我们的实验表明，对于具有数百万点的点云，所提出的表示方法至少快 10 倍。它还显著提高了多视图重建的速度和准确性。我们的代码和模型可在 https://github.com/Dengzhi-USTC/Neural-Geometry-Reconstruction 上找到。




]]></description>
      <guid>http://link.springer.com/10.1007/s41095-023-0340-x</guid>
      <pubDate>Sat, 01 Jun 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DAR-MVSNet：一种用于多视角立体视觉的新型双重注意残差网络</title>
      <link>http://link.springer.com/10.1007/s11760-024-03276-0</link>
      <description><![CDATA[摘要
基于学习的多视角立体视觉（MVS）在三维重建领域展现出巨大的应用前景。然而，现有的MVS方法在特征学习过程中，存在感受野大小固定的问题，导致信息丢失，影响对场景几何结构的理解，对具有复杂几何结构和光照条件的区域的重建质量构成挑战。因此，我们提出了DAR-MVSNet，它由双注意引导的特征金字塔网络（DA-FPN）和3D残差U-net模块（3D-RUM）组成。DA-FPN包括两个模块：基于注意的上下文提取模块（ACEM）和基于自注意的模块（SAM）。ACEM通过多个扩张卷积、空间和通道注意来扩张感受野并初步过滤深度特征。为了进一步消除冗余特征，提出SAM来增强深度特征的表示能力。此外，3D-RUM旨在增强特征传递和信息流动，从而解决严重的全局特征信息丢失问题。本文通过大量实验证明了DAR-MVSNet的有效性。在DTU数据集和Tanks and Temples基准上的结果与最先进的MVS方法的比较证明了其优越的性能。
]]></description>
      <guid>http://link.springer.com/10.1007/s11760-024-03276-0</guid>
      <pubDate>Fri, 31 May 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于神经辐射场的多视角内窥镜场景重建用于手术模拟</title>
      <link>http://link.springer.com/10.1007/s11548-024-03080-8</link>
      <description><![CDATA[摘要

目的
在虚拟手术中，由 CT 图像构建的 3D 模型外观缺乏真实感，导致住院医生产生误解。因此，使用内窥镜捕获的多视图图像重建逼真的内窥镜场景至关重要。


方法
我们提出了一种 Endoscope-NeRF 网络，用于非固定光源下内窥镜场景的隐式辐射场重建，并使用体积渲染合成新视图。具有多个 MLP 网络和射线变换器网络的内窥镜-NeRF 网络将内窥镜场景表示为具有连续 5D 矢量（3D 位置和 2D 方向）的颜色和体积密度的隐式场函数。通过使用体积渲染聚合目标相机每条射线上的所有采样点，获得最终合成图像。我们的方法考虑了从光源到采样点的距离对场景辐射度的影响。


结果
我们的网络在我们的设备采集的猪的肺、肝、肾和心脏上得到了验证。结果表明，我们的方法合成的内窥镜场景新视图在 PSNR、SSIM 和 LPIPS 指标方面优于现有方法（NeRF 和 IBRNet）。


结论
我们的网络可以有效地学习具有泛化能力的辐射场函数。在新的内窥镜场景上对预训练模型进行微调，以进一步优化场景的神经辐射场，从而可以为手术模拟提供更逼真的高分辨率渲染图像。
]]></description>
      <guid>http://link.springer.com/10.1007/s11548-024-03080-8</guid>
      <pubDate>Wed, 01 May 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CT-MVSNet：带有 Transformer 的曲率引导多视角立体视觉</title>
      <link>http://link.springer.com/10.1007/s11042-024-19227-3</link>
      <description><![CDATA[摘要
多视角立体视觉（MVS）可以从一组多视角图像中实现密集的三维重建。尽管基于深度学习的 MVS 显著提高了重建性能，但重建精度和完整性仍需要改进才能满足三维内容生成实际应用的需求。因此，提出了一种新的 MVS 方法，即带变换器的曲率引导多视角立体视觉。通过探索视角间关系并利用表面曲率测量感受野的大小和图像表面上的特征信息，该方法可以适应各种候选曲率尺度，以自适应地提取更详细的特征，从而实现精确的成本计算。此外，提出了一种基于变换器的特征匹配网络，以更好地识别视角间相似性并提高特征匹配精度。此外，基于特征匹配的相似度测量模块将曲率和视角间相似度测量紧密结合，以进一步提高重建精度。在DTU数据集和Tanks and Temples数据集上的实验验证了所提方法的有效性。]]></description>
      <guid>http://link.springer.com/10.1007/s11042-024-19227-3</guid>
      <pubDate>Tue, 23 Apr 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多视角立体视觉的特征分布归一化网络</title>
      <link>http://link.springer.com/10.1007/s00371-024-03334-1</link>
      <description><![CDATA[摘要
作为三维重建的关键技术，多视角立体视觉（MVS）的研究随着深度学习的发展取得了重大进展。然而，由于不同视角之间的特征分布不一致，MVS 面临着挑战。这一现象被认为是 MVS 的一个重要瓶颈。为了克服这一限制，我们提出了一种称为特征分布归一化网络（FDN-MVS）的创新方法。为了减轻由不一致的相对姿势引起的误差，我们利用多个视图之间的单应性变换并提出分布残差细化（DRR）模块。扩大尺度、残差连接和坐标系固定可以实现更有意义的特征分布学习。为了进一步缩小不同视图之间的差异，我们建议进行特征归一化，将特征的计算限制在一个共同的分布内。实验结果证明了我们方法的有效性。与常用的基准算法 CasMVSNet 相比，我们在 DTU 数据集上的误差降低了 13.52%，在 Tanks and Temples 数据集上的 F 得分提高了 18.77%。代码可在 https://github.com/ZYangChen/FDN-MVS 获取。]]></description>
      <guid>http://link.springer.com/10.1007/s00371-024-03334-1</guid>
      <pubDate>Wed, 17 Apr 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于城市场景新颖视图合成的多视角立体调节 NeRF</title>
      <link>http://link.springer.com/10.1007/s00371-024-03321-6</link>
      <description><![CDATA[摘要
神经辐射场 (NeRF) 将场景编码为神经表示，在单个物体和小空间区域上表现出令人印象深刻的新颖视图合成质量。然而，当面对城市户外环境时，NeRF 受到单个 MLP 容量和输入视图不足的限制，导致几何形状不正确，阻碍了逼真渲染的产生。在本文中，我们提出了 MVSRegNeRF，这是神经辐射场的扩展，专注于大规模自动驾驶场景。我们采用传统的基于面片匹配的多视图立体 (MVS) 方法来生成密集深度图，我们利用这些深度图来调节 NeRF 的几何优化。我们还将多分辨率哈希编码集成到我们的神经场景表示中，以加速训练过程。由于我们的方法具有相对精确的几何约束，我们在现实世界的大规模街道场景中实现了高质量的新颖视图合成。我们在 KITTI-360 数据集上进行的实验表明，MVSRegNeRF 在新颖视图外观合成任务中的表现优于最先进的方法。
]]></description>
      <guid>http://link.springer.com/10.1007/s00371-024-03321-6</guid>
      <pubDate>Wed, 27 Mar 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>BSI-MVS：具有双向语义信息的多视角立体网络</title>
      <link>http://link.springer.com/10.1038/s41598-024-55612-6</link>
      <description><![CDATA[摘要
多视角立体视觉（MVS）的基本原理是通过从多个视角提取深度信息来进行三维重建。目前大多数 SOTA MVS 网络都是基于 Vision Transformer 的，这通常意味着昂贵的计算复杂度。为了降低计算复杂度并提高深度图精度，我们提出了一种具有双向语义信息的 MVS 网络（BSI-MVS）。首先，我们设计了一个多级空间金字塔模块来生成多层特征图以提取多尺度信息。然后我们提出了一个 2D 双向-LSTM 模块来捕获水平和垂直方向上不同时间步骤的双向语义信息，其中包含丰富的深度信息。最后，基于各级特征图构建成本量以优化最终的深度图。我们在 DTU 和 BlendedMVS 数据集上进行了实验。结果表明，我们的网络在整体指标上分别超越TransMVSNet、CasMVSNet、CVP-MVSNet和AACVP-MVSNet 17.84%、36.42%、14.96%和4.86%，在客观指标和可视化方面也显示出明显的性能提升。]]></description>
      <guid>http://link.springer.com/10.1038/s41598-024-55612-6</guid>
      <pubDate>Thu, 21 Mar 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PLKA-MVSNet：具有大核卷积注意力机制的并行多视角立体视觉</title>
      <link>http://link.springer.com/10.1007/978-981-99-8145-8_10</link>
      <description><![CDATA[摘要
本文提出了PLKA-MVSNet，以解决基于学习的多视角立体（MVS）方法深度估计中剩余的挑战，特别是在低纹理区域、弱光照条件和非朗伯表面等具有挑战性的区域中深度估计不准确的问题。我们将这个问题归因于特征提取器性能不足和MVS管道传输造成的信息丢失，并给出了优化方案。具体来说，我们引入并行大核注意力（PLKA），通过使用多个小卷积代替单个大卷积来增强对纹理和结构信息的感知，这使我们能够捕获更大的感受野和长距离信息。为了适应由粗到细的MVS管道，我们使用PLKA构建了一个多阶段特征提取器。此外，我们提出了并行成本体积聚合（PCVA）来增强聚合成本体积的鲁棒性。它在二维维度上引入了两个决策注意点来弥补三维卷积压缩中的信息丢失和像素遗漏。特别地，我们的方法在DTU数据集上表现出超越基于Transformer的方法的最佳整体性能，并在具有挑战性的Tanks and Temples高级数据集上取得了最好的结果。
]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8145-8_10</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>增强级联 MVSNet 的三维植物重建</title>
      <link>http://link.springer.com/10.1007/978-981-99-8432-9_23</link>
      <description><![CDATA[摘要
三维重建是恢复植物形态结构的重要方法，完整准确的植物三维点云可以更好地反映植物的表型参数，如株高、植株体积、叶面积等。为了获得更加完整准确的植物点云，本文基于Cascade-MVSNet网络提出了一系列增强措施。为了提高植物弱纹理区域的重建效果，在特征提取阶段加入了轻量级的注意力机制。为了增强植物点云的有效点，抑制无效点，采用focal loss作为损失函数，将深度估计问题视为分类任务，以获得更准确的深度信息。此外，为了提高模型的效率，我们用深度可分离卷积代替标准卷积，在保持性能的同时减少了参数数量和计算复杂度。将上述工作应用到植物数据集上，重建结果表明无效点明显减少，重建点云更加清晰。除了对植物数据集进行研究外，我们还在公开的DTU数据集上评估了我们的方法。实验结果表明，在DTU数据集上重建的完整性明显提高，整体性能具有竞争力。]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8432-9_23</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CT-MVSNet：具有跨尺度变换器的高效多视角立体视觉</title>
      <link>http://link.springer.com/10.1007/978-3-031-53308-2_29</link>
      <description><![CDATA[摘要
最近的深度多视角立体 (MVS) 方法已广泛将变换器纳入级联网络以实现高分辨率深度估计，取得了令人印象深刻的效果。然而，现有的基于变换器的方法受到计算成本的限制，阻碍了它们扩展到更精细的阶段。在本文中，我们提出了一种新颖的跨尺度变换器 (CT)，它可以在不同阶段处理特征表示而无需额外的计算。具体来说，我们引入了一种自适应匹配感知变换器 (AMT)，它在多个尺度上采用不同的交互式注意组合。这种组合策略使我们的网络能够捕获图像内上下文信息并增强图像间特征关系。此外，我们提出了一种双特征引导聚合 (DFGA)，将粗略的全局语义信息嵌入到更精细的成本体积构造中，以进一步加强全局和局部特征感知。同时，我们设计了一个特征度量损失 (FM Loss)，用于评估变换前后的特征偏差，以减少特征不匹配对深度估计的影响。在 DTU 数据集和 Tanks and Temples (T&amp;T) 基准上进行的大量实验表明，我们的方法取得了最先进的结果。代码可在 https://github.com/wscstrive/CT-MVSNet. 获取。]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-53308-2_29</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>