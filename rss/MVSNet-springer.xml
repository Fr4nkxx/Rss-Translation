<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最新结果</title>
    <link>http://link.springer.com</link>
    <description>Springer 提供的最新内容</description>
    <lastBuildDate>Mon, 29 Jan 2024 03:11:30 GMT</lastBuildDate>
    <item>
      <title>PLKA-MVSNet：具有大型内核卷积注意力的并行多视图立体</title>
      <link>http://link.springer.com/10.1007/978-981-99-8145-8_10</link>
      <description><![CDATA[摘要
在本文中，我们提出 PLKA-MVSNet 来解决基于学习的多视图立体（MVS）方法的深度估计中剩余的挑战，特别是不准确的深度估计在具有挑战性的区域，例如低纹理区域、弱照明条件和非朗伯表面。我们将此问题归因于特征提取器性能不足以及MVS管道传输造成的信息丢失，并给出了我们的优化方案。具体来说，我们通过使用多个小卷积而不是单个大卷积引入并行大核注意（PLKA），以增强纹理和结构信息的感知，这使我们能够捕获更大的感受野和长距离信息。为了适应从粗到细的 MVS 流程，我们采用 PLKA 构建多级特征提取器。此外，我们提出并行成本量聚合（PCVA）来增强聚合成本量的鲁棒性。它在2D维度上引入了两个决策注意力，以弥补3D卷积压缩中的信息损失和像素遗漏。特别是，我们的方法在 DTU 数据集上显示出超越基于 Transformer 的方法的最佳整体性能，并在具有挑战性的 Tanks 和 Temples 高级数据集上取得了最佳结果。
      ]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8145-8_10</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用增强级联进行三维植物重建-MVSNet</title>
      <link>http://link.springer.com/10.1007/978-981-99-8432-9_23</link>
      <description><![CDATA[摘要
三维重建是恢复植物形态结构的重要方法，完整、准确的植物3D点云可以更好地反映植物的表型参数，如植物植物的高度、体积和叶面积。为了获得更加完整、准确的植物点云，本文在Cascade-MVSNet网络的基础上提出了一系列的增强。为了改善植物弱纹理区域的重建，在特征提取阶段引入了轻量级注意机制。为了增强植物点云的有效点并抑制无效点，采用焦点损失作为损失函数，将深度估计问题视为分类任务以获得更准确的深度信息。此外，为了提高模型的效率，我们用深度可分离卷积代替标准卷积，在保持性能的同时减少参数数量和计算复杂度。上述工作应用于植物数据集，重建显示无效点显着减少，重建点云更清晰。除了对植物数据集进行研究外，我们还在公开的 DTU 数据集上评估了我们的方法。实验结果表明，DTU 数据集的重建完整性显着提高，整体表现具有竞争力。]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8432-9_23</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CT-MVSNet：具有跨尺度变压器的高效多视图立体</title>
      <link>http://link.springer.com/10.1007/978-3-031-53308-2_29</link>
      <description><![CDATA[摘要
最近的深度多视图立体（MVS）方法已广泛将变压器纳入级联网络中以进行高分辨率深度估计，取得了令人印象深刻的结果。然而，现有的基于变压器的方法受到计算成本的限制，阻碍了它们扩展到更精细的阶段。在本文中，我们提出了一种新颖的跨尺度变换器（CT），它可以在不同阶段处理特征表示，而无需额外的计算。具体来说，我们引入了一种自适应匹配感知变压器（AMT），它在多个尺度上采用不同的交互式注意力组合。这种组合策略使我们的网络能够捕获图像内上下文信息并增强图像间特征关系。此外，我们提出了一种双特征引导聚合（DFGA），将粗略的全局语义信息嵌入到更精细的成本卷构造中，以进一步加强全局和局部特征感知。同时，我们设计了一个特征度量损失（FM Loss）来评估变换前后的特征偏差，以减少特征不匹配对深度估计的影响。对 DTU 数据集和 Tanks and Temples (T &amp;T) 基准的大量实验表明，我们的方法取得了最先进的结果。代码可在 https://github.com/wscstrive/CT-MVSNet 获取。]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-53308-2_29</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>模式识别和计算机视觉</title>
      <link>http://link.springer.com/10.1007/978-981-99-8432-9</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8432-9</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多媒体建模</title>
      <link>http://link.springer.com/10.1007/978-3-031-53308-2</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-53308-2</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>神经信息处理</title>
      <link>http://link.springer.com/10.1007/978-981-99-8145-8</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8145-8</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多媒体建模</title>
      <link>http://link.springer.com/10.1007/978-3-031-53311-2</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-53311-2</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于多视图立体和并行优化的自监督边缘结构学习</title>
      <link>http://link.springer.com/10.1007/978-3-031-53311-2_33</link>
      <description><![CDATA[摘要
最近的研究表明，许多自监督方法在多视图立体（MVS）方面取得了明显的进展。然而，现有方法忽略了重建目标的边缘结构信息，包括外部轮廓和内部结构的边缘信息。这可能导致重建结果的边缘和完整性不太令人满意。为了解决这个问题，我们提出了一种提取边缘结构图的提取器，并创新地设计了边缘结构Loss来约束网络更多地关注参考视图的边缘结构特征，以改善重建结果的纹理细节。特别地，我们利用多视图立体中构造成本量的思想，并将源视图的边缘结构图扭曲到参考视图，以提供可靠的自监督。此外，我们设计了一种结合局部和全局属性的掩蔽机制，保证了鲁棒性并提高了模型对复杂样本的重建完整性。此外，我们采用有效的并行加速方法来提高训练速度和重建效率。对 DTU 和 Tanks &amp;Temples 基准的大量实验表明，与其他无监督工作相比，我们的方法提高了准确性和完整性。此外，我们的并行方法在保证准确性的同时提高了效率。代码将被发布。]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-53311-2_33</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有分层几何约束的可推广神经辐射场</title>
      <link>http://link.springer.com/10.1007/978-981-99-8432-9_26</link>
      <description><![CDATA[摘要
广义神经辐射场模型的出现极大地扩展了新颖的视图合成任务的适用性。然而，现有的方法主要依赖从相邻视图获得的 2D 特征来辅助渲染，这通常会导致看不见的视点的退化。一些方法在模型中引入成本量来提供几何先验，但这种粗略的几何信息无法有效地约束模型，导致渲染图像具有丰富的伪影。在这项工作中，我们建议为模型提供分层几何约束，以实现更好的渲染结果。我们引入级联 MVSNet 来提供分层场景结构特征，用于推断底层场景几何形状。它限制了模型在看不见的视点下的渲染。此外，分层特征提供了有助于细节重建的精细表示。此外，我们利用级联 MVSNet 生成的分层深度图来约束采样过程，确保采样点集中在场景表面附近。这种采样策略过滤掉了大量无用的采样点，提高了采样效率和渲染质量。与之前的广义方法不同，我们采用 Neus 提出的新权重函数来消除密度场中的固有偏差，从而将颜色和密度的预测与带符号的距离场分开。大量实验表明，我们提出的方法显着提高了渲染质量，并且优于以前的方法。]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8432-9_26</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>神经信息处理</title>
      <link>http://link.springer.com/10.1007/978-981-99-8070-3</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8070-3</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计算机图形学的进展</title>
      <link>http://link.springer.com/10.1007/978-3-031-50072-5</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-50072-5</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过融合单目和深度表示方法组合的多视图立体</title>
      <link>http://link.springer.com/10.1007/978-981-99-8070-3_23</link>
      <description><![CDATA[摘要
平面扫描深度 MVS 的设计主要依赖于基于块相似度的匹配。然而，当处理场景中的低纹理、相似纹理和反射区域时，这种方法变得不切实际，导致匹配结果不准确。避免这种错误的方法之一是在匹配过程中结合语义信息。在本文中，我们提出了一种使用单目深度估计向深度 MVS 添加语义信息的端到端方法。此外，我们分析了两种主要深度表示的优缺点，并提出了一种协作方法来减轻它们的缺点。最后，我们引入了一种新颖的过滤准则“分布一致性”，它可以有效地过滤掉概率分布较差（例如均匀分布）的异常值，从而进一步提高重建质量。]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8070-3_23</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DSC-MVSNet：基于多视图立体深度可分离卷积的注意感知成本体积正则化</title>
      <link>http://link.springer.com/10.1007/s40747-023-01106-3</link>
      <description><![CDATA[摘要
深度学习最近已被证明可以在多视图立体 (MVS) 方面提供出色的性能。然而，基于深度学习的 MVS 方法很难平衡其效率和效果。为此，我们提出了 DSC-MVSNet，这是一种新颖的从粗到细和端到端的框架，用于在 MVS 中进行更高效、更准确的深度估计。特别是，我们提出了一种注意力感知 3D UNet 形状网络，它首先使用深度可分离卷积进行成本量正则化。该机制通过将代价量上的普通卷积转变为深度卷积和逐点卷积，实现了信息的有效聚合，并显着减少了模型参数和计算量。此外，还提出了3D-Attention模块来缓解成本量正则化中的特征不匹配问题，并在三个维度（即通道、空间和深度）聚合成本量的重要信息。此外，我们提出了一种高效的特征传输模块，可将低分辨率（LR）深度图上采样为高分辨率（HR）深度图，以实现更高的精度。在两个基准数据集（即 DTU 和 Tanks &amp;）上进行了大量实验。 Temples，我们证明模型的参数显着减少到 
\(25\%\)
 最先进的模型 MVSNet。此外，我们的方法优于或保持与最先进模型相同的准确性。我们的源代码位于 https://github.com/zs670980918/DSC-MVSNet.]]></description>
      <guid>http://link.springer.com/10.1007/s40747-023-01106-3</guid>
      <pubDate>Fri, 01 Dec 2023 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用几何先验从稀疏视图进行神经 3D 重建</title>
      <link>http://link.springer.com/10.1007/s41095-023-0337-5</link>
      <description><![CDATA[摘要
随着神经隐式 3D 表示的发展，稀疏视图 3D 重建引起了越来越多的关注。现有方法通常仅使用 2D 视图，需要一组密集的输入视图才能进行精确的 3D 重建。在本文中，我们证明了通过将几何先验融入神经隐式 3D 重建中可以实现精确的 3D 重建。我们的方法采用有符号距离函数作为 3D 表示，并从稀疏视图中学习可推广的 3D 表面重建模型。具体来说，我们通过使用相应的深度图从输入视图构建更有效和稀疏的特征量，深度图可以由深度传感器提供或直接从输入视图预测。在训练神经隐式 3D 表示时，除了颜色损失之外，我们还通过施加深度和表面法线约束来恢复更好的几何细节。实验表明，我们的方法不仅优于最先进的方法，而且具有良好的泛化性。




]]></description>
      <guid>http://link.springer.com/10.1007/s41095-023-0337-5</guid>
      <pubDate>Fri, 01 Dec 2023 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有点注意力的多视图立体网络</title>
      <link>http://link.springer.com/10.1007/s10489-023-04806-y</link>
      <description><![CDATA[摘要
近年来，基于学习的多视图立体（MVS）重建与传统方法相比取得了优越性。在本文中，我们介绍了一种新颖的点注意力网络，具有基于点云结构的注意力机制。在重建过程中，我们具有注意机制的方法可以引导网络更多地关注复杂区域，例如薄结构和低纹理表面。我们首先使用修改后的经典 MVS 深度框架推断粗略深度图，并将其转换为相应的点云。然后，我们将原始图像的高频特征和不同分辨率特征添加到点云中。最后，我们的网络通过注意力机制引导不同维度的点的权重分布，并迭代计算每个点的深度位移作为深度残差，将其添加到粗略深度预测中以获得最终的高分辨率深度图。实验结果表明，我们提出的点注意力架构可以在一些场景中在没有合理几何假设的情况下在DTU数据集和Tanks and Temples 数据集，表明我们的方法具有很强的泛化能力。
      ]]></description>
      <guid>http://link.springer.com/10.1007/s10489-023-04806-y</guid>
      <pubDate>Wed, 01 Nov 2023 00:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>