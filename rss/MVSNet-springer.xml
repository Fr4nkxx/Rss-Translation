<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最新结果</title>
    <link>http://link.springer.com</link>
    <description>Springer 提供的最新内容</description>
    <lastBuildDate>Thu, 13 Feb 2025 01:05:08 GMT</lastBuildDate>
    <item>
      <title>单次超分辨条纹投影轮廓术 (SSSR-FPP)：基于深度学习的 100,000 帧/秒 3D 成像</title>
      <link>http://link.springer.com/10.1038/s41377-024-01721-w</link>
      <description><![CDATA[摘要
为了揭示力学、物理学和生物学中各种瞬态事件背后隐藏的基本方面，人们长期以来一直在寻求以超快时间分辨率获取三维 (3D) 图像的能力。作为最常用的 3D 传感技术之一，条纹投影轮廓术 (FPP) 可以根据使用连续结构化照明拍摄的立体图像重建场景的深度。然而，当前 FPP 方法的成像速度通常限制在几 kHz，这受到投影仪-相机硬件以及相位检索和展开所需的条纹图案数量的限制。在这里，我们报告了一种基于学习的新型超快 3D 成像技术，称为单次超分辨 FPP (SSSR-FPP)，它能够以 100,000 Hz 的速度实现超快 3D 成像。 SSSR-FPP 仅使用一对低信噪比 (SNR)、低分辨率和像素化的条纹图案作为输入，而高分辨率的展开相位和条纹顺序可以用经过特定训练的深度神经网络进行解读。我们的方法利用了通过缩小传统高速摄像机的成像窗口而实现的显著速度增益，同时通过深度学习“再生”丢失的空间分辨率。为了展示 SSSR-FPP 的高时空分辨率，我们展示了几个瞬态场景的 3D 视频，包括旋转的涡扇叶片、爆炸的积木和蒸汽机的往复运动等，这些场景以前很难甚至不可能用传统方法捕捉到。实验结果证实 SSSR-FPP 是 3D 光学传感领域向前迈出的重要一步，为跨各个科学学科的广泛动态过程提供了新的见解。]]></description>
      <guid>http://link.springer.com/10.1038/s41377-024-01721-w</guid>
      <pubDate>Fri, 07 Feb 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>重新思考多视角立体的概率体积：一种概率分析方法</title>
      <link>http://link.springer.com/10.1007/s10489-025-06284-w</link>
      <description><![CDATA[摘要
现有的基于学习的多视角立体（MVS）模型主要侧重于通过级联结构预测深度图，以实现更为稳健的重建结果。然而，它们往往强调提高立体匹配的质量，而忽略了深度假设的重要性。本文从概率体积分析的角度提出了一种新的 MVS 模型。首先，考虑概率体积对深度细化的引导作用。理想情况下，概率体积沿深度维度的概率分布遵循单峰模式。我们设计了一条单峰曲线来拟合这种模式。然后，基于预定义的概率阈值，自适应地为每个像素位置选择合理的深度细化范围。此外，考虑到匹配噪声可能导致概率体积呈现为模糊的单峰峰，我们设计了概率体积分裂合并模块（PVS-PVM）。该模块根据条件约束进行峰值搜索，将概率体积分为主概率体积和子概率体积，然后从中计算两组深度假设。最后，根据这些深度假设计算新的主概率体积和子概率体积，并合并以预测深度。这种方法可以更全面地考虑概率较高的区域，从而提高深度假设的稳健性。实验结果表明，我们的方法有效地利用了概率体积信息来指导深度图细化，并在 DTU 和 Tanks &amp; Temples 数据集上获得了增强的重建结果。我们的代码将在 https://github.com/zongh5a/ProbMVSNet 发布。]]></description>
      <guid>http://link.springer.com/10.1007/s10489-025-06284-w</guid>
      <pubDate>Mon, 03 Feb 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于深度学习的多视角立体算法综述</title>
      <link>http://link.springer.com/10.1007/s11042-024-20464-9</link>
      <description><![CDATA[摘要
多视角立体视觉 (MVS) 是计算机视觉领域一项长期存在的基础性任务，旨在从一组重叠图像中重建场景的 3D 几何形状。在已知相机参数的情况下，MVS 会匹配图像中的像素以计算密集对应关系并恢复 3D 点。传统的 MVS 方法由于鲁棒性限制，在处理复杂场景时经常会遇到困难。然而，基于深度学习的 MVS 的最新进展已显示出显著的性能提升，这归功于其增强的特征提取和成本量处理能力。这项全面的调查深入研究了基于深度学习的 MVS 算法，强调了未来探索的主要挑战和途径。我们的重点是旨在减轻算法内存需求和提高处理速度的研究。为了更好地理解该领域的发展，我们提出了一种新的分类法。基于深度学习的 MVS 算法分为两类：面向场景的方法和面向视图的方法。面向场景的方法通常使用体素或神经隐式表示作为场景的代表形式，并直接推断完整场景。面向视图的方法专注于估计单个视图的深度图。每个类别都进行了详细的分类和评论。此外，我们介绍了几个广泛使用的数据集和指标，并在三个流行的数据集上对代表性算法进行了实证评估。本文最后讨论了 MVS 领域中普遍存在的挑战和未来有希望的研究方向。]]></description>
      <guid>http://link.springer.com/10.1007/s11042-024-20464-9</guid>
      <pubDate>Sat, 01 Feb 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于高效边缘保留深度估计的上下文感知多视图立体网络</title>
      <link>http://link.springer.com/10.1007/s11263-024-02337-8</link>
      <description><![CDATA[摘要
近年来，基于学习的多视角立体方法通过采用由粗到细的深度估计框架取得了长足的进步。然而，现有的方法在恢复无特征区域、物体边界和薄结构的深度方面仍然遇到困难，这主要是由于低纹理区域中匹配线索的可区分性差、用于成本体积正则化的 3D 卷积神经网络固有的平滑特性以及最粗尺度特征的信息丢失。为了解决这些问题，我们提出了一种上下文感知的多视角立体网络 (CANet)，利用图像中的上下文线索来实现有效的边缘保留深度估计。引入的自相似性参与成本聚合模块利用参考视图中的结构自相似性信息在成本体积中执行远程依赖关系建模，从而提高无特征区域的可匹配性。随后利用参考视图中的上下文信息通过提出的分层边缘保留残差学习模块逐步细化多尺度深度估计，从而实现边缘处的精细深度估计。为了丰富最粗尺度的特征，使其更加关注精细区域，提出了一个焦点选择模块，该模块可以增强初始深度的恢复，并具有更精细的细节，例如薄结构。通过将上述策略集成到精心设计的轻量级级联框架中，CANet 实现了卓越的性能和效率权衡。大量实验表明，所提出的方法具有快速的推理速度和较低的内存使用率，实现了最先进的性能。值得注意的是，CANet 在具有挑战性的 Tanks and Temples 高级数据集和 ETH3D 高分辨率基准上排名第一，在所有已发表的基于学习的方法中。]]></description>
      <guid>http://link.springer.com/10.1007/s11263-024-02337-8</guid>
      <pubDate>Tue, 07 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SS-BEV：基于多尺度空间结构理解的多摄像头 BEV 物体检测</title>
      <link>http://link.springer.com/10.1007/s11760-024-03762-5</link>
      <description><![CDATA[摘要
基于多摄像头的鸟瞰 (BEV) 物体检测已成为自动驾驶领域的主流范式。然而，我们发现许多现有方法在检测极大或极小的物体时表现不佳。为了解决这个问题，本文提出了一种称为 SS-BEV 的多摄像头 BEV 检测新框架。首先，我们设计了一个更具表现力的特征提取模块 A-WBFP，它使用级联方法将并行空洞卷积和加权双向特征金字塔块集成到主干网络中。这有助于防止更深的网络层中小物体信息的丢失并增强感受野，从而生成富含上下文信息的特征图。然后我们引入了 MORD 模块，它利用来自雷达点云的精确深度信息来提高模型对大物体和小物体的空间结构理解。通过学习物体内部结构与所选参考点之间的相对深度，构建相应的损失函数来监督最终的检测性能。SS-BEV 在具有挑战性的 nuScenes 验证集上的表现优于基线模型，NDS 检测分数提高了 2.1 分。在 nuScenes 测试集上，它对障碍物和工程车辆的检测准确率分别达到 66.1% 和 22.3% mAP，超越了一些基于多摄像头和 LiDAR 融合的方法。]]></description>
      <guid>http://link.springer.com/10.1007/s11760-024-03762-5</guid>
      <pubDate>Thu, 02 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>全光成像和处理</title>
      <link>http://link.springer.com/10.1007/978-981-97-6915-5</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-981-97-6915-5</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>迈向大规模全光重建</title>
      <link>http://link.springer.com/10.1007/978-981-97-6915-5_5</link>
      <description><![CDATA[摘要
重建具有无与伦比的真实感和细节水平的真实世界场景一直是计算机视觉和图形领域的长期目标。实现这一目标需要在传感技术和全光重建算法方面进行协调努力。]]></description>
      <guid>http://link.springer.com/10.1007/978-981-97-6915-5_5</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉 – ECCV 2024</title>
      <link>http://link.springer.com/10.1007/978-3-031-73036-8</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-73036-8</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多视角立体视觉的特征分布归一化网络</title>
      <link>http://link.springer.com/10.1007/s00371-024-03334-1</link>
      <description><![CDATA[摘要
作为三维重建的关键技术，多视角立体视觉（MVS）的研究随着深度学习的发展取得了重大进展。然而，由于不同视角之间的特征分布不一致，MVS 面临着挑战。这一现象被认为是 MVS 的一个重要瓶颈。为了克服这一限制，我们提出了一种称为特征分布规范化网络（FDN-MVS）的创新方法。为了减轻由不一致的相对姿势引起的误差，我们利用多个视图之间的单应性变换并提出分布残差细化（DRR）模块。扩大尺度、残差连接和坐标系固定可以实现更有意义的特征分布学习。为了进一步缩小不同视图之间的差异，我们建议进行特征规范化，将特征的计算限制在一个共同的分布内。实验结果证明了我们方法的有效性。与常用的基准算法 CasMVSNet 相比，我们在 DTU 数据集上的误差降低了 13.52%，在 Tanks and Temples 数据集上的 F 得分提高了 18.77%。代码可在 https://github.com/ZYangChen/FDN-MVS 获取。]]></description>
      <guid>http://link.springer.com/10.1007/s00371-024-03334-1</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>模式识别</title>
      <link>http://link.springer.com/10.1007/978-3-031-78456-9</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-78456-9</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉 – ECCV 2024</title>
      <link>http://link.springer.com/10.1007/978-3-031-73030-6</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-73030-6</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>平滑度、合成和采样：重新思考带有 DIV 损失的无监督多视角立体视觉</title>
      <link>http://link.springer.com/10.1007/978-3-031-73036-8_22</link>
      <description><![CDATA[摘要
尽管无监督多视图立体 (MVS) 取得了重大进展，但核心损失公式自推出以来基本保持不变。然而，我们确定了这种核心损失的基本限制，并提出了三项重大变化，以改进深度先验、遮挡和视图相关效应的建模。首先，我们使用限制深度平滑度约束消除了预测深度图中突出的阶梯状和边缘伪影。其次，我们提出了一种学习视图合成方法来生成光度损失的图像，避免使用手工编码的启发式方法来处理视图相关效应。最后，除了用作 MVS 输入的视图之外，我们还对其他视图进行采样以进行监督，挑战网络预测与未见视图相匹配的深度。这些贡献共同形成了一种改进的监督策略，我们称之为 DIV 损失。我们的 DIV 损失的主要优势在于它可以轻松地放入现有的无监督 MVS 训练流程中，从而显著提高竞争性重建基准，并以最小的训练成本显著提高对象边界周围的定性性能。]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-73036-8_22</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>模式识别与计算机视觉</title>
      <link>http://link.springer.com/10.1007/978-981-97-8508-7</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-981-97-8508-7</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于多视图立体匹配的可视性感知像素视图选择</title>
      <link>http://link.springer.com/10.1007/978-3-031-78456-9_9</link>
      <description><![CDATA[摘要
基于 PatchMatch 的多视图立体算法的性能在很大程度上受到用于匹配成本计算的所选源视图的影响。现有方法通常以相当临时的方式检测遮挡，这可能会对计算产生负面影响。相比之下，我们的论文介绍了一种刻意模拟视图可见性的创新方法。我们提出了一种新颖的可见性引导像素视图选择方案，该方案使用来自经过验证的解决方案的可见性信息逐步细化参考视图中每个像素的源视图集。此外，利用人工多蜂群 (AMBC) 算法并行搜索不同像素的最优解决方案。为了确保相邻像素的平滑度并更好地管理无纹理区域，将奖励分配给来自经过验证的来源的解决方案。我们的方法通过对两个数据集的实验进行了验证，提高了遮挡和低纹理区域的细节恢复能力，在要求苛刻的场景中表现出了显著的性能。我们的实现可在https://github.com/Ricky-S/Visibility-Aware-Pixelwise-View-Selection上找到。]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-78456-9_9</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉 – ECCV 2024</title>
      <link>http://link.springer.com/10.1007/978-3-031-73220-1</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-73220-1</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>