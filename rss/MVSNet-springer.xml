<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最新结果</title>
    <link>http://link.springer.com</link>
    <description>Springer 提供的最新内容</description>
    <lastBuildDate>Sat, 09 Dec 2023 16:56:24 GMT</lastBuildDate>
    <item>
      <title>PLKA-MVSNet：具有大型内核卷积注意力的并行多视图立体</title>
      <link>http://link.springer.com/10.1007/978-981-99-8145-8_10</link>
      <description><![CDATA[摘要
在本文中，我们提出 PLKA-MVSNet 来解决基于学习的多视图立体（MVS）方法的深度估计中剩余的挑战，特别是不准确的深度估计在具有挑战性的区域，例如低纹理区域、弱照明条件和非朗伯表面。我们将此问题归因于特征提取器性能不足以及MVS管道传输造成的信息丢失，并给出了我们的优化方案。具体来说，我们通过使用多个小卷积而不是单个大卷积引入并行大核注意（PLKA），以增强纹理和结构信息的感知，这使我们能够捕获更大的感受野和长距离信息。为了适应从粗到细的 MVS 流程，我们采用 PLKA 构建多级特征提取器。此外，我们提出并行成本量聚合（PCVA）来增强聚合成本量的鲁棒性。它在2D维度上引入了两个决策注意力，以弥补3D卷积压缩中的信息损失和像素遗漏。特别是，我们的方法在 DTU 数据集上显示出超越基于 Transformer 的方法的最佳整体性能，并在具有挑战性的 Tanks 和 Temples 高级数据集上取得了最佳结果。
      ]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8145-8_10</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>神经信息处理</title>
      <link>http://link.springer.com/10.1007/978-981-99-8145-8</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8145-8</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>神经信息处理</title>
      <link>http://link.springer.com/10.1007/978-981-99-8070-3</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8070-3</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过融合单目和深度表示方法组合的多视图立体</title>
      <link>http://link.springer.com/10.1007/978-981-99-8070-3_23</link>
      <description><![CDATA[摘要
平面扫描深度 MVS 的设计主要依赖于基于块相似度的匹配。然而，当处理场景中的低纹理、相似纹理和反射区域时，这种方法变得不切实际，导致匹配结果不准确。避免这种错误的方法之一是在匹配过程中结合语义信息。在本文中，我们提出了一种使用单目深度估计向深度 MVS 添加语义信息的端到端方法。此外，我们分析了两种主要深度表示的优缺点，并提出了一种协作方法来减轻它们的缺点。最后，我们引入了一种新颖的过滤准则“分布一致性”，它可以有效地过滤掉概率分布较差（例如均匀分布）的异常值，从而进一步提高重建质量。]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8070-3_23</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DSC-MVSNet：基于多视图立体深度可分离卷积的注意感知成本体积正则化</title>
      <link>http://link.springer.com/10.1007/s40747-023-01106-3</link>
      <description><![CDATA[摘要
深度学习最近已被证明可以在多视图立体 (MVS) 方面提供出色的性能。然而，基于深度学习的 MVS 方法很难平衡其效率和效果。为此，我们提出了 DSC-MVSNet，这是一种新颖的从粗到细和端到端的框架，用于在 MVS 中进行更高效、更准确的深度估计。特别是，我们提出了一种注意力感知 3D UNet 形状网络，它首先使用深度可分离卷积进行成本量正则化。该机制通过将代价量上的普通卷积转变为深度卷积和逐点卷积，实现了信息的有效聚合，并显着减少了模型参数和计算量。此外，还提出了3D-Attention模块来缓解成本量正则化中的特征不匹配问题，并在三个维度（即通道、空间和深度）聚合成本量的重要信息。此外，我们提出了一种高效的特征传输模块，可将低分辨率（LR）深度图上采样为高分辨率（HR）深度图，以实现更高的精度。在两个基准数据集（即 DTU 和 Tanks &amp;）上进行了大量实验。 Temples，我们证明模型的参数显着减少到 
\(25\%\)
 最先进的模型 MVSNet。此外，我们的方法优于或保持与最先进模型相同的准确性。我们的源代码位于 https://github.com/zs670980918/DSC-MVSNet.]]></description>
      <guid>http://link.springer.com/10.1007/s40747-023-01106-3</guid>
      <pubDate>Fri, 01 Dec 2023 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用几何先验从稀疏视图进行神经 3D 重建</title>
      <link>http://link.springer.com/10.1007/s41095-023-0337-5</link>
      <description><![CDATA[摘要
随着神经隐式 3D 表示的发展，稀疏视图 3D 重建引起了越来越多的关注。现有方法通常仅使用 2D 视图，需要一组密集的输入视图才能进行精确的 3D 重建。在本文中，我们证明了通过将几何先验融入神经隐式 3D 重建中可以实现精确的 3D 重建。我们的方法采用有符号距离函数作为 3D 表示，并从稀疏视图中学习可推广的 3D 表面重建模型。具体来说，我们通过使用相应的深度图从输入视图构建更有效和稀疏的特征量，深度图可以由深度传感器提供或直接从输入视图预测。在训练神经隐式 3D 表示时，除了颜色损失之外，我们还通过施加深度和表面法线约束来恢复更好的几何细节。实验表明，我们的方法不仅优于最先进的方法，而且具有良好的泛化性。




]]></description>
      <guid>http://link.springer.com/10.1007/s41095-023-0337-5</guid>
      <pubDate>Fri, 01 Dec 2023 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有点注意力的多视图立体网络</title>
      <link>http://link.springer.com/10.1007/s10489-023-04806-y</link>
      <description><![CDATA[摘要
近年来，与传统方法相比，基于学习的多视图立体（MVS）重建取得了优越性。在本文中，我们介绍了一种新颖的点注意力网络，具有基于点云结构的注意力机制。在重建过程中，我们具有注意机制的方法可以引导网络更多地关注复杂区域，例如薄结构和低纹理表面。我们首先使用修改后的经典 MVS 深度框架推断粗略深度图，并将其转换为相应的点云。然后，我们将原始图像的高频特征和不同分辨率特征添加到点云中。最后，我们的网络通过注意力机制引导不同维度上的点的权重分布，并迭代计算每个点的深度位移作为深度残差，将其添加到粗略深度预测中以获得最终的高分辨率深度图。实验结果表明，我们提出的点注意力架构可以在一些场景中在 DTU 数据集和 Tanks and Temples 数据集，表明我们的方法具有很强的泛化能力。
      ]]></description>
      <guid>http://link.springer.com/10.1007/s10489-023-04806-y</guid>
      <pubDate>Wed, 01 Nov 2023 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多视图立体自适应传播的不确定性意识</title>
      <link>http://link.springer.com/10.1007/s10489-023-04910-z</link>
      <description><![CDATA[摘要
基于学习的多视角立体方法以由粗到细的方式预测不同尺度的深度图，有效提高重建的质量和效率。为了解决在该技术中获得自适应深度细化间隔和轻量级功能的挑战，我们提出了多视图立体自适应传播（AP-UCSNet）的不确定性意识作为解决方案。在对深度假设进行采样时，我们采用卷积运算来计算每个像素始终位于同一物理表面上的一组相邻点。然后，我们对每个像素及其所有邻居的不确定性感知结果进行加权和平均，以得出空间相关的深度假设样本，这一过程可以减轻某些弱纹理区域的噪声影响。此外，我们将网络扩展到四尺度结构以增强其性能。前三个尺度利用 3D UNet 结构来正则化成本量，而对于最终尺度，概率量直接从特征图构建以简化正则化过程。我们的实验结果表明，所提出的方法提供了卓越的结果和性能。与 UCSNet 相比，完整性误差和总体误差分别减少了 0.051 毫米和 0.021 毫米。在Quadro RTX 5000 GPU上，预测分辨率为1600 × 1184的深度图仅需要0.57s和4398M，分别减少了约19.7%和34.2%。]]></description>
      <guid>http://link.springer.com/10.1007/s10489-023-04910-z</guid>
      <pubDate>Wed, 01 Nov 2023 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于深度学习的 3D 重建：一项调查</title>
      <link>http://link.springer.com/10.1007/s10462-023-10399-2</link>
      <description><![CDATA[摘要
基于图像的 3D 重建是计算机视觉和图形领域定义的一个长期存在的不适定问题。基于图像的 3D 重建的目的是从一组输入图像中检索目标对象或场景的 3D 结构和几何形状。该任务在机器人、虚拟现实和医学成像等各个领域都有广泛的应用。近年来，基于学习的 3D 重建方法吸引了世界各地的许多研究人员。这些新颖的方法可以以端到端的方式隐式估计对象或场景的 3D 形状，从而无需开发关键点检测和匹配等多个阶段。此外，这些新颖的方法可以从单个输入图像重建物体的形状。由于该领域的快速进步，以及提高 3D 重建方法性能的大量机会，因此似乎有必要对该领域的算法进行彻底审查。因此，这项研究提供了基于图像的 3D 重建领域最新发展的完整概述。从输入类型、模型结构、输出表示和训练策略等多个角度对所研究的方法进行了检验。还为读者提供了详细的比较。最后，讨论了未解决的挑战、潜在问题和未来可能的工作。]]></description>
      <guid>http://link.springer.com/10.1007/s10462-023-10399-2</guid>
      <pubDate>Fri, 01 Sep 2023 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>对具有复杂表面反射率的物体进行全 3D 重建的一系列方法</title>
      <link>http://link.springer.com/10.1007/s11263-023-01795-w</link>
      <description><![CDATA[摘要
一般场景的 3D 重建仍然是一个公开的挑战，当前技术通常依赖于场景表面反射率的假设，这限制了可建模对象的范围。亥姆霍兹立体视觉提供了一个有吸引力的框架，使建模过程与表面反射率无关。然而，以前的公式几乎完全局限于 2.5D 建模。为了解决这一差距，本文介绍了一系列重建方法，这些方法利用亥姆霍兹互易性来生成具有任意未知反射率的物体的完整 3D 模型。这包括基于（正交或透视）视图相关重建融合的方法、优化体素网格内表面位置的体积方法以及优化给定网格拓扑的顶点位置的基于网格的公式。贡献的方法在合成和真实数据集上进行评估，包括与本文一起公开发布的新颖的完整 3D 数据集，并与各种竞争方法进行实验比较。结果证明了不同方法的优势及其实现复杂对象的高质量全 3D 重建的能力。]]></description>
      <guid>http://link.springer.com/10.1007/s11263-023-01795-w</guid>
      <pubDate>Fri, 01 Sep 2023 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多视图立体的多分布拟合</title>
      <link>http://link.springer.com/10.1007/s00138-023-01449-4</link>
      <description><![CDATA[摘要
我们提出了一种基于多分布拟合的多视图立体网络（MDF-Net），它实现了低内存和高效率的高分辨率深度图预测。该方法采用四级级联结构，主要有以下三个贡献。首先，提出了视图成本正则化来削弱匹配噪声对构建成本量的影响。其次，建议使用多分布拟合（MDF）自适应地计算深度细化区间。高斯分布拟合用于在大区间内细化和校正深度，然后使用拉普拉斯分布拟合来精确估计小区间内的深度。第三，在第四阶段应用轻量级图像超分辨率网络对深度图进行上采样，以减少运行时间和内存需求。 DTU数据集上的实验结果表明MDF-Net取得了最先进的结果。它在高分辨率重建方法中具有最低的内存消耗和运行时间，仅需要大约4.29G内存来预测分辨率为1600 × 1184的深度图。此外，我们在Tanks和Temples数据集上验证了泛化能力，取得极具竞争力的业绩。代码已发布于 https://github.com/zongh5a/MDF-Net.]]></description>
      <guid>http://link.springer.com/10.1007/s00138-023-01449-4</guid>
      <pubDate>Wed, 30 Aug 2023 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于生成对抗网络的3D实体模型生成方法</title>
      <link>http://link.springer.com/10.1007/s10489-022-04381-8</link>
      <description><![CDATA[摘要
三维（3D）实体模型生成技术是实现智能生成结构设计的基础，但这一问题尚未得到有效解决。本文结合3D生成对抗网络（GAN）和逆向工程（RE）技术，提出了一种用于3D实体模型的综合生成方法3D-JointGAN。首先介绍3D-JointGAN的基本思想、相关理论和具体实现过程。然后将该方法应用于实际工程中三分支铸钢接头的生成，并对评价后选取的代表性接头的力学性能进行综合计算。最后，利用3D打印技术制造代表性关节的缩小比例模型，以验证生成模型的可制造性。通过与工程中常见的其他三种类型的关节进行比较，结果表明，该方法具有出色的生成和优化能力，可以生成各种创新且高度生动的3D实体模型。此外，经过评估后选择的代表性接头具有更好的力学性能。本文提出的方法解决了智能生成结构设计的瓶颈问题，具有广阔的应用前景。]]></description>
      <guid>http://link.springer.com/10.1007/s10489-022-04381-8</guid>
      <pubDate>Sat, 01 Jul 2023 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过基于深度学习的深度图分析和 3D 重建对单视图目标图像进行准确的体积估计</title>
      <link>http://link.springer.com/10.1007/s11042-023-14615-7</link>
      <description><![CDATA[摘要
从单视图物体图像估计刚性物体的体积是许多基于自动视觉的系统的重要需求。多视图图像上的体积估计很容易估计。但单视目标图像的体积估计是一个困难的过程，并且在体积估计中具有重要意义。这项工作提出了规则和不规则单视图对象图像中的有效对象体积估计。最初，使用均值中值滤波对单视图输入图像进行预处理。然后，利用基于高斯边缘的拉普拉斯算子提取边缘特征，并利用尺度不变特征变换（SIFT）特征提取关键点。提取的特征被考虑用于对象的形状分析。随后，基于提取的特征，利用VGG-ResNet框架进行深度分析。通过提取的特征获得用于体积估计的点云生成。最后，通过混合3维U-Net和图神经网络（Hybrid 3DU-GNet）有效地实现了单视图物体的体积估计。该框架提供了 3D 几何创建以实现精确的体积估计。这对体积估计提供了显着的改进。所提出的方法有效地估计了规则和不规则单视图对象图像的体积。该方法在MATLAB工作平台上实现。使用不同的现有方法对所提出的工作的实验结果进行了分析，并证明了性能指标的显着改进。性能指标包括准确度 (98.59%)、精确度 (98.21%)、召回率 (97.09%)、计算时间（3.2 秒）、R 平方 (98.2%)、（平均绝对百分比误差）MAPE (6.1%) 和（均方根误差）RMSE (0.93)。]]></description>
      <guid>http://link.springer.com/10.1007/s11042-023-14615-7</guid>
      <pubDate>Sat, 01 Jul 2023 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过遮挡感知采样和深度视觉特征融合进行稳健的局部光场合成</title>
      <link>http://link.springer.com/10.1007/s11633-022-1381-9</link>
      <description><![CDATA[摘要
新颖的视图合成最近因其在虚拟现实和沉浸式远程呈现中的应用而引起了极大的研究关注。基于任意大基线 RGB 参考渲染局部沉浸式光场 (LF) 是一个具有挑战性的问题，现有新颖的视图合成技术缺乏有效的解决方案。在这项工作中，我们的目标是基于大基线 LF 捕获和目标视图中的单个 RGB 图像真实地渲染局部沉浸式新颖视图/LF 图像。为了充分探索源 LF 捕获中的宝贵信息，我们提出了一种新颖的遮挡感知源采样器（OSS）模块，该模块以遮挡感知方式有效地将源视图的像素传输到目标视图的视锥体。提出了一种基于注意力的深度视觉融合模块，将显示的遮挡背景内容与初步 LF 融合成最终的精炼 LF。所提出的源采样和融合机制不仅有助于从不同观察角度提供遮挡区域的信息，而且被证明能够有效提高视觉渲染质量。实验结果表明，我们提出的方法能够使用稀疏 RGB 参考渲染高质量的 LF 图像/新颖视图，并且优于最先进的 LF 渲染和新颖的视图合成方法。]]></description>
      <guid>http://link.springer.com/10.1007/s11633-022-1381-9</guid>
      <pubDate>Thu, 01 Jun 2023 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于U型特征提取的成本体分离与融合的分层MVSNet</title>
      <link>http://link.springer.com/10.1007/s00530-022-01009-2</link>
      <description><![CDATA[摘要
基于深度学习的多视立体（MVS）方法近年来发展迅速，但由于特征提取效果一般、成本体之间相关性差等原因导致重建不准确。仍然存在，为提高重建准确性和完整性提供了可能性。因此，我们开发了一种具有成本量分离和融合的分层 MVS 网络模型，以缓解这些问题。首先，为了从输入图像中获得更完整、更准确的特征信息，设计了一个U型特征提取模块，根据由三个不同尺度组成的层次结构同时输出特征信息。然后，为了增强网络结构对特征的学习能力，我们对提取的特征引入了注意力机制，关注并学习突出显示的特征。最后，在成本量正则化阶段，设计了层次级联结构的成本量分离与融合模块。该模块分离小规模成本体积内的信息，将其传递到较低级别的成本体积进行融合，并执行从粗到细的深度图估计。该模型显着提高了重建的准确性和完整性。在 DTU 数据集上进行的大量实验结果表明，我们的方法比 Cascade-MVSNet 的精度误差 (acc.) 好约 10.2%，完整性误差 (comp.) 好 7.6%，总体误差 (overall) 好 9.0% ，在重建完整性方面具有相似的性能，显示了我们模块的有效性。]]></description>
      <guid>http://link.springer.com/10.1007/s00530-022-01009-2</guid>
      <pubDate>Wed, 01 Feb 2023 00:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>