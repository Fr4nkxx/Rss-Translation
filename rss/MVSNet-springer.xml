<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最新结果</title>
    <link>http://link.springer.com</link>
    <description>Springer可用的最新内容</description>
    <lastBuildDate>Thu, 10 Apr 2025 12:30:42 GMT</lastBuildDate>
    <item>
      <title>对多孔介质复杂微观结构的3D重建方法的评论：成像设备，技术和应用</title>
      <link>https://link.springer.com/article/10.1007/s00603-025-04516-6</link>
      <description><![CDATA[由于不同多孔介质的复杂和可变的微观结构，使用3D重建方法平衡3D模型的准确性和效率仍然是一项艰巨的任务，这显着影响了各种计算字段中基于3D重建模型的精确性和收敛性。本评论总结了来自成像设备，技术和应用程序方面的多孔介质复杂微结构的3D重建方法。在这篇综述中，主要得出的六种3D重建算法包括立体断层扫描重建，随机重建，理论重建，混合重建，3D点云重建和基于基于基本原理的基本原理和研究3D的重构算法和研究3D点的重构算法和研究3D。通过3D重建模型，结束了机械性能，开裂行为，多孔介质的液压和热性能的数值研究。该评论为多孔媒体的各种属性研究提供了各种实用且可访问的3D重建方法，这对诸如深度能源和资源开发等众多工程领域有帮助。]]></description>
      <guid>https://link.springer.com/article/10.1007/s00603-025-04516-6</guid>
      <pubDate>Thu, 10 Apr 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>来自多视图特征立体匹配先验的健壮的小说视图综合</title>
      <link>https://link.springer.com/article/10.1007/s00530-025-01757-x</link>
      <description><![CDATA[ nerf（神经辐射场）表现出从未知视图中合成图像的能力。但是，它面临着诸如闭塞，非斜体表面，稀疏输入以及多视图图像中通常遇到的弱质地等因素所面临的挑战。这些复杂性通常会导致拟合错误的场景几何形状，从而导致次优的新型视图综合质量。本文通过利用基于功能的多视立体匹配（MVS）先验的潜力来解决这一挑战。该方法的区别是其基于MVS估算的深度值，不确定性和深度间隔的自适应构建高斯函数的区别，从而为任意缩放场景提供了灵活性和适应性。在此基础上，通过比较该分布之间的差异与量渲染射线上采样点的重量分布之间的差异来实现NERF训练过程的优化。此外，我们提出了一种有效的Riemann和近似策略，以进一步提高深度损失的性能。适用于三个实际场景数据集的定量指标，即LLFF，IBRNET和DTU，表明本文提供的方法显着提高了与当前的先进方法相比，新型视图合成的质量，可实现3.8％至26.9％的增强。可视化实验揭示了强大的优化结果，尤其是在常规NERF遇到困难的挑战区域中。
                图形摘要
                
]]></description>
      <guid>https://link.springer.com/article/10.1007/s00530-025-01757-x</guid>
      <pubDate>Tue, 25 Mar 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>sp-a $$ \ text {i}^{2} $$：稀疏的先前指导成本构建以及自适应内部和规模间成本汇总，用于多视图深度估算</title>
      <link>https://link.springer.com/article/10.1007/s11760-025-03985-0</link>
      <description><![CDATA[基于学习的多视图深度估计方法取得了显着的成功，主要是通过准确匹配参考和源视图之间的对应关系，以构建可区分的成本量。使用有序和预设采样的平面扫地的现有方法无法辨别初始成本量。此外，不足的成本汇总加剧了在低文本区域和物体边界内实现准确的深度估计方面所面临的挑战。在本文中，我们提出了一个新颖的多视图深度估计网络，称为$$ \ text {sp-a} \ text {i}^{2} $$（稀疏的先前指导性成本构建以及自适应内部和间刻度成本组合）。具体而言，我们提出了一种稀疏的先前指导策略，用于跨深度范围内的动态和不均匀采样，以构建成本量，这引入了合理且细粒的空间分区，以更高的精度来完善深度。此外，为了提高具有挑战性的区域的深度质量，提出了一种新颖的自适应内部和规模相间的成本聚合（$$ \ text {a} \ text {i}^{2} $$ -sa）模块，以增强特征表示的力量。提出的$$ {\ hbox {sp-a}} \ text {i}^{2} $$是训练的端到端，实验结果表明，我们的方法在各种基准数据集上实现了最新的结果。&gt; ]]></description>
      <guid>https://link.springer.com/article/10.1007/s11760-025-03985-0</guid>
      <pubDate>Mon, 10 Mar 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于语义一致性约束的深度多视图立体声的指示微调</title>
      <link>https://link.springer.com/article/10.1007/s10489-025-06382-9</link>
      <description><![CDATA[现有的基于深度图的多视图立体声（MVS）方法通常假定纹理特征在不同的观点上保持一致。但是，诸如照明变化，遮挡和弱纹理区域之类的因素会导致不一致的纹理特征，从而构成特征提取的挑战。结果，仅依靠纹理一致性并不总是会在某些情况下产生高质量的重建。相反，与相同对象相对应的高级语义概念在不同的观点中保持一致，我们将其定义为语义一致性。由于从头开始设计和培训新的MVS网络既昂贵又是劳动力密集的，因此我们通过纳入语义一致性限制来提高地区的重建质量，在测试阶段进行微观调整，从而对现有的深度地图MVS网络进行良好的效果。考虑到接地-SAM的可靠开放式检测和零摄像分段功能，我们首先使用接地SAM来基于文本指令在多视图图像中为任意对象生成语义分割掩码。然后，这些掩码用于通过将它们从不同的视点对齐到参考角度来微调预训练的MVS网络，并根据建议的语义一致性损失函数优化深度图。我们的方法被设计为一种测试时间方法，可适应广泛的基于地图的MVS网络，仅需要调整少量与深度相关的参数。跨不同MVS网络和大规模场景的全面实验评估表明，我们的方法有效地以较低的计算成本增强了重建质量。]]></description>
      <guid>https://link.springer.com/article/10.1007/s10489-025-06382-9</guid>
      <pubDate>Tue, 25 Feb 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>重新思考多视图立体声的概率量：概率分析方法</title>
      <link>https://link.springer.com/article/10.1007/s10489-025-06284-w</link>
      <description><![CDATA[现有的基于学习的多视图立体声（MVS）模型主要集中于通过级联结构预测深度图，以实现更强大的重建结果。但是，他们经常强调提高立体声匹配的质量，同时忽略深度假设的重要性。在本文中，我们从概率体积分析的角度提出了一种新型的MVS模型。首先，考虑深度细化的概率量的指导效应。理想情况下，沿概率体积深度维度的概率分布遵循单峰模式。我们设计了一个单峰曲线以适合这种模式。然后，根据预定义的概率阈值，为每个像素位置自适应选择合理的深度细化范围。此外，考虑到匹配噪声可能导致概率量显示为模糊的单峰峰，我们设计了概率体积分式 - 工程模块（PVS-PVM）。该模块基于条件约束执行峰值搜索，将概率体积分为主概率和子概率量，然后计算出它们的两组深度假设。最后，根据这些深度假设计算新的主概率和子概率量，并合并以预测深度。这种方法可以更全面地考虑具有更高概率的区域，从而提高了深度假设的鲁棒性。实验结果表明，我们的方法有效地利用了概率信息信息来指导深度图的细化，并产生增强的DTU和TAMP上的重建结果。寺庙数据集。我们的代码将于 https://github.com/zongh5a/probmvsnet 。]]></description>
      <guid>https://link.springer.com/article/10.1007/s10489-025-06284-w</guid>
      <pubDate>Mon, 03 Feb 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>上下文感知的多视图立体声网络，用于有效的边缘保护深度估计</title>
      <link>https://link.springer.com/article/10.1007/s11263-024-02337-8</link>
      <description><![CDATA[近年来，基于学习的多视图立体观点通过采用粗到1的深度估计框架取得了巨大进步。但是，现有方法在恢复无特征区域，物体边界和薄结构的深度方面仍然遇到困难，这主要是由于低纹理区域中匹配线索的区分性差，这是用于成本量正规化的3D卷积神经网络的固有光滑属性，以及Coarsest量表特征的信息丢失。为了解决这些问题，我们提出了一个上下文感知的多视图立体网络（CANET），该网络（CANET）利用图像中的上下文提示来实现有效的边缘延伸深度估计。参考视图中的结构自相似性信息由引入的自相似性的成本聚合模块利用，以在成本量中执行远程依赖关系建模，这可以提高无特征区域的对位。随后，参考视图中的上下文信息通过提出的层次层次呈现剩余学习模块逐渐完善多尺度的深度估计，从而在边缘进行了微妙的深度估计。为了通过使其更加专注于精致的区域来丰富最佳量表，提出了一个焦点选择模块，可以通过更细节（例如薄结构）来增强初始深度的恢复。通过将上述策略整合到设计精良的轻质级联框架中，Canet可以实现卓越的性能和效率权衡。广泛的实验表明，所提出的方法以快速推理速度和低内存使用量实现最先进的性能。值得注意的是，在所有已发表的基于学习的方法中，CANET在具有挑战性的坦克和寺庙中排名第一。]]></description>
      <guid>https://link.springer.com/article/10.1007/s11263-024-02337-8</guid>
      <pubDate>Tue, 07 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SS-BEV：基于多尺度空间结构理解的多相机BEV对象检测</title>
      <link>https://link.springer.com/article/10.1007/s11760-024-03762-5</link>
      <description><![CDATA[ bev（鸟视图）基于多个摄像机的对象检测已成为自主驾驶领域的主流范式。但是，我们已经确定在检测到极大或小物体时，许多现有方法的性能较差。为了解决此问题，本文提出了一个新的称为SS-BEV的多相机BEV检测的框架。首先，我们设计了一个更具表现力的特征提取模块A-WBFP，该模块通过级联的方法集成了平行的非常弯曲的卷积和加权双向特征金字塔块进入骨干网络。这有助于防止更深的网络层中小型对象信息的丢失并增强接受场，从而产生充满上下文信息的特征图。然后，我们介绍MORD模块，该模块利用雷达点云的准确深度信息来改善模型对大对象和小物体的空间结构理解。通过学习对象的内部结构和选定的参考点之间的相对深度，构建了相应的损耗函数以监督最终检测性能。 SS-BEV在具有挑战性的Nuscenes验证设置上胜过基线模型，NDS检测分数有2.1分。在Nuscenes测试集中，它的检测准确度分别为66.1％和22.3％的障碍物和建筑车辆地图，基于多型相机和激光雷达融合，超过了一些方法。。]]></description>
      <guid>https://link.springer.com/article/10.1007/s11760-024-03762-5</guid>
      <pubDate>Thu, 02 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>朝向大规模的元素重建</title>
      <link>https://link.springer.com/chapter/10.1007/978-981-97-6915-5_5</link>
      <description><![CDATA[在计算机视觉和图形领域，以无与伦比的现实主义和细节来重建现实世界的现实场景一直是一个长期的目标。实现这一目标需要在感应技术和元素重建算法中进行协调的努力。]]></description>
      <guid>https://link.springer.com/chapter/10.1007/978-981-97-6915-5_5</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MVSGAUSSIAN：多视图立体声</title>
      <link>https://link.springer.com/chapter/10.1007/978-3-031-72649-1_3</link>
      <description><![CDATA[我们提出了MVSgaussian，这是一种可推广的新的3D高斯表示方法，该方法源自多视图立体声（MVS），可以有效地重建看不见的场景。具体而言，1）我们利用MVS编码几何形状 - 意识到高斯表示，并将其解码为高斯参数。 2）为了进一步提高性能，我们提出了一种混合高斯渲染，该渲染整合了新型视图合成的有效体积渲染设计。 3）为了支持特定场景的快速微调，我们引入了多视图几何一致的聚合策略，以有效地汇总了由可推广模型产生的点云，作为每场局部优化的初始化。与以前可推广的基于NERF的方法相比，通常需要进行微调和秒渲染的几分钟，MVSgaussian可以为每个场景提供更好的合成质量实时渲染。与Vanilla 3D-GS相比，MVSGaussian以较少的训练计算成本获得了更好的视图合成。对DTU，实际向前的，NERF合成以及坦克和神庙数据集进行了广泛的实验，以验证MVSgaussian具有令人信服的通用性，实时渲染速度和快速的每场曲（每场启动）的最佳性能。]]></description>
      <guid>https://link.springer.com/chapter/10.1007/978-3-031-72649-1_3</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>OMNI-RECON：利用基于图像的通用神经辐射场的渲染</title>
      <link>https://link.springer.com/chapter/10.1007/978-3-031-72640-8_9</link>
      <description><![CDATA[神经辐射场（NERF）的最新突破已经引发了对它们集成到现实世界3D应用程序中的大量需求。但是，不同3D应用所需的各种功能通常需要具有各种管道的不同NERF模型，从而为每个目标任务和繁琐的试用试验实验提供了乏味的NERF培训。我们的工作旨在从新兴基金会模型的概括能力和适应性中汲取灵感，旨在开发一个通用的NERF来处理各种3D任务。我们通过提出一个名为Omni-Recon的框架来实现这一目标，该框架能够（1）可推广的3D重建和零击的多任务场景的理解，以及（2）对多样化的下游3D应用程序的适应性，例如实时渲染和场景编辑。我们的主要见解是，具有准确的几何形状和外观估算的基于图像的渲染管道可以将2D图像特征提升到其3D对应物中，从而以可推广的方式将2D任务扩展到3D世界。具体而言，我们的Omni-Recon使用基于图像的渲染具有两个解耦分支的渲染：一个基于图像的分支：一个基于图像的渲染：一个基于复杂的变压器的分支，逐渐融合了几何形状和外观特征，以进行准确的几何学估计，以及一个可预测混合源视图的轻量级分支。该设计实现了最新的（SOTA）可推广的3D表面重建质量，并在各种任务中重复使用的重量可重复使用，以了解零照片的多任务场景。此外，它可以在将复杂的几何分支烘烤到网格中后实现实时渲染，快速适应以实现SOTA可推广的3D理解性能，并与2D扩散模型无缝集成，用于文本指导的3D编辑。我们的代码可用： 
              https://github.com/gatech-eic/omni-recon
              
            。]]></description>
      <guid>https://link.springer.com/chapter/10.1007/978-3-031-72640-8_9</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Caesarnerf：校准的语义表示，用于几个可概括的神经渲染</title>
      <link>https://link.springer.com/chapter/10.1007/978-3-031-72658-3_5</link>
      <description><![CDATA[普遍性和少数学习是神经辐射场（NERF）的主要挑战，通常是由于对像素级渲染缺乏整体理解。我们介绍了Caesarnerf，这是一种端到端的方法，它利用场景级校准的语义表示以及像素级表示，以促进几乎没有弹性的可推广的神经渲染，从而促进整体理解而不损害高质量的细节。 Caesarnerf明确模型构成参考视图的差异，以结合场景级的语义表示，提供了校准的整体理解。该校准过程将各种观点与精确的位置保持一致，并通过顺序细化进一步增强，以捕获不同的细节。在公共数据集上进行了广泛的实验，包括LLFF，Shiny，MIP-NYF 360和MVIMGNET，表明Caesarnerf在不同的参考视图中提供最先进的性能，即使单个参考图像也有效。]]></description>
      <guid>https://link.springer.com/chapter/10.1007/978-3-031-72658-3_5</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>以表面为中心的高保真性神经表面重建的建模</title>
      <link>https://link.springer.com/chapter/10.1007/978-3-031-73411-3_11</link>
      <description><![CDATA[从多视图图像（尤其是稀疏图像）中重建高保真表面是一项关键而实用的任务，近年来引起了广泛关注。但是，现有方法会受到内存约束或地面深度的要求，无法恢复令人满意的几何细节。为此，我们提出了Surf，这是一个新的以表面为中心的框架，该框架结合了基于匹配领域的新区域稀疏，在性能，效率和可扩展性之间实现了良好的权衡。据我们所知，这是第一个实现由引入匹配场供电的端到端稀疏的方法，该匹配场利用重量分布有效地定位包含表面的边界区域。我们没有预测每个体素的SDF值，而是通过判断体素是否在表面区域内，提出了一种新的区域稀疏方法来稀疏体积。通过这种方式，我们的模型可以利用表面周围的较高频率特征，而记忆和计算消耗较少。在包含复杂大规模场景的多个基准测试上进行的广泛实验表明，我们的重建表现出高质量的细节并实现新的最先进的性能，即提高了46％的改善，记忆消耗减少80％。代码可在 
              https://github.com/prstrive/surf
              
            。]]></description>
      <guid>https://link.springer.com/chapter/10.1007/978-3-031-73411-3_11</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LARA：有效的大型基线辐射场</title>
      <link>https://link.springer.com/chapter/10.1007/978-3-031-73247-8_20</link>
      <description><![CDATA[辐射场方法已经实现了逼真的新型视图合成和几何重建。但是它们主要用于人均优化或小型基线设置。尽管最近的几项工作通过使用变压器来调查大型基线的进料重建，但它们都以标准的全球注意力机制运行，因此忽略了3D重建的局部性质。我们提出了一种在变压器层中统一本地和全球推理的方法，从而提高了质量和更快的收敛性。我们的模型代表场景作为高斯体积，并将其与图像编码器和小组注意层结合起来，以有效地进行进料重建。实验结果表明，我们的模型在四个GPU上接受了两天的训练，证明了重建360的高保真度
              
                
              
              $$^{\ circ} $$
              
             辐射场，以及零射和室外测试的鲁棒性。]]></description>
      <guid>https://link.springer.com/chapter/10.1007/978-3-031-73247-8_20</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用对比的学习和知识蒸馏，无用的无用的多视图立体声</title>
      <link>https://link.springer.com/chapter/10.1007/978-981-96-2440-9_7</link>
      <description><![CDATA[3D重建是飞机视觉处理中的新颖而有意义的任务之一。从飞机上携带的摄像机拍摄的2D照片或视频中重建3D模型可以有效地构建地面目标，从而促进对地面目标的研究并帮助研究人员进行更深入，更全面的工作。当前验证的大多数3D重建数据集基于室内场景和小对象。配备摄像头无人机捕获的室外建筑数据集的相对验证很少。因此，在本文中，我们探讨了一种3D重建算法，该算法可应用于无人机捕获的场景。为了提高准确性和鲁棒性，我们基于对比度学习和知识蒸馏设计了无监督的多视图立体声算法，该算法可以预测无人机而无需从头开始训练的无人机捕获的场景深度。具体而言，首先，我们使用群体级的对比损失，图像级对比损失和光度一致性来构建无监督的MVS模型并训练教师模型。然后，我们应用知识蒸馏以提取教师模型中的有效概率分布以训练学生模型，从而可以降低模型大小的同时提高准确性和完整性；最后，在DTU数据集上训练的模型用于对包含无人机捕获的真实场景的Urbanscene数据集执行零弹药推断。已验证的是，所提出的模型可以在DTU上获得良好的定量结果，并且可以在Urbanscene上制定准确而完整的3D重建模型。]]></description>
      <guid>https://link.springer.com/chapter/10.1007/978-981-96-2440-9_7</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>平滑度，综合和抽样：重新考虑无监督的多视图立体声，DIV损失</title>
      <link>https://link.springer.com/chapter/10.1007/978-3-031-73036-8_22</link>
      <description><![CDATA[尽管无监督的多视图立体声（MV）取得了重大进展，但自引入以来，核心损失配方在很大程度上保持不变。但是，我们确定了对该核心损失的基本局限性，并提出了三个主要变化，以改善深度先验，遮挡和观察依赖性效果的建模。首先，我们使用夹紧的深度平滑度约束在预测的深度图中消除了突出的楼梯式和边缘伪像。其次，我们提出了一种学习的视图合成方法，以生成用于光度损失的图像，避免使用手工编码的启发式方法来处理观点依赖性效果。最后，我们将其他视图示例视图以超出用作MVS输入的视图，从而挑战网络以预测与看不见的视图相匹配的深度。这些贡献共同构成了我们称为Div损失的改进的监督策略。我们DIV损失的主要优点是，它可以很容易地放入现有的无监督MVS培训管道中，从而显着改善了竞争性重建基准，并在对象界面上的质量性能极大地提高了，以获得最小的培训成本。]]></description>
      <guid>https://link.springer.com/chapter/10.1007/978-3-031-73036-8_22</guid>
      <pubDate>Wed, 01 Jan 2025 00:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>