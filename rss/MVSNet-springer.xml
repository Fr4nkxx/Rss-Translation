<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最新结果</title>
    <link>http://link.springer.com</link>
    <description>Springer 提供的最新内容</description>
    <lastBuildDate>Fri, 29 Dec 2023 00:41:09 GMT</lastBuildDate>
    <item>
      <title>PLKA-MVSNet：具有大型内核卷积注意力的并行多视图立体</title>
      <link>http://link.springer.com/10.1007/978-981-99-8145-8_10</link>
      <description><![CDATA[摘要
在本文中，我们提出 PLKA-MVSNet 来解决基于学习的多视图立体（MVS）方法的深度估计中剩余的挑战，特别是不准确的深度估计在具有挑战性的区域，例如低纹理区域、弱照明条件和非朗伯表面。我们将此问题归因于特征提取器性能不足以及MVS管道传输造成的信息丢失，并给出了我们的优化方案。具体来说，我们通过使用多个小卷积而不是单个大卷积引入并行大核注意（PLKA），以增强纹理和结构信息的感知，这使我们能够捕获更大的感受野和长距离信息。为了适应从粗到细的 MVS 流程，我们采用 PLKA 构建多级特征提取器。此外，我们提出并行成本量聚合（PCVA）来增强聚合成本量的鲁棒性。它在2D维度上引入了两个决策注意力，以弥补3D卷积压缩中的信息损失和像素遗漏。特别是，我们的方法在 DTU 数据集上显示出超越基于 Transformer 的方法的最佳整体性能，并在具有挑战性的 Tanks 和 Temples 高级数据集上取得了最佳结果。
      ]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8145-8_10</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用增强级联进行三维植物重建-MVSNet</title>
      <link>http://link.springer.com/10.1007/978-981-99-8432-9_23</link>
      <description><![CDATA[摘要
三维重建是恢复植物形态结构的重要方法，完整、准确的植物3D点云可以更好地反映植物的表型参数，例如植物植物的高度、体积和叶面积。为了获得更加完整、准确的植物点云，本文在Cascade-MVSNet网络的基础上提出了一系列的增强。为了改善植物弱纹理区域的重建，在特征提取阶段引入了轻量级注意机制。为了增强植物点云的有效点并抑制无效点，采用焦点损失作为损失函数，将深度估计问题视为分类任务以获得更准确的深度信息。此外，为了提高模型的效率，我们用深度可分离卷积代替标准卷积，在保持性能的同时减少参数数量和计算复杂度。上述工作应用于植物数据集，重建显示无效点显着减少，重建点云更清晰。除了对植物数据集进行研究外，我们还在公开的 DTU 数据集上评估了我们的方法。实验结果表明，DTU 数据集的重建完整性显着提高，整体表现具有竞争力。]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8432-9_23</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>模式识别和计算机视觉</title>
      <link>http://link.springer.com/10.1007/978-981-99-8432-9</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8432-9</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>神经信息处理</title>
      <link>http://link.springer.com/10.1007/978-981-99-8145-8</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8145-8</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有分层几何约束的可推广神经辐射场</title>
      <link>http://link.springer.com/10.1007/978-981-99-8432-9_26</link>
      <description><![CDATA[摘要
广义神经辐射场模型的出现极大地扩展了新颖的视图合成任务的适用性。然而，现有的方法主要依靠从相邻视图获得的 2D 特征来辅助渲染，这通常会导致看不见的视点的退化。一些方法在模型中引入成本量来提供几何先验，但这种粗略的几何信息无法有效地约束模型，导致渲染图像具有丰富的伪影。在这项工作中，我们建议为模型提供分层几何约束，以实现更好的渲染结果。我们引入级联 MVSNet 来提供分层场景结构特征，用于推断底层场景几何形状。它限制了模型在看不见的视点下的渲染。此外，分层特征提供了有助于细节重建的精细表示。此外，我们利用级联 MVSNet 生成的分层深度图来约束采样过程，确保采样点集中在场景表面附近。这种采样策略过滤掉了大量无用的采样点，提高了采样效率和渲染质量。与之前的广义方法不同，我们采用 Neus 提出的新权重函数来消除密度场中的固有偏差，从而将颜色和密度的预测与带符号的距离场分开。大量实验表明，我们提出的方法显着提高了渲染质量，并且优于以前的方法。]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8432-9_26</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>神经信息处理</title>
      <link>http://link.springer.com/10.1007/978-981-99-8070-3</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8070-3</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计算机图形学的进展</title>
      <link>http://link.springer.com/10.1007/978-3-031-50072-5</link>
      <description><![CDATA[]]></description>
      <guid>http://link.springer.com/10.1007/978-3-031-50072-5</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过融合单目和深度表示方法组合的多视图立体</title>
      <link>http://link.springer.com/10.1007/978-981-99-8070-3_23</link>
      <description><![CDATA[摘要
平面扫描深度 MVS 的设计主要依赖于基于块相似度的匹配。然而，当处理场景中的低纹理、相似纹理和反射区域时，这种方法变得不切实际，导致匹配结果不准确。避免这种错误的方法之一是在匹配过程中结合语义信息。在本文中，我们提出了一种使用单目深度估计向深度 MVS 添加语义信息的端到端方法。此外，我们分析了两种主要深度表示的优缺点，并提出了一种协作方法来减轻它们的缺点。最后，我们引入了一种新颖的过滤准则“分布一致性”，它可以有效地过滤掉概率分布较差（例如均匀分布）的异常值，从而进一步提高重建质量。]]></description>
      <guid>http://link.springer.com/10.1007/978-981-99-8070-3_23</guid>
      <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DSC-MVSNet：基于多视图立体深度可分离卷积的注意感知成本体积正则化</title>
      <link>http://link.springer.com/10.1007/s40747-023-01106-3</link>
      <description><![CDATA[摘要
深度学习最近已被证明可以在多视图立体 (MVS) 方面提供出色的性能。然而，基于深度学习的 MVS 方法很难平衡其效率和效果。为此，我们提出了 DSC-MVSNet，这是一种新颖的从粗到细和端到端的框架，用于在 MVS 中进行更高效、更准确的深度估计。特别是，我们提出了一种注意力感知 3D UNet 形状网络，它首先使用深度可分离卷积进行成本量正则化。该机制通过将代价量上的普通卷积转变为深度卷积和逐点卷积，实现了信息的有效聚合，并显着减少了模型参数和计算量。此外，还提出了3D-Attention模块来缓解成本量正则化中的特征不匹配问题，并在三个维度（即通道、空间和深度）聚合成本量的重要信息。此外，我们提出了一种高效的特征传输模块，可将低分辨率（LR）深度图上采样为高分辨率（HR）深度图，以实现更高的精度。在两个基准数据集（即 DTU 和 Tanks &amp;）上进行了大量实验。 Temples，我们证明模型的参数显着减少到 
\(25\%\)
 最先进的模型 MVSNet。此外，我们的方法优于或保持与最先进模型相同的准确性。我们的源代码位于 https://github.com/zs670980918/DSC-MVSNet.]]></description>
      <guid>http://link.springer.com/10.1007/s40747-023-01106-3</guid>
      <pubDate>Fri, 01 Dec 2023 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用几何先验从稀疏视图进行神经 3D 重建</title>
      <link>http://link.springer.com/10.1007/s41095-023-0337-5</link>
      <description><![CDATA[摘要
随着神经隐式 3D 表示的发展，稀疏视图 3D 重建引起了越来越多的关注。现有方法通常仅使用 2D 视图，需要一组密集的输入视图才能进行精确的 3D 重建。在本文中，我们证明了通过将几何先验融入神经隐式 3D 重建中可以实现精确的 3D 重建。我们的方法采用有符号距离函数作为 3D 表示，并从稀疏视图中学习可推广的 3D 表面重建模型。具体来说，我们通过使用相应的深度图从输入视图构建更有效和稀疏的特征量，深度图可以由深度传感器提供或直接从输入视图预测。在训练神经隐式 3D 表示时，除了颜色损失之外，我们还通过施加深度和表面法线约束来恢复更好的几何细节。实验表明，我们的方法不仅优于最先进的方法，而且具有良好的泛化性。




]]></description>
      <guid>http://link.springer.com/10.1007/s41095-023-0337-5</guid>
      <pubDate>Fri, 01 Dec 2023 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有点注意力的多视图立体网络</title>
      <link>http://link.springer.com/10.1007/s10489-023-04806-y</link>
      <description><![CDATA[摘要
近年来，与传统方法相比，基于学习的多视图立体（MVS）重建取得了优越性。在本文中，我们介绍了一种新颖的点注意力网络，具有基于点云结构的注意力机制。在重建过程中，我们具有注意机制的方法可以引导网络更多地关注复杂区域，例如薄结构和低纹理表面。我们首先使用修改后的经典 MVS 深度框架推断粗略深度图，并将其转换为相应的点云。然后，我们将原始图像的高频特征和不同分辨率特征添加到点云中。最后，我们的网络通过注意力机制引导不同维度上的点的权重分布，并迭代计算每个点的深度位移作为深度残差，将其添加到粗略深度预测中以获得最终的高分辨率深度图。实验结果表明，我们提出的点注意力架构可以在一些场景中在 DTU 数据集和 Tanks and Temples 数据集，表明我们的方法具有很强的泛化能力。
      ]]></description>
      <guid>http://link.springer.com/10.1007/s10489-023-04806-y</guid>
      <pubDate>Wed, 01 Nov 2023 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多视图立体自适应传播的不确定性意识</title>
      <link>http://link.springer.com/10.1007/s10489-023-04910-z</link>
      <description><![CDATA[摘要
基于学习的多视角立体方法以由粗到细的方式预测不同尺度的深度图，有效提高重建的质量和效率。为了解决在该技术中获得自适应深度细化间隔和轻量级功能的挑战，我们提出了多视图立体自适应传播（AP-UCSNet）的不确定性意识作为解决方案。在对深度假设进行采样时，我们采用卷积运算来计算每个像素始终位于同一物理表面上的一组相邻点。然后，我们对每个像素及其所有邻居的不确定性感知结果进行加权和平均，以得出空间相关的深度假设样本，这一过程可以减轻某些弱纹理区域的噪声影响。此外，我们将网络扩展到四尺度结构以增强其性能。前三个尺度利用 3D UNet 结构来正则化成本量，而对于最终尺度，概率量直接从特征图构建以简化正则化过程。我们的实验结果表明，所提出的方法提供了卓越的结果和性能。与UCSNet相比，完整性误差和整体误差分别减少了0.051毫米和0.021毫米。在Quadro RTX 5000 GPU上，预测分辨率为1600 × 1184的深度图仅需要0.57s和4398M，分别减少了约19.7%和34.2%。]]></description>
      <guid>http://link.springer.com/10.1007/s10489-023-04910-z</guid>
      <pubDate>Wed, 01 Nov 2023 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于深度学习的 3D 重建：一项调查</title>
      <link>http://link.springer.com/10.1007/s10462-023-10399-2</link>
      <description><![CDATA[摘要
基于图像的 3D 重建是计算机视觉和图形领域定义的一个长期存在的不适定问题。基于图像的 3D 重建的目的是从一组输入图像中检索目标对象或场景的 3D 结构和几何形状。该任务在机器人、虚拟现实和医学成像等各个领域都有广泛的应用。近年来，基于学习的 3D 重建方法吸引了世界各地的许多研究人员。这些新颖的方法可以以端到端的方式隐式估计对象或场景的 3D 形状，从而无需开发关键点检测和匹配等多个阶段。此外，这些新颖的方法可以从单个输入图像重建物体的形状。由于该领域的快速进步，以及提高 3D 重建方法性能的大量机会，因此似乎有必要对该领域的算法进行彻底审查。因此，这项研究提供了基于图像的 3D 重建领域最新发展的完整概述。从输入类型、模型结构、输出表示和训练策略等多个角度对所研究的方法进行了检验。还为读者提供了详细的比较。最后，讨论了未解决的挑战、潜在问题和未来可能的工作。]]></description>
      <guid>http://link.springer.com/10.1007/s10462-023-10399-2</guid>
      <pubDate>Fri, 01 Sep 2023 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>对具有复杂表面反射率的物体进行全 3D 重建的一系列方法</title>
      <link>http://link.springer.com/10.1007/s11263-023-01795-w</link>
      <description><![CDATA[摘要
一般场景的 3D 重建仍然是一个公开的挑战，当前技术通常依赖于场景表面反射率的假设，这限制了可建模对象的范围。亥姆霍兹立体视觉提供了一个有吸引力的框架，使建模过程与表面反射率无关。然而，以前的公式几乎完全局限于 2.5D 建模。为了解决这一差距，本文介绍了一系列重建方法，这些方法利用亥姆霍兹互易性来生成具有任意未知反射率的物体的完整 3D 模型。这包括基于（正交或透视）视图相关重建融合的方法、优化体素网格内表面位置的体积方法以及优化给定网格拓扑的顶点位置的基于网格的公式。贡献的方法在合成和真实数据集上进行评估，包括与本文一起公开发布的新颖的完整 3D 数据集，并与各种竞争方法进行实验比较。结果证明了不同方法的优势及其实现复杂对象的高质量全 3D 重建的能力。]]></description>
      <guid>http://link.springer.com/10.1007/s11263-023-01795-w</guid>
      <pubDate>Fri, 01 Sep 2023 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多视图立体的多分布拟合</title>
      <link>http://link.springer.com/10.1007/s00138-023-01449-4</link>
      <description><![CDATA[摘要
我们提出了一种基于多分布拟合的多视图立体网络（MDF-Net），它实现了低内存和高效率的高分辨率深度图预测。该方法采用四级级联结构，主要有以下三个贡献。首先，提出了视图成本正则化来削弱匹配噪声对构建成本量的影响。其次，建议使用多分布拟合（MDF）自适应地计算深度细化区间。高斯分布拟合用于在大区间内细化和校正深度，然后使用拉普拉斯分布拟合来精确估计小区间内的深度。第三，在第四阶段应用轻量级图像超分辨率网络对深度图进行上采样，以减少运行时间和内存需求。 DTU数据集上的实验结果表明MDF-Net取得了最先进的结果。它在高分辨率重建方法中具有最低的内存消耗和运行时间，仅需要大约4.29G内存来预测分辨率为1600 × 1184的深度图。此外，我们在Tanks和Temples数据集上验证了泛化能力，取得极具竞争力的业绩。代码已发布于 https://github.com/zongh5a/MDF-Net.]]></description>
      <guid>http://link.springer.com/10.1007/s00138-023-01449-4</guid>
      <pubDate>Wed, 30 Aug 2023 00:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>