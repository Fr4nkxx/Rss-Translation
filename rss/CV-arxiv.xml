<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Thu, 22 Aug 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>利用高分辨率、多传感器卫星图像预测全球地上生物量、冠层高度和覆盖度的统一深度学习模型</title>
      <link>https://arxiv.org/abs/2408.11234</link>
      <description><![CDATA[arXiv:2408.11234v1 公告类型：新
摘要：定期测量世界森林中的碳储量对于国家和国际气候倡议下的碳核算和报告以及科学研究至关重要，但由于缺乏地面评估，其可扩展性和时间分辨率受到很大限制。人们通过整合遥感数据，越来越多地努力应对这些挑战。我们提出了一种新方法，该方法使用分辨率为 10 米的多传感器、多光谱图像和基于深度学习的模型，该模型统一了地上生物量密度 (AGBD)、冠层高度 (CH)、冠层覆盖率 (CC) 的预测以及这三个量的不确定性估计。该模型经过数百万次全球采样的 GEDI-L2/L4 测量的训练。我们通过在 2023 年将模型部署到全球以及从 2016 年到 2023 年每年在选定区域部署模型来验证模型的能力。该模型在全球采样的测试数据集上实现了 AGBD（CH、CC）的平均绝对误差 26.1 Mg/ha（3.7 m，9.9 %）和均方根误差 50.6 Mg/ha（5.4 m，15.8 %），与之前发布的结果相比有显著改善。我们还报告了模型与文献中发表的独立收集的地面测量值的性能，这些测量值在不同条件下显示出高度的相关性。我们进一步表明，由于我们的预训练模型具有多头架构，因此可以无缝转移到其他 GEDI 变量。]]></description>
      <guid>https://arxiv.org/abs/2408.11234</guid>
      <pubDate>Fri, 23 Aug 2024 03:13:54 GMT</pubDate>
    </item>
    <item>
      <title>CooPre：V2X 协作感知的协作预训练</title>
      <link>https://arxiv.org/abs/2408.11241</link>
      <description><![CDATA[arXiv:2408.11241v1 公告类型：新
摘要：现有的车对万物 (V2X) 协作感知方法依赖于准确的多智能体 3D 注释。然而，收集和注释真实世界数据非常耗时且成本高昂，尤其是对于 V2X 系统而言。在本文中，我们提出了一种用于 V2X 协作感知的自监督学习方法，该方法利用大量未标记的 3D V2X 数据来增强感知性能。除了简单地扩展以前的点云表示学习预训练方法之外，我们还引入了一种针对协作场景定制的新型自监督协作预训练框架 (称为 CooPre)。我们指出，协作点云感知可以弥补智能体之间的信息丢失。这促使我们为 3D 编码器设计一种新颖的代理任务，以跨不同智能体重建 LiDAR 点云。此外，我们开发了一种 V2X 鸟瞰 (BEV) 引导掩蔽策略，该策略有效地允许模型在 BEV 空间中关注异构 V2X 代理（即车辆和基础设施）的 3D 特征。值得注意的是，这种掩蔽策略有效地预训练了 3D 编码器，并且与主流协作感知主干兼容。我们的方法通过对代表性数据集（即 V2X-Real、V2V4Real 和 OPV2V）的大量实验得到验证，可提高所有 V2X 设置的性能。此外，我们还展示了该框架在具有挑战性的场景下跨域可转移性、数据效率和鲁棒性的改进。代码将公开发布。]]></description>
      <guid>https://arxiv.org/abs/2408.11241</guid>
      <pubDate>Fri, 23 Aug 2024 03:13:54 GMT</pubDate>
    </item>
    <item>
      <title>基于 CNN 的标记裂纹检测用于图像注释</title>
      <link>https://arxiv.org/abs/2408.11250</link>
      <description><![CDATA[arXiv:2408.11250v1 公告类型：新
摘要：已采用多种图像处理技术 (IPT) 来检测裂纹缺陷，为人工现场检查提供了替代方案。这些 IPT 处理图像以提取缺陷特征，特别是通过增材制造 (AM) 产生的表面裂纹。本文介绍了一种基于视觉的方法，该方法利用深度卷积神经网络 (CNN) 来检测 AM 表面中的裂纹。传统的图像处理技术面临着多样化现实场景和不同裂纹类型的挑战。为了克服这些挑战，我们提出的方法利用了 CNN，无需进行广泛的特征提取。LabelImg 无需额外的 IPT 即可为 CNN 训练提供注释。经过 OpenCV 预处理技术的增强，经过训练的 CNN 在 14,982 张带注释图像的数据集上实现了出色的 99.54% 的准确率，分辨率为 1536 x 1103 像素。超过 96% 的准确率、98% 的召回率和 97% 的 F1 分数的评估指标凸显了整个过程的精确度和有效性。]]></description>
      <guid>https://arxiv.org/abs/2408.11250</guid>
      <pubDate>Fri, 23 Aug 2024 03:13:54 GMT</pubDate>
    </item>
    <item>
      <title>重新审视对抗训练中的最小-最大优化问题</title>
      <link>https://arxiv.org/abs/2408.11218</link>
      <description><![CDATA[arXiv:2408.11218v1 公告类型：新
摘要：现实世界中计算机视觉应用的兴起使深度神经网络的安全性面临风险。最近的研究表明，卷积神经网络容易受到对抗性示例的影响 - 输入图像看起来与自然图像相似，但被模型错误分类。为了反驳这个问题，我们提出了一种新方法，通过重新制定 \cite{madry2017towards} 中的鞍点优化问题来构建抵御对抗性攻击的稳健深度神经网络。我们提出的方法提供了显着的抵抗力和针对多个对手的具体安全保障。本文的目标是作为深度学习模型新变体的垫脚石，从而实现完全稳健的深度学习模型。]]></description>
      <guid>https://arxiv.org/abs/2408.11218</guid>
      <pubDate>Fri, 23 Aug 2024 03:13:53 GMT</pubDate>
    </item>
    <item>
      <title>开放词汇模型在异常街景中物体检测的潜力</title>
      <link>https://arxiv.org/abs/2408.11221</link>
      <description><![CDATA[arXiv:2408.11221v1 公告类型：新
摘要：分布外 (OOD) 对象检测是一项关键任务，重点是检测来自与训练数据不同的数据分布的对象。在这项研究中，我们调查了最先进的开放词汇对象检测器在多大程度上可以检测街道场景中的异常物体，这些物体在常见街道场景数据集中被视为 OOD 或罕见场景。具体来说，我们在 OoDIS 基准上评估了它们的性能，该基准扩展了 SegmentMeIfYouCan 的 RoadAnomaly21 和 RoadObstacle21，以及最近扩展到对象级注释的 LostAndFound。我们研究的目的是揭示当代对象检测器在具有挑战性的现实世界中，特别是在开放世界场景中的缺点。我们的实验表明，开放词汇模型对于 OOD 对象检测场景很有前景，但远非完美。在它们能够可靠地部署到现实世界的应用中之前，还需要进行实质性的改进。我们在三个不同的数据集上对四种最先进的开放词汇对象检测模型进行了基准测试。值得注意的是，在我们的研究中，Grounding DINO 在 RoadObstacle21 和 LostAndFound 上取得了最佳效果，AP 分别为 48.3% 和 25.4%。YOLO-World 在 RoadAnomaly21 上表现出色，AP 为 21.2%。]]></description>
      <guid>https://arxiv.org/abs/2408.11221</guid>
      <pubDate>Fri, 23 Aug 2024 03:13:53 GMT</pubDate>
    </item>
    <item>
      <title>PooDLe：来自自然视频的池化和密集自监督学习</title>
      <link>https://arxiv.org/abs/2408.11208</link>
      <description><![CDATA[arXiv:2408.11208v1 公告类型：新
摘要：自监督学习推动了从单一主题标志性图像中学习的重大进展。然而，关于使用最低限度策划的自然视频数据，仍然存在未解答的问题，这些数据包含具有许多独立对象的密集场景、不平衡的类分布和不同的对象大小。在本文中，我们提出了一种新颖的方法，该方法将基于不变性的池化表示 SSL 目标与强制光流扭曲等方差的密集 SSL 目标相结合。我们的研究结果表明，在多个特征尺度上应用统一的目标对于从高分辨率自然视频中学习有效的图像表示至关重要。我们在 BDD100K 驾驶视频数据集和 Walking Tours 第一人称视频数据集上验证了我们的方法，证明了它能够从密集的目标中捕获空间理解，并通过池化表示目标捕获语义理解。]]></description>
      <guid>https://arxiv.org/abs/2408.11208</guid>
      <pubDate>Fri, 23 Aug 2024 03:13:52 GMT</pubDate>
    </item>
    <item>
      <title>SAM2 在 3D CT 图像分割中的表现的简要回顾和评估</title>
      <link>https://arxiv.org/abs/2408.11210</link>
      <description><![CDATA[arXiv:2408.11210v1 公告类型：新
摘要：自 Segment Anything 2 (SAM2) 发布以来，医学成像界一直在积极评估其在 3D 医学图像分割方面的性能。然而，不同的研究采用了不同的评估流程，导致结果相互矛盾，模糊了对 SAM2 功能和潜在应用的清晰理解。我们简要回顾了现有的基准，并指出 SAM2 论文清楚地概述了一个零样本评估流程，该流程最多可迭代八次模拟用户点击。我们在 3D CT 数据集上重现了这种交互式注释模拟，并提供了结果和代码~\url{https://github.com/Project-MONAI/VISTA}。我们的研究结果表明，以零样本方式直接将 SAM2 应用于 3D 医学成像远远不能令人满意。当前景物体消失时，它很容易产生误报，注释更多切片无法完全抵消这种趋势。对于肾脏和主动脉等较小的单连接对象，SAM2 表现相当不错，但对于大多数器官，它仍然远远落后于最先进的 3D 注释方法。3D 医学成像界需要进行更多研究和创新才能正确使用 SAM2。]]></description>
      <guid>https://arxiv.org/abs/2408.11210</guid>
      <pubDate>Fri, 23 Aug 2024 03:13:52 GMT</pubDate>
    </item>
    <item>
      <title>针对自动驾驶汽车中传感器错位的稳健远程感知</title>
      <link>https://arxiv.org/abs/2408.11196</link>
      <description><![CDATA[arXiv:2408.11196v1 公告类型：新
摘要：传感器融合机器学习算法的进步显著改善了对其他道路使用者的检测和预测，从而提高了安全性。然而，即使传感器放置位置有很小的角度位移也会导致输出显著下降，尤其是在远距离时。在本文中，我们展示了一种简单但通用且高效的多任务学习方法，它不仅可以检测不同传感器模式之间的错位，而且对于远距离感知也具有很强的鲁棒性。除了错位量之外，我们的方法还可以预测校准后的不确定性，这对于过滤和融合随时间推移预测的错位值很有用。此外，我们表明预测的错位参数可用于自校正输入传感器数据，从而进一步提高传感器错位下的感知性能。]]></description>
      <guid>https://arxiv.org/abs/2408.11196</guid>
      <pubDate>Fri, 23 Aug 2024 03:13:51 GMT</pubDate>
    </item>
    <item>
      <title>量子逆上下文视觉变换器 (Q-ICVT)：自动驾驶汽车 3D 物体检测的新前沿</title>
      <link>https://arxiv.org/abs/2408.11207</link>
      <description><![CDATA[arXiv:2408.11207v1 公告类型：新
摘要：自动驾驶汽车 (AV) 领域主要利用 LiDAR 和摄像头数据的多模态集成来实现比使用单一模态更好的性能。然而，由于摄像头的高分辨率与 LiDAR 的稀疏数据之间的差异，融合过程在检测远距离物体时遇到挑战。全局视角与局部细节的整合不足导致融合性能不佳。为了解决这个问题，我们开发了一种创新的两阶段融合过程，称为量子逆上下文视觉变换器 (Q-ICVT)。这种方法利用量子概念中的绝热量计算来创建一种称为全局绝热量变换器 (GAT) 的新型可逆视觉变换器。GAT 将稀疏的 LiDAR 特征与密集图像中的语义特征聚合在一起，以全局形式进行跨模态集成。此外，稀疏局部融合专家 (SELF) 模块使用门控点融合方法将稀疏的 LiDAR 3D 提案和原始点云的位置信息映射到密集的相机特征空间上。我们的实验表明，Q-ICVT 在 Waymo 数据集上实现了 L2 难度的 82.54 mAPH，比目前最先进的融合方法提高了 1.88%。我们还在消融研究中分析了 GAT 和 SELF，以突出 Q-ICVT 的影响。我们的代码可在 https://github.com/sanjay-810/Qicvt Q-ICVT 上找到]]></description>
      <guid>https://arxiv.org/abs/2408.11207</guid>
      <pubDate>Fri, 23 Aug 2024 03:13:51 GMT</pubDate>
    </item>
    <item>
      <title>一种可解释的深度学习方法，用于形态脚本类型分析</title>
      <link>https://arxiv.org/abs/2408.11150</link>
      <description><![CDATA[arXiv:2408.11150v1 公告类型：新
摘要：定义脚本类型并建立中世纪手写体的分类标准是古文字分析的核心方面。然而，现有的类型学经常遇到方法论挑战，例如描述限制和主观标准。我们提出了一种可解释的基于深度学习的形态脚本类型分析方法，该方法可以进行系统和客观的分析，并有助于弥合定性观察和定量测量之间的差距。更准确地说，我们采用深度实例分割方法来学习可比较的字符原型，代表字母形态，并提供定性和定量工具来比较和分析它们。我们通过将我们的方法应用于 Textualis Formata 脚本类型及其由 A. Derolez 形式化的两种子类型来展示我们的方法：北方和南方 Textualis]]></description>
      <guid>https://arxiv.org/abs/2408.11150</guid>
      <pubDate>Fri, 23 Aug 2024 03:13:50 GMT</pubDate>
    </item>
    <item>
      <title>数据集构建的统计挑战：为什么你永远不会有足够的图像</title>
      <link>https://arxiv.org/abs/2408.11160</link>
      <description><![CDATA[arXiv:2408.11160v1 公告类型：新
摘要：近年来，深度神经网络在许多计算机视觉基准测试中取得了令人印象深刻的表现。但是，我们能否确信基准测试中的出色表现将转化为现实环境中的强大性能？现实世界中的许多环境都是安全至关重要的，即使是轻微的模型故障也可能造成灾难性的后果。因此，在部署之前严格测试模型至关重要。我们通过统计理论和经验证据论证说，在许多领域选择代表性图像数据集来测试模型可能是不合理的。此外，使用非代表性图像数据集计算的性能统计数据非常不可靠。因此，我们无法保证在保留的测试图像上表现良好的模型在现实世界中也会表现良好。创建越来越大的数据集不会有帮助，而偏差感知数据集也无法解决这个问题。最终，使用保留的测试集评估模型的统计基础很少。我们建议未来的评估方法侧重于评估模型的决策过程，而不是准确性等指标。]]></description>
      <guid>https://arxiv.org/abs/2408.11160</guid>
      <pubDate>Fri, 23 Aug 2024 03:13:50 GMT</pubDate>
    </item>
    <item>
      <title>条件扩散采样中的压缩指导</title>
      <link>https://arxiv.org/abs/2408.11194</link>
      <description><![CDATA[arXiv:2408.11194v1 公告类型：新
摘要：由于模型拟合问题，在整个采样过程中强制执行指导通常会适得其反，其中生成的样本是为了匹配分类器的参数而不是概括预期条件。这项工作识别并量化了问题，表明减少或排除多个时间步的指导可以缓解此问题。通过在过程的早期阶段密集分布指导，我们观察到图像质量和多样性的显着改善，同时还将所需的指导时间步减少了近 40%。这种方法解决了将指导有效应用于生成任务的重大挑战。因此，我们提出的方法称为压缩指导，允许排除大量指导时间步，同时在图像质量方面仍然超越基线模型。我们通过对各种数据集和模型中的标签条件和文本到图像生成任务的基准测试来验证我们的方法。]]></description>
      <guid>https://arxiv.org/abs/2408.11194</guid>
      <pubDate>Fri, 23 Aug 2024 03:13:50 GMT</pubDate>
    </item>
    <item>
      <title>双目模型：采用双波长成像高温法进行在线熔池温度分析的深度学习解决方案</title>
      <link>https://arxiv.org/abs/2408.11126</link>
      <description><![CDATA[arXiv:2408.11126v1 公告类型：新
摘要：在金属增材制造 (AM) 中，监测熔池 (MP) 的温度对于确保零件质量、工艺稳定性、缺陷预防和整体工艺优化至关重要。传统方法收敛速度慢，需要大量手动工作才能将数据转化为可操作的见解，因此不适用于实时监控和控制。为了应对这一挑战，我们提出了一种基于人工智能 (AI) 的解决方案，旨在减少对手动数据处理的依赖并提高从数据到见解的转换效率。在我们的研究中，我们使用了一个包含双波长实时过程监控数据和相应温度图的数据集。我们引入了一种称为“双目模型”的深度学习模型，该模型利用双输入观察对激光粉末床熔合 (L-PBF) 中的 MP 温度进行精确分析。通过先进的深度学习技术，我们将原始数据无缝转换为温度图，大大简化了流程，并实现了高达每秒 750 帧的批处理速度，比传统方法快约 1000 倍。我们的双目模型在温度估计方面实现了高精度，R 平方得分为 0.95，同时将处理效率提高了 $\sim1000x$ 倍。该模型直接解决了实时 MP 温度监测的挑战，并深入了解了遇到的限制以及我们基于深度学习的方法的优势。通过结合效率和精度，我们的工作促进了 L-PBF 温度监测的进步，从而推动了金属 AM 领域的进步。]]></description>
      <guid>https://arxiv.org/abs/2408.11126</guid>
      <pubDate>Fri, 23 Aug 2024 03:13:49 GMT</pubDate>
    </item>
    <item>
      <title>ISLES 2024：首个纵向多模式多中心真实世界数据集，研究（亚）急性中风</title>
      <link>https://arxiv.org/abs/2408.11142</link>
      <description><![CDATA[arXiv:2408.11142v1 公告类型：新
摘要：中风仍然是全球发病率和死亡率的主要原因，带来了沉重的社会经济负担。在过去十年中，血管内再灌注治疗的进展以及使用 CT 和 MRI 成像进行治疗指导显著改善了患者的治疗效果，现在已成为临床实践的标准。为了开发机器学习算法，以便从中风图像中提取有意义且可重复的脑功能模型用于临床和研究目的 - 特别是用于病变识别、脑健康量化和预后 - 大型、多样化且注释良好的公共数据集至关重要。虽然以前只有少数包含（亚）急性中风数据的数据集可用，但最近已经有几个大型高质量数据集向公众开放。但是，这些现有数据集仅包含 MRI 数据。相比之下，我们的数据集是第一个提供全面纵向中风数据的数据集，包括带血管造影和灌注的急性 CT 成像、2-9 天的随访 MRI，以及长达三个月的急性和纵向临床数据。该数据集包括 n = 150 的训练数据集和 n = 100 次扫描的测试数据集。训练数据是公开的，而测试数据将专门用于模型验证。我们将此数据集作为 2024 年缺血性中风病变分割 (ISLES) 挑战赛 (https://www.isles-challenge.org/) 的一部分提供，该挑战赛旨在持续建立急性和亚急性缺血性中风病变分割的基准方法，帮助创建开放的中风成像数据集并评估尖端的图像处理算法。]]></description>
      <guid>https://arxiv.org/abs/2408.11142</guid>
      <pubDate>Fri, 23 Aug 2024 03:13:49 GMT</pubDate>
    </item>
    <item>
      <title>GSLoc：通过 3D 高斯溅射实现高效的相机姿态细化</title>
      <link>https://arxiv.org/abs/2408.11085</link>
      <description><![CDATA[arXiv:2408.11085v1 公告类型：新
摘要：我们利用 3D Gaussian Splatting (3DGS) 作为场景表示，并提出了一种新颖的测试时间相机姿势细化框架 GSLoc。该框架提高了最先进的绝对姿势回归和场景坐标回归方法的定位精度。3DGS 模型呈现高质量的合成图像和深度图，以促进 2D-3D 对应的建立。GSLoc 通过直接在 RGB 图像上操作，利用 3D 视觉基础模型 MASt3R 进行精确的 2D 匹配，消除了训练特征提取器或描述符的需要。为了提高我们的模型在具有挑战性的户外环境中的稳健性，我们在 3DGS 框架中加入了曝光自适应模块。因此，GSLoc 可以在给定单个 RGB 查询和粗略的初始姿势估计的情况下实现有效的姿势细化。我们提出的方法在室内和室外视觉定位基准上的准确性和运行时间都超越了领先的基于 NeRF 的优化方法，在两个室内数据集上实现了最先进的准确性。]]></description>
      <guid>https://arxiv.org/abs/2408.11085</guid>
      <pubDate>Fri, 23 Aug 2024 03:13:48 GMT</pubDate>
    </item>
    </channel>
</rss>