<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Tue, 13 Aug 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>SAM-FNet：用于喉咽肿瘤检测的 SAM 引导融合网络</title>
      <link>https://arxiv.org/abs/2408.05426</link>
      <description><![CDATA[arXiv:2408.05426v1 公告类型：新
摘要：喉咽癌 (LPC) 是一种影响头颈部的高度致命恶性疾病。先前对内窥镜肿瘤检测的研究，特别是利用双分支网络架构的研究，已显示出肿瘤检测的重大进步。这些研究强调了双分支网络通过有效整合全局和局部（病变）特征提取来提高诊断准确性的潜力。然而，它们在准确定位病变区域和捕获全局和局部分支之间的判别特征信息方面的能力仍然有限。为了解决这些问题，我们提出了一种新型的 SAM 引导融合网络 (SAM-FNet)，这是一种用于喉咽肿瘤检测的双分支网络。通过利用 Segment Anything 模型 (SAM) 强大的对象分割功能，我们将 SAM 引入 SAM-FNet 以准确分割病变区域。此外，我们提出了一个类似GAN的特征优化（GFO）模块来捕获全局和局部分支之间的判别特征，增强融合特征的互补性。此外，我们从中山大学附属第一医院（FAHSYSU）和中山大学附属第六医院（SAHSYSU）收集了两个LPC数据集。FAHSYSU数据集用作训练模型的内部数据集，而SAHSYSU数据集用作评估模型性能的外部数据集。在FAHSYSU和SAHSYSU两个数据集上进行的大量实验表明，SAM-FNet可以取得有竞争力的结果，优于最先进的同类产品。SAM-FNet的源代码可在https://github.com/VVJia/SAM-FNet的URL上找到。]]></description>
      <guid>https://arxiv.org/abs/2408.05426</guid>
      <pubDate>Tue, 13 Aug 2024 06:18:24 GMT</pubDate>
    </item>
    <item>
      <title>通过基于地标的扩散模型实现高保真、口型同步的说话人脸合成</title>
      <link>https://arxiv.org/abs/2408.05416</link>
      <description><![CDATA[arXiv:2408.05416v1 公告类型：新
摘要：音频驱动的说话脸视频生成因其巨大的工业潜力而受到越来越多的关注。一些以前的方法专注于学习从音频到视觉内容的直接映射。尽管取得了进展，但它们经常在映射过程的模糊性方面遇到困难，导致结果有缺陷。另一种策略涉及面部结构表征（例如面部标志）作为中介。这种多阶段方法可以更好地保留外观细节，但由于不同阶段的独立优化而遭受错误积累。此外，大多数以前的方法依赖于生成对抗网络，容易出现训练不稳定和模式崩溃。为了应对这些挑战，我们的研究提出了一种新的基于标志的说话脸部生成扩散模型，该模型利用面部标志作为中间表示，同时实现端到端优化。具体来说，我们首先建立从音频到嘴唇和下巴标志运动的不太模糊的映射。然后，我们引入了一个名为 TalkFormer 的创新调节模块，通过可微分交叉注意将合成运动与地标所表示的运动对齐，从而实现端到端优化，以改善唇部同步。此外，TalkFormer 采用隐式特征扭曲将参考图像特征与目标运动对齐，以保留更多外观细节。大量实验表明，我们的方法可以合成高保真和唇部同步的说话脸部视频，并从参考图像中保留更多主体外观细节。]]></description>
      <guid>https://arxiv.org/abs/2408.05416</guid>
      <pubDate>Tue, 13 Aug 2024 06:18:23 GMT</pubDate>
    </item>
    <item>
      <title>EPAM-Net：一种用于视频动作识别的高效姿势驱动注意力引导多模态网络</title>
      <link>https://arxiv.org/abs/2408.05421</link>
      <description><![CDATA[arXiv:2408.05421v1 公告类型：新
摘要：现有的基于多模态的人体动作识别方法要么计算成本高昂，限制了它们在实时场景中的适用性，要么无法利用多种数据模态的时空信息。在这项工作中，我们提出了一种高效的姿势驱动注意力引导多模态网络 (EPAM-Net) 用于视频中的动作识别。具体来说，我们针对 RGB 和姿势流调整了 X3D 网络，以从 RGB 视频及其骨架序列中捕获时空特征。然后利用骨架特征帮助视觉网络流使用时空注意块聚焦于关键帧及其显着的空间区域。最后，将所提出的网络的两个流的分数融合在一起进行最终分类。实验结果表明，我们的方法在 NTU-D 60 和 NTU RGB-D 120 基准数据集上取得了具有竞争力的性能。此外，我们的模型将 FLOP（浮点运算，乘法加法次数）减少了 6.2-9.9 倍，将网络参数数量减少了 9-9.6 倍。代码将在 https://github.com/ahmed-nady/Multimodal-Action-Recognition 上提供。]]></description>
      <guid>https://arxiv.org/abs/2408.05421</guid>
      <pubDate>Tue, 13 Aug 2024 06:18:23 GMT</pubDate>
    </item>
    <item>
      <title>音频如何影响全向视频中的视觉注意力？数据库和模型</title>
      <link>https://arxiv.org/abs/2408.05411</link>
      <description><![CDATA[arXiv:2408.05411v1 公告类型：新
摘要：理解和预测全向视频 (ODV) 中的观看者注意力对于增强虚拟和增强现实应用中的用户参与度至关重要。尽管音频和视觉模态对于 ODV 中的显着性预测都至关重要，但这两种模态的联合利用受到限制，主要是因为缺乏大规模的视听显着性数据库和全面的分析。本文从主观和客观角度全面研究了 ODV 中的视听注意力。具体来说，我们首先介绍一个新的全向视频视听显着性数据库，称为 AVS-ODV 数据库，其中包含 162 个 ODV 和相应的眼动数据，这些数据是在静音、单声道和环绕声三种音频模式下从 60 个受试者收集的。基于构建的 AVS-ODV 数据库，我们对音频如何影响 ODV 中的视觉注意力进行了深入分析。为了推进 ODV 的视听显著性预测研究，我们通过测试大量最先进的显著性模型（包括纯视觉模型和视听模型），进一步基于 AVS-ODV 数据库建立了新的基准。此外，考虑到当前模型的局限性，我们提出了一种创新的全向视听显著性预测网络 (OmniAVS)，该网络基于 U-Net 架构构建，并分层融合来自多模态对齐嵌入空间的音频和视觉特征。大量实验结果表明，所提出的 OmniAVS 模型在 ODV AVS 预测和传统 AVS 预测任务上均优于其他最先进的模型。AVS-ODV 数据库和 OmniAVS 模型将发布以促进未来的研究。]]></description>
      <guid>https://arxiv.org/abs/2408.05411</guid>
      <pubDate>Tue, 13 Aug 2024 06:18:22 GMT</pubDate>
    </item>
    <item>
      <title>通过音频感知风格参考实现保留风格的口型同步</title>
      <link>https://arxiv.org/abs/2408.05412</link>
      <description><![CDATA[arXiv:2408.05412v1 公告类型：新
摘要：音频驱动的唇形同步最近因其在多媒体领域的广泛应用而引起了广泛关注。由于个人独特的说话风格，每个人在说同一句话时会表现出不同的唇形，这对音频驱动的唇形同步提出了显著的挑战。早期的此类任务方法通常会绕过个性化说话风格的建模，导致符合一般风格的次优唇形同步。最近的唇形同步技术试图通过聚合来自风格参考视频的信息来指导任意音频的唇形同步，但由于它们在风格聚合方面的不准确性，它们无法很好地保留说话风格。这项工作提出了一种创新的音频感知风格参考方案，该方案有效地利用输入音频和来自风格参考视频的参考音频之间的关系来解决保留风格的音频驱动唇形同步问题。具体来说，我们首先开发一种基于 Transformer 的高级模型，该模型擅长预测与输入音频相对应的唇部运动，并通过交叉注意层从风格参考视频中聚合的风格信息进行增强。然后，为了更好地将唇部运动渲染成逼真的说话面部视频，我们设计了一个条件潜在扩散模型，通过调制卷积层整合唇部运动，并通过空间交叉注意层融合参考面部图像。大量实验验证了所提出方法在实现精确唇部同步、保留说话风格和生成高保真、逼真的说话面部视频方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2408.05412</guid>
      <pubDate>Tue, 13 Aug 2024 06:18:22 GMT</pubDate>
    </item>
    <item>
      <title>基于网格变形的细眼镜框单视图三维重建及可区分渲染</title>
      <link>https://arxiv.org/abs/2408.05402</link>
      <description><![CDATA[arXiv:2408.05402v1 公告类型：新
摘要：在虚拟现实 (VR) 和增强现实 (AR) 技术的支持下，3D 虚拟眼镜试戴应用程序正在成为一种新的流行解决方案，它提供“试戴”选项，让您在家中舒适地选择一副完美的眼镜。使用传统的深度和基于图像的方法从单个图像重建眼镜框架非常困难，因为它们具有缺乏足够的纹理特征、薄元素和严重的自遮挡等独特特征。在本文中，我们提出了第一个基于网格变形的重建框架，用于从单个 RGB 图像恢复高精度 3D 全框眼镜模型，利用先验和特定领域的知识。具体而言，基于合成眼镜架数据集的构建，我们首先定义具有预定义关键点的特定类的眼镜架模板。然后，给定一个结构薄且纹理特征较少的输入眼镜框图像，我们设计一个关键点检测器和细化器，以由粗到细的方式检测预定义的关键点，以准确估计相机姿势。之后，使用可微分渲染，我们提出了一种新颖的优化方法，通过在模板网格上逐步执行自由变形 (FFD) 来生成正确的几何图形。我们定义了一系列损失函数来强制渲染结果与相应的 RGB 输入之间的一致性，利用固有结构、轮廓、关键点、每像素阴影信息等的约束。在合成数据集和真实图像上的实验结果证明了所提算法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2408.05402</guid>
      <pubDate>Tue, 13 Aug 2024 06:18:21 GMT</pubDate>
    </item>
    <item>
      <title>RSL-BA：卷帘线束调整</title>
      <link>https://arxiv.org/abs/2408.05409</link>
      <description><![CDATA[arXiv:2408.05409v1 Announce Type: new 
摘要：线是人造环境中普遍存在的元素，它本身就编码了空间结构信息，因此在实际应用中，它是一种更稳健的特征表示选择。尽管以前的滚动快门束调整 (RSBA) 方法具有明显的优势，但它仅支持稀疏特征点，这缺乏稳健性，尤其是在退化环境中。在本文中，我们介绍了第一个基于滚动快门线的束调整解决方案 RSL-BA。具体而言，我们首先利用 Pl\&quot;ucker 线参数化建立了滚动快门相机线投影理论。随后，我们推导出一系列稳定有效的重投影误差公式。最后，我们从理论和实验上证明了我们的方法可以防止三种常见的退化，其中一种是本文首次发现的。大量的合成和真实数据实验表明，我们的方法实现了与现有基于点的滚动快门束调整解决方案相当的效率和准确性。]]></description>
      <guid>https://arxiv.org/abs/2408.05409</guid>
      <pubDate>Tue, 13 Aug 2024 06:18:21 GMT</pubDate>
    </item>
    <item>
      <title>球形世界锁定用于自我中心视频中的视听定位</title>
      <link>https://arxiv.org/abs/2408.05364</link>
      <description><![CDATA[arXiv:2408.05364v1 公告类型：新
摘要：以自我为中心的视频为用户和场景理解提供了全面的背景，涵盖了从多感官感知到行为交互的各个方面。我们提出了球面世界锁定 (SWL) 作为以自我为中心的场景表示的通用框架，它隐式地根据头部方向的测量转换多感官流。与具有 2D 平面视野的传统头部锁定以自我为中心的表示相比，SWL 有效地抵消了自我运动带来的挑战，从而改善了输入模态之间的空间同步。使用一组世界锁定球体上的多感官嵌入，我们设计了一个统一的编码器-解码器转换器架构，该架构保留了场景表示的球面结构，而无需在图像和世界坐标系之间进行昂贵的投影。我们评估了所提出的框架在以自我为中心的视频理解的多个基准任务上的有效性，包括视听主动说话者定位、听觉球面源定位和日常活动中的行为预测。]]></description>
      <guid>https://arxiv.org/abs/2408.05364</guid>
      <pubDate>Tue, 13 Aug 2024 06:18:20 GMT</pubDate>
    </item>
    <item>
      <title>DeepSpeak 数据集 v1.0</title>
      <link>https://arxiv.org/abs/2408.05366</link>
      <description><![CDATA[arXiv:2408.05366v1 公告类型：新
摘要：我们描述了一个大规模数据集——{\em DeepSpeak}——包含人们在网络摄像头前说话和做手势的真实和深度伪造镜头。该数据集的第一个版本中的真实视频由来自 220 个不同个体的 9 小时镜头组成。假视频包含超过 25 小时的镜头，包括一系列不同的最先进的换脸和口型同步深度伪造，带有自然和人工智能生成的声音。我们预计将发布该数据集的未来版本，其中包含不同的和更新的深度伪造技术。该数据集可免费用于研究和非商业用途；商业用途的请求将被考虑。]]></description>
      <guid>https://arxiv.org/abs/2408.05366</guid>
      <pubDate>Tue, 13 Aug 2024 06:18:20 GMT</pubDate>
    </item>
    <item>
      <title>PersonViT：用于行人重新识别的大规模自监督视觉转换器</title>
      <link>https://arxiv.org/abs/2408.05398</link>
      <description><![CDATA[arXiv:2408.05398v1 Announce Type: new 
摘要：行人重识别（Person Re-Identification，ReID）旨在从非重叠的摄像头图像中检索相关个体，在公共安全领域有着广泛的应用。近年来，随着 Vision Transformer（ViT）和自监督学习技术的发展，基于自监督预训练的行人重识别性能得到了很大的提升。行人重识别需要提取人体高判别性的局部细粒度特征，而传统 ViT 擅长提取与上下文相关的全局特征，难以聚焦局部人体特征。为此，本文将近期兴起的 Masked Image Modeling (MIM) 自监督学习方法引入到行人 ReID 中，通过结合遮罩图像建模和判别性对比学习，通过大规模无监督预训练有效提取出高质量的全局和局部特征，然后在行人 ReID 任务中进行有监督的微调训练。这种基于遮罩图像建模的 ViT (PersonViT) 的行人特征提取方法具有无监督、可扩展、泛化能力强的良好特性，克服了有监督行人 ReID 中标注困难的问题，并在公开的基准数据集上取得了最佳效果，包括 MSMT17、Market1501、DukeMTMC-reID、Occluded-Duke。PersonViT 方法的代码和预训练模型发布在 https://github.com/hustvl/PersonViT，以促进行人 ReID 领域的进一步研究。]]></description>
      <guid>https://arxiv.org/abs/2408.05398</guid>
      <pubDate>Tue, 13 Aug 2024 06:18:20 GMT</pubDate>
    </item>
    <item>
      <title>实现快速、准确的众包注释，以进行高程感知洪水范围测绘</title>
      <link>https://arxiv.org/abs/2408.05350</link>
      <description><![CDATA[arXiv:2408.05350v1 公告类型：新
摘要：为了评估损失并适当分配救援工作，绘制洪水事件的范围是灾害管理的一个必要且重要的方面。近年来，深度学习方法已经发展成为一种有效的工具，可以快速标记高分辨率图像并提供必要的洪水范围映射。然而，这些方法需要大量带注释的训练数据来创建准确且对新洪水图像具有鲁棒性的模型。在这项工作中，我们提供了 FloodTrace，这是一个应用程序，可以有效地众包洪水区域注释机器学习训练数据，消除了仅由研究人员进行注释的要求。我们通过应用程序中的两种正交方法来实现这一点，并根据领域专家的要求提供信息。首先，我们利用高程引导注释工具和 3D 渲染，通过数字高程模型数据为用户注释决策提供信息，从而提高注释准确性。为此，我们提供了一种独特的注释方法，该方法使用拓扑数据分析来超越最先进的高程引导注释工具的效率。其次，我们为研究人员提供了一个框架，让他们使用受不确定性可视化启发的方法来审查聚合的众包注释并纠正不准确的信息。我们进行了一项用户研究，以确认应用程序的有效性，其中 266 名研究生对北卡罗来纳州飓风马修的高分辨率航拍图像进行了注释。实验结果表明，我们的应用程序的准确性和效率优势甚至适用于未经训练的用户。此外，使用我们的聚合和校正框架，在众包注释上训练的洪水检测模型能够实现与在专家标记注释上训练的模型相同的性能，而研究人员只需要花费一小部分时间。]]></description>
      <guid>https://arxiv.org/abs/2408.05350</guid>
      <pubDate>Tue, 13 Aug 2024 06:18:19 GMT</pubDate>
    </item>
    <item>
      <title>AyE-Edge：自动部署空间搜索，实现边缘上准确而高效的实时物体检测</title>
      <link>https://arxiv.org/abs/2408.05363</link>
      <description><![CDATA[arXiv:2408.05363v1 公告类型：新 
摘要：边缘物体检测（Edge-OD）因其日益广泛的应用前景而日益受到需求的约束。然而，该领域的发展受到同时实现高精度、高能效和满足严格实时要求的部署困境的严格限制。为了解决这一难题，我们提出了 AyE-Edge，这是第一种此类开发工具，它探索自动算法设备部署空间搜索，以实现准确且节能的实时边缘物体检测。通过对关键帧选择、CPU-GPU 配置和 DNN 修剪策略的协同探索，AyE-Edge 在移动设备上进行的大量实际实验中表现出色。结果一致证明了 AyE-Edge 的有效性，与最先进的 (SOTA) 竞争对手相比，实现了出色的实时性能、检测精度，尤其是功耗降低了 96.7%。]]></description>
      <guid>https://arxiv.org/abs/2408.05363</guid>
      <pubDate>Tue, 13 Aug 2024 06:18:19 GMT</pubDate>
    </item>
    <item>
      <title>VACoDe：视觉增强对比解码</title>
      <link>https://arxiv.org/abs/2408.05337</link>
      <description><![CDATA[arXiv:2408.05337v1 公告类型：新
摘要：尽管最近的大型视觉语言模型 (LVLM) 表现惊人，但这些模型通常会产生不准确的响应。为了解决这个问题，以前的研究重点是通过对增强图像采用对比解码 (CD) 来减轻幻觉，这会放大与原始图像的对比度。然而，这些方法有局限性，包括依赖单一增强，这对某些任务有限制，以及使用外部知识的成本高。在本研究中，我们通过探索如何利用多个图像增强来解决这些限制。通过大量的实验，我们观察到不同的增强会根据任务产生不同程度的对比度。基于这一观察，我们引入了一种名为 VACoDe（视觉增强对比解码）的新方法。该方法使用提出的 softmax 距离度量自适应地为每个任务选择对比度最高的增强。我们的实证测试表明，\alg 的表现优于以前的方法，并提高了各种视觉语言任务的输出质量。此外，VACoDe 可以普遍应用于不同类型和大小的模型，而无需额外训练或使用外部模型和数据。]]></description>
      <guid>https://arxiv.org/abs/2408.05337</guid>
      <pubDate>Tue, 13 Aug 2024 06:18:18 GMT</pubDate>
    </item>
    <item>
      <title>CAR：具有对比度不变潜在正则化的对比度不可知可变形医学图像配准</title>
      <link>https://arxiv.org/abs/2408.05341</link>
      <description><![CDATA[arXiv:2408.05341v1 公告类型：新
摘要：由于不同成像对比度之间存在复杂的强度关系，多对比度图像配准是一项具有挑战性的任务。传统的图像配准方法通常基于对每个输入图像对的迭代优化，这既耗时又对对比度变化敏感。虽然基于学习的方法在推理阶段要快得多，但由于通用性问题，它们通常只能应用于训练阶段观察到的固定对比度。在这项工作中，我们提出了一种新颖的对比度无关可变形图像配准框架，该框架可以推广到任意对比度图像，而无需在训练期间观察它们。特别是，我们提出了一种基于随机卷积的对比度增强方案，该方案在单个图像对比度上模拟图像的任意对比度，同时保留其固有的结构信息。为了确保网络能够学习对比度不变的表示以促进对比度无关的配准，我们进一步引入了对比度不变的潜在正则化 (CLR)，它通过对比度不变损失来规范潜在空间中的表示。实验表明，CAR 在配准精度方面优于基线方法，并且对看不见的成像对比度具有更好的泛化能力。代码可在 \url{https://github.com/Yinsong0510/CAR} 获得。]]></description>
      <guid>https://arxiv.org/abs/2408.05341</guid>
      <pubDate>Tue, 13 Aug 2024 06:18:18 GMT</pubDate>
    </item>
    <item>
      <title>基于事件的对象检测的循环 YOLOv8 框架</title>
      <link>https://arxiv.org/abs/2408.05321</link>
      <description><![CDATA[arXiv:2408.05321v1 公告类型：新
摘要：物体检测在各种尖端应用中至关重要，例如自动驾驶汽车和先进的机器人系统，主要依赖于来自传统基于帧的 RGB 传感器的数据。然而，这些传感器通常会遇到诸如运动模糊和在具有挑战性的照明条件下性能不佳等问题。为了应对这些挑战，基于事件的相机已成为一种创新范式。这些相机模仿人眼，在快速运动和极端照明条件下表现出卓越的性能，同时消耗更少的电量。本研究介绍了 ReYOLOv8，这是一种先进的物体检测框架，它通过时空建模功能增强了领先的基于帧的检测系统。我们实现了一种低延迟、内存高效的事件数据编码方法，以提高系统的性能。我们还开发了一种新颖的数据增强技术，旨在利用事件数据的独特属性，从而提高检测准确性。我们的模型在 GEN1 数据集中的表现优于所有同类方法，专注于汽车应用，在纳米、小规模和中等规模上分别实现了 5%、2.8% 和 2.5% 的平均精度 (mAP) 提升。在实现这些改进的同时，可训练参数的数量平均减少了 4.43%，实时处理速度保持在 9.2 毫秒到 15.5 毫秒之间。在针对机器人应用的 PEDRo 数据集上，我们的模型的 mAP 改进范围为 9% 到 18%，模型尺寸分别缩小了 14.5 倍和 3.8 倍，平均速度提高了 1.67 倍。]]></description>
      <guid>https://arxiv.org/abs/2408.05321</guid>
      <pubDate>Tue, 13 Aug 2024 06:18:17 GMT</pubDate>
    </item>
    </channel>
</rss>