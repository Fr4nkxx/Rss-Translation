<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Mon, 02 Sep 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>STEREO：从文本到图像生成模型中实现对抗性鲁棒概念擦除</title>
      <link>https://arxiv.org/abs/2408.16807</link>
      <description><![CDATA[arXiv:2408.16807v1 公告类型：新
摘要：大规模文本到图像生成 (T2IG) 模型的快速普及引发了人们对其可能被滥用于生成有害内容的担忧。尽管已经提出了许多方法来从 T2IG 模型中删除不需要的概念，但它们只提供了一种虚假的安全感，因为最近的研究表明，概念擦除模型 (CEM) 很容易被欺骗通过对抗性攻击生成被擦除的概念。对抗性鲁棒概念擦除问题而不会显着降低模型效用（生成良性概念的能力）仍然是一个尚未解决的挑战，特别是在对手可以访问 CEM 的白盒设置中。为了解决这一差距，我们提出了一种称为 STEREO 的方法，它涉及两个不同的阶段。第一阶段通过利用对抗性训练中的鲁棒优化原理，充分彻底地搜索强大而多样的对抗性提示，这些提示可以从 CEM 中重新生成被擦除的概念。在第二个稳健擦除一次阶段，我们引入了一个基于锚概念的组合目标，以稳健地一次性擦除目标概念，同时尝试最大限度地减少模型效用的下降。通过在三种对抗性攻击下将所提出的 STEREO 方法与四种最先进的概念擦除方法进行基准测试，我们证明了它能够实现更好的稳健性与效用权衡。我们的代码和模型可在 https://github.com/koushiksrivats/robust-concept-erasing 上找到。]]></description>
      <guid>https://arxiv.org/abs/2408.16807</guid>
      <pubDate>Mon, 02 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>看还是猜：反事实正则化图像字幕</title>
      <link>https://arxiv.org/abs/2408.16809</link>
      <description><![CDATA[arXiv:2408.16809v1 公告类型：新
摘要：图像字幕生成图像中视觉信息的自然语言描述，是视觉语言研究中的一项关键任务。以前的模型通常通过对现有数据集进行统计拟合，将机器的生成能力与人类智能相结合来解决此任务。虽然它们对正常图像有效，但它们可能难以准确描述图像某些部分被遮挡或编辑的图像，而人类则擅长此类情况。它们表现出的这些弱点，包括幻觉和有限的可解释性，通常会阻碍在具有转移关联模式的场景中的表现。在本文中，我们提出了一个通用的图像字幕框架，该框架采用因果推理使现有模型更有能力完成干预任务，并具有反事实可解释性。我们的方法包括两种利用总效应或自然直接效应的变体。将它们集成到训练过程中使模型能够处理反事实场景，从而提高其通用性。在各种数据集上进行的大量实验表明，我们的方法有效地减少了幻觉并提高了模型对图像的忠实度，在小规模和大规模图像到文本模型中都表现出很高的可移植性。代码可在 https://github.com/Aman-4-Real/See-or-Guess 上找到。]]></description>
      <guid>https://arxiv.org/abs/2408.16809</guid>
      <pubDate>Mon, 02 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用自训练奖励模型实现流畅、准确的图像字幕</title>
      <link>https://arxiv.org/abs/2408.16827</link>
      <description><![CDATA[arXiv:2408.16827v1 公告类型：新
摘要：使用手工制作的奖励（如 CIDEr 指标）对图像字幕模型进行微调一直是在序列级别提升字幕质量的经典策略。然而，众所周知，这种方法会限制描述性和语义丰富性，并倾向于将模型推向真实句子的风格，从而失去细节和特异性。相反，最近尝试使用 CLIP 等图像文本模型作为奖励，导致了语法错误和重复的字幕。在本文中，我们提出了 Self-Cap，这是一种字幕方法，它依赖于基于自生成负值的可学习奖励模型，该模型可以根据字幕与图像的一致性对其进行区分。具体来说，我们的鉴别器是一个经过微调的对比图像文本模型，经过训练可以提高字幕的正确性，同时避免使用基于 CLIP 的奖励进行训练时通常发生的偏差。为此，我们的鉴别器直接合并了冻结字幕制作者的负样本，这不仅显著提高了生成的字幕的质量和丰富度，而且与使用 CIDEr 分数作为优化的唯一指标相比，还减少了微调时间。实验结果证明了我们的训练策略在标准和零样本图像字幕数据集上的有效性。]]></description>
      <guid>https://arxiv.org/abs/2408.16827</guid>
      <pubDate>Mon, 02 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过联合和个体成分分析实现扩散模型中的局部编辑</title>
      <link>https://arxiv.org/abs/2408.16845</link>
      <description><![CDATA[arXiv:2408.16845v1 公告类型：新
摘要：扩散模型 (DM) 的最新进展已在视觉合成和编辑任务中取得了重大进展，使其成为生成对抗网络 (GAN) 的强大竞争对手。然而，DM 的潜在空间并不像 GAN 那样被很好地理解。最近的研究集中于通过利用去噪网络的瓶颈层在 DM 的潜在空间中进行无监督语义发现，这已被证明具有语义潜在空间的属性。然而，这些方法仅限于发现全局属性。在本文中，我们解决了 DM 中局部图像处理的挑战，并介绍了一种无监督方法来分解预训练 DM 的去噪网络学习到的潜在语义。给定任意图像和定义的感兴趣区域，我们利用去噪网络的雅可比矩阵建立感兴趣区域与潜在空间中相应子空间之间的关系。此外，我们解开这些子空间的联合和单个组件，以识别能够进行局部图像处理的潜在方向。一旦发现，这些方向就可以应用于不同的图像以产生语义一致的编辑，使我们的方法适合实际应用。在各种数据集上的实验结果表明，与最先进的方法相比，我们的方法可以产生更本地化且保真度更高的语义编辑。]]></description>
      <guid>https://arxiv.org/abs/2408.16845</guid>
      <pubDate>Mon, 02 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GameIR：用于游戏内容图像恢复的大规模合成真实数据集</title>
      <link>https://arxiv.org/abs/2408.16866</link>
      <description><![CDATA[arXiv:2408.16866v1 公告类型：新
摘要：超分辨率和图像合成等图像恢复方法已成功应用于 NVIDIA 的 DLSS 等商业云游戏产品。然而，游戏内容的恢复尚未得到公众的充分研究。这种差异主要是由于缺乏与测试用例相匹配的真实游戏训练数据造成的。由于游戏内容的独特特性，通过降级原始 HR 图像来生成伪训练数据的常用方法会导致恢复性能较差。在这项工作中，我们开发了 GameIR，这是一个大规模高质量计算机合成的真实数据集来填补空白，针对两种不同的应用。第一个是延迟渲染的超分辨率，以支持仅渲染和传输 LR 图像并在客户端恢复 HR 图像的游戏解决方案。我们为这项任务提供了 19200 个 LR-HR 配对真实帧，这些帧来自 640 个以 720p 和 1440p 渲染的视频。第二个是新颖的视图合成 (NVS)，以支持多视图游戏解决方案，即渲染和传输部分多视图帧并在客户端生成剩余帧。这项任务有 57,600 个 HR 帧，来自 160 个场景的 960 个视频，有 6 个摄像机视图。除了 RGB 帧之外，还提供了延迟渲染阶段的 GBuffers，可用于帮助恢复。此外，我们在我们的数据集上评估了几种 SOTA 超分辨率算法和基于 NeRF 的 NVS 算法，这证明了我们的真实 GameIR 数据在提高游戏内容恢复性能方面的有效性。此外，我们还测试了将 GBuffers 作为附加输入信息合并以帮助超分辨率和 NVS 的方法。我们向公众发布我们的数据集和模型，以促进对游戏内容恢复方法的研究。]]></description>
      <guid>https://arxiv.org/abs/2408.16866</guid>
      <pubDate>Mon, 02 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MSLIQA：通过多尺度学习增强图像质量评估的学习表征</title>
      <link>https://arxiv.org/abs/2408.16879</link>
      <description><![CDATA[arXiv:2408.16879v1 公告类型：新
摘要：由于失真的多样性和缺乏大型注释数据集，无参考图像质量评估 (NR-IQA) 仍然是一项具有挑战性的任务。许多研究试图通过开发更准确的 NR-IQA 模型来应对这些挑战，这些模型通常采用复杂且计算成本高昂的网络，或者通过弥合各种失真之间的领域差距来提高测试数据集的性能。在我们的工作中，我们通过引入一种新颖的增强策略来提高通用轻量级 NR-IQA 模型的性能，该策略将其性能提高了近 28%。这种增强策略使网络能够通过放大和缩小来更好地区分图像各个部分的不同失真。此外，测试时间增强的加入进一步提高了性能，使我们的轻量级网络的结果与当前最先进的模型相当，只需使用增强即可。]]></description>
      <guid>https://arxiv.org/abs/2408.16879</guid>
      <pubDate>Mon, 02 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FineFACE：利用细粒度特征进行公平面部属性分类</title>
      <link>https://arxiv.org/abs/2408.16881</link>
      <description><![CDATA[arXiv:2408.16881v1 公告类型：新
摘要：已发表的研究强调了自动面部属性分类算法中存在人口统计学偏见，尤其影响女性和肤色较深的人。现有的偏见缓解技术通常需要人口统计学注释，并且经常在公平性和准确性之间取得权衡，即帕累托效率低下。面部属性，无论是性别等常见属性，还是“胖乎乎”或“高颧骨”等其他属性，都表现出较高的类间相似性和跨人口统计学的类内差异，导致准确性不平等。这需要使用局部和细微的线索，使用细粒度分析进行区分。本文提出了一种公平面部属性分类的新方法，将其定义为细粒度分类问题。我们的方法通过跨层相互注意学习有效地整合了低级局部特征（如边缘和颜色）和高级语义特征（如形状和结构）。在这里，从浅到深的 CNN 层充当专家，提供类别预测和注意区域。对面部属性注释数据集的详尽评估表明，与 SOTA 偏差缓解技术相比，我们的 FineFACE 模型将准确度提高了 1.32% 至 1.74%，公平性提高了 67% 至 83.6%。重要的是，我们的方法在人口统计群体之间的准确度和公平性之间取得了帕累托效率平衡。此外，我们的方法不需要人口统计注释，适用于各种下游分类任务。为了便于重复，代码和数据集信息可在 https://github.com/VCBSL-Fairness/FineFACE 上找到。]]></description>
      <guid>https://arxiv.org/abs/2408.16881</guid>
      <pubDate>Mon, 02 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Tex-ViT：一种通用的、稳健的、基于纹理的双分支交叉注意力深度伪造检测器</title>
      <link>https://arxiv.org/abs/2408.16892</link>
      <description><![CDATA[arXiv:2408.16892v1 公告类型：新
摘要：Deepfakes 使用 GAN 来制作高度逼真的面部修改，被广泛认为是主流方法。传统的 CNN 已经能够识别虚假媒体，但它们在不同的数据集上表现不佳，并且由于缺乏鲁棒性而容易受到对抗性攻击。视觉转换器已在图像分类问题领域展现出潜力，但它们需要足够的训练数据。受这些限制的启发，本出版物介绍了 Tex-ViT（纹理视觉转换器），它通过将 ResNet 与视觉转换器相结合来增强 CNN 特征。该模型将传统的 ResNet 特征与纹理模块相结合，该模块在每次下采样操作之前在 ResNet 的各个部分上并行运行。然后，纹理模块作为交叉注意视觉转换器的双分支的输入。它特别侧重于改进全局纹理模块，该模块提取特征图相关性。实证分析表明，假图像呈现出平滑的纹理，在操作过程中，这些纹理在长距离上无法保持一致。实验在 FF++ 的不同类别（例如 DF、f2f、FS 和 NT）以及跨域场景中的其他类型的 GAN 数据集上进行。此外，还在 FF++、DFDCPreview 和 Celeb-DF 数据集上进行了实验，并经历了模糊、压缩和噪声等多种后处理情况。该模型在泛化方面超越了最先进的模型，在跨域场景中的准确率达到 98%。这表明它能够学习被操纵样本中共享的区分纹理特征。这些实验证明，所提出的模型能够应用于各种情况，并且能够抵抗许多后处理程序。]]></description>
      <guid>https://arxiv.org/abs/2408.16892</guid>
      <pubDate>Mon, 02 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Ig3D：在面部表情推理中整合 3D 面部表征</title>
      <link>https://arxiv.org/abs/2408.16907</link>
      <description><![CDATA[arXiv:2408.16907v1 公告类型：新
摘要：从单个图像重建具有面部几何形状的 3D 面部，为动画、生成模型和虚拟现实带来了重大进步。然而，面部表情推理 (FEI) 社区尚未充分探索这种用 3D 特征表示面部的能力。因此，本研究旨在调查将这种 3D 表示集成到 FEI 任务中的影响，特别是对于面部表情分类和基于面部的效价唤醒 (VA) 估计。为此，我们首先评估两个 3D 面部表示（均基于 3D 可变形模型 FLAME）对 FEI 任务的性能。我们进一步探索两种融合架构，即中间融合和后期融合，以将 3D 面部表示与现有的 2D 推理框架集成。为了评估我们提出的架构，我们提取了相应的 3D 表示并对 AffectNet 和 RAF-DB 数据集进行了广泛的测试。我们的实验结果表明，我们提出的方法优于最先进的 AffectNet VA 估计和 RAF-DB 分类任务。此外，我们的方法可以作为其他现有方法的补充，以提高许多情感推理任务的性能。]]></description>
      <guid>https://arxiv.org/abs/2408.16907</guid>
      <pubDate>Mon, 02 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用亲子二元组积木游戏协议和注意力增强型 GCN-xLSTM 混合深度学习框架增强自闭症谱系障碍的早期检测</title>
      <link>https://arxiv.org/abs/2408.16924</link>
      <description><![CDATA[arXiv:2408.16924v1 公告类型：新
摘要：自闭症谱系障碍 (ASD) 是一种快速增长的神经发育障碍。及时进行干预对于患有 ASD 的幼儿的成长至关重要，但传统的临床筛查方法缺乏客观性。这项研究介绍了一种早期检测 ASD 的创新方法。贡献有三方面。首先，这项工作提出了一种新颖的亲子二元组积木游戏 (PCB) 协议，该协议以运动学和神经科学研究为基础，用于识别区分 ASD 和正常发育 (TD) 幼儿的行为模式。其次，我们编制了一个大量的视频数据集，其中包括 40 名 ASD 和 89 名 TD 幼儿与父母一起进行积木游戏。该数据集在参与者规模和单个会话长度方面都超过了以前的努力。第三，我们对视频中动作的分析方法采用了混合深度学习框架，将双流图卷积网络与注意力增强型 xLSTM（2sGCN-AxLSTM）相结合。该框架擅长通过提取与上半身和头部运动相关的空间特征并关注动作序列随时间变化的全局上下文信息来捕捉幼儿与父母之间的动态互动。通过学习这些具有时空相关性的全局特征，我们的 2sGCN-AxLSTM 可以有效分析动态人类行为模式，并在早期检测 ASD 方面表现出前所未有的 89.6% 的准确率。我们的方法通过准确分析亲子互动，显示出增强早期 ASD 诊断的巨大潜力，为支持及时和明智的临床决策提供了重要工具。]]></description>
      <guid>https://arxiv.org/abs/2408.16924</guid>
      <pubDate>Mon, 02 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>VLM-KD：从 VLM 中提取知识以进行长尾视觉识别</title>
      <link>https://arxiv.org/abs/2408.16930</link>
      <description><![CDATA[arXiv:2408.16930v1 公告类型：新
摘要：对于视觉识别，知识蒸馏通常涉及将知识从大型、训练有素的教师模型转移到较小的学生模型。在本文中，我们介绍了一种从现成的视觉语言模型 (VLM) 中提取知识的有效方法，证明它除了传统的纯视觉教师模型之外还提供了新颖的监督。我们的关键技术贡献是开发了一个框架，该框架可生成新颖的文本监督并将自由格式的文本蒸馏到视觉编码器中。我们展示了我们的方法（称为 VLM-KD）在各种基准数据集上的有效性，表明它超越了几种最先进的长尾视觉分类器。据我们所知，这项工作是第一个利用现成 VLM 生成的文本监督进行知识蒸馏并将其应用于原始随机初始化的视觉编码器的工作。]]></description>
      <guid>https://arxiv.org/abs/2408.16930</guid>
      <pubDate>Mon, 02 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于自动驾驶的瞬态容错语义分割</title>
      <link>https://arxiv.org/abs/2408.16952</link>
      <description><![CDATA[arXiv:2408.16952v1 公告类型：新
摘要：深度学习模型对于自动驾驶汽车感知至关重要，但其可靠性受到算法限制和硬件故障的挑战。我们通过检查语义分割模型中的容错性来解决后者。使用已建立的硬件故障模型，我们从准确性和不确定性方面评估现有的强化技术，并引入 ReLUMax，这是一种新颖的简单激活函数，旨在增强对瞬态故障的恢复能力。ReLUMax 无缝集成到现有架构中，无需时间开销。我们的实验表明，ReLUMax 有效地提高了鲁棒性，保持了性能并提高了预测信心，从而有助于开发可靠的自动驾驶系统。]]></description>
      <guid>https://arxiv.org/abs/2408.16952</guid>
      <pubDate>Mon, 02 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>HiTSR：基于参考的超分辨率分层变换器</title>
      <link>https://arxiv.org/abs/2408.16959</link>
      <description><![CDATA[arXiv:2408.16959v1 公告类型：新
摘要：在本文中，我们提出了 HiTSR，一种用于基于参考的图像超分辨率的分层变换模型，它通过学习来自高分辨率参考图像的匹配对应关系来增强低分辨率输入图像。与现有的多网络、多阶段方法不同，我们通过结合 GAN 文献中的双重注意块来简化架构和训练流程。我们独立处理两个视觉流，通过门控注意策略融合自注意和交叉注意块。该模型集成了一个挤压和激励模块来从输入图像中捕获全局上下文，从而促进基于窗口的注意块内的远程空间交互。浅层和深层之间的长跳跃连接进一步增强了信息流。我们的模型在三个数据集（包括 SUN80、Urban100 和 Manga109）上表现出色。具体来说，在 SUN80 数据集上，我们的模型实现了 30.24/0.821 的 PSNR/SSIM 值。这些结果强调了注意力机制在基于参考的图像超分辨率中的有效性。基于 Transformer 的模型无需专门构建的子网络、知识提炼或多阶段训练即可获得最佳结果，这强调了注意力机制在满足基于参考的图像超分辨率要求方面的潜力。]]></description>
      <guid>https://arxiv.org/abs/2408.16959</guid>
      <pubDate>Mon, 02 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于因果表征的凝视估计领域泛化</title>
      <link>https://arxiv.org/abs/2408.16964</link>
      <description><![CDATA[arXiv:2408.16964v1 公告类型：新
摘要：包含每个受试者凝视信息的大量数据集的可用性显著提高了凝视估计的准确性。然而，领域之间的差异严重影响了针对特定领域明确训练的模型的性能。在本文中，我们提出了基于因果表示的凝视估计领域泛化 (CauGE) 框架，该框架基于因果机制的一般原理设计，与领域差异一致。我们采用对抗性训练方式和额外的惩罚项来提取领域不变特征。提取特征后，我们定位注意层以使特征足以推断实际凝视。通过利用这些模块，CauGE 确保神经网络从符合因果机制一般原理的表示中学习。通过这种方式，CauGE 通过提取领域不变特征跨领域推广，并且虚假相关性不会影响模型。我们的方法在凝视估计基准的领域泛化中实现了最先进的性能。]]></description>
      <guid>https://arxiv.org/abs/2408.16964</guid>
      <pubDate>Mon, 02 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>对比学习与综合正面</title>
      <link>https://arxiv.org/abs/2408.16965</link>
      <description><![CDATA[arXiv:2408.16965v1 公告类型：新
摘要：通过利用同一类中多个实例的相似性，最近邻对比学习已被证明是最有效的自监督学习 (SSL) 技术之一。然而，它的有效性受到限制，因为最近邻算法主要识别“容易”的正对，其中表示已经在嵌入空间中紧密定位。在本文中，我们介绍了一种称为合成正例对比学习 (CLSP) 的新方法，该方法利用由无条件扩散模型生成的合成图像作为附加正例，以帮助模型从不同的正例中学习。通过扩散模型采样过程中的特征插值，我们生成具有不同背景但语义内容与锚图像相似的图像。这些图像被视为锚图像的“硬”正例，当作为对比损失中的补充正例时，它们在线性评估中与之前的 NNCLR 和 All4One 方法相比，在多个基准数据集（如 CIFAR10）中，性能提升超过 2% 和 1%，达到最先进的方法。在迁移学习基准测试中，CLSP 在 8 个下游数据集中的 6 个上优于现有的 SSL 框架。我们相信 CLSP 为未来在训练过程中纳入合成数据的 SSL 研究建立了宝贵的基础。]]></description>
      <guid>https://arxiv.org/abs/2408.16965</guid>
      <pubDate>Mon, 02 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>