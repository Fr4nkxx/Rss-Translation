<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Tue, 17 Sep 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>PrimeDepth：具有稳定扩散原像的高效单目深度估计</title>
      <link>https://arxiv.org/abs/2409.09144</link>
      <description><![CDATA[arXiv:2409.09144v1 公告类型：新
摘要：这项工作解决了零样本单目深度估计的任务。该领域的最新进展是利用文本到图像基础模型（例如稳定扩散）的想法。基础模型提供了丰富而通用的图像表示，因此，只需很少的训练数据即可将其重新表述为深度估计模型，该模型可以预测高度详细的深度图并具有良好的泛化能力。然而，到目前为止，这个想法的实现导致了一些方法，不幸的是，由于底层的迭代去噪过程，这些方法在测试时效率极低。在这项工作中，我们提出了这个想法的不同实现，并提出了 PrimeDepth，这是一种在测试时效率极高的方法，同时保留甚至增强了基于扩散的方法的积极方面。我们的关键思想是通过运行单个去噪步骤从稳定扩散中提取丰富但冻结的图像表示。这种表示（我们称之为原像）随后被输入到具有架构归纳偏差的细化网络中，然后进入下游任务。我们通过实验验证了 PrimeDepth 比领先的基于扩散的方法 Marigold 快两个数量级，同时对于具有挑战性的场景更为稳健，在数量上也略有优势。因此，我们缩小了与目前领先的数据驱动方法 Depth Anything 的差距，虽然它在数量上仍然更胜一筹，但预测的深度图细节较少，并且需要 20 倍以上的标记数据。由于我们方法的互补性，即使是 PrimeDepth 和 Depth Anything 预测之间的简单平均也可以改进这两种方法，并在零样本单目深度估计中树立新的最先进水平。未来，数据驱动的方法也可能受益于整合我们的原像。]]></description>
      <guid>https://arxiv.org/abs/2409.09144</guid>
      <pubDate>Tue, 17 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用区域感知循环损失进行数字人手合成的自适应多模态控制</title>
      <link>https://arxiv.org/abs/2409.09149</link>
      <description><![CDATA[arXiv:2409.09149v1 公告类型：新
摘要：扩散模型已经展示了其合成图像的卓越能力，包括生成特定姿势的人类。然而，当前的模型在充分表达详细手势生成的条件控制方面面临挑战，导致手部区域严重扭曲。为了解决这个问题，我们首先整理 How2Sign 数据集以提供更丰富、更准确的手势注释。此外，我们引入了自适应多模态融合来整合以不同模态表达的角色身体特征，例如骨架、深度和表面法线。此外，我们提出了一种新颖的区域感知循环损失 (RACL)，使扩散模型训练能够专注于改善手部区域，从而提高生成的手势质量。更具体地说，提出的 RACL 计算生成图像中的全身姿势关键点与地面实况之间的加权关键点距离，以生成更高质量的手势，同时平衡整体姿势准确性。此外，我们使用两个手部区域指标（称为手部 PSNR 和手部距离）来评估手势生成。我们的实验评估证明了我们提出的方法在使用扩散模型提高数字人体姿势生成质量方面的有效性，尤其是手部区域的质量。源代码可在 https://github.com/fuqifan/Region-Aware-Cycle-Loss 上找到。]]></description>
      <guid>https://arxiv.org/abs/2409.09149</guid>
      <pubDate>Tue, 17 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于同步运动字幕的受控注意力变压器</title>
      <link>https://arxiv.org/abs/2409.09177</link>
      <description><![CDATA[arXiv:2409.09177v1 公告类型：新
摘要：在本文中，我们解决了一项具有挑战性的任务，即同步运动字幕，旨在生成与人类运动序列同步的语言描述。此任务涉及许多应用，例如对齐的手语转录、无监督动作分割和时间基础。我们的方法引入了控制 Transformer 的自我和交叉注意力分布的机制，从而实现可解释性和时间对齐的文本生成。我们通过掩蔽策略和结构化损失来实现这一点，这些策略和结构化损失推动模型最大限度地关注对生成运动词有贡献的最重要的帧。这些约束旨在防止注意力图中的信息不必要地混合，并在 token 之间提供单调的注意力分布。因此，token 的交叉注意力用于与人类运动序列同步的渐进式文本生成。我们通过对两个可用的基准数据集 KIT-ML 和 HumanML3D 进行评估，证明了我们方法的卓越性能。由于视觉评估对于这项任务至关重要，我们在代码库中提供了一套全面的动画视觉插图：https://github.com/rd20karim/Synch-Transformer。]]></description>
      <guid>https://arxiv.org/abs/2409.09177</guid>
      <pubDate>Tue, 17 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>稀疏神经网络是更好的硬样本学习者吗？</title>
      <link>https://arxiv.org/abs/2409.09196</link>
      <description><![CDATA[arXiv:2409.09196v1 公告类型：新
摘要：虽然深度学习已经取得了令人瞩目的进展，但从硬样本中学习仍然是一项艰巨的挑战，因为这些样本通常嘈杂且复杂。这些硬样本在深度神经网络的最佳性能中起着至关重要的作用。大多数关于稀疏神经网络 (SNN) 的研究都集中在标准训练数据上，在理解它们对复杂和具有挑战性的数据的有效性方面存在差距。本文对各种场景的广泛调查表明，大多数在具有挑战性的样本上训练的 SNN 通常可以在某些稀疏程度下匹配或超越密集模型的准确度，尤其是在数据有限的情况下。我们观察到，逐层密度比往往在 SNN 性能中起着重要作用，特别是对于从头开始训练而没有预训练初始化的方法。这些见解增强了我们对 SNN 行为的理解以及在以数据为中心的 AI 中有效学习方法的潜力。我们的代码公开在：\url{https://github.com/QiaoXiao7282/hard_sample_learners}。]]></description>
      <guid>https://arxiv.org/abs/2409.09196</guid>
      <pubDate>Tue, 17 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多模态语音变换器解码器：何时使用多种模态可以提高准确性？</title>
      <link>https://arxiv.org/abs/2409.09221</link>
      <description><![CDATA[arXiv:2409.09221v1 公告类型：新
摘要：仅解码器的离散标记语言模型最近在自动语音识别中取得了重大成功。然而，关于不同模态如何影响特定场景下性能的系统分析仍然有限。在本文中，我们研究了多种模态对合成和真实世界数据集上识别准确性的影响。我们的实验表明：（1）整合更多模态可以提高准确性；特别是，据我们所知，我们的论文是第一篇展示结合音频、图像上下文和唇部信息的好处的论文；（2）图像作为语音识别的补充模态在中等噪声水平下提供了最大的好处，而且，与唇部运动等固有同步的模态相比，它们表现出不同的趋势；（3）当最相关的视觉信息作为预处理步骤被过滤时，合成和真实世界数据集上的性能都会提高。]]></description>
      <guid>https://arxiv.org/abs/2409.09221</guid>
      <pubDate>Tue, 17 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于高光谱图像分类的分层光谱视觉变换器架构研究</title>
      <link>https://arxiv.org/abs/2409.09244</link>
      <description><![CDATA[arXiv:2409.09244v1 公告类型：新 
摘要：在过去三年中，人们对使用视觉 Transformers 分析遥感数据进行高光谱图像 (HSI) 分类产生了浓厚的兴趣。先前的研究主要集中于卷积神经网络 (CNN) 的经验集成，以增强网络提取局部特征信息的能力。然而，视觉 Transformers 在 HSI 分类中优于 CNN 架构的理论依据仍然是一个问题。为了解决这个问题，研究了一种专门针对 HSI 分类的统一分层光谱视觉 Transformer 架构。在这种精简但有效的视觉 Transformer 架构中，多个混合器模块被分别战略性地集成。其中包括执行卷积运算的 CNN 混合器；空间自注意 (SSA) 混合器和通道自注意 (CSA) 混合器，它们都是经典自注意块的改编；以及将卷积与自注意力操作相结合的混合模型，例如 SSA+CNN-mixer 和 CSA+CNN-mixer。这种集成有助于开发针对 HSI 分类的各种基于视觉 Transformer 的模型。在训练过程方面，进行了全面的分析，对比了经典 CNN 模型和基于视觉 Transformer 的模型，特别关注抗干扰鲁棒性和 Hessian 最大特征值的分布。从对基于统一架构的各种 mixer 模型进行的评估中得出结论，视觉 Transformer 的独特优势可以归因于它们的总体架构，而不是仅仅依赖于单个多头自注意力 (MSA) 组件。]]></description>
      <guid>https://arxiv.org/abs/2409.09244</guid>
      <pubDate>Tue, 17 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>VSFormer：挖掘灵活视图集中的相关性以实现多视图 3D 形状理解</title>
      <link>https://arxiv.org/abs/2409.09254</link>
      <description><![CDATA[arXiv:2409.09254v1 公告类型：新
摘要：基于视图的方法在 3D 形状理解中表现出色。然而，它们倾向于对视图之间的关系做出强有力的假设或间接学习多视图相关性，这限制了探索视图间相关性的灵活性和目标任务的有效性。为了克服上述问题，本文研究了多视图的灵活组织和显式相关性学习。具体来说，我们建议将 3D 形状的不同视图合并到一个置换不变集中，称为 \emph{View Set}，它消除了严格的关系假设并促进了视图之间的充分信息交换和融合。基于此，我们设计了一个灵活的 Transformer 模型，名为 \emph{VSFormer}，以明确捕获集合中所有元素的成对和高阶相关性。同时，我们从理论上揭示了视图集的笛卡尔积与注意机制中的相关矩阵之间的自然对应关系，这支持了我们的模型设计。综合实验表明，VSFormer 具有更好的灵活性、高效的推理效率和卓越的性能。值得注意的是，VSFormer 在各种 3D 识别数据集上都达到了最先进的结果，包括 ModelNet40、ScanObjectNN 和 RGBD。它还在 SHREC&#39;17 检索基准上创下了新纪录。代码和数据集可在 \url{https://github.com/auniquesun/VSFormer} 上找到。]]></description>
      <guid>https://arxiv.org/abs/2409.09254</guid>
      <pubDate>Tue, 17 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>指导跨任务、跨领域、跨知识类型的视觉问答的视觉语言模型选择</title>
      <link>https://arxiv.org/abs/2409.09269</link>
      <description><![CDATA[arXiv:2409.09269v1 公告类型：新
摘要：视觉问答 (VQA) 已成为多个应用程序中的关键用例，以改善用户体验，尤其是在视觉语言模型 (VLM) 在零样本推理中取得良好效果之后。但是在实际环境中使用标准化框架评估不同 VLM 是否符合应用要求仍然具有挑战性。本文介绍了一个全面的框架，用于评估实际环境中针对 VQA 任务量身定制的 VLM。我们提供了一个从已建立的 VQA 基准中衍生的新数据集，其中注释了任务类型、应用领域和知识类型，这是任务可能发生变化的三个关键实际方面。我们还介绍了使用 GPT-4o 开发的多模态评估指标 GoEval，与人类判断的相关系数达到 56.71%。我们对十个最先进的 VLM 进行的实验表明，没有一个模型具有普遍的卓越性，因此适当的选择是一个关键的设计决策。 Gemini-1.5-Pro 和 GPT-4o-mini 等专有模型通常表现优于其他模型，但 InternVL-2-8B 和 CogVLM-2-Llama-3-19B 等开源模型在特定情况下表现出竞争优势，同时提供额外的优势。本研究根据特定任务要求和资源限制指导 VLM 的选择，也可以扩展到其他视觉语言任务。]]></description>
      <guid>https://arxiv.org/abs/2409.09269</guid>
      <pubDate>Tue, 17 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LabellessFace：无需属性标签的公平度量学习人脸识别</title>
      <link>https://arxiv.org/abs/2409.09274</link>
      <description><![CDATA[arXiv:2409.09274v1 公告类型：新
摘要：人口偏见是人脸识别系统面临的主要挑战之一。现有的大多数关于人口偏见的研究都严重依赖于特定的人口群体或人口分类器，因此很难解决无法识别的群体的表现问题。本文介绍了“LabellessFace”，这是一种新颖的框架，可以改善人脸识别中的人口偏见，而无需出于公平考虑而通常需要的人口群体标签。我们提出了一种称为类别偏袒水平的新型公平性增强指标，该指标可评估整个数据集中对特定类别的偏袒程度。利用这一指标，我们引入了公平类别边际惩罚，这是现有基于边际的度量学习的扩展。该方法根据类别偏袒水平动态调整学习参数，从而促进所有属性的公平性。通过在人脸识别系统中将每个类别视为个体，我们促进了学习，从而最大限度地减少了个体之间身份验证准确性的偏差。全面的实验表明，我们提出的方法可以有效地提高公平性，同时保持身份验证准确性。]]></description>
      <guid>https://arxiv.org/abs/2409.09274</guid>
      <pubDate>Tue, 17 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SAM-OCTA2：使用精细调整的 Segment Anything 模型 2 进行层序列 OCTA 分割</title>
      <link>https://arxiv.org/abs/2409.09286</link>
      <description><![CDATA[arXiv:2409.09286v1 公告类型：新
摘要：指示目标的分割有助于精确分析光学相干断层血管造影 (OCTA) 样本。现有的分割方法通常在 2D 投影目标上执行，因此很难通过 3D 体积捕获分割对象的方差。为了解决这一限制，采用低秩自适应技术对 Segment Anything Model (SAM) 版本 2 进行微调，从而实现跨 OCTA 扫描层序列的指定对象的跟踪和分割。为了进一步开展这项工作，提出了一种帧序列中的提示点生成策略和一种获取视网膜血管 (RV) 层掩模的稀疏注释方法。该方法被命名为 SAM-OCTA2，并已在 OCTA-500 数据集上进行了实验。它在常规 2D 正面分割中心凹无血管区 (FAZ) 方面实现了最先进的性能，并有效地跟踪了扫描层序列中的局部血管。代码可在以下位置获取：https://github.com/ShellRedia/SAM-OCTA2。]]></description>
      <guid>https://arxiv.org/abs/2409.09286</guid>
      <pubDate>Tue, 17 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有层次化人类感知的红外和可见光图像融合</title>
      <link>https://arxiv.org/abs/2409.09291</link>
      <description><![CDATA[arXiv:2409.09291v1 公告类型：新
摘要：图像融合将来自多个域的图像组合成一幅图像，包含来自源域的互补信息。现有方法以像素强度、纹理和高级视觉任务信息为标准来确定信息的保留，缺乏对人类感知的增强。我们介绍了一种图像融合方法，分层感知融合（HPFusion），它利用大型视觉语言模型来整合分层的人类语义先验，保留满足人类视觉系统的互补信息。我们提出人类在查看图像对时关注的多个问题，并通过大型视觉语言模型根据图像生成答案。答案的文本被编码到融合网络中，优化还旨在引导融合图像的人类语义分布更类似于源图像，探索人类感知域内的互补信息。大量实验表明，我们的 HPFusoin 可以在信息保存和人类视觉增强方面实现高质量的融合结果。]]></description>
      <guid>https://arxiv.org/abs/2409.09291</guid>
      <pubDate>Tue, 17 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>StyleTalk++：控制说话头像的统一框架</title>
      <link>https://arxiv.org/abs/2409.09292</link>
      <description><![CDATA[arXiv:2409.09292v1 公告类型：新 
摘要：个人具有独特的面部表情和头部姿势风格，反映了他们个性化的说话风格。现有的单镜头说话头部方法无法捕捉这种个性化特征，因此无法在最终视频中产生多样化的说话风格。为了应对这一挑战，我们提出了一种单镜头风格可控的说话面部生成方法，该方法可以从参考说话视频中获取说话风格，并驱动单镜头肖像使用参考说话风格和另一段音频说话。我们的方法旨在在统一的框架中合成 3D 可变形模型 (3DMM) 的风格可控系数，包括面部表情和头部运动。具体而言，所提出的框架首先利用风格编码器从参考视频中提取所需的说话风格并将其转换为风格代码。然后，该框架使用风格感知解码器从音频输入和风格代码中合成 3DMM 的系数。在解码过程中，我们的框架采用双分支架构，分别生成风格化的面部表情系数和风格化的头部运动系数。在获得 3DMM 的系数后，图像渲染器将表情系数渲染到特定人物的头部说话视频中。大量实验表明，我们的方法仅从一张肖像图像和一段音频剪辑即可生成具有多种说话风格的视觉真实头部说话视频。]]></description>
      <guid>https://arxiv.org/abs/2409.09292</guid>
      <pubDate>Tue, 17 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>关联所有检测到的事物：促进对未知事物的检测跟踪</title>
      <link>https://arxiv.org/abs/2409.09293</link>
      <description><![CDATA[arXiv:2409.09293v1 公告类型：新
摘要：多目标跟踪 (MOT) 是计算机视觉领域中一个关键且极具前景的分支。经典的封闭词汇 MOT (CV-MOT) 方法旨在跟踪预定义类别的对象。最近，一些开放词汇 MOT (OV-MOT) 方法成功解决了跟踪未知类别的问题。然而，我们发现 CV-MOT 和 OV-MOT 方法都难以在对方的任务中脱颖而出。在本文中，我们提出了一个统一的框架 Associate Everything Detected (AED)，它通过与任何现成的检测器集成同时解决 CV-MOT 和 OV-MOT 问题并支持未知类别。与现有的通过检测进行跟踪的 MOT 方法不同，AED 摆脱了先验知识（例如运动线索），仅依靠高度鲁棒的特征学习来处理 OV-MOT 任务中的复杂轨迹，同时保持 CV-MOT 任务中的优异性能。具体而言，我们将关联任务建模为相似性解码问题，并提出一种具有关联中心学习机制的 sim 解码器。sim 解码器从空间、时间和交叉剪辑三个方面计算相似性。随后，关联中心学习利用这三重相似性来确保提取的特征适合连续跟踪，并且足够鲁棒以推广到未知类别。与现有的强大的 OV-MOT 和 CV-MOT 方法相比，AED 在没有任何先验知识的情况下在 TAO、SportsMOT 和 DanceTrack 上取得了卓越的性能。我们的代码可在 https://github.com/balabooooo/AED 获得。]]></description>
      <guid>https://arxiv.org/abs/2409.09293</guid>
      <pubDate>Tue, 17 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ManiDext：通过连续对应嵌入和残差引导扩散实现手部-物体操控合成</title>
      <link>https://arxiv.org/abs/2409.09300</link>
      <description><![CDATA[arXiv:2409.09300v1 公告类型：新
摘要：动态和灵巧地操纵物体是一项复杂的挑战，需要将手部运动与物体的轨迹同步，以实现无缝且物理上合理的交互。在这项工作中，我们引入了 ManiDext，这是一个统一的分层扩散框架，用于基于 3D 物体轨迹生成手部操纵和抓握姿势。我们的关键见解是，在交互过程中准确建模物体和手之间的接触对应关系至关重要。因此，我们提出了一种连续对应嵌入表示，该表示在顶点级别指定物体和手之间的详细手部对应关系。该嵌入以自监督的方式直接在手部网格上进行优化，嵌入之间的距离反映测地线距离。我们的框架首先在物体表面生成接触图和对应嵌入。基于这些细粒度的对应关系，我们引入了一种新方法，该方法在手势生成的第二阶段将迭代细化过程集成到扩散过程中。在去噪过程的每个步骤中，我们将当前手势残差作为细化目标纳入网络，引导网络纠正不准确的手势。将残差引入每个去噪步骤本质上与传统优化过程一致，有效地将生成和细化合并到一个统一的框架中。大量实验表明，我们的方法可以为各种任务生成物理上合理且高度逼真的动作，包括单手和双手抓握以及操纵刚性和铰接物体。代码将用于研究目的。]]></description>
      <guid>https://arxiv.org/abs/2409.09300</guid>
      <pubDate>Tue, 17 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>关键点集成指令跟随数据生成，用于增强多模态模型中的人体姿势理解</title>
      <link>https://arxiv.org/abs/2409.09306</link>
      <description><![CDATA[arXiv:2409.09306v1 公告类型：新
摘要：当前的多模态模型非常适合一般的视觉理解任务。然而，它们在处理与人体姿势和动作相关的复杂视觉任务时表现不佳，这主要是由于缺乏专门的指令遵循数据。我们引入了一种生成此类数据的新方法，即将人体关键点与传统的视觉特征（如字幕和边界框）相结合。我们的方法生成的数据集旨在微调模型，使其在以人为中心的活动中表现出色，重点关注三种特定类型：对话、详细描述和复杂推理。我们使用这个新数据集对 LLaVA-7B 模型进行了微调，在各种与人体姿势相关的任务中取得了显着的改进。实验结果表明，与原始 LLaVA-7B 模型相比，总体提高了 21.18%。这些发现证明了关键点辅助数据在增强多模态模型方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2409.09306</guid>
      <pubDate>Tue, 17 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>