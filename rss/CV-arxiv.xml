<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Tue, 07 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>STEAM-EEG：使用马尔可夫转移场和注意力 CNN 进行时空脑电图分析</title>
      <link>https://arxiv.org/abs/2501.01959</link>
      <description><![CDATA[arXiv:2501.01959v1 公告类型：新
摘要：脑电图 (EEG) 信号在生物医学研究和临床应用中起着关键作用，包括癫痫诊断、睡眠障碍分析和脑机接口。然而，有效分析和解释这些复杂信号往往带来重大挑战。本文提出了一种将计算机图形技术与生物信号模式识别相结合的新方法，具体使用马尔可夫转移场 (MTF) 进行 EEG 时间序列成像。所提出的框架 (STEAM-EEG) 利用 MTF 的功能来捕获 EEG 信号的时空动态，将其转换为视觉信息丰富的图像。然后使用最先进的计算机图形技术对这些图像进行渲染、可视化和建模，从而促进增强的数据探索、模式识别和决策。代码可以从 GitHub 访问。]]></description>
      <guid>https://arxiv.org/abs/2501.01959</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GAF-FusionNet：通过 Gramian 角场和分割注意力进行多模态心电图分析</title>
      <link>https://arxiv.org/abs/2501.01960</link>
      <description><![CDATA[arXiv:2501.01960v1 公告类型：新
摘要：心电图 (ECG) 分析在诊断心血管疾病中起着至关重要的作用，但准确解释这些复杂信号仍然具有挑战性。本文介绍了一种用于 ECG 分类的新型多模态框架 (GAF-FusionNet)，该框架将时间序列分析与使用 Gramian Angular Fields (GAF) 的基于图像的表示相结合。我们的方法采用双层跨通道分割注意力模块来自适应地融合时间和空间特征，从而实现互补信息的细微整合。我们在三个不同的 ECG 数据集上评估了 GAF-FusionNet：ECG200、ECG5000 和 MIT-BIH 心律失常数据库。结果表明，与最先进的方法相比，我们的模型有显着的改进，在相应的数据集上实现了 94.5\%、96.9\% 和 99.6\% 的准确率。我们的代码将很快在https://github.com/Cross-Innovation-Lab/GAF-FusionNet.git 上提供。]]></description>
      <guid>https://arxiv.org/abs/2501.01960</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>INFELM：大型文本转图像模型的深入公平性评估</title>
      <link>https://arxiv.org/abs/2501.01973</link>
      <description><![CDATA[arXiv:2501.01973v2 公告类型：新
摘要：大型语言模型 (LLM) 和大型视觉模型 (LVM) 的快速发展推动了多模态 AI 系统的发展，这些系统通过模拟类似人类的认知展示了巨大的工业应用潜力。然而，它们也带来了重大的道德挑战，包括放大有害内容和强化社会偏见。例如，一些工业图像生成模型中的偏见凸显了对稳健公平性评估的迫切需求。大多数现有的评估框架都侧重于模型各个方面的全面性，但它们表现出严重的局限性，包括对内容生成一致性和社会偏见敏感领域的关注不足。更重要的是，它们对像素检测技术的依赖容易导致不准确。
为了解决这些问题，本文提出了 INFELM，这是一种对广泛使用的文本到图像模型的深入公平性评估。我们的主要贡献是：(1) 先进的肤色分类器结合了面部拓扑和精细的皮肤像素表示，可将分类精度提高至少 16.04%；(2) 偏差敏感的内容对齐测量，用于了解社会影响；(3) 适用于不同人口群体的可推广的表征偏差评估；(4) 大量实验，分析六个社会偏差敏感领域的大规模文本到图像模型输出。我们发现研究中的现有模型通常不符合经验公平标准，表征偏差通常比对齐误差更明显。INFELM 为公平性评估建立了强大的基准，支持符合道德和以人为本原则的多模态 AI 系统的开发。]]></description>
      <guid>https://arxiv.org/abs/2501.01973</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>动态人脸的极化 BSSRDF 获取</title>
      <link>https://arxiv.org/abs/2501.01980</link>
      <description><![CDATA[arXiv:2501.01980v1 公告类型：新
摘要：偏振光反射和散射的采集和建模有助于揭示物体的形状、结构和物理特性，这在计算机图形学中越来越重要。然而，目前的偏振采集系统仅限于静态和不透明物体。另一方面，人脸则面临着特别困难的挑战，因为它们的结构和反射特性复杂，存在强烈的空间变化的次表面散射，以及它们的动态特性。我们提出了一种新的动态人脸偏振采集方法，该方法侧重于捕捉空间变化的外观和精确的几何形状，涵盖广泛的肤色和面部表情。它包括单一和异质次表面散射、折射率、镜面粗糙度和强度等参数，同时揭示基于生物物理的成分，如内层和外层血红蛋白、真黑素和褐黑素。我们的方法利用这些成分独特的多光谱吸收曲线来量化它们的浓度，从而为我们的模型提供有关皮肤层内发生的复杂相互作用的信息。据我们所知，我们的工作是第一个同时获取偏振和光谱反射信息以及基于生物物理的皮肤参数和动态人脸几何形状的工作。此外，我们的偏振皮肤模型可以无缝集成到各种渲染管道中。]]></description>
      <guid>https://arxiv.org/abs/2501.01980</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用卷积神经网络对阿育王婆罗米文铭文进行光学字符识别</title>
      <link>https://arxiv.org/abs/2501.01981</link>
      <description><![CDATA[arXiv:2501.01981v1 公告类型：新
摘要：本研究论文深入探讨了使用卷积神经网络开发用于识别阿育王婆罗米文字符的光学字符识别 (OCR) 系统。它利用字符图像的综合数据集来训练模型，并使用数据增强技术来优化训练过程。此外，本文还结合了图像预处理以消除噪声，以及图像分割以促进线条和字符分割。该研究主要关注三个预训练的 CNN，即 LeNet、VGG-16 和 MobileNet，并比较它们的准确性。采用迁移学习将预训练模型适应阿育王婆罗米文字符数据集。研究结果表明，MobileNet 在准确性方面优于其他两个模型，验证准确率为 95.94%，验证损失为 0.129。本文对使用 MobileNet 的实施过程进行了深入分析，并讨论了研究结果的含义。
使用 OCR 进行字符识别在铭文领域具有重要意义，特别是对于古代文字的保存和数字化。本研究论文的结果证明了使用预训练的 CNN 识别阿育王婆罗米文字符的有效性。]]></description>
      <guid>https://arxiv.org/abs/2501.01981</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>您的图像是一个优秀的故事讲述者吗？</title>
      <link>https://arxiv.org/abs/2501.01982</link>
      <description><![CDATA[arXiv:2501.01982v1 公告类型：新
摘要：在实体级别量化图像复杂度很简单，但对语义复杂度的评估却被忽视了。事实上，不同图像的语义复杂度存在差异。语义更丰富的图像可以讲述生动有趣的故事，并提供广泛的应用场景。例如，Cookie Theft 图片就是这样一种图像，由于其语义复杂度更高，被广泛用于评估人类语言和认知能力。此外，语义丰富的图像可以有利于视觉模型的开发，因为语义有限的图像对它们来说变得不那么具有挑战性。然而，这样的图像很稀缺，凸显了对更多图像的需求。例如，需要更多像 Cookie Theft 这样的图像来迎合来自不同文化背景和时代的人们。评估语义复杂性需要人类专家和经验证据。自动评估图像的语义丰富程度将是挖掘或生成更多具有丰富语义的图像的第一步，并有利于人类认知评估、人工智能和各种其他应用。为此，我们提出了图像语义评估 (ISA) 任务来解决此问题。我们介绍了第一个 ISA 数据集和一种利用语言解决此视觉问题的新方法。在我们的数据集上进行的实验证明了我们方法的有效性。我们的数据和代码可在以下网址获取：https://github.com/xiujiesong/ISA。]]></description>
      <guid>https://arxiv.org/abs/2501.01982</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过 PPG 在 ECG 引导下进行个人识别</title>
      <link>https://arxiv.org/abs/2501.01983</link>
      <description><![CDATA[arXiv:2501.01983v1 公告类型：新
摘要：基于光电容积描记法 (PPG) 的个体识别旨在通过内在心血管活动识别人类，由于其高安全性和抗模仿性而受到广泛关注。然而，由于信息密度低的限制，这种技术的效果并不理想。为此，心电图 (ECG) 信号已被引入作为一种新的模态来增强输入信息的密度。具体而言，实施了一种新颖的跨模态知识蒸馏框架，以将判别知识从 ECG 模态传播到 PPG 模态，而不会在推理阶段产生额外的计算需求。此外，为了确保有效的知识传播，分别提出了基于对比语言图像预训练 (CLIP) 的知识对齐和跨知识评估模块。进行了全面的实验，结果表明，我们的框架在可见和不可见个体识别的总体准确率方面优于基线模型，分别提高了 2.8% 和 3.0%。]]></description>
      <guid>https://arxiv.org/abs/2501.01983</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用智能监控摄像系统检测乘客电梯中的跌倒：YoloV8 Nano 模型的应用</title>
      <link>https://arxiv.org/abs/2501.01985</link>
      <description><![CDATA[arXiv:2501.01985v1 公告类型：新
摘要：计算机视觉技术通过深度学习算法分析摄像机拍摄的图像和视频，极大地推动了人体跌倒检测领域的发展。本研究重点关注 YoloV8 Nano 模型在识别乘客电梯内跌倒事件中的应用，由于封闭的环境和不同的照明条件，这种情况带来了独特的挑战。通过在包含不同电梯类型的 10,000 多张图像的强大数据集上训练模型，我们旨在提高检测精度和召回率。该模型在跌倒检测中的准确率为 85%，召回率为 82%，凸显了其集成到现有电梯安全系统中以实现快速干预的潜力。]]></description>
      <guid>https://arxiv.org/abs/2501.01985</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FrameFusion：结合相似性和重要性，在大型视觉语言模型上减少视频标记</title>
      <link>https://arxiv.org/abs/2501.01986</link>
      <description><![CDATA[arXiv:2501.01986v1 公告类型：新
摘要：由于视觉标记数量巨大，对长视频和高分辨率视频的处理需求不断增长，这严重加重了大型视觉语言模型 (LVLM) 的负担。现有的标记减少方法主要侧重于基于重要性的标记修剪，这忽略了由帧相似性和重复的视觉元素引起的冗余。在本文中，我们分析了 LVLM 中的高视觉标记相似性。我们发现，随着层的加深，标记相似性分布会收缩，同时保持排名一致性。利用相似性优于重要性的独特属性，我们引入了 FrameFusion，这是一种新颖的方法，它将基于相似性的合并与基于重要性的修剪相结合，以更好地减少 LVLM 中的标记。FrameFusion 在修剪之前识别和合并相似的标记，为标记减少开辟了新的视角。我们在各种 LVLM（包括 Llava-Video-{7B,32B,72B} 和 MiniCPM-V-8B）上对 FrameFusion 进行了视频理解、问答和检索基准测试。实验表明，FrameFusion 将视觉标记减少了 70$\%$，实现了 3.4-4.4 倍的 LLM 速度提升和 1.6-1.9 倍的端到端速度提升，平均性能影响不到 3$\%$。我们的代码可在 https://github.com/thu-nics/FrameFusion 上找到。]]></description>
      <guid>https://arxiv.org/abs/2501.01986</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>文本转视频生成模型中的性别偏见：Sora 案例研究</title>
      <link>https://arxiv.org/abs/2501.01987</link>
      <description><![CDATA[arXiv:2501.01987v1 公告类型：新
摘要：文本到视频生成模型的出现彻底改变了内容创作，因为它可以根据文本提示生成高质量的视频。然而，对此类模型固有偏见的担忧引发了人们的关注，尤其是在性别代表性方面。我们的研究调查了 OpenAI 的 Sora（一种最先进的文本到视频生成模型）中是否存在性别偏见。我们通过分析从一组不同的性别中立和刻板提示中生成的视频，发现了明显的偏见证据。结果表明，Sora 将特定性别与刻板行为和职业不成比例地联系起来，这反映了其训练数据中存在的社会偏见。]]></description>
      <guid>https://arxiv.org/abs/2501.01987</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CRRG-CLIP：自动生成胸部放射学报告并对胸部X光片进行分类</title>
      <link>https://arxiv.org/abs/2501.01989</link>
      <description><![CDATA[arXiv:2501.01989v1 公告类型：新 
摘要：堆叠成像的复杂性和大量的射线照片使编写放射学报告变得复杂且效率低下。即使是经验丰富的放射科医生，在长时间的高强度工作下，也难以保持对射线照片的准确和一致解释。为了解决这些问题，这项工作提出了 CRRG-CLIP 模型（胸部放射学报告生成和射线照片分类模型），这是一种用于自动报告生成和射线照片分类的端到端模型。该模型由两个模块组成：放射学报告生成模块和射线照片分类模块。生成模块使用 Faster R-CNN 识别射线照片中的解剖区域，使用二元分类器选择关键区域，并使用 GPT-2 生成语义一致的报告。分类模块使用无监督的对比语言图像预训练 (CLIP) 模型，解决了高成本标记数据集和特征不足的挑战。结果表明，生成模块在 BLEU、METEOR 和 ROUGE-L 指标上的表现与高性能基线模型相当，在 BLEU-2、BLEU-3、BLEU-4 和 ROUGE-L 指标上的表现优于 GPT-4o 模型。分类模块在 AUC 和准确度方面明显超越了最先进的模型。这表明所提出的模型在报告生成方面实现了较高的准确度、可读性和流畅性，而使用未标记的射线照片报告对进行多模态对比训练可提高分类性能。]]></description>
      <guid>https://arxiv.org/abs/2501.01989</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于准确检测和验证脑肿瘤的混合深度学习和模型检查框架</title>
      <link>https://arxiv.org/abs/2501.01991</link>
      <description><![CDATA[arXiv:2501.01991v1 公告类型：新
摘要：模型检查是一种形式化验证技术，可确保系统满足预定义要求，在开发过程中对减少错误和提高质量起着至关重要的作用。本文介绍了一种将模型检查与深度学习相结合的新型混合框架，用于医学成像中的脑肿瘤检测和验证。通过将模型检查原理与基于 CNN 的特征提取和 K-FCM 聚类相结合进行分割，所提出的方法提高了肿瘤检测和分割的可靠性。实验结果突出了该框架的有效性，实现了 98% 的准确率、96.15% 的精确率和 100% 的召回率，证明了其作为高级医学图像分析的强大工具的潜力。]]></description>
      <guid>https://arxiv.org/abs/2501.01991</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一种基于卷积和注意力机制的新型 6D 物体姿态估计模型</title>
      <link>https://arxiv.org/abs/2501.01993</link>
      <description><![CDATA[arXiv:2501.01993v1 公告类型：新
摘要：从 RGB 图像估计 6D 物体姿势具有挑战性，因为缺乏深度信息需要从 2D 投影推断三维结构。传统方法通常依赖于基于网格的数据结构的深度学习，但难以捕捉提取特征之间的复杂依赖关系。为了解决这个问题，我们引入了一种直接从图像中得出的基于图形的表示，其中每个像素的空间时间特征用作节点，它们之间的关系通过节点连接和空间交互来定义。我们还采用了使用空间注意和自注意蒸馏的特征选择机制，以及利用 Legendre 多项式正交性实现数值稳定性的 Legendre 卷积层。在 LINEMOD、Occluded LINEMOD 和 YCB Video 数据集上的实验表明，我们的方法优于现有的九种方法，并在物体姿势估计方面达到了最先进的基准。]]></description>
      <guid>https://arxiv.org/abs/2501.01993</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SmartSpatial：增强稳定扩散模型的 3D 空间排列能力并引入新颖的 3D 空间评估框架</title>
      <link>https://arxiv.org/abs/2501.01998</link>
      <description><![CDATA[arXiv:2501.01998v1 公告类型：新
摘要：稳定扩散模型在根据文本提示生成逼真的图像方面取得了显著进步，但在准确表示复杂空间排列（尤其是涉及复杂的 3D 关系）时往往会失败。为了解决这一限制，我们引入了 SmartSpatial，这是一种创新方法，通过 3D 感知条件和注意力引导机制增强了稳定扩散模型的空间排列能力。SmartSpatial 结合深度信息并采用交叉注意力控制来确保精确的物体放置，从而显着提高了空间精度指标。与 SmartSpatial 结合，我们提出了 SmartSpatialEval，这是一个旨在评估空间关系的综合评估框架。该框架利用视觉语言模型和基于图的依赖关系解析进行性能分析。在 COCO 和 SpatialPrompts 数据集上的实验结果表明，SmartSpatial 明显优于现有方法，为图像生成的空间排列精度树立了新的标杆。]]></description>
      <guid>https://arxiv.org/abs/2501.01998</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>等方差和对称性破坏在点云深度学习架构中的实用性</title>
      <link>https://arxiv.org/abs/2501.01999</link>
      <description><![CDATA[arXiv:2501.01999v1 公告类型：新
摘要：本文探讨了影响点云模型性能的关键因素，这些因素适用于不同几何复杂度的任务。在这项工作中，我们探索了等变层引入的灵活性和权重共享之间的权衡，评估等变何时会提高或降低性能。人们经常认为，提供更多信息作为输入可以提高模型的性能。但是，如果这些额外的信息破坏了某些属性，例如 $\SE(3)$ 等变，它是否仍然有益？我们通过在复杂度不断增加的多个数据集上对分割、回归和生成任务进行基准测试，确定了等变和非等变架构在不同任务中取得成功的关键方面。我们观察到等变的积极影响，这种影响随着任务复杂性的增加而变得更加明显，即使在不需要严格的等变时也是如此。]]></description>
      <guid>https://arxiv.org/abs/2501.01999</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>