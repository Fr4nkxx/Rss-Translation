<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Wed, 13 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>ViTOC：视觉转换器和对象感知字幕器</title>
      <link>https://arxiv.org/abs/2411.07265</link>
      <description><![CDATA[arXiv:2411.07265v1 公告类型：新
摘要：本文介绍了 ViTOC（Vision Transformer and Object-aware Captioner），这是一种用于图像字幕的新型视觉语言模型，可解决生成描述的准确性和多样性挑战。与传统方法不同，ViTOC 采用基于 Vision Transformer 和对象检测器的双路径架构，通过可学习向量有效地融合全局视觉特征和局部对象信息。该模型引入了一种创新的对象感知提示策略，大大增强了其处理长尾数据的能力。在标准 COCO 数据集上的实验表明，ViTOC 在所有评估指标上均优于基线模型，在 CIDEr 和 SPICE 上分别达到 71.26 和 17.82。此外，我们提出了一种基于 CLIP 的无参考评估方法，以进一步验证模型的有效性。通过利用预训练的视觉模型参数，ViTOC 实现了高效的端到端训练。]]></description>
      <guid>https://arxiv.org/abs/2411.07265</guid>
      <pubDate>Wed, 13 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GPU 加速逆向光刻，实现高质量曲线掩模生成</title>
      <link>https://arxiv.org/abs/2411.07311</link>
      <description><![CDATA[arXiv:2411.07311v1 公告类型：新
摘要：逆向光刻技术 (ILT) 已成为光掩模设计和优化的有前途的解决方案。依靠多光束掩模写入器，ILT 能够创建自由形式的曲线掩模形状，从而提高印刷晶圆的图像质量和工艺窗口。然而，在大规模生产中实施曲线 ILT 的一个主要挑战是掩模规则检查，这是目前代工厂和 EDA 供应商正在开发的一个领域。虽然最近的研究已将掩模复杂性纳入优化过程，但其中大部分研究都集中在减少电子束发射上，这与曲线 ILT 的目标不一致。在本文中，我们介绍了一种 GPU 加速的 ILT 算法，它不仅可以提高轮廓质量和工艺窗口，还可以提高曲线掩模形状的精度。我们在开放基准上的实验证明了我们的算法比领先的学术 ILT 引擎具有显着优势。]]></description>
      <guid>https://arxiv.org/abs/2411.07311</guid>
      <pubDate>Wed, 13 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于隐式多视图深度估计的 $SE(3)$ 等变射线嵌入</title>
      <link>https://arxiv.org/abs/2411.07326</link>
      <description><![CDATA[arXiv:2411.07326v1 公告类型：新
摘要：通过嵌入几何实体（例如射线）作为输入来结合归纳偏差已被证明在多视图学习中是成功的。然而，采用这种技术的方法通常缺乏等方差，而等方差对于有效的 3D 学习至关重要。等方差是一种有价值的归纳先验，有助于生成用于 3D 场景理解的稳健多视图特征。在本文中，我们探索了等方差多视图学习在深度估计中的应用，不仅认识到它对计算机视觉和机器人技术的重要性，而且还解决了先前研究的局限性。大多数先前的研究要么忽略了这种设置中的等方差，要么仅通过数据增强实现了近似等方差，这通常会导致不同参考帧之间的不一致。为了解决这个问题，我们建议将 $SE(3)$ 等方差嵌入到 Perceiver IO 架构中。我们采用球谐函数进行位置编码，以确保 3D 旋转等变性，并在 Perceiver IO 架构中开发专门的等变编码器和解码器。为了验证我们的模型，我们将其应用于立体深度估计任务，在现实世界的数据集上取得了最先进的结果，而无需明确的几何约束或大量数据增强。]]></description>
      <guid>https://arxiv.org/abs/2411.07326</guid>
      <pubDate>Wed, 13 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>将 Brady-Yong 快速霍夫变换算法推广至任意图像大小</title>
      <link>https://arxiv.org/abs/2411.07351</link>
      <description><![CDATA[arXiv:2411.07351v1 公告类型：新
摘要：如今，霍夫（离散 Radon）变换 (HT/DRT) 已被证明是一种非常强大且广泛使用的工具，可用于从一般图像处理到 X 射线计算机断层扫描等许多应用领域。有效利用 HT 解决应用问题需要加速并提高准确性。除此之外，大多数用于计算 HT 的快速算法，尤其是开创性的 Brady-Yong 算法，都针对 2 的幂大小的输入图像进行操作，并且不适用于任意大小的图像。本文提出了一种用于计算任意大小图像的 HT 的新算法。它概括了 Brady-Yong 算法，并从中继承了最佳计算复杂度。此外，与现有算法相比，该算法可以以更高的精度计算 HT。在此，本文对所提算法的计算复杂度和准确性进行了理论分析。所进行的实验的结论与理论结果一致。]]></description>
      <guid>https://arxiv.org/abs/2411.07351</guid>
      <pubDate>Wed, 13 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>特征空间语义不变性：增强开放集域泛化的 OOD 检测</title>
      <link>https://arxiv.org/abs/2411.07392</link>
      <description><![CDATA[arXiv:2411.07392v1 公告类型：新
摘要：开放集域泛化解决了现实世界中的一个挑战：训练一个模型来泛化未知域（域泛化），同时检测训练期间未遇到的未知类的样本（开放集识别）。然而，大多数现有方法都是单独解决这些问题，限制了它们的实际适用性。为了克服这一限制，我们通过引入特征空间语义不变性 (FSI) 提出了一个统一的开放集域泛化框架。FSI 在特征空间内的不同域之间保持语义一致性，从而能够更准确地检测未知域中的 OOD 实例。此外，我们采用生成模型来生成具有新域样式或类标签的合成数据，从而增强模型的鲁棒性。初步实验表明，我们的方法在 ColoredMNIST 上将 AUROC 提高了 9.1% 至 18.9%，同时还显着提高了分布内分类准确率。]]></description>
      <guid>https://arxiv.org/abs/2411.07392</guid>
      <pubDate>Wed, 13 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>XPoint：一种基于自监督视觉状态空间的多光谱图像配准架构</title>
      <link>https://arxiv.org/abs/2411.07430</link>
      <description><![CDATA[arXiv:2411.07430v1 公告类型：新
摘要：由于光谱模态之间的非线性强度变化、极端视点变化以及标记数据集的稀缺性，准确的多光谱图像匹配带来了重大挑战。当前最先进的方法通常专门针对单一光谱差异，例如可见红外，并且由于依赖昂贵的监督（例如深度图或相机姿势），因此难以适应其他模态。为了满足跨模态快速适应的需求，我们引入了 XPoint，这是一个自监督的模块化图像匹配框架，旨在对对齐的多光谱数据集进行自适应训练和微调，允许用户根据其特定任务自定义关键组件。XPoint 采用模块化和自监督来调整诸如基础检测器之类的元素，它可以生成与视点和光谱变化无关的伪地面真实关键点。该框架集成了一个在分割任务上预先训练过的 VMamba 编码器，用于稳健的特征提取，并包括三个联合解码器头：两个专用于兴趣点和描述符提取；一个特定于任务的单应性回归头施加几何约束，以在图像配准等任务中实现卓越性能。这种灵活的架构能够快速适应各种模态，通过在光学热数据上进行训练并在视觉近红外、视觉红外、视觉长波红外和视觉合成孔径雷达等设置上进行微调来证明这一点。实验结果表明，XPoint 在五个不同的多光谱数据集上的特征匹配和图像配准任务中始终优于或匹配最先进的方法。我们的源代码可在 https://github.com/canyagmur/XPoint 上找到。]]></description>
      <guid>https://arxiv.org/abs/2411.07430</guid>
      <pubDate>Wed, 13 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过自适应退化感知自提示模型实现一体化天气退化图像修复</title>
      <link>https://arxiv.org/abs/2411.07445</link>
      <description><![CDATA[arXiv:2411.07445v1 公告类型：新 
摘要：现有的一体化天气退化图像恢复方法在利用退化感知先验方面存在效率低下的问题，导致在适应不同天气条件时性能不佳。为此，我们开发了一种自适应退化感知自提示模型 (ADSM)，用于一体化天气退化图像恢复。具体而言，我们的模型采用对比语言图像预训练模型 (CLIP) 来促进我们提出的潜在提示生成器 (LPG) 的训练，这些提示代表三种类型的潜在提示，以表征退化类型、退化属性和图像标题。此外，我们将获得的退化感知提示集成到扩散模型的时间嵌入中以改善退化感知。同时，我们使用潜在标题提示通过交叉注意机制来指导反向采样过程，从而指导准确的图像重建。此外，为了加速扩散模型的反向采样过程并解决频率感知的局限性，我们引入了一个面向小波的噪声估计网络（WNE-Net）。在八个公开可用的数据集上进行的大量实验证明了我们提出的方法在特定任务和一体化应用中的有效性。]]></description>
      <guid>https://arxiv.org/abs/2411.07445</guid>
      <pubDate>Wed, 13 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>追根溯源：利用扩散轨迹的时间动态进行来源归因</title>
      <link>https://arxiv.org/abs/2411.07449</link>
      <description><![CDATA[arXiv:2411.07449v1 公告类型：新
摘要：扩散模型彻底改变了图像合成，近年来引起了广泛的研究兴趣。扩散是一种迭代算法，其中从纯噪声开始逐步生成样本。此过程引入了扩散轨迹的概念，即从标准高斯分布到目标图像分布的路径。在此背景下，我们研究了在这些轨迹上运行的判别算法。具体而言，给定一个预先训练的扩散模型，我们考虑将图像分类为训练数据集的一部分、由模型生成或源自外部源的问题。我们的方法证明了跨步骤的模式的存在，这些模式可用于分类。我们还进行了消融研究，结果表明，使用高阶梯度特征来表征轨迹可以显着提高性能并提高算法的鲁棒性。]]></description>
      <guid>https://arxiv.org/abs/2411.07449</guid>
      <pubDate>Wed, 13 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>BLIP3-KALE：知识增强大规模密集字幕</title>
      <link>https://arxiv.org/abs/2411.07461</link>
      <description><![CDATA[arXiv:2411.07461v1 公告类型：新
摘要：我们推出了 BLIP3-KALE，这是一个包含 2.18 亿个图像文本对的数据集，它弥补了描述性合成字幕和事实网络规模替代文本之间的差距。KALE 使用网络规模替代文本增强合成密集图像字幕，以生成基于事实的图像字幕。我们的两阶段方法利用大型视觉语言模型和语言模型来创建知识增强字幕，然后用于训练专门的 VLM 以扩大数据集。我们在 KALE 上训练视觉语言模型并展示视觉语言任务的改进。我们的实验展示了 KALE 在训练更有能力和知识更丰富的多模态模型方面的实用性。我们在 https://huggingface.co/datasets/Salesforce/blip3-kale 发布 KALE 数据集]]></description>
      <guid>https://arxiv.org/abs/2411.07461</guid>
      <pubDate>Wed, 13 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MureObjectStitch：多参考图像合成</title>
      <link>https://arxiv.org/abs/2411.07462</link>
      <description><![CDATA[arXiv:2411.07462v1 公告类型：新
摘要：生成图像合成旨在在背景图像中重新生成给定的前景对象以生成逼真的合成图像。在这项工作中，我们提出了一种有效的生成图像合成模型微调策略，其中我们使用一张或多张包含相同前景对象的图像对预训练模型进行微调。此外，我们提出了一种多参考策略，允许模型采用前景对象的多张参考图。在 MureCOM 数据集上的实验验证了我们方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2411.07462</guid>
      <pubDate>Wed, 13 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MSEG-VCUQ：具有增强视觉基础模型、卷积神经网络和高速视频相位检测数据不确定性量化的多模态分割</title>
      <link>https://arxiv.org/abs/2411.07463</link>
      <description><![CDATA[arXiv:2411.07463v1 公告类型：新
摘要：目的：高速视频 (HSV) 相位检测 (PD) 分割对于核反应堆、化学处理和电子冷却中检测蒸汽、液体和微层相至关重要。传统分割模型在多模态数据中面临像素级准确性和泛化问题。MSEG-VCUQ 引入了 VideoSAM，这是一个混合框架，利用卷积神经网络 (CNN) 和基于变压器的视觉模型来提高复杂多模态 PD 任务的分割准确性和泛化能力。方法：VideoSAM 结合 U-Net CNN 和 Segment Anything 模型 (SAM)，用于跨各种 HSV PD 模态进行高级特征提取和分割，涵盖不同热通量条件下的水、FC-72、氮气和氩气等流体。该框架还结合了不确定性量化 (UQ) 来评估基于像素的离散化误差，在实验条件下提供可靠的指标，例如接触线密度和干燥面积分数。结果：VideoSAM 在分割精度方面优于 SAM 和特定模态的 CNN 模型，在具有复杂相边界、重叠气泡和动态液体-蒸汽相互作用的环境中表现出色。其混合架构支持跨数据集泛化，可有效适应不同的模态。UQ 模块提供准确的误差估计，增强了高级 HSV PD 研究的分割输出的可靠性。结论：通过 VideoSAM，MSEG-VCUQ 为 HSV PD 分割提供了强大的解决方案，解决了高级深度学习和 UQ 技术以前的局限性。引入的开源数据集和工具可实现多模态 PD 数据集的可扩展、精确和适应性强的分割，支持 HSV 分析和自主实验的进步。]]></description>
      <guid>https://arxiv.org/abs/2411.07463</guid>
      <pubDate>Wed, 13 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>半真半假：用于评估 AI 生成图像检测器鲁棒性的大规模 AI 增强图像数据集</title>
      <link>https://arxiv.org/abs/2411.07472</link>
      <description><![CDATA[arXiv:2411.07472v1 公告类型：新
摘要：文本到图像的扩散模型在艺术、设计和娱乐领域有着深远的应用，但这些技术也带来了重大风险，因为它们可以创造和传播错误信息。尽管最近的进展已经产生了声称对各种增强具有鲁棒性的人工智能生成的图像检测器，但它们的真实有效性仍不确定。这些检测器是否能可靠地识别具有不同增强级别的图像？它们是否偏向特定场景或数据分布？为了进行调查，我们引入了 SEMI-TRUTHS，其中包含 27,600 张真实图像、223,400 个蒙版和 1,472,700 张人工智能增强图像，这些图像具有使用各种增强技术、扩散模型和数据分布产生的有针对性和局部扰动。每个增强图像都附有元数据，用于对检测器鲁棒性进行标准化和有针对性的评估。我们的研究结果表明，最先进的检测器对所用扰动的类型和程度、数据分布和增强方法表现出不同的敏感度，这为其性能和局限性提供了新的见解。增强和评估流程的代码可在 https://github.com/J-Kruk/SemiTruths 上找到。]]></description>
      <guid>https://arxiv.org/abs/2411.07472</guid>
      <pubDate>Wed, 13 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GUS-IR：用于逆向渲染的高斯溅射统一着色</title>
      <link>https://arxiv.org/abs/2411.07478</link>
      <description><![CDATA[arXiv:2411.07478v1 公告类型：新
摘要：从图像中恢复场景的固有物理属性（通常称为逆渲染问题）一直是计算机视觉和计算机图形学中的核心和具有挑战性的任务。在本文中，我们介绍了 GUS-IR，这是一种新颖的框架，旨在解决具有粗糙和光滑表面的复杂场景的逆渲染问题。本文首先分析和比较了两种常用于逆渲染的突出着色技术，即前向着色和延迟着色，以及处理复杂材料的有效性。更重要的是，我们提出了一种统一的着色解决方案，结合了两种技术的优点，以实现更好的分解。此外，我们分析了 3D 高斯溅射 (3DGS) 中的法线建模，并利用 GUS-IR 中每个粒子的最短轴作为法线，以及与深度相关的正则化，从而改善了几何表示并更好地重建了形状。此外，我们增强了 GS-IR 提出的基于探针的烘焙方案，以实现更精确的环境光遮蔽建模，从而更好地处理间接照明。大量实验证明了 GUS-IR 在实现精确的本征分解和几何表示方面的卓越性能，支持计算机视觉、图形和扩展现实中的许多下游任务（例如重新照明、修饰）。]]></description>
      <guid>https://arxiv.org/abs/2411.07478</guid>
      <pubDate>Wed, 13 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SparrowVQE：课程内容理解的视觉问题解释</title>
      <link>https://arxiv.org/abs/2411.07516</link>
      <description><![CDATA[arXiv:2411.07516v1 公告类型：新
摘要：视觉问答 (VQA) 研究旨在创建 AI 系统来回答图像中的自然语言问题，但 VQA 方法通常会产生过于简单和简短的答案。本文旨在通过引入视觉问题解释 (VQE) 来推动该领域的发展，这增强了 VQA 提供详细解释而不是简短回答的能力，并满足了与视觉内容进行更复杂交互的需求。我们首先从 14 周的流式视频机器学习课程中创建了一个 MLVQE 数据集，其中包括 885 张幻灯片图像、110,407 个单词的成绩单和 9,416 个设计的问题答案 (QA) 对。接下来，我们提出了一种新颖的 SparrowVQE，这是一个 30 亿参数的小型多模态模型。我们采用三阶段训练机制训练模型，包括多模态预训练（幻灯片图像和转录本特征对齐）、指令调整（使用转录本和 QA 对调整预训练模型）和域微调（微调幻灯片图像和 QA 对）。最终，我们的 SparrowVQE 可以使用 SigLIP 模型理解视觉信息，并使用带有 MLP 适配器的 Phi-2 语言模型将转录本与 SigLIP 模型连接起来。实验结果表明，我们的 SparrowVQE 在我们开发的 MLVQE 数据集中实现了更好的性能，并且在其他五个基准 VQA 数据集中的表现优于最先进的方法。源代码可在 \url{https://github.com/YoushanZhang/SparrowVQE} 获得。]]></description>
      <guid>https://arxiv.org/abs/2411.07516</guid>
      <pubDate>Wed, 13 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>HiCoM：具有 3D 高斯溅射的可流式动态场景的分层连贯运动</title>
      <link>https://arxiv.org/abs/2411.07541</link>
      <description><![CDATA[arXiv:2411.07541v1 公告类型：新
摘要：从多视角流媒体视频中在线重建动态场景面临着训练、渲染和存储效率方面的重大挑战。利用卓越的学习速度和实时渲染能力，3D 高斯分层 (3DGS) 最近在该领域表现出了巨大的潜力。然而，3DGS 在存储方面效率低下，并且容易因过度增长的高斯而过度拟合，尤其是在视图有限的情况下。本文提出了一个高效的框架，称为 HiCoM，它包含三个关键组件。首先，我们使用扰动平滑策略构建一个紧凑而强大的初始 3DGS 表示。接下来，我们引入了一种分层相干运动机制，该机制利用 3D 高斯固有的非均匀分布和局部一致性来快速准确地学习跨帧运动。最后，我们不断用额外的高斯函数细化 3DGS，这些函数随后被合并到初始 3DGS 中，以保持与不断发展的场景的一致性。为了保持紧凑的表示，在处理后续帧之前，会移除对表示影响最小的等量低不透明度高斯函数。在两个广泛使用的数据集上进行的大量实验表明，我们的框架将最先进方法的学习效率提高了约 $20\%$，并将数据存储减少了 $85\%$，实现了具有竞争力的自由视点视频合成质量，但具有更高的鲁棒性和稳定性。此外，通过同时并行学习多个帧，我们的 HiCoM 将平均训练时间减少到每帧 $&lt;2$ 秒，而性能下降几乎可以忽略不计，大大提高了现实世界的适用性和响应能力。]]></description>
      <guid>https://arxiv.org/abs/2411.07541</guid>
      <pubDate>Wed, 13 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>