<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Fri, 11 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>用于分布外检测的边界边界置信度分数</title>
      <link>https://arxiv.org/abs/2410.07185</link>
      <description><![CDATA[arXiv:2410.07185v1 公告类型：新
摘要：在许多关键的机器学习应用中，例如自动驾驶和医学图像诊断，检测分布外 (OOD) 样本与准确分类分布内 (ID) 输入一样重要。最近，基于异常值暴露 (OE) 的方法已显示出通过使用辅助异常值数据进行模型微调来检测 OOD 输入的良好结果。然而，大多数以前基于 OE 的方法更强调合成额外的异常值样本或引入正则化来多样化 OOD 样本空间，这在实践中是难以量化的。在这项工作中，我们提出了一种新颖而直接的方法，称为边际有界置信度分数 (MaCS)，通过扩大 ID 和 OOD 分数之间的差异来解决非平凡的 OOD 检测问题，这反过来又使决策边界更加紧凑，从而通过简单的阈值促进有效隔离。具体来说，我们用补充约束增强了 OE 正则化分类器的学习目标，这会惩罚 OOD 输入（与 ID 相比）的高置信度分数，并在保持 ID 分类准确度的同时显著提高 OOD 检测性能。在各种图像分类任务基准数据集上进行的大量实验证明了所提方法的有效性，它在各种基准指标上的表现显著优于最先进 (S.O.T.A) 方法。代码可在 https://github.com/lakpa-tamang9/margin_ood 上公开获取]]></description>
      <guid>https://arxiv.org/abs/2410.07185</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>技术报告：Modelscope-Sora 竞争解决方案</title>
      <link>https://arxiv.org/abs/2410.07194</link>
      <description><![CDATA[arXiv:2410.07194v1 公告类型：新
摘要：本报告介绍了 Modelscope-Sora 挑战赛中采用的方法，该方法侧重于微调视频生成模型的数据。该挑战赛评估参与者在特定计算约束下分析、清理和生成高质量数据集的能力，用于基于视频的文本到视频任务。所提供的方法涉及数据处理技术，例如视频描述生成、过滤和加速。本报告概述了用于提高训练数据质量的程序和工具，以确保提高文本到视频生成模型的性能。]]></description>
      <guid>https://arxiv.org/abs/2410.07194</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SpaRG：用于通用 fMRI 分析的稀疏重建图</title>
      <link>https://arxiv.org/abs/2410.07201</link>
      <description><![CDATA[arXiv:2410.07201v1 公告类型：新
摘要：深度学习可以帮助揭示与精神疾病和个人特征相关的静息状态功能性磁共振成像 (rs-fMRI) 模式。然而，解释深度学习发现的问题很少比 fMRI 分析更明显，因为数据对扫描效果很敏感，而且本质上难以可视化。我们提出了一种基于稀疏化和自我监督的简单方法来缓解这些挑战。我们不是提取事后特征归因来发现对目标任务很重要的功能连接，而是在训练期间识别一小部分高度信息丰富的连接并遮挡其余连接。为此，我们以端到端的方式联合训练 (1) 稀疏输入掩码、(2) 变分自动编码器 (VAE) 和 (3) 下游分类器。虽然我们需要一部分标记样本来训练分类器，但我们使用来自其他采集站点的未标记数据优化了稀疏掩码和 VAE，仅保留了具有良好泛化能力的输入特征。我们在公共 ABIDE 数据集上评估了我们的方法 - 稀疏重构图 (SpaRG) - 以进行性别分类，使用来自 18 个站点的标记案例进行训练，并使用一部分未标记样本将模型调整到两个额外的分布外站点。对于相对粗略的分区（64 个区域），SpaRG 仅利用了 1% 的原始连接，同时提高了跨域的分类准确性。我们的代码可以在 github.com/yanismiraoui/SpaRG 找到。]]></description>
      <guid>https://arxiv.org/abs/2410.07201</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>神经对比：利用生成编辑进行图形设计推荐</title>
      <link>https://arxiv.org/abs/2410.07211</link>
      <description><![CDATA[arXiv:2410.07211v1 公告类型：新
摘要：创建视觉上吸引人的合成图需要优化文本和背景以实现兼容性。以前的方法侧重于简单的设计策略，例如更改文本颜色或添加背景形状以形成对比。这些方法通常具有破坏性，会改变文本颜色或部分遮挡背景图像。另一种方法是将设计元素放置在非显着和对比区域，但这并不总是有效的，尤其是在有图案的背景中。为了应对这些挑战，我们提出了一种使用扩散模型的生成方法。该方法可确保设计资产下方的改变区域表现出低显着性，同时增强对比度，从而提高设计资产的可见性。]]></description>
      <guid>https://arxiv.org/abs/2410.07211</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Spiking GS：通过基于脉冲神经元的高斯溅射实现高精度、低成本表面重建</title>
      <link>https://arxiv.org/abs/2410.07266</link>
      <description><![CDATA[arXiv:2410.07266v1 公告类型：新
摘要：3D Gaussian Splatting 能够在几分钟内重建 3D 场景。尽管最近在提高表面重建精度方面取得了进展，但重建结果仍然存在偏差，并且在存储和训练方面效率低下。本文对效率低下和重建偏差的原因提供了不同的观察，这归因于生成的高斯的低不透明度部分 (LOP) 的集成。我们表明 LOP 由具有整体低不透明度的高斯 (LOG) 和高斯的低不透明度尾部 (LOT) 组成。我们提出 Spiking GS 通过将脉冲神经元集成到 Gaussian Splatting 管道中来减少这两类 LOP。具体来说，我们分别将全局和局部全精度积分激发脉冲神经元引入到扁平 3D 高斯的不透明度和表示函数中。此外，我们利用脉冲神经元的阈值和高斯尺度上的新标准增强了密度控制策略。我们的方法可以以更低的成本表示更准确的重建表面。代码可在 \url{https://github.com/shippoT/Spiking_GS} 获得。]]></description>
      <guid>https://arxiv.org/abs/2410.07266</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过鸟瞰视图表示学习内容感知的多模态联合输入修剪</title>
      <link>https://arxiv.org/abs/2410.07268</link>
      <description><![CDATA[arXiv:2410.07268v1 公告类型：新
摘要：在自动驾驶领域，鸟瞰图 (BEV) 表示最近引起了大量学术关注，成为融合多模态传感器输入的变革框架。这种 BEV 范式有效地将传感器融合挑战从基于规则的方法转变为以数据为中心的方法，从而有助于从一系列异构传感器中提取更细微的特征。尽管基于 BEV 的技术具有明显的优点，但其计算开销通常需要高容量硬件基础设施，因此对实际的现实实现构成了挑战。为了缓解这一限制，我们引入了一种新颖的内容感知多模态联合输入修剪技术。我们的方法利用 BEV 作为共享锚点，在将非必要传感器区域引入感知模型的主干之前，通过算法识别和消除它们。我们通过对 NuScenes 数据集进行大量实验来验证我们方法的有效性，证明了在不牺牲感知准确性的情况下具有显著的计算效率。据我们所知，这项工作代表了首次尝试从输入修剪点减轻计算负担。]]></description>
      <guid>https://arxiv.org/abs/2410.07268</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>BELM：用于扩散模型精确反演的双向显式线性多步采样器</title>
      <link>https://arxiv.org/abs/2410.07273</link>
      <description><![CDATA[arXiv:2410.07273v1 公告类型：新 
摘要：扩散模型采样反演旨在找到样本相应的初始噪声，在各种任务中起着至关重要的作用。最近，已经提出了几种启发式精确反演采样器，以无需训练的方式解决不精确反演问题。然而，这些启发式采样器的理论特性仍然未知，而且它们通常表现出平庸的采样质量。在本文中，我们介绍了一种通用公式，即精确反演采样器的 \emph{双向显式线性多步} (BELM) 采样器，其中包括所有以前提出的启发式精确反演采样器作为特例。BELM 公式是通过集成双向显式约束从可变步长可变公式线性多步法推导出来的。我们强调这种双向显式约束是数学精确反演的关键。我们系统地研究了 BELM 框架内的局部截断误差 (LTE)，并表明现有的精确反演采样器的启发式设计会产生次优 LTE。因此，我们通过 LTE 最小化方法提出了最佳 BELM (O-BELM) 采样器。我们进行了额外的分析，以证实所提出的最佳采样器的理论稳定性和全局收敛性。全面的实验表明，我们的 O-BELM 采样器在实现高质量采样的同时建立了精确反演特性。在图像编辑和图像插值方面的额外实验凸显了 O-BELM 在不同应用中应用的广泛潜力。]]></description>
      <guid>https://arxiv.org/abs/2410.07273</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>减轻自动面部非语言行为生成中的性别偏见</title>
      <link>https://arxiv.org/abs/2410.07274</link>
      <description><![CDATA[arXiv:2410.07274v1 公告类型：新
摘要：社交互动代理的非语言行为生成研究主要关注非语言提示与语音的可信度和同步性。然而，现有的模型主要基于深度学习架构，往往会延续训练数据中固有的偏见。这引发了道德问题，具体取决于这些代理的预期应用。本文首先通过研究性别对面部非语言行为的影响来解决这些问题。我们专注于凝视、头部运动和面部表情。我们引入了一个分类器，能够从非语言提示中辨别说话者的性别。该分类器在使用最先进工具提取的真实行为数据和从先前工作中开发的模型生成的合成数据上都实现了高精度。在此基础上，我们提出了一个新模型 FairGenderGen，它将性别鉴别器和梯度反转层集成到我们之前的行为生成模型中。这种新模型根据语音特征生成面部非语言行为，从而减轻了生成行为中的性别敏感性。我们的实验表明，在初始阶段开发的分类器不再能有效地从生成的非语言行为中区分说话者的性别。]]></description>
      <guid>https://arxiv.org/abs/2410.07274</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>检索替换缩减：一种通过语义匹配实现的有效视觉标记缩减方法</title>
      <link>https://arxiv.org/abs/2410.07278</link>
      <description><![CDATA[arXiv:2410.07278v1 公告类型：新
摘要：多模态大型语言模型 (MLLM) 已在各种任务中表现出色，无需从头开始训练。然而，它们面临着巨大的计算和内存限制，特别是在处理超过上下文长度的多模态输入时，这限制了它们的可扩展性。在本文中，我们介绍了一种新方法 \textbf{TRSM}（\textbf{T}oken \textbf{R}eduction via \textbf{S}emantic \textbf{M}atch），它有效地减少了视觉标记的数量，而不会影响 MLLM 性能。受人类处理多模态任务的方式的启发，TRSM 利用一种模态的语义信息来匹配另一种模态中的相关语义，从而减少视觉标记的数量。具体而言，为了保留与任务相关的视觉标记，我们使用文本提示作为查询向量来从视觉提示中检索最相似的向量并将它们与文本标记合并。根据实验结果，当应用于 LLaVA-1.5\cite{liu2023} 时，我们的方法将视觉标记压缩了 20\%，在不同的视觉问答和推理任务中实现了可比的性能。]]></description>
      <guid>https://arxiv.org/abs/2410.07278</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ReinDiffuse：利用强化扩散模型制作物理上合理的运动</title>
      <link>https://arxiv.org/abs/2410.07296</link>
      <description><![CDATA[arXiv:2410.07296v1 公告类型：新
摘要：根据文本描述生成人体运动是一项具有挑战性的任务。现有的方法要么难以实现物理可信度，要么受到物理模拟复杂性的限制。在本文中，我们提出了 \emph{ReinDiffuse}，它将强化学习与运动扩散模型相结合，以生成与文本描述一致的物理可信的人体运动。我们的方法采用运动扩散模型来输出参数化的动作分布，使其与强化学习范式兼容。我们采用强化学习，目的是最大化物理上合理的奖励，以优化运动生成以实现物理保真度。我们的方法在两个主要数据集 HumanML3D 和 KIT-ML 上的表现优于现有的最先进模型，在物理可信度和运动质量方面取得了显着的改进。项目：\url{https://reindiffuse.github.io/}]]></description>
      <guid>https://arxiv.org/abs/2410.07296</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>增强具有一致性损失的点云完成网络的性能</title>
      <link>https://arxiv.org/abs/2410.07298</link>
      <description><![CDATA[arXiv:2410.07298v1 公告类型：新
摘要：点云补全网络通常经过训练以最小化已完成点云与地面实况点云之间的差异。然而，当单独检查不完整的对象级点云时，它可以有多个有效的补全解决方案。这种一对多映射问题可能导致网络产生矛盾的监督信号，因为损失函数可能会为网络的相同输入输出对产生不同的值。在许多情况下，这个问题可能会对网络优化过程产生不利影响。在这项工作中，我们建议使用一种新颖的补全一致性损失来缓解一对多映射问题，以增强传统的学习目标。具体而言，所提出的一致性损失确保点云补全网络为来自同一源点云的不完整对象生成一致的补全解决方案。在多个成熟的数据集和基准上的实验结果表明，所提出的补全一致性损失具有出色的能力，可以在不修改网络设计的情况下增强各种现有网络的补全性能。所提出的一致性损失在不影响推理速度的情况下增强了点云补全网络的性能，从而提高了点云补全的准确性。值得注意的是，使用所提出的一致性损失训练的最先进的点云补全网络可以在具有挑战性的新 MVP 数据集上实现最先进的准确性。使用所提出的一致性损失对各种点云补全模型进行实验的代码和结果将在以下位置提供：https://github.com/kaist-avelab/ConsistencyLoss。]]></description>
      <guid>https://arxiv.org/abs/2410.07298</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>整流扩散：整流流中不需要直线性</title>
      <link>https://arxiv.org/abs/2410.07303</link>
      <description><![CDATA[arXiv:2410.07303v1 公告类型：新
摘要：扩散模型极大地改善了视觉生成，但由于求解生成 ODE 的计算密集型特性，生成速度缓慢。整流是一种被广泛认可的解决方案，它通过拉直 ODE 路径来提高生成速度。其关键组件包括：1）使用流匹配的扩散形式，2）采用 $\boldsymbol v$ 预测，3）执行整流（又称回流）。在本文中，我们认为整流的成功主要在于使用预训练的扩散模型来获得匹配的噪声和样本对，然后用这些匹配的噪声样本对进行重新训练。基于此，组件 1）和 2）是不必要的。此外，我们强调直线性不是整流的基本训练目标；相反，它是流匹配模型的一个特例。更关键的训练目标是实现一阶近似 ODE 路径，对于 DDPM 和 Sub-VP 等模型来说，该路径本质上是弯曲的。基于这一见解，我们提出了 Rectified Diffusion，它概括了整流的设计空间和应用范围，以涵盖更广泛的扩散模型类别，而不仅限于流匹配模型。我们在 Stable Diffusion v1-5 和 Stable Diffusion XL 上验证了我们的方法。我们的方法不仅大大简化了基于整流的先前作品（例如 InstaFlow）的训练过程，而且还以更低的训练成本实现了卓越的性能。我们的代码可在 https://github.com/G-U-N/Rectified-Diffusion 上找到。]]></description>
      <guid>https://arxiv.org/abs/2410.07303</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于视觉和语言评估和训练的正增强对比学习</title>
      <link>https://arxiv.org/abs/2410.07336</link>
      <description><![CDATA[arXiv:2410.07336v1 公告类型：新
摘要：尽管字幕生成取得了重大进展，但现有的评估指标往往无法捕捉字幕的全部质量或细粒度细节。这主要是因为它们依赖于非特定的人工编写的参考资料或嘈杂的预训练数据。尽管如此，找到一个有效的指标不仅对字幕评估而且对生成阶段都至关重要。指标确实可以在字幕模型的微调阶段发挥关键作用，最终提高生成的字幕的质量。在本文中，我们提出了 PAC-S++，这是一种可学习的指标，它利用 CLIP 模型，在网络收集和清理的数据上进行预训练，并通过生成的额外视觉和文本正样本对进行正则化。利用这种更强大且精心策划的预训练，我们还将 PAC-S++ 作为通常用于微调字幕模型的自我批判序列训练 (SCST) 阶段的奖励。对不同图像和视频数据集进行的大量实验凸显了 PAC-S++ 与该任务的流行指标相比的有效性，包括其对物体幻觉的敏感性。此外，我们表明，将 PAC-S++ 集成到字幕模型的微调阶段会产生语义更丰富的字幕，重复和语法错误更少。域外基准的评估进一步证明了我们的微调方法在增强模型能力方面的有效性。源代码和训练模型可公开获取：https://github.com/aimagelab/pacscore。]]></description>
      <guid>https://arxiv.org/abs/2410.07336</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 Micro-CT 对小物体进行整体扫描和自动表面处理</title>
      <link>https://arxiv.org/abs/2410.07385</link>
      <description><![CDATA[arXiv:2410.07385v1 公告类型：新
摘要：现代考古方法越来越多地利用物体的 3D 虚拟表示、计算密集型分析、高分辨率扫描、大数据集和机器学习。随着更高分辨率的扫描，围绕计算能力、内存和文件存储的挑战迅速出现。处理和分析高分辨率扫描通常需要内存密集型工作流程，这对于大多数计算机来说是不可行的，并且越来越需要使用超级计算机或创新方法在标准计算机上进行处理。在这里，我们介绍了一种用于对小物体进行大规模微型 CT 扫描的新协议，该协议具有在内存有限的环境中运行的{\em 大部分自动化}处理工作流程。我们仅使用 10 次微型 CT 扫描就扫描了 1,112 个动物骨骼碎片，并将其后处理为单独的 PLY 文件。值得注意的是，我们的方法可以应用于任何物体（与包装材料具有可辨别的密度），这使得该方法适用于各种研究和领域，包括古生物学、地质学、电气工程和材料科学。此外，扫描机构可能会立即采用我们的方法，将客户订单汇集在一起​​并提供更实惠的扫描。本文介绍的工作是国际和多学科研究联盟“考古和动物考古证据的人类学和数学分析”（AMAAZE）推动的一项大型计划的一部分。AMAAZE 联合人类学、数学和计算机科学专家，开发大规模虚拟考古研究的新方法。总体而言，我们的新扫描方法和处理工作流程为未来大规模高分辨率扫描研究奠定了基础并树立了标准。]]></description>
      <guid>https://arxiv.org/abs/2410.07385</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用开放词汇对象检测器进行结构化空间推理</title>
      <link>https://arxiv.org/abs/2410.07394</link>
      <description><![CDATA[arXiv:2410.07394v1 公告类型：新
摘要：推理物体之间的空间关系对于许多现实世界的机器人任务至关重要，例如获取和交付、物体重新排列和物体搜索。检测和消除不同物体的歧义并确定其位置的能力是成功完成这些任务的关键。最近的几项研究已经使用强大的视觉和语言模型 (VLM) 来解锁机器人代理的这种能力。在本文中，我们介绍了一种结构化的概率方法，该方法将丰富的 3D 几何特征与最先进的开放词汇物体检测器相结合，以增强机器人感知的空间推理能力。对该方法进行了评估，并与最先进的视觉和语言模型 (VLM) 在空间推理任务上的零样本性能进行了比较。为了进行这种比较，我们在现实世界的 RGB-D 主动视觉数据集 [1] 中注释了空间子句，并对该数据集和合成的语义抽象 [2] 数据集进行了实验。结果证明了所提出方法的有效性，其接地空间关系性能比最先进的开源 VLM 高出 20% 以上。]]></description>
      <guid>https://arxiv.org/abs/2410.07394</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>