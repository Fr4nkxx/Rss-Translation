<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>CS.CV更新arxiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.cv更新arxiv.org e-print存档。</description>
    <lastBuildDate>Mon, 31 Mar 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>Skip-Vision：加速视觉模型的综合框架</title>
      <link>https://arxiv.org/abs/2503.21817</link>
      <description><![CDATA[ARXIV：2503.21817V1公告类型：新 
摘要：基于变压器的模型已推动了多模式大语言模型（MLLM）的重大进步，但是在扩展分辨率，培训数据和模型参数时，其计算成本却大幅增加。一个关键的瓶颈源于用于理解细粒的图像所需的视觉令牌的扩散。我们提出了Skip-Vision，这是一个统一的框架，涉及视觉模型中的训练和推理效率低下。除了传统的令牌压缩方法外，我们的方法还引入了两种互补的加速策略。对于训练加速度，我们观察到视觉令牌上的前馈网络（FFN）计算会引起边际功能更新。这激发了我们的跳过策略，该策略绕过了FFN层的冗余视觉令牌。为了推断加速度，我们设计了一种选择性的KV-CACHE去除机制，该机制在解码过程中在保留模型性能的同时，在解码过程中修剪了跳过的键值对。实验结果表明，Skip-Vision将训练时间最多减少了35 \％，推理拖曳量增加了75 \％，而潜伏期则减少了45 \％，同时获得了与现有方法的可比性或优越的性能。我们的工作提供了一种实用的解决方案，可通过提高效率来扩展高性能MLLM。]]></description>
      <guid>https://arxiv.org/abs/2503.21817</guid>
      <pubDate>Mon, 31 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>UFM：统一的功能与多模式图像助手匹配的预训练</title>
      <link>https://arxiv.org/abs/2503.21820</link>
      <description><![CDATA[ARXIV：2503.21820V1公告类型：新 
摘要：图像特征匹配是计算机视觉中的基础任务，对于多模式图像应用程序仍然具有挑战性，通常需要在特定数据集中进行复杂的培训。在本文中，我们引入了统一的功能匹配的预训练模型（UFM），旨在解决各种模态图像的特征匹配挑战。我们提出了多模式图像助手（MIA）变压器，可调节的结构擅长处理各种特征匹配问题。 UFM在解决同一模态和不同模态的功能匹配任务方面表现出多功能性。此外，我们提出了一种数据增强算法和分阶段的预训练策略，以有效地应对特定模式和不平衡模态数据集中数据稀疏引起的挑战。实验结果表明，UFM在各种特征匹配任务中擅长概括和性能。该代码将在以下网址发布：https：//github.com/liaoyun0x0/ufm。]]></description>
      <guid>https://arxiv.org/abs/2503.21820</guid>
      <pubDate>Mon, 31 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>对刚体目标ISAR成像的预训练稳定扩散的低排名适应</title>
      <link>https://arxiv.org/abs/2503.21823</link>
      <description><![CDATA[ARXIV：2503.21823V1公告类型：新 
摘要：由于时间频率分析的局限性（TFA），传统的范围内传染性多普勒（RID）方法通常会遭受低分辨率的损失。为了应对这一挑战，我们的主要重点是从低分辨率对应物中获得高分辨率的时频表示（TFR）。认识到TFR的曲线功能是一种特定类型的纹理功能，我们认为，诸如稳定扩散（SD）之类的经过训练的生成模型非常适合增强TFR，这要归功于它们在捕获纹理表示方面的强大能力。在此洞察力的基础上，我们提出了一种用于刚体靶标的新型逆合成孔径（ISAR）成像方法，利用了预训练的SD模型的低级别适应性（LORA）。我们的方法采用了SD涡轮增压的基本结构和预训练的参数，同时结合了用于洛拉和对抗训练的其他线性操作，以实现超分辨率和抑制噪声。然后，我们将Lora-SD集成到基于RID的ISAR成像中，从而可以通过超分辨率功能引起人们的焦点和去核成像。我们使用模拟和实际雷达数据评估我们的方法。实验结果表明，与传统方法相比，我们在频率ES时和ISAR成像中的方法的优势。值得注意的是，通过对模拟雷达数据进行培训并测试测得的雷达数据，可以验证概括能力。]]></description>
      <guid>https://arxiv.org/abs/2503.21823</guid>
      <pubDate>Mon, 31 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>保护您的视频内容：破坏基于视频的自动化LLM注释</title>
      <link>https://arxiv.org/abs/2503.21824</link>
      <description><![CDATA[ARXIV：2503.21824V1公告类型：新 
摘要：最近，基于视频的大语言模型（基于视频的LLM）在各种视频理解任务中取得了令人印象深刻的表现。但是，这种快速的进步引起了严重的隐私和安全问题，特别是关于在基于视频的LLM的自动注释中未经授权使用的个人视频数据的使用。然后可以使用这些未经授权的注释的视频文本对来改善下游任务的性能，例如文本到视频生成。为了保护个人视频免受未经授权的使用，我们提出了两种具有不可察觉的对抗性扰动的保护性视频水印，命名为漫游和静音。具体而言，漫游者旨在误导基于视频的LLM，以生成视频的不准确字幕，从而通过视频内容和字幕之间的不一致来降低视频注释的质量。另一方面，静音旨在提示基于视频的LLM，以产生异常简短的字幕，缺乏描述性细节。广泛的实验表明，我们的视频水印方法通过显着降低各种基于视频的LLM的视频注释性能有效地保护视频数据，从而展示了保护个人视频内容的隐秘性和鲁棒性。我们的代码可从https://github.com/ttthhl/protecting_your_video_content获得。]]></description>
      <guid>https://arxiv.org/abs/2503.21824</guid>
      <pubDate>Mon, 31 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>边缘检测的混合多阶段学习框架：调查</title>
      <link>https://arxiv.org/abs/2503.21827</link>
      <description><![CDATA[ARXIV：2503.21827V1公告类型：新 
摘要：边缘检测仍然是计算机视觉中的一项基本但具有挑战性的任务，尤其是在不同的照明，噪音和复杂的场景条件下。本文介绍了混合多阶段学习框架，该框架将卷积神经网络（CNN）的特征提取与支持向量机（SVM）分类器集成在一起，以提高边缘定位和结构精度。与传统的端到端深度学习模型不同，我们的方法脱离了代表和分类阶段，增强了鲁棒性和解释性。在BSDS500和NYUDV2等基准数据集上进行的广泛实验表明，在最佳数据集量表（ODS）和最佳图像量表（OIS）方面，所提出的框架优于传统的边缘检测器，甚至超过传统的边缘检测器，甚至超过了最新的基于学习的方法，同时维持竞争性的平均精度（AP）。定性和定量结果都强调了通过我们的方法实现的边缘连续性，噪声抑制和感知清晰度的增强性能。这项工作不仅桥接了古典和深度学习范式，而且为可扩展，可解释和高质量的边缘检测解决方案树立了新的方向。]]></description>
      <guid>https://arxiv.org/abs/2503.21827</guid>
      <pubDate>Mon, 31 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过重量空间学习形成生产</title>
      <link>https://arxiv.org/abs/2503.21830</link>
      <description><![CDATA[ARXIV：2503.21830V1公告类型：新 
摘要：3D形成生成的基础模型最近显示出在全球和局部维度上编码丰富的几何先验的显着能力。但是，由于现实世界中的数据通常很少或嘈杂，因此利用这些先验的下游任务可能会具有挑战性，并且传统的微调可能导致灾难性的遗忘。在这项工作中，我们将大型3D形状生成模型的重量空间视为可以直接探索的数据模式。我们假设在此高维重量空间内的子延伸物可以分别调节拓扑特性或细粒部分特征，从而通过两个实验证明了早期证据。首先，当在调节空间中插值时，我们观察到全球连通性的急剧过渡，这表明体重空间的较小变化可以大大改变拓扑。其次，我们表明，即使数据非常有限，低维度重量化也会产生控制的局部几何形状的变化。这些结果突出了体重空间学习的潜力，以解锁3D形状生成和专业微调的新方法。]]></description>
      <guid>https://arxiv.org/abs/2503.21830</guid>
      <pubDate>Mon, 31 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>船舶轨迹预测的多模式知识增强框架</title>
      <link>https://arxiv.org/abs/2503.21834</link>
      <description><![CDATA[ARXIV：2503.21834V1公告类型：新 
摘要：准确的船舶轨迹预测有助于改善导航安全，路由和环境保护。但是，现有的预测方法受到船舶跟踪来自全球AIS系统的数据的不规则采样时间间隔和船舶运动的复杂性的挑战。这些方面使模型学习和泛化变得困难。为了应对这些挑战并改善血管轨迹预测，我们提出了用于血管轨迹预测的多模式知识增强框架（Maker）。为了更好地与不规则的采样时间间隔抗衡，Maker具有大型语言模型引导的知识转移（LKT）模块，该模块利用预训练的语言模型有效地传递了特定于轨迹的上下文知识。为了增强学习复杂轨迹模式的能力，制造商结合了基于知识的自定进度学习（KSL）模块。该模块采用运动学知识来逐步整合训练过程中的复杂模式，从而可以进行自适应学习和增强概括。两个血管轨迹数据集的实验结果表明，制造商可以将最先进方法的预测准确性提高12.08％-17.86％。]]></description>
      <guid>https://arxiv.org/abs/2503.21834</guid>
      <pubDate>Mon, 31 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>IMEdimage技术报告</title>
      <link>https://arxiv.org/abs/2503.21836</link>
      <description><![CDATA[ARXIV：2503.21836V1公告类型：新 
摘要：背景：染色体核型分析对于诊断遗传性疾病至关重要，但是检测结构异常仍然具有挑战性。虽然AI在医学成像中表现出了希望，但其有效性在各种方式方面有所不同。利用基础模型的进步，将多模式医学成像整合以进行鲁棒特征提取和准确的诊断，我们开发了IMEdimage，这是一种用于一般医学图像识别的端到端模型，在包括染色体异常检测在内的多个成像任务中表现出强大的性能。材料和方法：我们构建了一个全面的医学图像数据集，其中包括来自公共医疗领域的多种模式，包括染色体，细胞，病理，超声，X射线，CT和MRI图像。基于此数据集，我们开发了IMEdimage模型，该模型包含以下关键特征：（1）用于各种模态输入和医学成像任务的统一表示方法； （2）通过思想链（COT）嵌入和混合专家（MOE）策略增强的多层次（情况级，图像级，贴片级）图像识别能力。结果：该测试组包括来自中国六个地区的12个机构的数据，涵盖了三个主流扫描设备，并包括自然分布的，未验证的异常病例。在这个多样化的数据集上，该模型达到了全自动的染色体分析工作流程，包括分割，核分型和异常检测，达到92.75％的灵敏度和91.5％的特异性。结论：我们提出了Imedimage，这是一种用于医学图像分析的端到端基础模型，证明了其在各种医学成像任务中的出色性能。 IMEdimage为临床医生提供了精确的成像分析工具，并有助于提高诊断准确性和疾病筛查。]]></description>
      <guid>https://arxiv.org/abs/2503.21836</guid>
      <pubDate>Mon, 31 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>M-Docsum：LVLM是否真的理解了文档摘要中的交织图像文本？</title>
      <link>https://arxiv.org/abs/2503.21839</link>
      <description><![CDATA[ARXIV：2503.21839V1公告类型：新 
摘要：我们研究了大型视力语言模型（LVLMS）中一个关键但探索次数的问题：LVLM是否真的理解文档中的交织图像文本？现有的文档理解基准通常会使用问题解答格式评估LVLM，这些格式是信息的，并且难以保证对远程依赖的覆盖范围。为了解决这个问题，我们介绍了一个新颖且具有挑战性的多式模式文档摘要基准（M-Docsum-Bench），该基准包括500个高质量的Arxiv论文，以及与人类偏爱相符的交织的多模式摘要。 M-Docsum-Bench是一项基于参考的生成任务，需要使用提供的参考图像来生成交织的图像文本摘要，从而同时评估复杂多模式文档方案中理解，推理，本地化和汇总的能力。为了促进此基准，我们开发了一个自动化框架来构建摘要并提出一种称为M-Doceval的细粒评估方法。此外，我们进一步开发了一个强大的摘要基线，即M-Docsum-7b，通过具有多种指导和偏好数据的进行性两阶段培训。在我们的M-Docsum台上的广泛结果表明，领先的LVLM努力保持连贯性并准确地将信息整合在长而相互交织的环境中，通常会在相似的图像和缺乏健壮性之间表现出混乱。值得注意的是，M-Docsum-7b与较大和闭合源模型相比（包括GPT-4O，Gemini Pro，Claude-3.5-Sonnet和Qwen2.5-VL-72B等），实现了最先进的性能，证明了LVLMS对改进的Interleaved Interleaved Interleaved Interleaved Intealed Image-Text的了解。代码，数据和模型可在https://github.com/stepfun-ai/m-docsum-bench上找到。]]></description>
      <guid>https://arxiv.org/abs/2503.21839</guid>
      <pubDate>Mon, 31 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>无与伦比：高光谱遥感图像的通道自适应和无调的基础模型</title>
      <link>https://arxiv.org/abs/2503.21841</link>
      <description><![CDATA[ARXIV：2503.21841V1公告类型：新 
摘要：高光谱遥感图像的高级解释使许多精确的地球观察任务受益。最近，Visual Foundation模型促进了遥感解释，但集中在RGB和多光谱图像上。由于高光谱渠道的各种渠道，现有的基础模型将面临逐图调音情况，对硬件和时间资源施加巨大压力。在本文中，我们通过调整现有的视觉及时工程设计，提出了一种称为Hyperfree的无调高光谱基础模型。为了处理多样化的通道号，我们设计了一个学到的权重字典，覆盖全谱的$ 0.4 \ sim 2.5 \，\ mu \ text {m} $，支持动态构建嵌入式图层。为了使及时设计更加易于处理，HyperFree可以通过将特征距离视为语义相似性来为一个提示生成多个语义感知面膜。在构建的大规模高分辨率高光谱图像上进行预训练过度无效之后，Hyperfree（1个提示）在5个任务和11个数据集上的专业模型（5次）显示出可比的结果。在https://rsidea.whu.edu.edu.edu.cn/hyhyperfree.htm.https：//rsidea.whu.edea.whu.edea.whu.edea.whu.edea.whu.edea.https-code和11个数据集上显示出了可比的结果。]]></description>
      <guid>https://arxiv.org/abs/2503.21841</guid>
      <pubDate>Mon, 31 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CMD-HAR：可穿戴人类活动识别的跨模式分解</title>
      <link>https://arxiv.org/abs/2503.21843</link>
      <description><![CDATA[ARXIV：2503.21843V1公告类型：新 
摘要：人类活动识别（HAR）是许多人类以人为中心的智能应用的基本技术。尽管已经利用深度学习方法来加速特征提取，但多模式数据混合，活动异质性和复杂模型部署等问题基本上仍未解决。本文的目的是解决基于传感器的人类活动识别中的多模式数据混合，活动异质性和复杂模型部署等问题。我们提出了一个时空注意模态分解融合策略，以解决传感器数据混合分布的问题。活动的关键歧视性特征是通过跨模式时空暂时性的表示的表示，并组合梯度调制以减轻数据异质性。此外，构建了可穿戴的部署模拟系统。我们对大量公共数据集进行了实验，证明了该模型的有效性。]]></description>
      <guid>https://arxiv.org/abs/2503.21843</guid>
      <pubDate>Mon, 31 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>自动化新闻视频细分的图像，视频和音频分类器的比较分析</title>
      <link>https://arxiv.org/abs/2503.21848</link>
      <description><![CDATA[ARXIV：2503.21848V1公告类型：新 
摘要：新闻视频需要有效的内容组织和检索系统，但是它们的非结构化性质对自动处理构成了重大挑战。本文对自动化新闻视频细分的图像，视频和音频分类器进行了全面的比较分析。这项工作介绍了多种深度学习方法的开发和评估，包括Resnet，Vivit，AST和多模式体系结构，以对五种不同的细分类型进行分类：广告，故事，工作室场景，过渡和可视化。我们的实验使用了包含1,832个场景剪辑的41个新闻视频的自定义注销数据集，与更复杂的时间模型相比，基于图像的分类器具有卓越的性能（84.34 \％精度）。值得注意的是，Resnet体系结构的表现优于最先进的视频分类器，同时需要更少的计算资源。二进制分类模型的过渡（94.23 \％）和广告（92.74 \％）实现了很高的精度。这些发现提高了对新闻视频细分的有效体系结构的理解，并为在媒体应用程序中实施自动化内容组织系统提供了实用的见解。其中包括媒体存档，个性化内容交付和智能视频搜索。]]></description>
      <guid>https://arxiv.org/abs/2503.21848</guid>
      <pubDate>Mon, 31 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在大型多模型作为开放世界图像分类器上</title>
      <link>https://arxiv.org/abs/2503.21851</link>
      <description><![CDATA[ARXIV：2503.21851V1公告类型：新 
摘要：传统图像分类需要一个预定义的语义类别列表。相比之下，大型多模式模型（LMM）可以通过使用自然语言直接对图像进行分类（例如，回答“图像中的主要对象？”）来避开此要求。尽管具有出色的功能，但大多数现有的关于LMM分类性能的研究在范围上还是有限的，通常假设具有预定义类别的封闭世界。在这项工作中，我们通过在真正开放世界的环境中彻底评估LMM分类性能来解决这一差距。我们首先将任务正式化并引入评估协议，定义了各种指标，以评估预测和地面真理类之间的一致性。然后，我们评估了10个基准测试的13个模型，其中包括原型，非原型，细粒度和非常细粒度的类别，这表明了LMMS在此任务中面临的挑战。基于拟议的指标的进一步分析揭示了LMMS造成的错误类型，强调了与粒度和细粒度相关的挑战，显示了量身定制的提示和推理可以减轻它们的量身定制的挑战。]]></description>
      <guid>https://arxiv.org/abs/2503.21851</guid>
      <pubDate>Mon, 31 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>foveated实例细分</title>
      <link>https://arxiv.org/abs/2503.21854</link>
      <description><![CDATA[ARXIV：2503.21854V1公告类型：新 
摘要：实例细分对于增强现实和虚拟现实（AR/VR）至关重要，因为它可以实现精确的对象识别和互动，从而增强了虚拟和真实世界元素的整合，以获得沉浸式体验。但是，分割的高计算开销将其应用程序限制在资源受限的AR/VR设备上，从而导致大量处理延迟并降低用户体验。与传统的方案相反，AR/VR用户通常仅关注其视野视野中的几个区域，然后才能转移透视图，从而使细分集中在特定于目光的领域上。这种洞察力驱动了需要优先考虑关注的处理实例，减少计算负载并增强实时性能的有效分割方法的需求。在本文中，我们提出了一个foveated实例细分（fovealseg）框架，该框架利用实时用户凝视数据专门执行实例细分，从而实现了实例细分，从而获得了大量的计算节省。评估结果表明，FSNET在ADE20K上达到0.56，在LVI上达到0.54，尤其超过了基线。该代码可在https://github.com/sai--]]></description>
      <guid>https://arxiv.org/abs/2503.21854</guid>
      <pubDate>Mon, 31 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>StarFlow：从草图图像生成结构化工作流量输出</title>
      <link>https://arxiv.org/abs/2503.21889</link>
      <description><![CDATA[ARXIV：2503.21889V1公告类型：新 
摘要：工作流程是企业平台中自动化的基本组成部分，从而使任务，数据处理和系统集成的编排。尽管被广泛使用，但建筑工作流程可能很复杂，通常需要通过低编码平台或视觉编程工具进行手动配置。为了简化此过程，我们探讨了生成基础模型，尤其是视觉语言模型（VLM）的使用，以自动从视觉输入中生成结构化的工作流。由于自由图纸的模棱两可，图样式的变化以及从视觉元素推断执行逻辑的难度，将手绘草图或计算机生成的图转换为可执行的工作流程。为了解决这个问题，我们介绍了Starflow，这是一个框架，用于使用视觉模型从草图中生成结构化工作流量。我们策划了各种工作流程图的数据集，包括合成，手动注释和现实世界样本，以实现强大的培训和评估。我们对多种视觉语言模型进行了验证和基准测试，进行了一系列消融研究，以分析我们方法的优势和局限性。我们的结果表明，填充显着增强了结构化的工作流程的产生，在此任务上表现优于大型视觉模型。]]></description>
      <guid>https://arxiv.org/abs/2503.21889</guid>
      <pubDate>Mon, 31 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>