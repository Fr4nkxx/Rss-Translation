<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Thu, 22 Aug 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>库布里克：用于合成视频生成的多模式代理协作</title>
      <link>https://arxiv.org/abs/2408.10453</link>
      <description><![CDATA[arXiv:2408.10453v1 公告类型：新
摘要：文本到视频的生成一直由基于端到端扩散或自回归的模型主导。一方面，这些新模型提供了合理的多功能性，但它们因物理正确性、阴影和照明、相机运动和时间一致性而受到批评。另一方面，电影业依赖于使用 3D 建模软件手动编辑的计算机生成图像 (CGI)。人工指导的 3D 合成视频和动画解决了上述缺点，但它非常繁琐，需要电影制作人和 3D 渲染专家之间的紧密合作。在本文中，我们介绍了一种基于 Vision 大型语言模型 (VLM) 代理协作的自动合成视频生成管道。给定视频的自然语言描述，多个 VLM 代理自动指导生成管道的各种过程。他们合作创建 Blender 脚本，以渲染与给定描述最一致的视频。基于电影制作灵感，并借助基于 Blender 的电影制作知识，导演代理将输入的基于文本的视频描述分解为子流程。对于每个子流程，程序员代理根据自定义函数组合和 API 调用生成基于 Python 的 Blender 脚本。然后，审阅者代理借助视频审阅、角色运动坐标和中间屏幕截图的知识，使用其构图推理能力向程序员代理提供反馈。程序员代理迭代改进脚本以产生最佳的整体视频效果。我们生成的视频在视频质量和指令遵循性能的 5 个指标上都比商业视频生成模型质量更好。此外，我们的框架在质量、一致性和合理性的综合用户研究中优于其他方法。]]></description>
      <guid>https://arxiv.org/abs/2408.10453</guid>
      <pubDate>Thu, 22 Aug 2024 06:17:32 GMT</pubDate>
    </item>
    <item>
      <title>实现人类衰变阶段识别的自动化：一种人工智能方法</title>
      <link>https://arxiv.org/abs/2408.10414</link>
      <description><![CDATA[arXiv:2408.10414v1 公告类型：新
摘要：确定腐烂阶段 (SOD) 对于估计死后间隔和识别人类遗骸至关重要。目前，为此目的使用劳动密集型的手动评分方法，但它们是主观的，并且不适用于新兴的大规模人体腐烂照片档案集。本研究探讨了使用人工智能 (AI) 自动化 Megyesi 和 Gelderman 提出的两种常见人体腐烂评分方法的可行性。我们评估了两种流行的深度学习模型 Inception V3 和 Xception，通过在大量人体腐烂图像数据集上对它们进行训练来对不同解剖区域（包括头部、躯干和四肢）的 SOD 进行分类。此外，还进行了一项评分者间研究，以评估 AI 模型与人类法医检查员在 SOD 识别方面的可靠性。 Xception 模型实现了最佳分类性能，在预测 Megyesi 的 SOD 时，头部、躯干和四肢的宏观平均 F1 得分分别为 .878、.881 和 .702，在​​预测 Gelderman 的 SOD 时，头部、躯干和四肢的宏观平均 F1 得分分别为 .872、.875 和 .76。评分者间研究结果支持 AI 能够以与人类专家相当的可靠性水平确定 SOD。这项工作展示了在大量人体分解图像数据集上训练的 AI 模型在自动识别 SOD 方面的潜力。]]></description>
      <guid>https://arxiv.org/abs/2408.10414</guid>
      <pubDate>Thu, 22 Aug 2024 06:17:31 GMT</pubDate>
    </item>
    <item>
      <title>CLIP-DPO：视觉语言模型作为修复 LVLM 幻觉的偏好来源</title>
      <link>https://arxiv.org/abs/2408.10433</link>
      <description><![CDATA[arXiv:2408.10433v1 公告类型：新
摘要：尽管最近取得了成功，但 LVLM 或大型视觉语言模型容易对诸如对象及其属性或关系之类的细节产生幻觉，从而限制了它们在现实世界中的部署。为了解决这个问题并提高其稳健性，我们提出了 CLIP-DPO，这是一种偏好优化方法，它利用对比预训练的视觉语言 (VL) 嵌入模型（例如 CLIP）进行基于 DPO 的 LVLM 优化。与之前解决 LVLM 幻觉的研究不同，我们的方法不依赖于付费 API，也不需要额外的训练数据或部署其他外部 LVLM。相反，从最初的监督微调数据池开始，我们生成一组不同的预测，这些预测根据它们的 CLIP 图像文本相似性进行排名，然后使用基于规则的稳健方法进行过滤，以获得一组用于基于 DPO 的训练的正负对。我们对 MobileVLM-v2 系列模型和 LlaVA-1.5 进行了 CLIP-DPO 微调，在所有情况下，与基线模型相比，幻觉减少方面均有显著改善。我们还观察到零样本分类的性能更好，表明基础能力有所提高，并验证了标准 LVLM 基准上的原始性能总体上得到保留。]]></description>
      <guid>https://arxiv.org/abs/2408.10433</guid>
      <pubDate>Thu, 22 Aug 2024 06:17:31 GMT</pubDate>
    </item>
    <item>
      <title>人工智能生成的图像水印技术的脆弱性：检查​​其对视觉释义攻击的鲁棒性</title>
      <link>https://arxiv.org/abs/2408.10446</link>
      <description><![CDATA[arXiv:2408.10446v1 公告类型：新
摘要：文本到图像生成系统的快速发展，例如 Stable Diffusion、Midjourney、Imagen 和 DALL-E 等模型，加剧了人们对其潜在滥用的担忧。作为回应，Meta 和 Google 等公司加大了在 AI 生成的图像上实施水印技术的力度，以遏制可能具有误导性的视觉效果的传播。然而，在本文中，我们认为当前的图像水印方法很脆弱，容易被视觉释义攻击所规避。所提出的视觉释义器分两个步骤运行。首先，它使用 KOSMOS-2（最新的先进图像字幕系统之一）为给定图像生成字幕。其次，它将原始图像和生成的字幕都传递给图像到图像的扩散系统。在扩散管道的去噪步骤中，系统会生成由文本标题引导的视觉相似图像。生成的图像是视觉释义，没有任何水印。我们的实证研究结果表明，视觉释义攻击可以有效地从图像中去除水印。本文提供了批判性评估，实证揭示了现有水印技术对视觉释义攻击的脆弱性。虽然我们没有提出解决这个问题的方案，但本文呼吁科学界优先开发更强大的水印技术。我们首创的视觉释义数据集和随附代码已公开。]]></description>
      <guid>https://arxiv.org/abs/2408.10446</guid>
      <pubDate>Thu, 22 Aug 2024 06:17:31 GMT</pubDate>
    </item>
    <item>
      <title>缩小导航愿景与行动之间的差距</title>
      <link>https://arxiv.org/abs/2408.10388</link>
      <description><![CDATA[arXiv:2408.10388v1 公告类型：新
摘要：现有的连续环境中的视觉和语言导航 (VLN-CE) 方法通常结合航点预测器来离散化环境。这将导航操作简化为视图选择任务，与使用低级操作直接训练相比，导航性能显著提高。然而，VLN-CE 代理与真实机器人仍然相差甚远，因为它们的视觉感知和执行的操作之间存在差距。首先，离散化视觉环境的 VLN-CE 代理主要通过高级视图选择进行训练，这导致它们忽略了低级动作运动中的关键空间推理。其次，在这些模型中，现有的航点预测器忽略了对象语义及其与可传递性相关的属性，这些属性可以有助于指示动作的可行性。为了解决这两个问题，我们引入了一个与高级动作预测联合训练的低级动作解码器，使当前的 VLN 代理能够学习并将所选的视觉视图与低级控件联系起来。此外，我们利用包含丰富语义信息的视觉表示并根据人类对行动可行性的先验知识明确遮盖障碍物，增强了当前的航点预测器。从经验上看，与高级和低级行动的强基线相比，我们的代理可以提高导航性能指标。]]></description>
      <guid>https://arxiv.org/abs/2408.10388</guid>
      <pubDate>Thu, 22 Aug 2024 06:17:30 GMT</pubDate>
    </item>
    <item>
      <title>使用事件相机评估基于图像的面部和眼球追踪</title>
      <link>https://arxiv.org/abs/2408.10395</link>
      <description><![CDATA[arXiv:2408.10395v1 公告类型：新
摘要：事件相机，也称为神经形态传感器，可在像素级别捕获局部光强度的变化，从而产生异步生成的数据，称为“事件”。这种独特的数据格式可以缓解传统相机中常见的问题，例如在捕捉快速移动的物体时采样不足，从而保留可能丢失的关键信息。然而，考虑到事件数据的独特属性，利用这些数据通常需要开发专门的、手工制作的事件表示，这些表示可以与传统的卷积神经网络 (CNN) 无缝集成。在本研究中，我们评估了基于事件的面部和眼球跟踪。我们研究的核心目标是展示将传统算法与基于事件的数据集成的可行性，将其转换为帧格式，同时保留事件相机的独特优势。为了验证我们的方法，我们通过模拟从可公开访问的 Helen 数据集派生的 RGB 帧之间的事件构建了一个基于帧的事件数据集。我们通过应用 GR-YOLO（一种源自 YOLOv3 的开创性技术）来评估其在人脸和眼睛检测任务中的实用性。此评估包括与使用 YOLOv8 训练数据集所得结果的比较分析。随后，在来自 Prophesee 事件摄像机不同迭代的真实事件流上测试了训练后的模型，并在事件流中的人脸 (FES) 基准数据集上进行了进一步评估。在我们的数据集上训练的模型在所有用于验证的数据集中显示出良好的预测性能，最佳结果为平均精度得分为 0.91。此外，训练后的模型在不同光照条件下对真实事件摄像机数据表现出稳健的性能。]]></description>
      <guid>https://arxiv.org/abs/2408.10395</guid>
      <pubDate>Thu, 22 Aug 2024 06:17:30 GMT</pubDate>
    </item>
    <item>
      <title>基于网络摄像头的瞳孔直径预测受益于升级</title>
      <link>https://arxiv.org/abs/2408.10397</link>
      <description><![CDATA[arXiv:2408.10397v1 公告类型：新
摘要：获取瞳孔直径对于评估心理和生理状态（例如压力水平和认知负荷）至关重要。然而，眼部数据集中图像的低分辨率通常会妨碍精确测量。本研究评估了从双三次插值到高级超分辨率等各种升级方法对瞳孔直径预测的影响。我们比较了几种预先训练的方法，包括 CodeFormer、GFPGAN、Real-ESRGAN、HAT 和 SRResNet。我们的研究结果表明，在升级数据集上训练的瞳孔直径预测模型对所选的升级方法和尺度高度敏感。我们的结果表明，升级方法始终如一地提高了瞳孔直径预测模型的准确性，凸显了升级在瞳孔测量中的重要性。总的来说，我们的工作为选择升级技术提供了宝贵的见解，为心理和生理研究中更准确的评估铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2408.10397</guid>
      <pubDate>Thu, 22 Aug 2024 06:17:30 GMT</pubDate>
    </item>
    <item>
      <title>机械和固态激光雷达点云地面分割的并行处理</title>
      <link>https://arxiv.org/abs/2408.10404</link>
      <description><![CDATA[arXiv:2408.10404v1 公告类型：新
摘要：在本研究中，我们引入了一种用于 FPGA 平台上实时点云地面分割的新型并行处理框架，旨在使 LiDAR 算法适应从机械到固态 LiDAR (SSL) 技术的不断发展的景观。专注于地面分割任务，我们探索现有方法的并行处理技术，并使其适应现实世界的 SSL 数据处理。我们在基于机械 LiDAR 的 SemanticKITTI 数据集上使用基于点、基于体素和基于范围图像的地面分割方法验证了基于帧分割的并行处理方法。结果揭示了范围图像方法的卓越性能和稳健性，尤其是在其对切片的弹性方面。此外，利用我们自建的 Camera-SSLSS 设备的自定义数据集，我们检查了常规 SSL 数据帧并验证了我们用于 SSL 传感器的并行方法的有效性。此外，我们在 FPGA 上为 SSL 传感器开创性地实现了距离图像地面分割，显著提高了处理速度和资源效率，处理速度比传统 CPU 设置快 50.3 倍。这些发现强调了并行处理策略的潜力，可以显著增强 LiDAR 技术，使其能够完成自主系统中的高级感知任务。发布后，数据和代码都将在 GitHub 上提供。]]></description>
      <guid>https://arxiv.org/abs/2408.10404</guid>
      <pubDate>Thu, 22 Aug 2024 06:17:30 GMT</pubDate>
    </item>
    <item>
      <title>Ricordi 档案手稿中的光学音乐识别</title>
      <link>https://arxiv.org/abs/2408.10260</link>
      <description><![CDATA[arXiv:2408.10260v1 公告类型：新
摘要：Ricordi 档案馆是著名歌剧作曲家如 Donizetti、Verdi 和 Puccini 的重要音乐手稿的著名收藏，现已数字化。这一过程使我们能够自动提取代表手稿上描绘的各种音乐元素的样本，包括音符、五线谱、谱号、擦除和作曲家的注释等。为了区分数字化噪声和实际音乐元素，这些图像的子集被多个人精心分组并标记为几个类别。在评估注释的一致性后，我们训练了多个基于神经网络的分类器来区分已识别的音乐元素。本研究的主要目的是评估这些分类器的可靠性，最终目标是使用它们对剩余的未注释数据集进行自动分类。这些实验中使用的数据集以及手动注释、模型和源代码均可公开访问，以用于复制目的。]]></description>
      <guid>https://arxiv.org/abs/2408.10260</guid>
      <pubDate>Thu, 22 Aug 2024 06:17:29 GMT</pubDate>
    </item>
    <item>
      <title>当代用户生成视觉艺术在复杂性-熵平面上的多样性和风格化</title>
      <link>https://arxiv.org/abs/2408.10356</link>
      <description><![CDATA[arXiv:2408.10356v2 公告类型：新 
摘要：近年来，计算和数值方法的出现为分析艺术史叙事和追踪其中艺术风格的演变提供了新途径。在这里，我们使用复杂性熵 (C-H) 平面研究了当代用户生成的视觉艺术风格的出现和风格化的进化过程，该平面量化了绘画中的局部结构。通过对 2010 年至 2020 年在 DeviantArt 和 Behance 平台上策划的 149,780 幅图像进行信息化，我们分析了 C-H 空间的局部信息与深度神经网络和特征提取算法生成的多级图像特征之间的关系。结果揭示了视觉艺术风格的 C-H 信息与艺术品组中多级图像特征随时间变化的差异之间存在显着的统计关系。通过揭示图像表现多样性明显体现的特定 C-H 区域，我们的分析揭示了新兴风格的经验条件，这些风格在 C-H 平面上既新颖又具有更大的风格多样性。我们的研究表明，视觉艺术分析与物理启发方法和机器学习相结合，可以提供宏观见解，定量映射特定群体和时间的未知视觉艺术创意风格化背后的进化过程的相关特征。]]></description>
      <guid>https://arxiv.org/abs/2408.10356</guid>
      <pubDate>Thu, 22 Aug 2024 06:17:29 GMT</pubDate>
    </item>
    <item>
      <title>HaSPeR：手影木偶识别图像库</title>
      <link>https://arxiv.org/abs/2408.10360</link>
      <description><![CDATA[arXiv:2408.10360v1 公告类型：新
摘要：手影戏，也称为阴影摄影或 ombromanie，是一种戏剧艺术和讲故事的形式，其中手影投射到平面上以产生生物的幻觉。熟练的表演者通过手部定位、手指动作和灵巧的手势来创建这些轮廓，以模仿动物和物体的阴影。由于缺乏从业者以及人们娱乐标准的巨大变化，这种艺术形式濒临灭绝。为了促进其保存并将其传播给更广泛的受众，我们引入了 ${\rm H{\small A}SP{\small E}R}$，这是一个新颖的数据集，由 11 个类别的 8,340 张手影戏图像组成，这些图像来自专业和业余手影戏剪辑。我们对数据集进行了详细的统计分析，并采用了一系列预训练的图像分类模型来建立基线。我们的研究结果表明，传统卷积模型的性能显著优于基于注意力机制的 Transformer 架构。我们还发现，适用于移动应用程序和嵌入式设备的轻量级模型（例如 MobileNetV2）的性能相对较好。我们推测，这种低延迟架构可用于开发混合教学工具，并且我们创建了一个原型应用程序来探索这一推测。我们将性能最佳的模型 InceptionV3 置于聚光灯下，进行全面的特征空间、可解释性和错误分析，以深入了解其决策过程。据我们所知，这是第一个有记录的数据集和研究工作，旨在使用计算机视觉方法为子孙后代保留这种濒临消亡的艺术。我们的代码和数据是公开的。]]></description>
      <guid>https://arxiv.org/abs/2408.10360</guid>
      <pubDate>Thu, 22 Aug 2024 06:17:29 GMT</pubDate>
    </item>
    <item>
      <title>VyAnG-Net：一种通过揭示视觉、听觉和词汇特征的新型多模态讽刺识别模型</title>
      <link>https://arxiv.org/abs/2408.10246</link>
      <description><![CDATA[arXiv:2408.10246v1 公告类型：新
摘要：各种语言和非语言线索，例如过分强调一个词、语调的变化或尴尬的表情，经常传达讽刺。对话中的讽刺识别的计算机视觉问题旨在识别日常对话中隐藏的讽刺、批评和隐喻信息。此前，讽刺识别主要集中在文本上。尽管如此，考虑所有文本信息、音频流、面部表情和身体姿势对于可靠的讽刺识别至关重要。因此，我们提出了一种新颖的方法，该方法结合了轻量级深度注意模块和自调节 ConvNet，以专注于视觉数据的最重要特征，并基于注意力标记器的策略从文本数据中提取最关键的上下文特定信息。以下是我们在执行多模态讽刺识别任务时所做的主要贡献：注意力标记器分支，用于从字幕提供的词汇表内容中获取有益的特征；视觉分支，用于从视频帧中获取最突出的特征；从声学内容中提取话语级特征；以及基于多头注意力的特征融合分支，用于混合从多种模态获得的特征。对基准视频数据集之一 MUSTaRD 进行的广泛测试，对于说话者相关配置的准确率为 79.86%，对于说话者无关配置的准确率为 76.94%，表明我们的方法优于现有方法。我们还进行了跨数据集分析，以测试 VyAnG-Net 对另一个数据集 MUStARD++ 的未见样本的适应性。]]></description>
      <guid>https://arxiv.org/abs/2408.10246</guid>
      <pubDate>Thu, 22 Aug 2024 06:17:28 GMT</pubDate>
    </item>
    <item>
      <title>通过使用视觉-情感字幕翻译网络和视觉-字幕对进行目标相关多模态情绪分析</title>
      <link>https://arxiv.org/abs/2408.10248</link>
      <description><![CDATA[arXiv:2408.10248v1 公告类型：新
摘要：自然语言处理和多媒体领域对多模态情绪识别的兴趣显著增加。因此，本研究旨在采用目标相关多模态情绪分析 (TDMSA) 来识别由视觉标题对组成的多模态帖子中陈述的每个目标（方面）相关的情绪水平。尽管多模态情绪识别最近取得了进展，但缺乏对视觉模态的情感线索的明确结合，特别是那些与面部表情有关的线索。当前的挑战是熟练地获取视觉和情感线索，然后将它们与文本内容同步。鉴于这一事实，本研究提出了一种称为视觉到情感标题翻译网络 (VECTN) 技术的新方法。该策略的主要目标是通过分析面部表情有效地获取视觉情绪线索。此外，它还能有效地将获得的情感线索与字幕模式的目标属性进行对齐和融合。实验结果表明，当应用于两个可公开访问的多模态 Twitter 数据集（即 Twitter-2015 和 Twitter-2017）时，我们的方法能够产生突破性的成果。实验结果表明，所提出的模型在 Twitter-15 数据集上的准确率和宏 F1 分别为 81.23% 和 80.61%，而在 Twitter-17 数据集上的准确率和宏 F1 分别为 77.42% 和 75.19%。观察到的性能改进表明，在使用面部表情收集多模态数据中的目标级情绪时，我们的模型优于其他模型。]]></description>
      <guid>https://arxiv.org/abs/2408.10248</guid>
      <pubDate>Thu, 22 Aug 2024 06:17:28 GMT</pubDate>
    </item>
    <item>
      <title>NeRF-US：从野外神经辐射场中去除超声成像伪影</title>
      <link>https://arxiv.org/abs/2408.10258</link>
      <description><![CDATA[arXiv:2408.10258v2 公告类型：新
摘要：当前在超声成像数据中执行 3D 重建和新视图合成 (NVS) 的方法在训练基于 NeRF 的方法时经常会面临严重的伪影。由于超声捕获的独特性质，当前方法产生的伪影与一般场景中的 NeRF 漂浮物不同。此外，当在临床环境中常见的非受控环境中捕获或随意获取超声数据时，现有模型无法产生合理的 3D 重建。因此，现有的重建和 NVS 方法难以处理超声运动，无法捕捉复杂的细节，并且无法对透明和反射表面进行建模。在这项工作中，我们引入了 NeRF-US，它将边界概率和散射密度的 3D 几何指导纳入 NeRF 训练中，同时还利用超声特定渲染而不是传统的体积渲染。这些 3D 先验是通过扩散模型学习的。通过对我们的新“野外超声”数据集进行的实验，我们观察到了准确、临床合理、无伪影的重建。]]></description>
      <guid>https://arxiv.org/abs/2408.10258</guid>
      <pubDate>Thu, 22 Aug 2024 06:17:28 GMT</pubDate>
    </item>
    <item>
      <title>扩散模型及其应用的综合概述</title>
      <link>https://arxiv.org/abs/2408.10207</link>
      <description><![CDATA[arXiv:2408.10207v1 公告类型：新
摘要：扩散模型是一种概率模型，通过模拟扩散过程来创建逼真的样本，逐渐添加和消除数据中的噪声。这些模型由于能够生成高质量的样本，在图像处理、语音合成和自然语言处理等领域广受欢迎。随着扩散模型被应用于各个领域，现有的文献综述通常侧重于计算机视觉或医学成像等特定领域，可能无法服务于多个领域的更广泛受众。因此，本综述全面概述了扩散模型，涵盖了它们的理论基础和算法创新。我们重点介绍了它们在媒体质量、真实性、合成、图像转换、医疗保健等不同领域的应用。通过整合当前知识并确定新兴趋势，本综述旨在促进对扩散模型的更深入理解和更广泛采用，并为未来跨学科的研究人员和从业者提供指导。]]></description>
      <guid>https://arxiv.org/abs/2408.10207</guid>
      <pubDate>Thu, 22 Aug 2024 06:17:27 GMT</pubDate>
    </item>
    </channel>
</rss>