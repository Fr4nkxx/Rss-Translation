<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Tue, 30 Jul 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>用于 3D 人体姿势估计的灵活图卷积网络</title>
      <link>https://arxiv.org/abs/2407.19077</link>
      <description><![CDATA[arXiv:2407.19077v1 公告类型：新
摘要：尽管图卷积网络在 3D 人体姿势估计中表现出良好的性能，但它们对单跳邻居的依赖限制了它们捕捉身体关节之间高阶依赖关系的能力，这对于减轻因遮挡或深度模糊引起的不确定性至关重要。为了解决这一限制，我们引入了 Flex-GCN，这是一种灵活的图卷积网络，旨在学习捕获更广泛的全局信息和依赖关系的图表示。其核心是灵活的图卷积，它聚合每个节点的直接和二阶邻居的特征，同时保持与标准卷积相同的时间和内存复杂度。我们的网络架构包括灵活图卷积层的残差块，以及用于全局特征聚合、规范化和校准的全局响应规范化层。定量和定性结果证明了我们模型的有效性，在基准数据集上取得了有竞争力的性能。]]></description>
      <guid>https://arxiv.org/abs/2407.19077</guid>
      <pubDate>Wed, 31 Jul 2024 03:14:03 GMT</pubDate>
    </item>
    <item>
      <title>UniForensics：通过一般面部特征进行人脸伪造检测</title>
      <link>https://arxiv.org/abs/2407.19079</link>
      <description><![CDATA[arXiv:2407.19079v1 公告类型：新
摘要：以前的深度伪造检测方法大多依赖于易受干扰的低级纹理特征，无法检测到看不见的伪造方法。相比之下，高级语义特征不易受干扰，并且不限于特定于伪造的伪造，因此具有更强的泛化能力。受此启发，我们提出了一种利用人脸高级语义特征来识别时间域不一致性的方法。我们引入了 UniForensics，这是一种新颖的深度伪造检测框架，它利用基于 Transformer 的视频分类网络，并使用元功能人脸编码器初始化以丰富人脸表示。通过这种方式，我们可以同时利用强大的时空模型和人脸的高级语义信息。此外，为了利用易于获取的真实人脸数据并引导模型关注时空特征，我们设计了一种动态视频自混合（DVSB）方法，使用真实人脸视频高效生成具有多样化时空伪造痕迹的训练样本。基于此，我们通过两阶段训练方法改进了我们的框架：第一阶段采用一种新颖的自监督对比学习，我们通过促使由相同伪造过程生成的视频具有相似的表示来鼓励网络关注伪造痕迹。在第一阶段学到的表示的基础上，第二阶段涉及在人脸伪造检测数据集上进行微调以构建深度伪造检测器。大量实验验证了 UniForensics 在泛化能力和鲁棒性方面优于现有的人脸伪造方法。特别是，我们的方法在具有挑战性的 Celeb-DFv2 和 DFDC 上分别实现了 95.3% 和 77.2% 的跨数据集 AUC。]]></description>
      <guid>https://arxiv.org/abs/2407.19079</guid>
      <pubDate>Wed, 31 Jul 2024 03:14:03 GMT</pubDate>
    </item>
    <item>
      <title>MangaUB：大型多模态模型的漫画理解基准</title>
      <link>https://arxiv.org/abs/2407.19034</link>
      <description><![CDATA[arXiv:2407.19034v1 公告类型：新
摘要：漫画是一种流行的媒介，它结合了风格化的绘画和文字来传达故事。由于漫画面板与自然图像不同，传统上必须专门为漫画设计计算系统。最近，现代大型多模态模型 (LMM) 的自适应性质显示了更通用方法的可能性。为了分析 LMM 当前在漫画理解任务中的能力并确定其改进领域，我们设计并评估了 MangaUB，这是一种用于 LMM 的新型漫画理解基准。MangaUB 旨在评估对单个面板中显示的内容以及跨多个面板传达的内容的识别和理解，从而可以对模型理解漫画所需的各种功能进行细粒度分析。我们的结果显示，它在图像内容识别方面表现出色，而理解跨多个面板传达的情感和信息仍然具有挑战性，这突出了未来在漫画理解方面对 LMM 的工作。]]></description>
      <guid>https://arxiv.org/abs/2407.19034</guid>
      <pubDate>Wed, 31 Jul 2024 03:14:02 GMT</pubDate>
    </item>
    <item>
      <title>ScalingGaussian：利用生成高斯溅射增强 3D 内容创作</title>
      <link>https://arxiv.org/abs/2407.19035</link>
      <description><![CDATA[arXiv:2407.19035v1 公告类型：新
摘要：创建高质量的 3D 资产对于数字遗产保护、娱乐和机器人技术的应用至关重要。传统上，此过程需要熟练的专业人员和专门的软件来对 3D 对象进行建模、纹理化和渲染。然而，游戏和虚拟现实 (VR) 对 3D 资产的需求不断增长，导致了可访问的图像到 3D 技术的创建，允许非专业人员制作 3D 内容并减少对专家输入的依赖。现有的 3D 内容生成方法难以同时实现详细的纹理和强大的几何一致性。我们引入了一个新颖的 3D 内容创建框架 ScalingGaussian，它结合了 3D 和 2D 扩散模型，以在生成的 3D 资产中实现详细的纹理和几何一致性。首先，3D 扩散模型生成点云，然后通过选择局部区域、引入高斯噪声、然后使用局部密度加权选择的过程对点云进行加密。为了细化 3D 高斯，我们利用带有分数蒸馏采样 (SDS) 损失的 2D 扩散模型，引导 3D 高斯进行克隆和分裂。最后，将 3D 高斯转换为网格，并使用均方误差 (MSE) 和梯度剖面先验 (GPP) 损失优化表面纹理。我们的方法解决了 3D 扩散中常见的稀疏点云问题，从而改进了几何结构和细节纹理。在图像到 3D 任务上的实验表明，我们的方法可以有效地生成高质量的 3D 资源。]]></description>
      <guid>https://arxiv.org/abs/2407.19035</guid>
      <pubDate>Wed, 31 Jul 2024 03:14:02 GMT</pubDate>
    </item>
    <item>
      <title>配置处理作为神经网络中稳健对象识别的优化策略</title>
      <link>https://arxiv.org/abs/2407.19072</link>
      <description><![CDATA[arXiv:2407.19072v1 公告类型：新
摘要：配置处理，即对物体组件之间空间关系的感知，对于物体识别至关重要。然而，尽管经过了几十年的研究，这种处理的目的论和潜在的神经计算机制仍然难以捉摸。我们假设，通过配置线索处理物体提供了一种比局部特征线索更强大的识别方法。我们通过设计复合字母刺激的识别任务并比较仅使用局部或配置线索训练的不同神经网络模型来评估这一假设。我们发现配置线索对旋转或缩放等几何变换的性能更为稳健。此外，当两种特征同时可用时，配置线索比局部特征线索更受青睐。分层分析表明，与局部特征线索相比，对配置线索的敏感性出现得较晚，这可能有助于对像素级变换的稳健性。值得注意的是，这种配置处理以纯前馈方式发生，无需进行循环计算。我们对字母刺激的发现成功扩展到自然人脸图像。因此，我们的研究提供了神经计算证据，表明配置处理出现在基于任务偶然性的简单网络中，并且有利于在不同观看条件下进行稳健的对象处理。]]></description>
      <guid>https://arxiv.org/abs/2407.19072</guid>
      <pubDate>Wed, 31 Jul 2024 03:14:02 GMT</pubDate>
    </item>
    <item>
      <title>用于视网膜血管分割的区域引导注意网络</title>
      <link>https://arxiv.org/abs/2407.18970</link>
      <description><![CDATA[arXiv:2407.18970v1 公告类型：新
摘要：视网膜成像已成为解决这一挑战的一种有前途的方法，它利用了视网膜的独特结构。视网膜是中枢神经系统的胚胎延伸，为神经系统健康提供了直接的体内窗口。最近的研究表明，视网膜血管的特定结构变化不仅可以作为各种疾病的早期指标，还有助于了解疾病进展。在这项工作中，我们提出了一种基于区域引导注意的编码器-解码器机制的轻量级视网膜血管分割网络。我们引入了具有区域引导注意的逆加法注意块，以关注前景区域并改善感兴趣区域的分割。为了进一步提高模型在视网膜血管分割方面的表现，我们采用了加权骰子损失。这种选择对于解决视网膜血管分割任务中经常遇到的类别不平衡问题特别有效。 Dice 损失对假阳性和假阴性的惩罚相同，鼓励模型生成更准确的分割，改善物体边界描绘并减少碎片化。在基准数据集上进行的大量实验表明，与最先进的方法相比，该方法具有更好的性能（召回率、准确率、准确度和 F1 分数分别为 0.8285、0.8098、0.9677 和 0.8166）。]]></description>
      <guid>https://arxiv.org/abs/2407.18970</guid>
      <pubDate>Wed, 31 Jul 2024 03:14:01 GMT</pubDate>
    </item>
    <item>
      <title>通过多模态大型语言模型进行基于图的无监督解缠结表征学习</title>
      <link>https://arxiv.org/abs/2407.18999</link>
      <description><![CDATA[arXiv:2407.18999v1 公告类型：新
摘要：解缠表示学习 (DRL) 旨在识别和分解观察背后的潜在因素，从而促进数据感知和生成。然而，当前的 DRL 方法通常依赖于不切实际的假设，即语义因素在统计上是独立的。实际上，这些因素可能表现出相关性，而现成的解决方案尚未正确解决这一问题。为了应对这一挑战，我们引入了一个双向加权图框架，以学习复杂数据中的分解属性及其相互关系。具体而言，我们提出了一个基于 $\beta$-VAE 的模块来提取因子作为图的初始节点，并利用多模态大语言模型 (MLLM) 来发现和排序潜在相关性，从而更新加权边。通过集成这些互补模块，我们的模型成功实现了细粒度、实用和无监督的解缠。实验证明了我们的方法在解缠和重建方面的卓越性能。此外，该模型继承了 MLLM 增强的可解释性和通用性。]]></description>
      <guid>https://arxiv.org/abs/2407.18999</guid>
      <pubDate>Wed, 31 Jul 2024 03:14:01 GMT</pubDate>
    </item>
    <item>
      <title>PromptCCD：学习高斯混合提示池以进行持续类别发现</title>
      <link>https://arxiv.org/abs/2407.19001</link>
      <description><![CDATA[arXiv:2407.19001v1 公告类型：新
摘要：我们解决了持续类别发现 (CCD) 的问题，该问题旨在自动在连续的未标记数据流中发现新类别，同时减轻灾难性遗忘的挑战——这是一个即使在传统的完全监督的持续学习中仍然存在的开放性问题。为了应对这一挑战，我们提出了 PromptCCD，这是一个简单而有效的框架，它利用高斯混合模型 (GMM) 作为 CCD 的提示方法。PromptCCD 的核心是高斯混合提示 (GMP) 模块，它充当动态池，随着时间的推移进行更新，以促进表示学习并防止在类别发现过程中遗忘。此外，GMP 支持即时估计类别编号，允许 PromptCCD 在没有事先知道类别编号的情况下发现未标记数据中的类别。我们将广义类别发现 (GCD) 的标准评估指标扩展到 CCD，并在各种公共数据集上对最先进的方法进行基准测试。PromptCCD 的表现明显优于现有方法，证明了其有效性。项目页面：https://visual-ai.github.io/promptccd 。]]></description>
      <guid>https://arxiv.org/abs/2407.19001</guid>
      <pubDate>Wed, 31 Jul 2024 03:14:01 GMT</pubDate>
    </item>
    <item>
      <title>稀疏细化以实现高效的高分辨率语义分割</title>
      <link>https://arxiv.org/abs/2407.19014</link>
      <description><![CDATA[arXiv:2407.19014v1 公告类型：新
摘要：语义分割为众多现实世界的应用提供支持，例如自动驾驶和增强/混合现实。这些应用程序通常在高分辨率图像（例如 8 兆像素）上运行以捕捉精细的细节。然而，这是以相当大的计算复杂性为代价的，阻碍了在延迟敏感场景中的部署。在本文中，我们介绍了 SparseRefine，这是一种通过稀疏高分辨率细化增强密集低分辨率预测的新方法。基于粗略的低分辨率输出，SparseRefine 首先使用熵选择器来识别具有高熵的稀疏像素集。然后，它使用稀疏特征提取器来有效地为感兴趣的像素生成细化。最后，它利用门控集成器将这些稀疏细化应用于初始粗略预测。 SparseRefine 可以无缝集成到任何现有的语义分割模型中，无论是基于 CNN 还是 ViT。SparseRefine 实现了显著的加速：在 Cityscapes 上应用于 HRNet-W48、SegFormer-B5、Mask2Former-T/L 和 SegNeXt-L 时，速度提高了 1.5 到 3.7 倍，而准确度几乎不会下降。我们的“密集+稀疏”范式为高效的高分辨率视觉计算铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2407.19014</guid>
      <pubDate>Wed, 31 Jul 2024 03:14:01 GMT</pubDate>
    </item>
    <item>
      <title>数字孪生工业 4.0 (I4) 系统的摄影测量</title>
      <link>https://arxiv.org/abs/2407.18951</link>
      <description><![CDATA[arXiv:2407.18951v1 公告类型：新
摘要：工业 4.0 的出现正在通过云计算、机器学习 (ML)、人工智能 (AI) 和通用网络连接的集成迅速改变制造业，从而优化性能并提高生产力。数字孪生 (DT) 就是这样一种转型技术，它利用软件系统复制物理过程行为，在数字环境中表示物理过程。本文旨在探索使用摄影测量法（即使用照片将物理对象重建为虚拟 3D 模型的过程）和 3D 扫描技术来创建“物理过程”的精确视觉表示，以便与基于 ML/AI 的行为模型进行交互。为了实现这一点，我们使用了一款随时可用的消费设备 iPhone 15 Pro，它具有立体视觉功能，可以捕捉工业 4.0 系统的深度。通过使用 3D 扫描工具处理这些图像，我们为 3D 建模和渲染软件创建了原始 3D 模型，以创建 DT 模型。本文通过测量地面实况（使用卷尺手动完成的测量）与使用此方法创建的最终 3D 模型之间的误差率来强调此方法的可靠性。地面实况测量值与其摄影测量对应值之间的总体平均误差为 4.97\%，总体标准偏差误差为 5.54\%。这项工作的结果表明，使用消费级设备的摄影测量可以成为一种高效且经济的创建智能制造 DT 的方法，同时该方法的灵活性允许随着时间的推移对模型进行迭代改进。]]></description>
      <guid>https://arxiv.org/abs/2407.18951</guid>
      <pubDate>Wed, 31 Jul 2024 03:14:00 GMT</pubDate>
    </item>
    <item>
      <title>真人脸视频动画平台</title>
      <link>https://arxiv.org/abs/2407.18955</link>
      <description><![CDATA[arXiv:2407.18955v1 公告类型：新
摘要：近年来，面部视频生成模型越来越受欢迎。然而，由于缺乏高质量的动漫风格人脸训练集，这些模型在处理夸张的动漫风格人脸时往往缺乏表现力。我们提出了一个面部动画平台，可以实时将真实人脸转换为卡通风格人脸，并支持多种模型。我们的平台基于 Gradio 框架构建，确保了出色的交互性和用户友好性。用户可以输入真实的面部视频或图像并选择他们想要的卡通风格。然后，系统将自动分析面部特征，执行必要的预处理，并调用适当的模型来生成富有表现力的动漫风格人脸。我们在系统中使用多种模型来处理 HDTF 数据集，从而创建动画面部视频数据集。]]></description>
      <guid>https://arxiv.org/abs/2407.18955</guid>
      <pubDate>Wed, 31 Jul 2024 03:14:00 GMT</pubDate>
    </item>
    <item>
      <title>LEMoN：使用多模态邻居检测标签错误</title>
      <link>https://arxiv.org/abs/2407.18941</link>
      <description><![CDATA[arXiv:2407.18941v1 公告类型：新
摘要：大型图像-字幕对存储库对于视觉语言模型的开发至关重要。然而，这些数据集通常是从网上抓取的嘈杂数据中提取的，包含许多错误标记的示例。为了提高下游模型的可靠性，识别和过滤带有错误字幕的图像非常重要。然而，除了基于图像-字幕嵌入相似性的过滤之外，之前还没有研究提出其他方法来过滤嘈杂的多模态数据，或者具体评估嘈杂的字幕数据对下游训练的影响。在这项工作中，我们提出了 LEMoN，一种自动识别多模态数据集中标签错误的方法。我们的方法利用对比预训练的多模态模型的潜在空间中图像-字幕对的多模态邻域。我们发现我们的方法在标签错误识别方面优于基线，并且使用我们的方法对过滤的数据集进行训练可以提高下游分类和字幕性能。]]></description>
      <guid>https://arxiv.org/abs/2407.18941</guid>
      <pubDate>Wed, 31 Jul 2024 03:13:59 GMT</pubDate>
    </item>
    <item>
      <title>WalkTheDog：通过相位流形实现跨形态运动对齐</title>
      <link>https://arxiv.org/abs/2407.18946</link>
      <description><![CDATA[arXiv:2407.18946v1 公告类型：新
摘要：我们提出了一种新方法来理解运动数据集的周期性结构和语义，独立于角色的形态和骨骼结构。与使用过于稀疏的高维潜在特征的现有方法不同，我们提出了一个由多条闭合曲线组成的相位流形，每条曲线对应一个潜在幅度。利用我们提出的矢量量化周期自动编码器，我们可以在没有任何监督的情况下为多个角色（例如人类和狗）学习共享相位流形。这是通过利用离散结构和浅层网络作为瓶颈来实现的，这样语义上相似的运动就会聚集到流形的同一曲线中，并且同一组件内的运动通过相位变量在时间上对齐。结合改进的运动匹配框架，我们展示了流形在多个应用中的定时和语义对齐能力，包括运动检索、传输和风格化。本文的代码和预训练模型可在 https://peizhuoli.github.io/walkthedog 上找到。]]></description>
      <guid>https://arxiv.org/abs/2407.18946</guid>
      <pubDate>Wed, 31 Jul 2024 03:13:59 GMT</pubDate>
    </item>
    <item>
      <title>预测《纽约客》周刊漫画的获胜标题</title>
      <link>https://arxiv.org/abs/2407.18949</link>
      <description><![CDATA[arXiv:2407.18949v1 公告类型：新
摘要：使用视觉变换器 (ViTs) 进行图像字幕制作代表了计算机视觉和自然语言处理的关键融合，具有增强用户体验、提高可访问性和提供视觉数据文本表示的潜力。本文探讨了图像字幕技术在《纽约客》漫画中的应用，旨在生成模仿《纽约客》漫画字幕大赛获奖作品的机智和幽默的字幕。这项任务需要复杂的视觉和语言处理，以及对文化细微差别和幽默的理解。我们提出了几个新的基准，用于使用视觉变换器编码器-解码器模型为《纽约客》漫画字幕大赛生成字幕。]]></description>
      <guid>https://arxiv.org/abs/2407.18949</guid>
      <pubDate>Wed, 31 Jul 2024 03:13:59 GMT</pubDate>
    </item>
    <item>
      <title>通过摄影测量驱动的数字孪生监测历史建筑随时间的变化</title>
      <link>https://arxiv.org/abs/2407.18925</link>
      <description><![CDATA[arXiv:2407.18925v1 公告类型：新
摘要：历史建筑对我们的社会很重要，但由于使用时间长和自然影响，它们可能容易出现结构性损坏。监测历史建筑的损坏情况对于利益相关者采取适当的干预措施至关重要。文献中现有的研究主要侧重于评估某一特定时刻的结构损坏，而不是评估损坏随时间的发展。为了解决这一差距，我们提出了一个新颖的五组分数字孪生框架来监测历史建筑随时间的变化。关岛索莱达堡的一个炮台试验台被选中来验证我们的框架。利用这个试验台，我们执行了数字孪生框架中的关键实施步骤。这项研究的结果证实，我们的数字孪生框架可以有效地监测随着时间的推移而发生的损坏，这是文化遗产保护界迫切需要的。]]></description>
      <guid>https://arxiv.org/abs/2407.18925</guid>
      <pubDate>Wed, 31 Jul 2024 03:13:58 GMT</pubDate>
    </item>
    </channel>
</rss>