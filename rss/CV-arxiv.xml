<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Thu, 09 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>NeRF 是镜像检测器：利用结构相似性进行具有 3D 表面基元的多视图镜像场景重建</title>
      <link>https://arxiv.org/abs/2501.04074</link>
      <description><![CDATA[arXiv:2501.04074v1 公告类型：新 
摘要：虽然神经辐射场 (NeRF) 导致了照片级真实感新视图合成的突破，但处理镜像表面仍然是一个特殊的挑战，因为它们会在场景表示中引入严重的不一致性。以前的尝试要么侧重于重建单个反射物体，要么依赖于强大的监督指导，即用户提供的镜子可见图像区域的额外注释，从而限制了实际可用性。相反，在本文中，我们提出了 NeRF-MD，这种方法表明 NeRF 可以被视为镜子检测器，并且能够重建包含镜像表面的场景的神经辐射场，而无需事先注释。为此，我们首先通过使用深度重投影损失训练标准 NeRF 来计算场景几何的初始估计。我们的关键见解在于，与镜面相对应的场景部分仍将表现出明显的光度不一致性，而其余部分已经以合理的方式重建。这使我们能够在训练的初始阶段通过将几何图元拟合到此类不一致区域来检测镜面。然后，利用此信息，我们在第二个训练阶段联合优化辐射场和镜子几何形状以改善其质量。我们展示了我们的方法能够忠实地检测场景中的镜子以及重建单个一致的场景表示的能力，并展示了它与基线和镜子感知方法相比的潜力。]]></description>
      <guid>https://arxiv.org/abs/2501.04074</guid>
      <pubDate>Thu, 09 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于图的多模态和多视图对齐的关键步骤识别</title>
      <link>https://arxiv.org/abs/2501.04121</link>
      <description><![CDATA[arXiv:2501.04121v1 公告类型：新
摘要：以自我为中心的视频从佩戴者的视角捕捉场景，导致背景动态、频繁运动和遮挡，对准确的关键步骤识别构成挑战。我们提出了一种灵活的图形学习框架，用于细粒度的关键步骤识别，该框架能够有效地利用以自我为中心的视频中的长期依赖关系，并在训练期间利用以自我为中心的和以外部为中心的视频之间的对齐来改进以自我为中心的视频的推理。我们的方法包括构建一个图形，其中以自我为中心的视频的每个视频片段对应于一个节点。在训练期间，我们将每个以外部为中心的视频的每个片段（如果有）视为附加节点。我们研究了几种定义这些节点之间连接的策略，并将关键步骤识别作为构建的图形上的节点分类任务。我们在 Ego-Exo4D 数据集上进行了广泛的实验，并表明我们提出的基于图形的灵活框架在准确度上明显优于现有方法 12 分以上。此外，构建的图稀疏且计算效率高。我们还介绍了一项研究，研究如何在异构图上利用多个多模态特征，包括叙述、深度和对象类别标签，并讨论它们对关键步骤识别性能的相应贡献。]]></description>
      <guid>https://arxiv.org/abs/2501.04121</guid>
      <pubDate>Thu, 09 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Chirpy3D：用于创意 3D 鸟类生成的连续部分潜在特征</title>
      <link>https://arxiv.org/abs/2501.04144</link>
      <description><![CDATA[arXiv:2501.04144v1 公告类型：新
摘要：在本文中，我们将细粒度 3D 生成的边界推向了真正具有创造性的领域。当前的方法要么缺乏复杂的细节，要么只是模仿现有对象——我们同时实现了这两种方法。通过多视图扩散将 2D 细粒度理解提升到 3D，并将部分潜在建模为连续分布，我们能够通过插值和采样生成全新但合理的部分。自监督特征一致性损失进一步确保了这些看不见的部分的稳定生成。结果是第一个能够创建具有超越现有示例的物种特定细节的新型 3D 对象的系统。虽然我们在鸟类身上展示了我们的方法，但底层框架超越了可以鸣叫的东西！代码将在 https://github.com/kamwoh/chirpy3d 发布。]]></description>
      <guid>https://arxiv.org/abs/2501.04144</guid>
      <pubDate>Thu, 09 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>对大型和小型 MLLM 进行基准测试</title>
      <link>https://arxiv.org/abs/2501.04150</link>
      <description><![CDATA[arXiv:2501.04150v1 公告类型：新 
摘要：大型多模态语言模型 (MLLM)（例如 GPT-4V 和 GPT-4o）在理解和生成多模态内容方面取得了显着进步，展示了跨各种任务的卓越质量和能力。然而，它们的部署面临着重大挑战，包括推理速度慢、计算成本高以及不适用于设备应用。相比之下，以 LLava 系列模型和 Phi-3-Vision 为代表的小型 MLLM 的出现提供了有希望的替代方案，具有更快的推理速度、更低的部署成本以及处理特定领域场景的能力。尽管它们的存在越来越多，但大型和小型 MLLM 之间的能力界限仍未得到充分探索。在这项工作中，我们进行了系统而全面的评估，以对小型和大型 MLLM 进行基准测试，涵盖了对象识别、时间推理和多模态理解等一般能力，以及工业和汽车等领域的实际应用。我们的评估表明，小型 MLLM 在特定场景下可以实现与大型模型相当的性能，但在需要更深入的推理或细致理解的复杂任务中会明显落后。此外，我们还确定了小型和大型 MLLM 中常见的故障案例，突出了即使是最先进的模型也难以应对的领域。我们希望我们的研究结果能够指导研究界突破 MLLM 的质量界限，提高其在不同应用中的可用性和有效性。]]></description>
      <guid>https://arxiv.org/abs/2501.04150</guid>
      <pubDate>Thu, 09 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MM-GEN：通过有针对性的多模式数据管理提高任务绩效</title>
      <link>https://arxiv.org/abs/2501.04155</link>
      <description><![CDATA[arXiv:2501.04155v1 公告类型：新
摘要：视觉语言模型 (VLM) 非常有效，但在专门的任务上往往表现不佳；例如，由于缺乏特定于任务的训练数据，Llava-1.5 在图表和图形理解方面表现不佳。现有的训练数据来自通用数据集，无法捕捉这些任务所需的细微细节。我们引入了 MM-Gen，这是一种可扩展的方法，它利用更强大的模型为候选图像生成特定于任务的高质量合成文本。MM-Gen 采用三阶段目标流程：将数据划分为子组、根据任务描述生成目标文本以及过滤掉冗余和异常数据。使用 MM-Gen 生成的数据对 VLM 进行微调可显着提高性能，包括 Llava-1.5 (7B) 的空间推理提高了 29%，图形理解提高了 15%。与人工整理的字幕数据相比，MM-Gen 使原始模型的性能提高了 1.6 倍，证明了其在增强特定任务的 VLM 性能以及弥合通用数据集与专业要求之间的差距方面的有效性。代码可在 https://github.com/sjoshi804/MM-Gen 上找到。]]></description>
      <guid>https://arxiv.org/abs/2501.04155</guid>
      <pubDate>Thu, 09 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MedicalNarratives：将医学视野和语言与本地化叙事相结合</title>
      <link>https://arxiv.org/abs/2501.04184</link>
      <description><![CDATA[arXiv:2501.04184v1 公告类型：新
摘要：我们提出了 MedicalNarratives，这是一个从医学教学视频中精选出来的数据集，其性质类似于 Think-Aloud 研究中收集的数据，并受到 Localized Narratives 的启发，它通过整理讲师的语音和鼠标光标移动来收集扎实的图像文本数据。MedicalNarratives 可以对语义和密集目标进行预训练，从而减轻了由于缺乏合理大小的数据集而对医学语义和密集任务进行不同训练的需要。我们的数据集包含来自视频和文章的 470 万个图像文本对，其中 100 万个样本包含以轨迹和边界框形式出现的密集注释。为了评估 MedicalNarratives 的实用性，我们使用涵盖 12 个医学领域的数据集基于 CLIP 架构训练 GenMedClip，并证明它在新构建的医学成像基准上优于以前最先进的模型，该基准全面评估了所有模态的性能。数据、演示、代码和模型可在 https://medical-narratives.github.io 上获取]]></description>
      <guid>https://arxiv.org/abs/2501.04184</guid>
      <pubDate>Thu, 09 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于自我知识提炼的生成数据集提炼</title>
      <link>https://arxiv.org/abs/2501.04202</link>
      <description><![CDATA[arXiv:2501.04202v1 公告类型：新
摘要：数据集蒸馏是一种有效的技术，通过将大数据集压缩为更小、更高效的版本，在保持性能的同时降低模型训练的成本和复杂性。在本文中，我们提出了一种新颖的生成数据集蒸馏方法，可以提高对齐预测逻辑的准确性。我们的方法集成了自我知识蒸馏，以实现合成数据和原始数据之间更精确的分布匹配，从而捕获数据中的整体结构和关系。为了进一步提高对齐的准确性，我们在执行分布匹配之前对逻辑引入了一个标准化步骤，确保逻辑范围内的一致性。通过大量实验，我们证明我们的方法优于现有的最先进方法，从而实现卓越的蒸馏性能。]]></description>
      <guid>https://arxiv.org/abs/2501.04202</guid>
      <pubDate>Thu, 09 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LipGen：用于增强视觉语音识别的 Viseme 引导唇部视频生成</title>
      <link>https://arxiv.org/abs/2501.04204</link>
      <description><![CDATA[arXiv:2501.04204v1 公告类型：新
摘要：视觉语音识别（VSR），俗称唇读，由于其广泛的实际应用而引起了广泛关注。深度学习技术的出现和硬件功能的进步显著提高了唇读模型的性能。尽管取得了这些进步，但现有数据集主要以稳定的视频记录为特色，唇部运动的变化有限。这种限制导致模型对现实场景中遇到的变化高度敏感。为了解决这个问题，我们提出了一个新框架 LipGen，旨在通过利用语音驱动的合成视觉数据来提高模型的鲁棒性，从而减轻当前数据集的限制。此外，我们引入了一个辅助任务，将视位分类与注意力机制结合起来。这种方法有助于有效整合时间信息，将模型的注意力引导到语音的相关片段上，从而增强判别能力。与目前最先进的野生唇读 (LRW) 数据集相比，我们的方法表现出更优异的性能，并且在具有挑战性的条件下表现出更明显的优势。]]></description>
      <guid>https://arxiv.org/abs/2501.04204</guid>
      <pubDate>Thu, 09 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于全局和像素优化的面向识别的低光图像增强</title>
      <link>https://arxiv.org/abs/2501.04210</link>
      <description><![CDATA[arXiv:2501.04210v1 公告类型：新
摘要：在本文中，我们提出了一种新颖的低光图像增强方法，旨在提高识别模型的性能。尽管深度学习最近取得了进展，但在低光条件下识别图像仍然是一个挑战。虽然现有的低光图像增强方法已经开发出来以提高人类视觉的图像可见性，但它们并没有特别关注提高识别模型的性能。我们提出的低光图像增强方法由两个关键模块组成：全局增强模块，用于调整输入图像的整体亮度和色彩平衡，以及像素级调整模块，用于在像素级细化图像特征。这些模块经过训练可以增强输入图像，从而有效地提高下游识别模型的性能。值得注意的是，所提出的方法可以用作前端过滤器来提高低光识别性能，而无需重新训练下游识别模型。实验结果表明，我们的方法提高了低光条件下预训练识别模型的性能及其有效性。]]></description>
      <guid>https://arxiv.org/abs/2501.04210</guid>
      <pubDate>Thu, 09 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>UPAQ：自动驾驶汽车中实时且节能的 3D 物体检测框架</title>
      <link>https://arxiv.org/abs/2501.04213</link>
      <description><![CDATA[arXiv:2501.04213v1 公告类型：新 
摘要：为了增强自动驾驶汽车 (AV) 的感知能力，最近的努力集中在 3D 物体检测器上，它比传统的 2D 物体检测器提供更全面的预测，但代价是增加了内存占用和计算资源使用量。我们提出了一个名为 UPAQ 的新框架，它利用半结构化模式修剪和量化来提高资源受限的嵌入式 AV 平台上 LiDAR 点云和基于摄像头的 3D 物体检测器的效率。在 Jetson Orin Nano 嵌入式平台上的实验结果表明，与 Pointpillar 和 SMOKE 模型上最先进的模型压缩框架相比，UPAQ 分别实现了高达 5.62 倍和 5.13 倍的模型压缩率、高达 1.97 倍和 1.86 倍的推理速度提升以及高达 2.07 倍和 1.87 倍的能耗降低。]]></description>
      <guid>https://arxiv.org/abs/2501.04213</guid>
      <pubDate>Thu, 09 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>考虑胸部 CT 图像中的医学领域知识的持续自监督学习</title>
      <link>https://arxiv.org/abs/2501.04217</link>
      <description><![CDATA[arXiv:2501.04217v1 公告类型：新
摘要：我们提出了一种新颖的持续自监督学习方法 (CSSL)，该方法考虑了胸部 CT 图像中的医学领域知识。我们的方法通过有效地捕捉不同阶段先前学习的知识与新信息之间的关系来解决顺序学习的挑战。通过将增强的 DER 合并到 CSSL 中并在 DER 的排练缓冲区中保持多样性和代表性，降低了预训练期间数据干扰的风险，使模型能够学习更丰富、更强大的特征表示。此外，我们结合了混合策略和特征蒸馏，以进一步增强模型学习有意义表示的能力。我们使用在两种不同成像条件下获得的胸部 CT 图像验证了我们的方法，与最先进的方法相比，其性能更优越。]]></description>
      <guid>https://arxiv.org/abs/2501.04217</guid>
      <pubDate>Thu, 09 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有稳健样本选择和边际引导模块的开放集标签噪声学习</title>
      <link>https://arxiv.org/abs/2501.04269</link>
      <description><![CDATA[arXiv:2501.04269v1 公告类型：新 
摘要：近年来，深度神经网络（DNN）在计算机视觉领域的显著成功很大程度上归功于大规模、高质量的标记数据集。直接在带有标签噪声的真实数据集上进行训练可能会导致过度拟合。传统方法仅限于处理闭集标签噪声，其中嘈杂的训练数据在已知标签空间内具有真实的类标签。然而，有些真实数据集包含开集标签噪声，这意味着一些样本属于已知标签空间之外的未知类。为了解决开集标签噪声问题，我们引入了一种基于稳健样本选择和边际引导模块（RSS-MGM）的方法。首先，与之前的干净样本选择方法仅选择有限数量的干净样本不同，稳健样本选择模块结合了小损失选择或高置信度样本选择以获得更多的干净样本。其次，为了有效区分开集标签噪声和闭集标签噪声，设计了边界函数对开集数据和闭集数据进行过滤。第三，针对不同类型的样本选择不同的处理方法，以充分利用数据的先验信息并优化整个模型。此外，对来自基准数据集和真实数据集（如 CIFAR-100N-C、CIFAR80N-O、WebFG-469 和 Food101N）的带噪声标记数据进行的大量实验结果表明，我们的方法优于许多最先进的标签噪声学习方法。特别是，它能够更准确地划分开集标签噪声样本和闭集样本。]]></description>
      <guid>https://arxiv.org/abs/2501.04269</guid>
      <pubDate>Thu, 09 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>增强多云图像场景中的场景分类：一种使用光学云层覆盖和 SAR 遥感图像的具有信息调节机制的协同传输方法</title>
      <link>https://arxiv.org/abs/2501.04283</link>
      <description><![CDATA[arXiv:2501.04283v1 公告类型：新 
摘要：在遥感场景分类中，利用训练有素的光学模型的迁移方法是克服标签稀缺的有效方法。然而，云污染会导致光学信息丢失并对特征分布产生重大影响，对迁移目标模型的可靠性和稳定性提出挑战。常见的解决方案包括对光学数据进行去云处理或直接在目标域中使用合成孔径雷达（SAR）数据。然而，去云需要大量辅助数据的支持和预训练，而直接使用SAR会忽略光学数据的未受遮挡部分。本研究提出了一种协同结合多模态数据的场景分类迁移方法，旨在以低成本将在无云光学数据上训练的源域模型迁移到包含多云光学和SAR数据的目标域。具体而言，该框架包含两部分：（1）基于知识提炼的协同迁移策略，实现跨异构数据的有效先验知识迁移； (2) 针对迁移过程中模态不平衡问题，提出了信息调节机制（IRM），利用辅助模型衡量各模态的贡献差异，在样本层面自动平衡目标模型学习过程中模态的信息利用率。在模拟和真实云数据集上进行了迁移实验，证明了所提方法在云覆盖场景下优于其他解决方案。我们还验证了 IRM 的重要性和局限性，并进一步讨论和可视化了模型迁移过程中的模态不平衡问题。代码位于 https://github.com/wangyuze-csu/ESCCS]]></description>
      <guid>https://arxiv.org/abs/2501.04283</guid>
      <pubDate>Thu, 09 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ContextMRI：通过元数据调节增强压缩感知 MRI</title>
      <link>https://arxiv.org/abs/2501.04284</link>
      <description><![CDATA[arXiv:2501.04284v1 公告类型：新
摘要：压缩感知 MRI 旨在通过采样更少的 k 空间测量值然后通过算法重建缺失数据来加速 MRI 采集过程。这些方法的成功通常依赖于强大的先验或学习到的统计模型。虽然最近基于扩散模型的先验显示出巨大的潜力，但以前的方法通常会忽略临床可用的元数据（例如患者人口统计、成像参数、切片特定信息）。实际上，元数据包含有关解剖结构和采集协议的有意义的线索，这表明它可以进一步限制重建问题。在这项工作中，我们提出了 ContextMRI，这是一种用于 MRI 的文本条件扩散模型，它将细粒度元数据集成到重建过程中。我们直接在经过最少处理的复值 MRI 图像上训练像素空间扩散模型。在推理过程中，元数据被转换为结构化文本提示并通过 CLIP 文本嵌入馈送到模型。通过对元数据进行先验调节，我们可以实现更准确的重建，并在多个数据集、加速因子和欠采样模式下显示出一致的增益。我们的实验表明，提高元数据的保真度（从切片位置和对比度到患者年龄、性别和病理）可以系统地提高重建性能。这项工作突出了利用临床背景解决逆问题的尚未开发的潜力，并为元数据驱动的 MRI 重建开辟了新方向。]]></description>
      <guid>https://arxiv.org/abs/2501.04284</guid>
      <pubDate>Thu, 09 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>TADFormer：用于高效多任务学习的任务自适应动态变换器</title>
      <link>https://arxiv.org/abs/2501.04293</link>
      <description><![CDATA[arXiv:2501.04293v1 公告类型：新
摘要：迁移学习范式推动了各种视觉任务的重大进步。然而，随着最先进模型的不断发展，经典的完全微调往往在计算上变得不切实际，特别是在多任务学习 (MTL) 设置中，其中训练复杂性与任务数量成正比增加。因此，最近的研究探索了 MTL 架构的参数高效微调 (PEFT)。尽管取得了一些进展，但这些方法在捕获对 MTL 至关重要的细粒度、特定于任务的特征方面仍然存在局限性。在本文中，我们介绍了任务自适应动态转换器，称为 TADFormer，这是一种新颖的 PEFT 框架，通过动态考虑特定于任务的输入上下文以细粒度的方式执行任务感知特征自适应。TADFormer 提出了参数高效的任务自适应提示和动态任务过滤器 (DTF) 来捕获以输入上下文为条件的任务信息。在 PASCAL-Context 基准上的实验表明，与 MTL 模型的完全微调相比，所提出的方法在密集场景理解任务中实现了更高的准确率，同时将可训练参数的数量减少了高达 8.4 倍。与最近的 PEFT 方法相比，TADFormer 还表现出卓越的参数效率和准确性。]]></description>
      <guid>https://arxiv.org/abs/2501.04293</guid>
      <pubDate>Thu, 09 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>