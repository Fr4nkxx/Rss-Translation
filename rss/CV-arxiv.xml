<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Wed, 15 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>BlobGEN-Vid：使用 Blob 视频表示法进行文本到视频的组合生成</title>
      <link>https://arxiv.org/abs/2501.07647</link>
      <description><![CDATA[arXiv:2501.07647v1 公告类型：新
摘要：现有的视频生成模型难以遵循复杂的文本提示并合成多个对象，因此需要额外的基础输入来提高可控性。在这项工作中，我们建议将视频分解为视觉基元 - blob 视频表示，这是可控视频生成的通用表示。基于 blob 条件，我们开发了一个基于 blob 的视频扩散模型，名为 BlobGEN-Vid，允许用户控制对象运动和细粒度对象外观。特别是，我们引入了一个蒙版 3D 注意模块，可有效提高跨帧的区域一致性。此外，我们引入了一个可学习的模块来插入文本嵌入，以便用户可以控制特定帧中的语义并获得平滑的对象过渡。我们表明我们的框架与模型无关，并基于 U-Net 和基于 DiT 的视频扩散模型构建 BlobGEN-Vid。大量实验结果表明，BlobGEN-Vid 在多个基准上实现了卓越的零样本视频生成能力和一流的布局可控性。当与 LLM 结合进行布局规划时，我们的框架在合成准确性方面甚至优于专有的文本转视频生成器。]]></description>
      <guid>https://arxiv.org/abs/2501.07647</guid>
      <pubDate>Wed, 15 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>C2PD：用于引导深度超分辨率的连续性约束像素变形</title>
      <link>https://arxiv.org/abs/2501.07688</link>
      <description><![CDATA[arXiv:2501.07688v1 公告类型：新
摘要：引导深度超分辨率（GDSR）在广泛领域表现出色，并提出了许多方法。然而，现有方法通常将深度图视为图像，其中阴影值是离散计算的，这使得它们难以有效恢复深度图固有的连续性。在本文中，我们提出了一种新方法，该方法最大限度地利用深度的空间特征，结合人类对现实世界物质的抽象感知，将 GDSR 问题转化为具有理想塑性的毛坯的变形，可以像连续物体一样受力变形。具体来说，我们首先设计了一个跨模态操作，即连续性约束的非对称像素操作（CAPO），它可以模拟通过外力使等体积柔性物体变形的过程。利用 CAPO 作为基本组件，我们开发了像素级交叉梯度变形 (PCGD)，它能够模拟理想塑料物体上的操作（不受体积限制）。值得注意的是，我们的方法在四个广泛采用的 GDSR 基准中表现出了最先进的性能，在大规模任务和通用性方面具有显著优势。]]></description>
      <guid>https://arxiv.org/abs/2501.07688</guid>
      <pubDate>Wed, 15 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于随机权重社交互动学习的行人轨迹预测</title>
      <link>https://arxiv.org/abs/2501.07711</link>
      <description><![CDATA[arXiv:2501.07711v1 公告类型：新
摘要：行人轨迹预测是自动驾驶汽车向完全人工智能发展的关键技术。近年来，人们越来越关注行人的轨迹以模拟他们的社交互动，对更准确的轨迹预测产生了浓厚的兴趣。然而，现有的行人社交互动建模方法依赖于预定义的规则，难以捕捉非显式的社交互动。在这项工作中，我们提出了一个名为 DTGAN 的新框架，它将生成对抗网络 (GAN) 的应用扩展到图序列数据，主要目标是自动捕捉隐式的社交互动并实现对行人轨迹的精确预测。DTGAN 创新地在每个图中加入了随机权重，从而无需预定义的交互规则。我们通过在对抗训练期间探索不同的任务损失函数来进一步提高 DTGAN 的性能，在 ADE 和 FDE 指标上分别实现了 16.7% 和 39.3% 的提升。我们的框架的有效性和准确性在两个公共数据集上得到了验证。实验结果表明，我们提出的 DTGAN 实现了卓越的性能，并且能够很好地理解行人的意图。]]></description>
      <guid>https://arxiv.org/abs/2501.07711</guid>
      <pubDate>Wed, 15 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用深度集成模型在人机交互中对分布内和分布外数据进行人手分割测试</title>
      <link>https://arxiv.org/abs/2501.07713</link>
      <description><![CDATA[arXiv:2501.07713v1 公告类型：新
摘要：可靠地检测和分割人手对于增强安全性和促进人机协作中的高级交互至关重要。当前的研究主要评估分布内 (ID) 数据下的手部分割，这反映了深度学习 (DL) 模型的训练数据。然而，这种方法无法解决现实世界中人机交互中经常出现的分布外 (OOD) 场景。在本研究中，我们提出了一种新方法，通过评估 ID 数据和更具挑战性的 OOD 场景下预训练的 DL 模型的性能。为了模拟现实的工业场景，我们设计了一个多样化的数据集，其中包含简单而杂乱的背景、工业工具、不同数量的手（0 到 4）以及戴手套和不戴手套的手。对于 OOD 场景，我们结合了独特和罕见的条件，例如手指交叉手势和快速移动的手的运动模糊，解决了认知和随机的不确定性。为了确保多个视角 (PoV)，我们利用安装在操作员头上的自我中心摄像机和静态摄像机来捕捉人机交互的 RGB 图像。这种方法使我们能够考虑多个摄像机视角，同时评估在现有自我中心数据集和静态摄像机数据集上训练的模型的性能。对于分割，我们使用由 UNet 和 RefineNet 组成的深度集成模型作为基础学习者。使用分割指标和通过预测熵量化不确定性来进行性能评估。结果表明，在工业数据集上训练的模型优于在非工业数据集上训练的模型，这凸显了针对特定情境进行训练的重要性。尽管所有模型都难以应对 OOD 场景，但那些在工业数据集上训练的模型表现出明显更好的泛化能力。]]></description>
      <guid>https://arxiv.org/abs/2501.07713</guid>
      <pubDate>Wed, 15 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用紧凑的文本感知一维标记使文本到图像的蒙版生成模型民主化</title>
      <link>https://arxiv.org/abs/2501.07730</link>
      <description><![CDATA[arXiv:2501.07730v1 公告类型：新
摘要：图像标记器构成了现代文本到图像生成模型的基础，但训练起来却非常困难。此外，大多数现有的文本到图像模型都依赖于大规模、高质量的私有数据集，这使得它们难以复制。在这项工作中，我们引入了基于文本感知 Transformer 的一维标记器 (TA-TiTok)，这是一种高效且功能强大的图像标记器，可以利用离散或连续的一维标记。TA-TiTok 在标记器解码阶段（即去标记化）独特地集成了文本信息，从而加速了收敛并提高了性能。TA-TiTok 还受益于简化但有效的单阶段训练过程，从而无需以前的一维标记器中使用的复杂两阶段蒸馏。这种设计允许无缝扩展到大型数据集。在此基础上，我们推出了一系列文本转图像蒙版生成模型 (MaskGen)，这些模型专门在开放数据上进行训练，同时实现了与在私有数据上训练的模型相当的性能。我们的目标是发布高效、强大的 TA-TiTok 分词器和开放数据、开放权重的 MaskGen 模型，以促进更广泛的访问并使文本转图像蒙版生成模型领域民主化。]]></description>
      <guid>https://arxiv.org/abs/2501.07730</guid>
      <pubDate>Wed, 15 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>修复单目深度的比例和偏移以进行相机姿态估计</title>
      <link>https://arxiv.org/abs/2501.07742</link>
      <description><![CDATA[arXiv:2501.07742v1 公告类型：新
摘要：单目深度预测的最新进展显著提高了深度预测精度。反过来，这使得各种应用程序都可以使用这种深度预测。在本文中，我们提出了一种新颖的框架，用于从与单目深度相关的点对应关系中估计两个相机之间的相对姿势。由于深度预测通常定义为未知的比例和偏移参数，我们的求解器将比例和偏移参数与相机姿势一起估计。我们为三种情况推导出有效的求解器：（1）两个校准的相机，（2）两个未校准的相机，焦距未知但共享，以及（3）两个未校准的相机，焦距未知且不同。对合成数据和真实数据的实验，包括使用 11 个不同的深度预测器估计的深度图的实验，证明了我们的求解器的实际可行性。与之前的工作相比，我们的求解器在两个大规模的真实世界数据集上取得了最先进的结果。源代码可以在 https://github.com/yaqding/pose_monodepth 上找到。]]></description>
      <guid>https://arxiv.org/abs/2501.07742</guid>
      <pubDate>Wed, 15 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过使用更少标签的半监督学习来增强巩膜分割</title>
      <link>https://arxiv.org/abs/2501.07750</link>
      <description><![CDATA[arXiv:2501.07750v1 公告类型：新
摘要：巩膜分割对于开发自动眼部医学计算机辅助诊断系统以及个人识别和验证至关重要，因为巩膜包含独特的个人特征。与依赖手工制作特征的传统方法相比，基于深度学习的巩膜分割取得了显着的成功，主要是因为它可以自主提取关键的输出相关特征，而无需考虑潜在的物理限制。然而，使用这些方法实现准确的巩膜分割具有挑战性，因为高质量、完全标记的数据集稀缺，而这些数据集依赖于昂贵、劳动密集型的医疗获取和专业知识。为了应对这一挑战，本文介绍了一种新颖的巩膜分割框架，该框架在有限的标记样本中表现出色。具体而言，我们采用半监督学习方法，将特定领域的改进和基于图像的空间变换相结合，以增强分割性能。此外，我们还开发了一个真实世界的眼部诊断数据集来丰富评估过程。在我们的数据集和另外两个公共数据集上进行的大量实验证明了我们提出的方法的有效性和优越性，特别是在标记样本明显较少的情况下。]]></description>
      <guid>https://arxiv.org/abs/2501.07750</guid>
      <pubDate>Wed, 15 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PSReg：用于点云配准的先验指导稀疏混合专家</title>
      <link>https://arxiv.org/abs/2501.07762</link>
      <description><![CDATA[arXiv:2501.07762v1 公告类型：新
摘要：判别特征对于点云配准至关重要。最近的方法通过区分非重叠和重叠区域点来提高特征判别性。然而，它们在区分重叠区域中的模糊结构方面仍然面临挑战。因此，它们提取的模糊特征导致重叠区域中出现大量异常匹配。为了解决这个问题，我们提出了一种基于先验引导的 SMoE 的配准方法，通过将潜在对应关系分派给相同的专家来提高特征的独特性。具体来说，我们提出了一个先验引导的 SMoE 模块，通过融合先验重叠和潜在对应嵌入进行路由，将 token 分配给最合适的专家进行处理。此外，我们提出了一个由 Transformer 层和先验引导的 SMoE 模块的特定组合组成的配准框架。所提出的方法不仅注重定位点云重叠区域的重要性，还致力于在重叠区域找到更准确的对应关系。我们进行了大量的实验，证明了我们方法的有效性，在 3DMatch/3DLoMatch 基准上实现了最先进的配准召回率 (95.7\%/79.3\%)。此外，我们还在 ModelNet40 上测试了性能，并展示了出色的性能。]]></description>
      <guid>https://arxiv.org/abs/2501.07762</guid>
      <pubDate>Wed, 15 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于视觉感知和多模态理解的参数倒置图像金字塔网络</title>
      <link>https://arxiv.org/abs/2501.07783</link>
      <description><![CDATA[arXiv:2501.07783v1 公告类型：新
摘要：图像金字塔被广泛用于表现最佳的方法中，以获得多尺度特征，从而实现精确的视觉感知和理解。然而，当前的图像金字塔使用相同的大规模模型来处理多种分辨率的图像，导致计算成本显著增加。为了应对这一挑战，我们提出了一种新的网络架构，称为参数倒置图像金字塔网络 (PIIP)。具体来说，PIIP 使用预训练模型 (ViTs 或 CNNs) 作为分支来处理多尺度图像，其中较高分辨率的图像由较小的网络分支处理，以平衡计算成本和性能。为了整合来自不同空间尺度的信息，我们进一步提出了一种新颖的跨分支特征交互机制。为了验证 PIIP，我们将其应用于各种感知模型和一个代表性的多模态大型语言模型 LLaVA，并对各种任务进行了广泛的实验，例如对象检测、分割、图像分类和多模态理解。与单分支和现有的多分辨率方法相比，PIIP 实现了更优异的性能，并且计算成本更低。当应用于大规模视觉基础模型 InternViT-6B 时，PIIP 可以在仅使用原始计算量的 40%-60% 的情况下，将其检测和分割性能提高 1%-2%，最终在 MS COCO 上实现 60.0 框 AP，在 ADE20K 上实现 59.7 mIoU。对于多模态理解，我们的 PIIP-LLaVA 在仅使用 2.8M 训练数据的情况下，在 TextVQA 上实现了 73.0% 的准确率，在 MMBench 上实现了 74.5% 的准确率。我们的代码发布在 https://github.com/OpenGVLab/PIIP。]]></description>
      <guid>https://arxiv.org/abs/2501.07783</guid>
      <pubDate>Wed, 15 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>BioPose：通过单目视频进行生物力学精确的 3D 姿势估计</title>
      <link>https://arxiv.org/abs/2501.07800</link>
      <description><![CDATA[arXiv:2501.07800v1 公告类型：新
摘要：单摄像头图像和视频的 3D 人体姿势估计的最新进展依赖于参数模型，例如 SMPL。然而，这些模型过于简化了解剖结构，限制了它们捕捉真实关节位置和运动的准确性，从而降低了它们在生物力学、医疗保健和机器人技术中的适用性。另一方面，生物力学上准确的姿势估计通常需要昂贵的基于标记的运动捕捉系统和专门实验室中的优化技术。为了弥补这一差距，我们提出了 BioPose，这是一种基于学习的新型框架，可直接从单目视频预测生物力学上准确的 3D 人体姿势。BioPose 包括三个关键组件：多查询人体网格恢复模型 (MQ-HMR)、神经逆运动学 (NeurIK) 模型和 2D 信息姿势细化技术。 MQ-HMR 利用多查询可变形变换器提取多尺度细粒度图像特征，实现精确的人体网格恢复。NeurIK 将网格顶点视为虚拟标记，应用时空网络在解剖约束下回归生物力学准确的 3D 姿势。为了进一步改善 3D 姿势估计，2D 信息细化步骤通过将 3D 结构与 2D 姿势观察对齐来优化推理过程中的查询标记。在基准数据集上进行的实验表明，BioPose 的表现明显优于最先进的方法。项目网站：\url{https://m-usamasaleem.github.io/publication/BioPose/BioPose.html}。]]></description>
      <guid>https://arxiv.org/abs/2501.07800</guid>
      <pubDate>Wed, 15 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>平衡发散以实现知识蒸馏</title>
      <link>https://arxiv.org/abs/2501.07804</link>
      <description><![CDATA[arXiv:2501.07804v1 公告类型：新
摘要：知识蒸馏已广泛应用于计算机视觉任务处理，因为它可以通过利用从繁琐的教师网络传输的知识来有效提高轻量级学生网络的性能。大多数现有的知识蒸馏方法利用 Kullback-Leibler 散度来模拟教师网络和学生网络之间的 logit 输出概率。尽管如此，这些方法可能会忽略教师“暗知识”的负面部分，因为散度计算可能会忽略教师 logit 输出中的微小概率的影响。这种缺陷可能导致蒸馏过程中 logit 模仿的性能不佳，并导致学生网络获取的信息不平衡。在本文中，我们研究了这种不平衡的影响，并提出了一种名为平衡散度蒸馏的新方法。通过引入使用逆 Kullback-Leibler 散度的补偿操作，我们的方法可以改进对来自老师的负数中极小值的建模，并保留对正数的学习能力。此外，我们测试了不同温度系数调整的影响，这些调整可以进一步平衡知识迁移。我们在几个计算机视觉任务上评估了所提出的方法，包括图像分类和语义分割。评估结果表明，我们的方法在 CIFAR-100 和 ImageNet 数据集上对轻量级学生的准确率提高了 1%~3%，在 Cityscapes 数据集上对 PSP-ResNet18 的 mIoU 提高了 4.55%。实验表明，我们的方法是一种简单而高效的解决方案，可以顺利应用于不同的知识提炼方法。]]></description>
      <guid>https://arxiv.org/abs/2501.07804</guid>
      <pubDate>Wed, 15 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>学习无监督视频对象分割的运动和时间线索</title>
      <link>https://arxiv.org/abs/2501.07806</link>
      <description><![CDATA[arXiv:2501.07806v1 公告类型：新
摘要：在本文中，我们通过提出一种称为 MTNet 的有效算法来解决无监督视频对象分割 (UVOS) 中的挑战，该算法同时利用运动和时间线索。与以前仅关注将外观与运动相结合或对时间关系进行建模的方法不同，我们的方法将这两个方面结合起来，将它们集成在一个统一的框架内。MTNet 的设计方式是在编码器内的特征提取过程中有效地合并外观和运动特征，从而促进更互补的表示。为了捕捉视频中嵌入的复杂的远程上下文动态和信息，引入了一个时间变换器模块，促进了整个视频片段中有效的帧间交互。此外，我们在所有特征级别上采用级联解码器来最佳地利用派生的特征，旨在生成越来越精确的分割蒙版。因此，MTNet 提供了一个强大而紧凑的框架，可以探索时间和跨模态知识，从而有效地在各种具有挑战性的场景中准确定位和跟踪主要对象。在各种基准上进行的大量实验最终表明，我们的方法不仅在无监督视频对象分割中达到了最先进的性能，而且在视频显着对象检测中也提供了具有竞争力的结果。这些发现突出了该方法的强大多功能性及其适应一系列分割任务的熟练程度。源代码可在 https://github.com/hy0523/MTNet 上找到。]]></description>
      <guid>https://arxiv.org/abs/2501.07806</guid>
      <pubDate>Wed, 15 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AVS-Mamba：探索用于音频视频分割的时间和多模式 Mamba</title>
      <link>https://arxiv.org/abs/2501.07810</link>
      <description><![CDATA[arXiv:2501.07810v1 公告类型：新
摘要：视听分割 (AVS) 的本质在于定位和描绘视频流中的发声对象。虽然基于 Transformer 的方法已显示出良好的前景，但由于二次计算成本，它们在处理长距离依赖关系方面遇到了困难，在复杂场景中成为瓶颈。为了克服这一限制并促进具有线性复杂性的复杂多模态理解，我们引入了 AVS-Mamba，这是一种用于解决 AVS 任务的选择性状态空间模型。我们的框架结合了两个用于视频理解和跨模态学习的关键组件：用于顺序视频处理的 Temporal Mamba 块和用于高级音频视觉集成的视觉到音频融合块。在此基础上，我们开发了多尺度时间编码器，旨在增强跨尺度视觉特征的学习，促进对帧内和帧间信息的感知。为了执行多模态融合，我们提出了模态聚合解码器，利用视觉到音频融合模块将视觉特征集成到帧和时间级别的音频特征中。此外，我们采用上下文集成金字塔来执行音频到视觉的时空上下文协作。通过这些创新贡献，我们的方法在 AVSBench-object 和 AVSBench-semantic 数据集上取得了新的最佳结果。我们的源代码和模型权重可在 AVS-Mamba 上找到。]]></description>
      <guid>https://arxiv.org/abs/2501.07810</guid>
      <pubDate>Wed, 15 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>3UR-LLM：用于 3D 场景理解的端到端多模态大型语言模型</title>
      <link>https://arxiv.org/abs/2501.07819</link>
      <description><![CDATA[arXiv:2501.07819v1 公告类型：新
摘要：多模态大型语言模型（MLLM）在二维任务中表现出令人印象深刻的能力，但在从二维表示转换为三维表示时，在辨别场景中的空间位置、相互关系和因果逻辑方面遇到挑战。我们发现限制主要在于：i）高注释成本限制了三维场景数据量的扩大，ii）缺乏一种直接有效的方法来感知三维信息，导致训练时间延长并使精简框架复杂化。为此，我们基于开源二维MLLM和LLM开发管道以生成高质量的三维文本对并构建3DS-160K，以增强预训练过程。利用这些高质量的预训练数据，我们推出了 3UR-LLM 模型，这是一种端到端的 3D MLLM，旨在精确解释 3D 场景，展示了在物理世界复杂性中导航的卓越能力。3UR-LLM 直接接收 3D 点云作为输入，并将与文本指令融合的 3D 特征投射到一组可管理的标记中。考虑到这些混合标记产生的计算负担，我们设计了一个 3D 压缩器模块来对 3D 空间线索和文本叙述进行压缩。3UR-LLM 相对于之前的 SOTA 取得了令人鼓舞的性能，例如，3UR-LLM 在 ScanQA 上比其同行高出 7.1% CIDEr，同时使用更少的训练资源。3UR-LLM 和 3DS-160K 基准的代码和模型权重可在 3UR-LLM 获得。]]></description>
      <guid>https://arxiv.org/abs/2501.07819</guid>
      <pubDate>Wed, 15 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>图像超分辨率的最新 Transformer 模型：技术、挑战和应用</title>
      <link>https://arxiv.org/abs/2501.07855</link>
      <description><![CDATA[arXiv:2501.07855v1 公告类型：新
摘要：图像超分辨率 (SR) 旨在从受特定退化过程影响的低分辨率图像中恢复高分辨率图像。这是通过增强细节和视觉质量来实现的。基于 Transformer 的方法的最新进展重塑了图像超分辨率，实现了超越 CNN 和 GAN 等以前深度学习方法的高质量重建。这有效地解决了以前方法的局限性，例如有限的接受场、较差的全局上下文捕获以及高频细节恢复中的挑战。此外，本文回顾了基于 Transformer 的 SR 模型的最新趋势和进展，探索了将 Transformer 与传统网络相结合以平衡全局和局部上下文的各种创新技术和架构。对这些新方法进行了批判性分析，揭示了有希望但尚未探索的差距和未来研究的潜在方向。其中包括模型和技术的几种可视化，以促进对最新趋势的整体理解。这项工作旨在为深度学习前沿的研究人员提供结构化的路线图，特别是探索 Transformer 对超分辨率技术的影响。]]></description>
      <guid>https://arxiv.org/abs/2501.07855</guid>
      <pubDate>Wed, 15 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>