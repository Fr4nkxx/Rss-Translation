<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>CS.CV更新arxiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.cv更新arxiv.org e-print存档。</description>
    <lastBuildDate>Thu, 03 Apr 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>基于圆柱体素的全向深度辅助占用预测，用于自动驾驶</title>
      <link>https://arxiv.org/abs/2504.01023</link>
      <description><![CDATA[ARXIV：2504.01023V1公告类型：新 
摘要：准确的3D感知对于自动驾驶至关重要。由于缺乏几何形状，传统方法通常与几何歧义相处。为了应对这些挑战，我们使用全向深度估计来引入几何事先。基于深度信息，我们提出了一个素描色框架OmnidePth-OCC。此外，我们的方法引入了基于极坐标的圆柱体素表示，以更好地与全景相机视图的径向性质保持一致。为了解决自动驾驶任务中缺少鱼眼摄像机数据集，我们还构建了一个带有六个鱼眼摄像机的虚拟场景数据集，并且数据量的达到了Semantickitti的两倍。实验结果表明，我们的素描颜色网络显着提高了3D感知性能。]]></description>
      <guid>https://arxiv.org/abs/2504.01023</guid>
      <pubDate>Thu, 03 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>凝视引导的3D手运动预测，用于检测以自我为中心的抓握任务</title>
      <link>https://arxiv.org/abs/2504.01024</link>
      <description><![CDATA[ARXIV：2504.01024V1公告类型：新 
摘要：用手运动预测的人类意图检测对于推动神经居住应用中的高级辅助机器人至关重要。但是，依赖生理信号测量的传统方法是限制性的，通常缺乏环境环境。我们提出了一种新的方法，可以预测手姿势和关节位置的未来序列。此方法将目光信息，历史手运动序列和环境对象数据集成在一起，并在没有预先了解预期的对象的情况下，动态地适应了患者的辅助需求。具体而言，我们使用矢量定量的变分自动编码器来用自回旋生成变压器编码稳健的手姿势，以进行有效的手运动序列预测。我们在一项具有健康受试者的试点研究中证明了这些新技术的可用性。为了训练和评估所提出的方法，我们收集了一个数据集，该数据集由来自多个受试者的不同对象的各种类型的掌握操作组成。通过广泛的实验，我们证明了所提出的方法可以成功预测顺序手动运动。尤其是，目光信息显示出预测能力的显着增强，尤其是在输入框架较少的情况下，突出了所提出的实际应用方法的潜力。]]></description>
      <guid>https://arxiv.org/abs/2504.01024</guid>
      <pubDate>Thu, 03 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>改善培训期间基于深度学习的代币分类模型的适用性</title>
      <link>https://arxiv.org/abs/2504.01028</link>
      <description><![CDATA[ARXIV：2504.01028V1公告类型：新 
摘要：本文表明，需要在模型培训期间进行进一步的评估指标来决定其推断中的适用性。例如，基于布局的模型接受了文档中的令牌分类的培训。这些文件是德国收据。我们表明，在我们的实验中以F1得分为代表的常规分类指标不足以评估机器学习模型在实践中的适用性。为了解决这个问题，我们介绍了一种新颖的指标，文档完整性精度（DIP），作为视觉文档理解和令牌分类任务的解决方案。据我们所知，在这种情况下，没有任何可比性。 DIP是一个严格的指标，描述了多少个测试数据集的文档需要手动干预。它使AI研究人员和软件开发人员能够对业务软件中的过程自动化水平进行深入研究。为了验证DIP，我们使用创建模型进行实验，以突出和分析DIP的影响和相关性，以评估是否应在不同的培训环境中部署模型。我们的结果表明，对于孤立的模型障碍而言，现有的指标几乎没有变化，而DIP表明该模型需要大量的人力干预。预测的实体集越大，传统指标的敏感性较小，其自动化质量差。相比之下，DIP仍然是为整个实体集解释的一个值。这强调了拥有指标专注于生产模型培训的业务任务的重要性。由于为令牌分类任务创建了DIP，因此需要进行更多的研究以找到适合其他培训任务的指标。]]></description>
      <guid>https://arxiv.org/abs/2504.01028</guid>
      <pubDate>Thu, 03 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>cal还是没有cal？ - 激光雷达和摄像头传感器的实时错误校准检测</title>
      <link>https://arxiv.org/abs/2504.01040</link>
      <description><![CDATA[ARXIV：2504.01040V1公告类型：新 
摘要：外部校准的目的是传感器数据的对齐，以确保周围环境的准确表示并启用传感器融合应用。从安全的角度来看，传感器校准是自动驾驶的关键推动力。在目前的最新状态下，可以观察到基于目标的离线校准到无目标在线校准的趋势。但是，在线校准受到严格的实时和资源限制，这些校准未通过最新方法来满足。这主要是由于要估计的参数数量很高，对几何特征的依赖或对特定车辆操纵的依赖性。为了满足这些要求并随时确保车辆的安全性，我们提出了一个错误校准检测框架，将焦点从校准参数的直接回归转移到校准状态的二进制分类，即校准或错误校准。因此，我们提出了一种对比的学习方法，该方法比较了潜在空间中嵌入式特征，以对两种不同传感器方式的校准状态进行分类。此外，我们对功能嵌入和挑战性校准错误提供了全面的分析，以突出我们的方法的性能。结果，我们的方法在检测性能，推理时间和资源需求方面优于当前的最新技术。该代码是开源的，可在https://github.com/tumftm/miscalebrationdetection上找到。]]></description>
      <guid>https://arxiv.org/abs/2504.01040</guid>
      <pubDate>Thu, 03 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过机器学习对演员的面部认识来预测电影制作年</title>
      <link>https://arxiv.org/abs/2504.01047</link>
      <description><![CDATA[ARXIV：2504.01047V1公告类型：新 
摘要：这项研究使用机器学习算法来识别演员并从从电影中随机拍摄的图像中提取演员年龄。从阿拉伯电影中拍摄的图像的使用包括诸如演员或演员或一组演员的演员以及多个元素的非均匀照明，不同和多个姿势。此外，使用化妆，假发，胡须和穿着不同的配饰和服装使系统难以确定同一演员的个性。阿拉伯演员数据集AAD包括574张图像，这些图像来自各种电影，包括黑白以及颜色构成。图像描绘了完整的场景或其碎片。采用了多种模型进行特征提取，并在分类和预测阶段使用了不同的机器学习算法来确定处理此类图像类型的最有效算法。该研究表明，与训练阶段的其他模型相比，逻辑回归模型的有效性表现出最佳性能，这是其AUC，Precision，CA和F1Score值分别为99％，86％，85.5％和84.2％的效果。这项研究的发现可用于提高各种用途的面部识别技术的精度和可靠性，包括电影搜索服务，电影建议算法和电影的体裁分类。]]></description>
      <guid>https://arxiv.org/abs/2504.01047</guid>
      <pubDate>Thu, 03 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>水印如何影响文档理解中的视觉语言模型？</title>
      <link>https://arxiv.org/abs/2504.01048</link>
      <description><![CDATA[ARXIV：2504.01048V1公告类型：新 
摘要：视觉语言模型（VLM）已成为文档理解任务的基础模型，该模型广泛用于处理跨金融，法律和学术界等领域的复杂多模式文档。但是，文档通常包含类似噪声的信息，例如水印，这不可避免地会导致我们询问：\ emph {水印是否会降低VLMS在文档理解中的性能？}以解决这一点，我们提出了一个新颖的评估框架来研究可见水印对VLMS性能的影响。我们考虑了各种因素，包括不同类型的文档数据，文档中水印的位置以及水印含量的变化。我们的实验结果表明，VLMS性能可能会因水印而显着损害，其性能下降速率达到36 \％。我们发现，\ emph {散射}水印比集中式的干扰更强，并且在水印中的\ emph {语义内容}比简单的视觉遮挡会产生更大的破坏。通过注意机制分析和嵌入相似性检查，我们发现性能下降主要归因于该水印1）力量重新分布，以及2）改变嵌入空间中的语义表示。我们的研究不仅强调了在部署VLM来了解文档理解方面面临的重大挑战，而且还为开发出在水印的文档上开发强大的推理机制提供了见解。]]></description>
      <guid>https://arxiv.org/abs/2504.01048</guid>
      <pubDate>Thu, 03 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SVIQA：无文本视觉问题回答的统一语音视图多模式模型</title>
      <link>https://arxiv.org/abs/2504.01049</link>
      <description><![CDATA[ARXIV：2504.01049V1公告类型：新 
摘要：集成语音和视觉的多模式模型具有推进人类计算机相互作用的重要潜力，尤其是在基于语音的视觉问题回答（SBVQA）中，有关图像的口语问题需要直接视听的理解。现有方法主要集中在文本视觉集成上，由于其固有的异质性，因此言语视野差距尚未被逐渐消失。为此，我们介绍了SVIQA，这是一个统一的语音视觉模型，该模型直接处理而没有文本转录的问题。在Llava架构的基础上，我们的框架通过两个关键的创新桥梁桥接听觉和视觉方式：（1）端到端语音特征提取消除了中间文本转换，以及（2）跨模式对准优化，实现有效的语音信号融合与视觉内容。 SBVQA基准的广泛实验结果证明了SVIQA最先进的性能，达到了75.62％的精度和竞争性的多模式概括。利用语音文本混合输入将表现提高到78.85％，比纯语音输入提高了3.23％，强调了SVIQA增强的鲁棒性和有效的跨模式注意对准。]]></description>
      <guid>https://arxiv.org/abs/2504.01049</guid>
      <pubDate>Thu, 03 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于知识库的语义图像传输使用剪辑</title>
      <link>https://arxiv.org/abs/2504.01053</link>
      <description><![CDATA[ARXIV：2504.01053V1公告类型：新 
摘要：本文提出了一个新颖的知识库（KB）辅助语义通信框架，用于图像传输。在接收方，基于Facebook AI的相似性搜索（FAISS）的向量数据库是通过使用对比的语言图像预训练（剪辑）模型从图像中提取语义嵌入来构建的。在传输过程中，发射器首先使用夹子模型提取512维语义功能，然后使用轻质神经网络来压缩它。接收信号后，接收器将功能重建回到512维度，并执行从KB匹配的相似性以检索最相似的图像。语义传输成功取决于传输图像和检索的图像之间的类别一致性，而不是传统的指标，例如峰值信噪比（PSNR）。提出的系统优先考虑语义准确性，为语义感知通信系统提供了新的评估范例。 CIFAR100的实验验证证明了该框架在实现语义图像传输方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2504.01053</guid>
      <pubDate>Thu, 03 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Shieldgemma 2：健壮和可拖动的图像内容适度</title>
      <link>https://arxiv.org/abs/2504.01081</link>
      <description><![CDATA[ARXIV：2504.01081V1公告类型：新 
摘要：我们介绍了建立在Gemma 3上的4B参数图像内容调节模型的Shieldgemma 2。该模型在以下关键危害类别中提供了强大的安全风险预测：性明确，暴力，暴力\＆Gore，以及合成图像的危险内容（例如，任何图像生成模型的输出）和自然图像（例如，任何图像输入到视觉模型）。我们对内部和外部基准测试进行了评估，以证明与llavaguard \ citep \ citep {helff2024llavaguard}，GPT-4O Mini \ citep \ citep {hurst2024gpt}相比，我们证明了最先进的性能。此外，我们提出了一种新颖的对抗数据生成管道，该管道能够产生受控，多样化和健壮的图像生成。 ShieldGemma 2提供了一种开放的图像审核工具，可提高多模式安全性和负责任的AI开发。]]></description>
      <guid>https://arxiv.org/abs/2504.01081</guid>
      <pubDate>Thu, 03 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>RIPVIS：RIP Currents视频实例细分基准用于海滩监控和安全性</title>
      <link>https://arxiv.org/abs/2504.01128</link>
      <description><![CDATA[ARXIV：2504.01128V2公告类型：新 
摘要：撕裂电流是强大的，局部的和狭窄的水流，它们向外流入大海，在全球造成了许多与海滩有关的伤害和死亡。由于其无定形性质和缺乏带注释的数据，对RIP电流的准确识别仍然具有挑战性，这通常需要专家知识。为了解决这些问题，我们提出了RIPVIS，这是一个大规模的视频实例细分基准明确设计用于RIP当前细分。 RIPVIS比以前的数据集大的数量级，其中包含$ 184 $的视频（$ 212,328 $框架），其中$ 150 $ $ 150 $的视频（$ 163,528 $帧）与Rip Currents一起收集，这些视频是从各种来源收集的，包括无人机，移动电话和固定的海滩喜剧片。我们的数据集涵盖了各种全球地点，包括美国，墨西哥，哥斯达黎加，葡萄牙，意大利，希腊，罗马尼亚，罗马尼亚，斯里兰卡，澳大利亚，澳大利亚和新西兰等多个全球地点，包括波浪破坏图案，沉积物流量和水色变化。大多数视频以$ 5 $ fps的注释，以确保在动态方案中的准确性，并附加了34美元的$ 34 $视频（$ 48,800 $帧），而无需撕裂电流。我们使用蒙版R-CNN，Cascade Mask R-CNN，SparseSinst和Yolo11进行全面的实验，以微调这些模型，以实现RIP当前分割的任务。结果是根据多个指标报告的，特别关注$ f_2 $得分，以优先考虑召回并减少虚假负面因素。为了提高分割性能，我们基于时间置信度聚集（TCA）引入了一个新的后处理步骤。 RIPVIS旨在为RIP当前细分设定新的标准，从而有助于更安全的海滩环境。我们提供一个基准网站，与研究社区共享数据，模型和结果，鼓励持续的合作和未来的贡献，网址为https://ripvis.ai。]]></description>
      <guid>https://arxiv.org/abs/2504.01128</guid>
      <pubDate>Thu, 03 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Gru-Aunet：无接触式指纹呈现攻击检测的域适应框架</title>
      <link>https://arxiv.org/abs/2504.01213</link>
      <description><![CDATA[ARXIV：2504.01213V1公告类型：新 
摘要：尽管非接触式指纹提供了用户舒适性，但它们更容易受到欺骗的影响。当前在非接触式指纹区域进行抗旋转的解决方案依赖于域的适应性学习，从而限制了它们的概括和可扩展性。为了解决这些局限性，我们引入了Gru-Aunet，这是一种域的适应方法，将基于Swin Transformer的UNET架构与Gru增强注意机制，瓶颈中的动态滤波器网络以及焦点和对比度损耗功能相结合。 Gru-aunet接受了真正的和欺骗指纹图像的培训，在Clarkson，Colfispoof和IIITD数据集中表现出了强大的抵御性抗韧性，平均BPCer为0.09 \％，APCer为1.2 \％。]]></description>
      <guid>https://arxiv.org/abs/2504.01213</guid>
      <pubDate>Thu, 03 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多边形：利用简化的多边形表示有效图像分类</title>
      <link>https://arxiv.org/abs/2504.01214</link>
      <description><![CDATA[ARXIV：2504.01214V1公告类型：新 
摘要：深度学习模型在各种与图像相关的任务中取得了重大成功。但是，他们经常遇到与计算复杂性和过度拟合有关的挑战。在本文中，我们提出了一种有效的方法，该方法使用主要点或轮廓坐标利用图像的多边形表示。通过将输入图像转换为这些紧凑的形式，我们的方法可大大降低计算要求，加速培训，并保守资源，使其适合实时和资源约束应用程序。这些表示在过滤噪声时固有地捕获了基本图像特征，从而提供了自然的正则化效果，从而减轻过度拟合。由此产生的轻质模型实现了使用完整分辨率图像在边缘设备上部署的同时，实现了与艺术方法的状态相媲美的性能。基准数据集的广泛实验验证了我们方法在降低复杂性，改善概括和促进边缘计算应用程序方面的有效性。这项工作证明了多边形表示在推进现实世界情景的高效和可扩展深度学习解决方案方面的潜力。本文实验的代码在https://github.com/salimkhazem/polygonet中提供。]]></description>
      <guid>https://arxiv.org/abs/2504.01214</guid>
      <pubDate>Thu, 03 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>RPPG-SYSDIAGAN：使用具有多域鉴别器的生成对抗网络在RPPG中的收缩期舒张特征定位</title>
      <link>https://arxiv.org/abs/2504.01220</link>
      <description><![CDATA[ARXIV：2504.01220V1公告类型：新 
摘要：远程光插图学（RPPG）提供了一种使用相机的无创监测生命体征（例如呼吸率）的新方法。尽管已经提出了几种监督和自我监督的方法，但它们通常无法准确地重建PPG信号，尤其是在区分收缩压和舒张期成分时。它们的主要重点往往仅仅是提取心率，这可能不能准确地代表完整的PPG信号。为了解决这一限制，本文提出了一种新颖的深度学习架构，使用生成的对抗网络引入多歧视器来从面部视频中提取RPPG信号。这些歧视因子专注于时间域，频域和原始时域信号的第二个导数。鉴别器集成了四个损失函数：减轻噪声引起的局部最小值的方差损失；动态时间扭曲损失，以解决由比对引起的局部最小值和可变长度的序列；心率调整的稀疏性损失以及差异损失，以确保PPG信号的收缩期和舒张期之间所需的频域和时间间隔的均匀分布。]]></description>
      <guid>https://arxiv.org/abs/2504.01220</guid>
      <pubDate>Thu, 03 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Tenad：用于视频分类的基于张量的低排名黑匣子对抗攻击</title>
      <link>https://arxiv.org/abs/2504.01228</link>
      <description><![CDATA[ARXIV：2504.01228V1公告类型：新 
摘要：深度学习模型在计算机视觉上取得了巨大的成功，但仍然容易受到对抗性攻击的影响，尤其是在模型细节未知的黑箱设置中。现有的对抗攻击方法（即使是与关键帧一起使用）通常将视频数据视为简单的向量，忽略了它们固有的多维结构，并且需要大量查询，从而使它们效率低下且可检测到。在本文中，我们提出了一种基于新颖的张量的低级对抗攻击\ textbf {tenad}，它通过将视频表示为四阶张量来利用视频数据的多维属性。通过利用低级攻击，我们的方法可大大减少搜索空间以及在黑框设置中生成对抗示例所需的查询数量。标准视频分类数据集的实验结果表明，\ textbf {tenad}有效地产生了不可察觉的对抗性扰动，同时与最先进的方法相比，获得了较高的攻击成功率和查询效率。我们的方法在成功率，查询效率和扰动不可识别方面优于现有的黑盒对抗攻击，突出了基于张量的方法对视频模型的对抗性攻击的潜力。]]></description>
      <guid>https://arxiv.org/abs/2504.01228</guid>
      <pubDate>Thu, 03 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>融合：频率引导的水下空间图像重建</title>
      <link>https://arxiv.org/abs/2504.01243</link>
      <description><![CDATA[ARXIV：2504.01243V1公告类型：新 
摘要：水下图像遭受严重的降解，包括颜色扭曲，可见度降低以及由于波长依赖性衰减和散射而导致的结构细节丧失。现有的增强方法主要集中于空间域处理，忽略了频率域捕获全局颜色分布和远距离依赖性的潜力。为了解决这些局限性，我们提出了融合，这是一个双重域深度学习框架，共同利用空间和频域信息。 Fusion通过多尺度的卷积内核和空间域中的自适应注意机制独立处理每个RGB通道，同时通过基于FFT的频率注意力同时提取全局结构信息。频率引导的融合模块整合了两个域的互补特征，然后进行通道间融合和自适应通道重新校准，以确保平衡的色彩分布。 Extensive experiments on benchmark datasets (UIEB, EUVP, SUIM-E) demonstrate that FUSION achieves state-of-the-art performance, consistently outperforming existing methods in reconstruction fidelity (highest PSNR of 23.717 dB and SSIM of 0.883 on UIEB), perceptual quality (lowest LPIPS of 0.112 on UIEB), and visual enhancement metrics (best UIQM在UIEB上为3.414），同时需要较少的参数（0.28m）和较低的计算复杂性，这表明其适合实时水下水下成像应用。]]></description>
      <guid>https://arxiv.org/abs/2504.01243</guid>
      <pubDate>Thu, 03 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>