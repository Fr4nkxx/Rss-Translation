<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Thu, 15 Aug 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>息肉分割中的集成架构</title>
      <link>https://arxiv.org/abs/2408.07262</link>
      <description><![CDATA[arXiv:2408.07262v1 公告类型：新
摘要：在本研究中，我们重新审视了语义分割的架构，并评估了在息肉分割方面表现出色的模型。我们引入了一个集成框架，该框架利用不同模型的优势来获得最佳结果。更具体地说，我们融合了从卷积和变换器模型中学习到的特征进行预测，我们将这种方法视为一种增强模型性能的集成技术。我们在息肉分割方面的实验表明，所提出的架构超越了其他顶级模型，表现出更好的学习能力和弹性。代码可在 https://github.com/HuangDLab/EnFormer 上找到。]]></description>
      <guid>https://arxiv.org/abs/2408.07262</guid>
      <pubDate>Fri, 16 Aug 2024 03:16:16 GMT</pubDate>
    </item>
    <item>
      <title>利用几何建模增强单目内窥镜场景的尺度感知深度估计</title>
      <link>https://arxiv.org/abs/2408.07266</link>
      <description><![CDATA[arXiv:2408.07266v1 公告类型：新
摘要：尺度感知的单目深度估计对计算机辅助内窥镜导航提出了重大挑战。然而，现有的不考虑几何先验的深度估计方法很难从单目内窥镜序列训练中学习绝对尺度。此外，传统方法在准确估计组织和器械边界的细节方面也面临困难。在本文中，我们通过提出一种新的增强尺度感知框架来解决这些问题，该框架仅使用具有几何建模的单目图像进行深度估计。具体而言，我们首先提出一种多分辨率深度融合策略来提高单目深度估计的质量。为了恢复相对深度和真实世界值之间的精确比例，我们进一步通过基于纯图像几何图元（即器械的边界和尖端）的代数几何计算内窥镜场景中器械的 3D 姿势。之后，手术器械的 3D 姿势可以实现相对深度图的比例恢复。通过结合比例因子和相对深度估计，可以估计单目内窥镜场景的比例感知深度。我们在内部内窥镜手术视频和模拟数据上评估了该流程。结果表明，我们的方法可以通过几何建模学习绝对比例，并准确估计单目场景的比例感知深度。]]></description>
      <guid>https://arxiv.org/abs/2408.07266</guid>
      <pubDate>Fri, 16 Aug 2024 03:16:16 GMT</pubDate>
    </item>
    <item>
      <title>基于深度学习和低成本手工描述符的手语识别</title>
      <link>https://arxiv.org/abs/2408.07244</link>
      <description><![CDATA[arXiv:2408.07244v1 公告类型：新
摘要：近年来，深度学习技术已用于开发手语识别系统，可能成为全球数百万听力障碍人士的交流工具。然而，创建这样的系统存在固有的挑战。首先，重要的是在手势执行中考虑尽可能多的语言参数，以避免单词之间的歧义。此外，为了促进所创建解决方案在现实世界中的采用，必须确保所选技术是现实的，避免使用昂贵、侵入性或低移动性的传感器，以及对计算要求很高的非常复杂的深度学习架构。基于此，我们的工作旨在提出一种利用低成本传感器和技术的高效手语识别系统。为此，专门训练了一个对象检测模型，用于检测翻译的脸部和手部，确保将焦点集中在图像中最相关的区域并为分类器生成具有更高语义值的输入。此外，我们还引入了一种新方法，利用从边界框质心位置获得的空间信息来获取表示手部位置和运动的特征，从而增强手势识别能力。结果证明了我们手工设计的特征的效率，在 AUTSL 数据集上将准确率提高了 7.96%，同时增加了不到 70 万个参数，并且额外推理时间不到 10 毫秒。这些发现凸显了我们的技术在计算成本和准确率之间取得良好平衡的潜力，使其成为实用手语识别应用的一种有前途的方法。]]></description>
      <guid>https://arxiv.org/abs/2408.07244</guid>
      <pubDate>Fri, 16 Aug 2024 03:16:15 GMT</pubDate>
    </item>
    <item>
      <title>GQE：增强文本视频检索的通用查询扩展</title>
      <link>https://arxiv.org/abs/2408.07249</link>
      <description><![CDATA[arXiv:2408.07249v1 公告类型：新
摘要：在快速扩展的网络视频内容领域，文本视频检索任务变得越来越重要，它弥合了文本查询和视频数据之间的语义鸿沟。本文介绍了一种新颖的以数据为中心的方法，即广义查询扩展 (GQE)，以解决文本和视频之间固有的信息不平衡问题，从而提高文本视频检索系统的有效性。与专注于设计复杂的跨模态交互机制的传统以模型为中心的方法不同，GQE 旨在在训练和测试阶段扩展与视频相关的文本查询。通过自适应地将视频分割成短片段并采用零镜头字幕，GQE 通过全面的场景描述丰富了训练数据集，有效地弥合了数据不平衡差距。此外，在检索过程中，GQE 利用大型语言模型 (LLM) 生成多样化的查询集，并使用查询选择模块根据相关性和多样性过滤这些查询，从而优化检索性能并降低计算开销。我们的贡献包括详细研究信息不平衡挑战、提出一种在视频文本数据集中扩展查询的新方法，以及引入一种在不增加计算成本的情况下提高检索准确性的查询选择策略。GQE 在多个基准测试中实现了最先进的性能，包括 MSR-VTT、MSVD、LSMDC 和 VATEX，证明了从数据中心角度解决文本视频检索的有效性。]]></description>
      <guid>https://arxiv.org/abs/2408.07249</guid>
      <pubDate>Fri, 16 Aug 2024 03:16:15 GMT</pubDate>
    </item>
    <item>
      <title>GRIF-DM：使用扩散模型生成丰富的印象字体</title>
      <link>https://arxiv.org/abs/2408.07259</link>
      <description><![CDATA[arXiv:2408.07259v1 公告类型：新
摘要：字体是创意工作、设计过程和艺术作品不可或缺的一部分。选择合适的字体可以显著增强艺术作品的效果，并赋予广告更高的表现力。尽管网上有各种各样的字体设计，但传统的基于检索的字体选择方法正日益被基于生成的方法所取代。这些新方法提供了增强的灵活性，迎合了特定的用户偏好并捕捉了独特的风格印象。然而，目前基于生成对抗网络 (GAN) 的印象字体技术需要利用多个辅助损失来在生成过程中提供指导。此外，这些方法通常采用加权求和来融合与印象相关的关键字。这会导致通用向量增加更多的印象关键字，最终缺乏细节生成能力。在本文中，我们介绍了一种基于扩散的方法，称为 \ourmethod，利用由单个字母和一组描述性印象关键词组成的输入来生成生动体现特定印象的字体。\ourmethod 的核心创新在于开发双重交叉注意模块，这些模块独立但协同地处理字母和印象关键词的特征，确保有效整合这两种类型的信息。我们在 MyFonts 数据集上进行的实验结果证实，该方法能够生成与用户规范紧密一致的逼真、生动和高保真字体。这证实了我们的方法通过满足广泛的用户驱动设计要求来彻底改变字体生成的潜力。我们的代码可在 \url{https://github.com/leitro/GRIF-DM} 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2408.07259</guid>
      <pubDate>Fri, 16 Aug 2024 03:16:15 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉伪标签综述</title>
      <link>https://arxiv.org/abs/2408.07221</link>
      <description><![CDATA[arXiv:2408.07221v1 公告类型：新
摘要：深度神经模型在计算机科学的广泛问题上取得了最先进的性能，尤其是在计算机视觉领域。然而，深度神经网络通常需要大量的标记样本数据集才能有效地概括，而一个重要的活跃研究领域是半监督学习，它试图利用大量（容易获得的）未标记样本。这个领域的一类方法是伪标记，这是一类使用模型输出为未标记样本分配标签的算法，然后在训练期间将其用作标记样本。这种分配的标签称为伪标签，最常与半监督学习领域相关。在这项工作中，我们探索了在自监督和无监督方法中对伪标签的更广泛解释。通过建立这些领域之间的联系，我们可以确定一个领域的进步可能使其他领域受益的新方向，例如课程学习和自监督正则化。]]></description>
      <guid>https://arxiv.org/abs/2408.07221</guid>
      <pubDate>Fri, 16 Aug 2024 03:16:14 GMT</pubDate>
    </item>
    <item>
      <title>儿童面孔识别的纵向评估及其潜在年龄的影响</title>
      <link>https://arxiv.org/abs/2408.07225</link>
      <description><![CDATA[arXiv:2408.07225v1 公告类型：新
摘要：各种新兴应用中对儿童可靠识别的需求引发了人们对利用儿童面部识别技术的兴趣。本研究介绍了一种纵向方法，用于儿童面部识别的注册和验证准确性，重点关注克拉克森大学 CITeR 研究小组在 8 年内以 6 个月为间隔收集的 YFA 数据库。]]></description>
      <guid>https://arxiv.org/abs/2408.07225</guid>
      <pubDate>Fri, 16 Aug 2024 03:16:14 GMT</pubDate>
    </item>
    <item>
      <title>通过语义分割训练期间的图像增强功能增强自动驾驶汽车在恶劣天气下的感知能力</title>
      <link>https://arxiv.org/abs/2408.07239</link>
      <description><![CDATA[arXiv:2408.07239v1 公告类型：新
摘要：稳健的感知对于自动驾驶汽车导航和定位至关重要。视觉处理任务（如语义分割）应该在不同的天气条件和一天中的不同时间进行。语义分割是将每个像素分配到一个类，这对于定位整体特征很有用 (1)。训练分割模型需要大量数据，而分割数据的标记过程尤其繁琐。此外，许多大型数据集仅包含在晴朗天气下拍摄的图像。这是一个问题，因为仅在晴朗天气数据上训练模型会阻碍在雾或雨等恶劣天气条件下的性能。我们假设，给定一个只有晴天图像的数据集，在训练期间应用图像增强（例如随机雨、雾和亮度）可以使领域适应不同的天气条件。我们使用 3D 逼真的自动驾驶汽车模拟器 CARLA 收集了来自 10 个不同城镇的 1200 张晴朗天气图像，由 29 个类别组成 (2)。我们还收集了 1200 张随机天气效果的图像。我们训练了编码器-解码器 UNet 模型来执行语义分割。应用增强功能显著改善了风雨夜晚条件下的分割效果 (p &lt; 0.001)。然而，除晴天外，在所有条件下，在天气数据上训练的模型的损失明显低于在增强数据上训练的模型。这表明领域自适应方法还有改进的空间。未来的工作应该测试更多类型的增强功能，并使用真实图像代替 CARLA。理想情况下，增强模型应达到或超过天气模型的性能。]]></description>
      <guid>https://arxiv.org/abs/2408.07239</guid>
      <pubDate>Fri, 16 Aug 2024 03:16:14 GMT</pubDate>
    </item>
    <item>
      <title>利用感知分数进行计算机视觉任务中的数据集修剪</title>
      <link>https://arxiv.org/abs/2408.07243</link>
      <description><![CDATA[arXiv:2408.07243v1 公告类型：新
摘要：在本文中，我们提出了一种图像分数，用于图像分类和语义分割任务中的核心集选择。分数是图像的熵，近似于其压缩版本的每像素位数。因此，分数是图像固有的，不需要监督或训练。由于所有图像都以压缩格式存储，因此计算起来非常简单且随时可用。我们选择分数的动机是，文献中提出的大多数其他分数计算起来都很昂贵。更重要的是，我们想要一个能够捕捉图像感知复杂性的分数。熵就是这样一种度量，杂乱的图像往往具有更高的熵。然而，仅对低熵标志性图像进行采样会导致学习偏差，并导致当前深度学习模型的测试性能整体下降。为了减轻偏差，我们使用一种基于图的方法来增加所选样本的空间多样性。我们表明，这个简单的分数产生了良好的结果，特别是对于语义分割任务。]]></description>
      <guid>https://arxiv.org/abs/2408.07243</guid>
      <pubDate>Fri, 16 Aug 2024 03:16:14 GMT</pubDate>
    </item>
    <item>
      <title>用手法控制世界</title>
      <link>https://arxiv.org/abs/2408.07147</link>
      <description><![CDATA[arXiv:2408.07147v1 公告类型：新
摘要：人类自然地建立了物体交互和动态的心理模型，使他们能够想象如果他们采取某种行动，周围环境将如何变化。虽然当今的生成模型在无条件或以文本为条件生成/编辑图像方面表现出令人印象深刻的结果，但当前方法不提供以动作为条件执行对象操作的能力，这是世界建模和行动规划的重要工具。因此，我们建议通过从未标记的人手与物体交互的视频中学习来学习动作条件生成模型。互联网上大量此类数据允许高效扩展，从而实现高性能的动作条件模型。给定图像和所需手部交互的形状/位置，CosHand 会合成交互发生后的未来图像。实验表明，生成的模型可以很好地预测手与物体交互的影响，具有很强的泛化能力，特别是对看不见的环境中看不见的物体的平移、拉伸和挤压交互。此外，CosHand 可以多次采样以预测多种可能的影响，从而对交互/环境中的力量的不确定性进行建模。最后，该方法可以推广到不同的实施例，包括非人类的手，即机器人手，这表明生成视频模型可以成为机器人技术的强大模型。]]></description>
      <guid>https://arxiv.org/abs/2408.07147</guid>
      <pubDate>Fri, 16 Aug 2024 03:16:13 GMT</pubDate>
    </item>
    <item>
      <title>通过分层形状匹配实现灵活的 3D 车道检测通过分层形状匹配实现灵活的 3D 车道检测</title>
      <link>https://arxiv.org/abs/2408.07163</link>
      <description><![CDATA[arXiv:2408.07163v1 公告类型：新
摘要：作为高清地图构建的基本而重要的技术之一，3D车道检测由于视觉条件多变、类型复杂、精度要求严格，仍然是一个悬而未决的问题。本文提出了一种端到端灵活、分层的车道检测器，可以从点云中精确预测3D车道线。具体来说，我们设计了一个分层网络，可以灵活地预测不同级别的车道形状表示，同时收集全局实例语义并避免局部错误。在全局范围内，我们提出回归参数曲线 w.r.t 自适应轴，以帮助对复杂场景做出更稳健的预测，而在局部视觉中，在沿全局预测曲线采样的每个动态锚单元中检测车道段的结构。此外，还设计了相应的全局和局部形状匹配损失和锚单元生成策略。在两个数据集上的实验表明我们在高精度标准下超越了目前顶尖的方法，而全消融研究也验证了我们方法的每个部分。我们的代码将在 https://github.com/Doo-do/FHLD 发布。]]></description>
      <guid>https://arxiv.org/abs/2408.07163</guid>
      <pubDate>Fri, 16 Aug 2024 03:16:13 GMT</pubDate>
    </item>
    <item>
      <title>SeLoRA：用于医学图像合成的自扩展低秩自适应潜在扩散模型</title>
      <link>https://arxiv.org/abs/2408.07196</link>
      <description><![CDATA[arXiv:2408.07196v1 公告类型：新
摘要：由于注释数据的稀缺以及需要合成“缺失模态”以进行多模态分析，医学图像合成面临持续挑战，这凸显了开发有效合成方法的必要性。最近，低秩自适应 (LoRA) 与潜在扩散模型 (LDM) 的结合已成为医学领域有效调整预训练大型语言模型的可行方法。然而，直接应用 LoRA 假设所有线性层的排名均匀，忽略了不同权重矩阵的重要性，导致结果不理想。先前对 LoRA 的研究优先考虑减少可训练参数，并且有机会进一步根据医学图像合成的复杂需求定制此自适应过程。为此，我们提出了 SeLoRA，一种自扩展低秩自适应模块，它在训练期间动态扩展其跨层排名，策略性地在关键层上放置额外排名，以允许模型在最重要的地方提升合成质量。所提出的方法不仅使 LDM 能够有效地对医疗数据进行微调，而且还使模型能够以最少的排名实现图像质量的提高。我们的 SeLoRA 方法的代码可在 https://anonymous.4open.science/r/SeLoRA-980D 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2408.07196</guid>
      <pubDate>Fri, 16 Aug 2024 03:16:13 GMT</pubDate>
    </item>
    <item>
      <title>用于纸笔计算机科学教育的手写代码识别</title>
      <link>https://arxiv.org/abs/2408.07220</link>
      <description><![CDATA[arXiv:2408.07220v1 公告类型：新
摘要：通过让学生在纸上手写程序来教授计算机科学 (CS) 具有关键的教学优势：与使用集成开发环境 (IDE) 和智能支持工具或“只是尝试”相比，它允许集中学习并需要仔细思考。熟悉的笔和纸环境也减轻了没有使用过计算机的学生的认知负担，对于他们来说，仅仅是基本的计算机使用就可能令人生畏。最后，这种教学方法为计算机使用有限的学生提供了学习机会。
然而，一个关键的障碍是目前缺乏使用和运行手写程序的教学方法和支持软件。手写代码的光学字符识别 (OCR) 具有挑战性：轻微的 OCR 错误（可能是由于不同的笔迹风格）很容易导致代码无法运行，并且识别缩进对于 Python 等语言至关重要，但由于手写水平间距不一致而很难做到。我们的方法整合了两种创新方法。第一种方法将 OCR 与缩进识别模块和专为 O​​CR 后纠错而设计的语言模型相结合，不会引入幻觉。据我们所知，这种方法在手写代码识别方面超越了所有现有系统。它将错误率从最先进的 30% 降低到 5%，同时将学生程序的逻辑修复幻觉降到最低。第二种方法利用多模态语言模型以端到端的方式识别手写程序。我们希望这一贡献能够促进进一步的教学研究，并为实现让 CS 教育普及的目标做出贡献。我们在 https://github.com/mdoumbouya/codeocr 上发布了手写程序和代码数据集，以支持未来的研究]]></description>
      <guid>https://arxiv.org/abs/2408.07220</guid>
      <pubDate>Fri, 16 Aug 2024 03:16:13 GMT</pubDate>
    </item>
    <item>
      <title>生成照片蒙太奇</title>
      <link>https://arxiv.org/abs/2408.07116</link>
      <description><![CDATA[arXiv:2408.07116v1 公告类型：新
摘要：文本到图像模型是用于创建图像的强大工具。但是，生成过程类似于掷骰子，很难获得一张能够捕捉用户想要的所有内容的图像。在本文中，我们提出了一个框架，通过合成生成的图像的各个部分来创建所需的图像，本质上形成生成照片蒙太奇。给定由 ControlNet 使用相同输入条件和不同种子生成的一堆图像，我们让用户使用笔触界面从生成的结果中选择所需的部分。我们引入了一种新技术，该技术吸收用户的笔触，使用基于图的扩散特征空间优化对生成的图像进行分割，然后通过新的特征空间混合方法合成分割的区域。我们的方法忠实地保留了用户选择的区域，同时和谐地合成它们。我们证明了我们的灵活框架可用于许多应用，包括生成新的外观组合、修复不正确的形状和伪影以及改进快速对齐。我们展示了每个应用的令人信服的结果，并证明我们的方法优于现有的图像混合方法和各种基线。]]></description>
      <guid>https://arxiv.org/abs/2408.07116</guid>
      <pubDate>Fri, 16 Aug 2024 03:16:12 GMT</pubDate>
    </item>
    <item>
      <title>用于解释和细粒度检测不同工作场所安全合规性的视觉语言模型</title>
      <link>https://arxiv.org/abs/2408.07146</link>
      <description><![CDATA[arXiv:2408.07146v1 公告类型：新
摘要：由于个人防护设备 (PPE) 不合规而导致的工作场所事故引发了严重的安全问题，并导致法律责任、经济处罚和声誉损害。虽然物体检测模型已经显示出通过识别安全项目来解决此问题的能力，但大多数现有模型（例如 YOLO、Faster R-CNN 和 SSD）在验证不同工作场所场景中 PPE 的细粒度属性方面受到限制。视觉语言模型 (VLM) 通过利用视觉和文本信息之间的协同作用，在检测任务中获得了关注，为 PPE 识别中传统物体检测的局限性提供了一个有希望的解决方案。尽管如此，由于工作场所环境的复杂性和多变性，VLM 在一致验证 PPE 属性方面面临挑战，需要它们同时解释特定于上下文的语言和视觉提示。我们推出了 Clip2Safety，这是一个用于多样化工作场所安全合规性的可解释检测框架，它包含四个主要模块：场景识别、视觉提示、安全物品检测和细粒度验证。场景识别识别当前场景以确定必要的安全装备。视觉提示制定检测过程所需的具体视觉提示。安全物品检测根据指定场景识别是否佩戴所需的安全装备。最后，细粒度验证评估所佩戴的安全设备是否满足细粒度属性要求。我们在六种不同场景中开展了真实案例研究。结果表明，Clip2Safety 不仅比最先进的基于问答的 VLM 具有准确性的提高，而且推理时间也提高了 200 倍。]]></description>
      <guid>https://arxiv.org/abs/2408.07146</guid>
      <pubDate>Fri, 16 Aug 2024 03:16:12 GMT</pubDate>
    </item>
    </channel>
</rss>