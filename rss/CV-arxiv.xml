<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Mon, 23 Sep 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>医学成像基础模型时代即将到来：放射学中大规模生成式人工智能应用的临床价值范围审查</title>
      <link>https://arxiv.org/abs/2409.12973</link>
      <description><![CDATA[arXiv:2409.12973v1 公告类型：新
摘要：放射科医生短缺引发的社会问题日益严重，人工智能被视为潜在的解决方案。最近出现的大规模生成式人工智能已经从大型语言模型 (LLM) 扩展到多模态模型，显示出彻底改变整个医学成像过程的潜力。然而，目前缺乏对其发展现状和未来挑战的全面评论。本范围审查遵循 PCC 指南，系统地组织了有关大规模生成式人工智能应用临床价值的现有文献。在 PubMed、EMbase、IEEE-Xplore 和 Google Scholar 四个数据库中进行了系统搜索，并审查了 15 项符合研究人员设定的纳入/排除标准的研究。这些研究大多侧重于提高解释过程特定部分的报告生成效率或翻译报告以帮助患者理解，最新研究扩展到执行直接解释的人工智能应用。所有研究均由临床医生进行定量评估，大多数研究使用 LLM，只有三项研究使用多模态模型。LLM 和多模态模型在特定领域都表现出色，但在诊断性能方面，没有一个能胜过放射科医生。大多数研究使用 GPT，很少有研究使用专门用于医学成像领域的模型。这项研究深入了解了医学成像领域大规模生成式 AI 应用的现状和局限性，提供了基础数据，并表明医学成像基础模型时代即将到来，这可能会在不久的将来从根本上改变临床实践。]]></description>
      <guid>https://arxiv.org/abs/2409.12973</guid>
      <pubDate>Mon, 23 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>测量你只需看一次 (YOLO) 多光谱物体检测的进展、应用和挑战</title>
      <link>https://arxiv.org/abs/2409.12977</link>
      <description><![CDATA[arXiv:2409.12977v1 公告类型：新
摘要：多光谱成像和深度学习已成为支持从自动驾驶汽车到农业、基础设施监测和环境评估等各种用例的强大工具。这些技术的结合已导致不可见光谱中的物体检测、分类和分割任务取得了重大进展。本文共考虑了 400 篇论文，详细回顾了 200 篇论文，以提供多光谱成像技术、深度学习模型及其应用的权威元评论，同时考虑到 You Only Look Once (YOLO) 方法的演变和适应性。地面收集是最普遍的方法，占所审查论文的 63%，尽管自 2020 年以来用于 YOLO-多光谱应用的无人驾驶航空系统 (UAS) 增加了一倍。最普遍的传感器融合是红-绿-蓝 (RGB) 与长波红外 (LWIR)，占文献的 39%。 YOLOv5 仍然是适应多光谱应用的最常用变体，占所有经过审查的修改后的 YOLO 模型的 33%。58% 的多光谱 YOLO 研究在中国进行，其研究质量与其他国家大致相同（平均期刊影响因子为 4.45，而非来自中国机构的论文为 4.36）。未来的研究需要关注以下方面：(i) 开发能够处理不同光谱输入且不需要进行大量架构修改的自适应 YOLO 架构，(ii) 探索生成大型合成多光谱数据集的方法，(iii) 推进多光谱 YOLO 迁移学习技术以解决数据集稀缺问题，以及 (iv) 创新与 RGB 和 LWIR 以外的其他传感器类型的融合研究。]]></description>
      <guid>https://arxiv.org/abs/2409.12977</guid>
      <pubDate>Mon, 23 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>语义元分割学习：一种用于小样本无线图像分类的 TinyML 方案</title>
      <link>https://arxiv.org/abs/2409.12978</link>
      <description><![CDATA[arXiv:2409.12978v1 公告类型：新
摘要：语义和目标导向 (SGO) 通信是一种新兴技术，它只传输给定任务的重要信息。语义通信面临许多挑战，例如最终用户的计算复杂性、数据可用性和隐私保护。这项工作提出了一种基于 TinyML 的语义通信框架，用于集成分割学习和元学习的少样本无线图像分类。我们利用分割学习来限制最终用户执行的计算，同时确保隐私保护。此外，元学习克服了数据可用性问题，并通过使用类似训练的任务来加快训练速度。使用手写字母图像数据集测试了所提出的算法。此外，我们使用共形预测 (CP) 技术对预测进行了不确定性分析。模拟结果表明，所提出的 Semantic-MSL 优于传统方案，使用更少的数据点实现了 20% 的分类准确率提升，但训练能耗更低。]]></description>
      <guid>https://arxiv.org/abs/2409.12978</guid>
      <pubDate>Mon, 23 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>新的人与物交互数据集和 NVS 基准</title>
      <link>https://arxiv.org/abs/2409.12980</link>
      <description><![CDATA[arXiv:2409.12980v1 Announce Type: new 
摘要：近来，人物交互场景中的 NVS 受到越来越多的关注。现有的人物交互数据集主要由视图有限的静态数据组成，仅提供 RGB 图像或视频，大多包含单个人与物体之间的交互。此外，这些数据集在光照环境中表现出复杂性、同步性差和分辨率低，阻碍了高质量的人物交互研究。在本文中，我们介绍了一个新的人物交互数据集，该数据集包含 38 系列 30 视图多人或单人 RGB-D 视频序列，并附带相机参数、前景蒙版、SMPL 模型、一些点云和网格文件。视频序列由 30 个 Kinect Azure 捕获，均匀围绕场景，每个 4K 分辨率 25 FPS，持续时间为 1$\sim$19 秒。同时，我们在我们的数据集上评估了一些 SOTA NVS 模型以建立 NVS 基准。我们希望我们的工作能够启发人机交互的进一步研究。]]></description>
      <guid>https://arxiv.org/abs/2409.12980</guid>
      <pubDate>Mon, 23 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DNI：用于扩散视频编辑的稀释噪声初始化</title>
      <link>https://arxiv.org/abs/2409.13037</link>
      <description><![CDATA[arXiv:2409.13037v1 公告类型：新
摘要：基于文本的扩散视频编辑系统已成功执行高保真度和文本对齐的编辑。然而，这种成功仅限于刚性类型编辑，例如风格转换和对象叠加，同时保留输入视频的原始结构。这种限制源于扩散视频编辑系统中使用的初始潜在噪声。扩散视频编辑系统通过逐渐将高斯噪声注入输入视频来准备初始潜在噪声以进行编辑。然而，我们观察到输入视频的视觉结构仍然存在于这种初始潜在噪声中，从而限制了非刚性编辑，例如需要结构修改的运动变化。为此，本文提出了稀释噪声初始化 (DNI) 框架，使编辑系统能够执行精确和动态的修改，包括非刚性编辑。 DNI 引入了“噪声稀释”的概念，即在要编辑的区域中添加更多噪声，以软化输入视频所造成的结构僵化，从而实现更接近目标提示的有效编辑。大量实验证明了 DNI 框架的有效性。]]></description>
      <guid>https://arxiv.org/abs/2409.13037</guid>
      <pubDate>Mon, 23 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>TACE：肿瘤意识反事实解释</title>
      <link>https://arxiv.org/abs/2409.13045</link>
      <description><![CDATA[arXiv:2409.13045v1 公告类型：新
摘要：深度学习在医学成像中的应用显著提高了诊断能力，提高了准确性和效率。尽管有这些好处，但这些人工智能模型（通常被称为“黑匣子”）缺乏透明度，引发了人们对其在临床环境中的可靠性的担忧。可解释人工智能 (XAI) 旨在通过开发使人工智能决策易于理解和可信的方法来缓解这些担忧。在本研究中，我们提出了肿瘤感知反事实解释 (TACE)，这是一个旨在为医学图像生成可靠反事实解释的框架。与现有方法不同，TACE 专注于在不改变整体器官结构的情况下修改肿瘤特定特征，确保反事实的忠实性。我们通过在生成过程中添加一个额外的步骤来实现这一点，该步骤允许仅修改感兴趣的区域 (ROI)，从而产生更可靠的反事实，因为器官的其余部分保持不变。我们在乳房 X 线摄影图像和脑部 MRI 上评估了我们的方法。我们发现，我们的方法在质量、忠实度和反事实生成速度方面远远超过了现有的最先进技术。事实上，更忠实的解释可以显著提高分类成功率，乳腺癌的成功率提高了 10.69%，脑肿瘤的成功率提高了 98.02%。我们的工作代码可在 https://github.com/ispamm/TACE 上找到。]]></description>
      <guid>https://arxiv.org/abs/2409.13045</guid>
      <pubDate>Mon, 23 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>跨手性掌纹验证：右掌纹左为右</title>
      <link>https://arxiv.org/abs/2409.13056</link>
      <description><![CDATA[arXiv:2409.13056v1 公告类型：新
摘要：掌纹识别已成为一种重要的生物特征认证方法，因为它具有很强的辨别力和用户友好的特性。本文介绍了一种新颖的跨手性掌纹验证 (CCPV) 框架，挑战了传统掌纹验证系统中的传统观点。与通常需要存储左右掌纹的现有方法不同，我们的方法允许使用任一手掌进行验证，同时仅存储一个掌纹模板。我们的 CCPV 框架的核心在于精心设计的匹配规则。该规则涉及翻转图库和查询掌纹，并计算每对之间的平均距离作为最终匹配距离。这种方法有效地降低了匹配方差并增强了整体系统的稳健性。我们引入了一种新颖的跨手性损失函数来构建一个具有判别力和稳健性的跨手性特征空间。这种损失加强了四种掌纹变体的表示一致性：左、右、左翻转和右翻转。由此产生的紧凑特征空间，加上模型增强的判别表示能力，确保了在各种场景中均具有稳健的性能。我们进行了广泛的实验来验证我们提出的方法的有效性。评估涵盖了多个公共数据集，并考虑了闭集和开集设置。结果证明了 CCPV 框架的有效性，并突出了其在掌纹认证系统中的实际应用潜力。]]></description>
      <guid>https://arxiv.org/abs/2409.13056</guid>
      <pubDate>Mon, 23 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用深度学习根据面部动态特征实时估计明显注意力</title>
      <link>https://arxiv.org/abs/2409.13084</link>
      <description><![CDATA[arXiv:2409.13084v1 公告类型：新
摘要：学生在课堂上经常注意力不集中。有效的教师认识到这一点，并在必要时重新吸引他们的注意力。随着向远程学习的转变，教师失去了适应不同学生参与度所需的视觉反馈。我们建议使用现成的正面视频根据眼睛、头部和面部的运动推断注意力水平。我们训练一个深度学习模型来根据明显的眼球运动预测注意力的测量值。具体来说，我们在学生观看相同的教育视频时以十秒为间隔测量受试者间眼球运动的相关性。在 3 个不同的实验（N=83）中，我们表明，训练后的模型在看不见的数据上预测注意力的客观指标为 $R^2$=0.38，在看不见的受试者上预测注意力的客观指标为 $R^2$=0.26-0.30。深度网络主要依赖于学生的眼球运动，但在某种程度上也依赖于眉毛、脸颊和头部的运动。与眼部受试者间相关性相比，该模型可以根据学生个体的动作来估计注意力投入程度，而无需来自注意力集中组的参考数据。这使得在线应用的范围更加广泛。该解决方案轻量级，可以在客户端运行，从而减轻了与在线注意力监控相关的一些隐私问题。]]></description>
      <guid>https://arxiv.org/abs/2409.13084</guid>
      <pubDate>Mon, 23 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>对难以分类的动作进行可解释的动作识别</title>
      <link>https://arxiv.org/abs/2409.13091</link>
      <description><![CDATA[arXiv:2409.13091v1 公告类型：新
摘要：我们研究一种类似人类的可解释视频理解模型。人类通过识别明确识别的物体和部件之间的关键时空关系来识别视频中的复杂活动，例如，物体进入容器的孔径。为了模仿这一点，我们建立了一个模型，该模型使用物体和手的位置及其动作来识别正在发生的活动。为了改进这个模型，我们专注于三个最混乱的类别（对于这个模型），并确定缺乏 3D 信息是主要问题。为了解决这个问题，我们通过两种方式添加 3D 感知来扩展我们的基本模型：（1）对最先进的物体检测模型进行了微调，以确定“容器”和“非容器”之间的差异，以便将物体形状信息集成到现有的物体特征中。（2）使用最先进的深度估计模型提取单个物体的深度值并计算深度关系以扩展我们可解释模型使用的现有关系。我们的基本模型的这些 3D 扩展在 Something-Something-v2 数据集中对三个表面上相似的“放置”动作子集进行了评估。结果表明，容器检测器并没有提高性能，但深度关系的增加显著提高了性能。]]></description>
      <guid>https://arxiv.org/abs/2409.13091</guid>
      <pubDate>Mon, 23 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ERIC：利用商品门铃摄像头估算降雨量，实现住宅精准灌溉</title>
      <link>https://arxiv.org/abs/2409.13104</link>
      <description><![CDATA[arXiv:2409.13104v1 公告类型：新
摘要：当前最先进的住宅灌溉系统（例如 WaterMyYard）依靠附近气象站的降雨数据来调整灌溉量。然而，降雨数据的准确性受到雨量计有限的空间分辨率和超本地降雨的显著变化的影响，导致大量的水浪费。为了提高灌溉效率，我们开发了一种经济高效的灌溉系统，称为 ERIC，它采用机器学习模型从商品门铃摄像头镜头中估算降雨量，并在没有人工干预的情况下优化灌溉计划。具体来说，我们：a) 使用轻量级神经网络模型设计了新颖的视觉和音频功能，以从边缘摄像头推断降雨量，保护用户隐私；b) 在 Raspberry Pi 4 上构建了一个完整的端到端灌溉系统，成本仅为 75 美元。我们在五个地点部署了该系统（收集了超过 750 小时的视频），背景和光照条件各不相同。综合评估验证了 ERIC 实现了最先进的降雨量估算性能（~5 毫米/天），每月节省 9,112 加仑的水，相当于每月节省 28.56 美元的水电费。]]></description>
      <guid>https://arxiv.org/abs/2409.13104</guid>
      <pubDate>Mon, 23 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>UL-VIO：具有抗噪测试时间自适应能力的超轻量级视觉惯性里程计</title>
      <link>https://arxiv.org/abs/2409.13106</link>
      <description><![CDATA[arXiv:2409.13106v1 公告类型：新
摘要：数据驱动的视觉惯性里程计 (VIO) 因其性能而备受关注，因为 VIO 是自主机器人的重要组成部分。然而，它们在资源受限的设备上部署并非易事，因为大型网络参数应该容纳在设备内存中。此外，由于测试时环境分布的变化，这些网络在部署后可能会出现故障。鉴于此，我们提出了 UL-VIO——一种超轻量级（&lt;1M）VIO 网络，能够基于视觉惯性一致性进行测试时间自适应 (TTA)。具体来说，我们对网络执行模型压缩，同时保留低级编码器部分，包括所有 BatchNorm 参数，以实现资源高效的测试时间自适应。它实现了比最先进的网络规模小 36 倍的网络规模，错误率仅增加了一分钟——在 KITTI 数据集上为 1%。对于测试时自适应，我们建议使用惯性参考网络输出作为伪标签，并更新 BatchNorm 参数以实现轻量级但有效的自适应。据我们所知，这是首次在 VIO 上执行抗噪 TTA 的工作。在 KITTI、EuRoC 和 Marulan 数据集上的实验结果证明了我们的资源高效自适应方法在具有动态域转移的各种 TTA 场景下的有效性。]]></description>
      <guid>https://arxiv.org/abs/2409.13106</guid>
      <pubDate>Mon, 23 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉和深度学习技术在分析混合建筑和拆除废物方面的发展与挑战</title>
      <link>https://arxiv.org/abs/2409.13112</link>
      <description><![CDATA[arXiv:2409.13112v1 公告类型：新
摘要：提高对建筑和拆除废物 (C&amp;DW) 成分的自动和及时识别对于提高业务回报、经济成果和可持续性至关重要。计算机视觉、人工智能 (AI)、机器人和物联网 (IoT) 等技术越来越多地融入废物处理以实现这些目标。虽然深度学习 (DL) 模型在识别同质 C&amp;DW 堆方面显示出良好的前景，但很少有研究评估它们在商业环境中对混合、高度污染材料的性能。凭借在澳大利亚悉尼的 C&amp;DW 材料回收设施 (MRF) 的丰富经验，我们探讨了开发先进的自动化混合 C&amp;DW 管理系统的挑战和机遇。我们首先概述建筑行业废物管理的发展，重点介绍其对环境、经济和社会的影响。我们回顾了各种 C&amp;DW 分析技术，得出结论：基于 DL 的视觉方法是最佳解决方案。此外，我们还研究了 C&amp;DW 分析的传感器和相机技术的发展，以及专注于物体检测和材料分割的 DL 算法的演变。我们还讨论了 C&amp;DW 数据集、它们的整理和创建它们的创新方法。最后，我们分享了关于 C&amp;DW 视觉分析的见解，解决了技术和商业挑战、研究趋势以及混合 C&amp;DW 分析的未来方向。本文旨在通过为这一关键领域正在进行和未来的研究和开发工作提供宝贵的见解来提高 C&amp;DW 管理的效率。]]></description>
      <guid>https://arxiv.org/abs/2409.13112</guid>
      <pubDate>Mon, 23 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>BGDB：具有改进的去噪扩散概率模型的伯努利高斯决策块</title>
      <link>https://arxiv.org/abs/2409.13116</link>
      <description><![CDATA[arXiv:2409.13116v1 公告类型：新
摘要：生成模型可以通过构建复杂的特征空间来增强判别分类器，从而提高复杂数据集的性能。传统方法通常使用更详细的特征表示来扩充数据集或增加维度以使非线性数据线性可分。仅将生成模型用于特征空间处理无法充分发挥其在分类器中的潜力，并且通常缺乏坚实的理论基础。我们的方法基于一个新假设：从单个模型训练中获得的概率信息（logit）可用于生成多个训练课程的等效信息。利用中心极限定理，预计这种合成的概率信息将更准确地收敛到真实概率。为了实现这一目标，我们提出了伯努利-高斯决策块（BGDB），这是一个受中心极限定理和多次伯努利试验的平均值近似于单次试验成功概率的概念启发的新模块。具体来说，我们利用改进的去噪扩散概率模型 (IDDPM) 来模拟伯努利试验的概率。我们的方法将重点从重建特征转移到重建逻辑，将逻辑从单次迭代转换为类似于来自多个实验的逻辑。我们通过数学分析为我们的方法提供理论基础，并通过使用各种数据集对多个成像任务（包括分类和分割）进行实验评估来验证其有效性。]]></description>
      <guid>https://arxiv.org/abs/2409.13116</guid>
      <pubDate>Mon, 23 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过重新标记蒸馏来解释深度网络的预测</title>
      <link>https://arxiv.org/abs/2409.13137</link>
      <description><![CDATA[arXiv:2409.13137v1 公告类型：新
摘要：解释黑盒深度网络的预测可以提高其部署的可靠性。在这项工作中，我们提出了一种重新标记蒸馏方法，以自我监督的方式学习从输入到预测的直接映射。将图像投影到 VAE 子空间中，通过随机扰动其潜在向量来生成一些合成图像。然后，通过识别这些合成图像的标签是否发生偏移，可以将它们注释为两个类之一。之后，使用深度网络注释的标签作为老师，训练一个线性学生模型，通过将这些合成图像映射到类别来近似注释。通过这种方式，这些重新标记的合成图像可以很好地描述深度网络的局部分类机制，而学习到的学生可以为预测提供更直观的解释。大量实验定性和定量验证了我们方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2409.13137</guid>
      <pubDate>Mon, 23 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>UniTabNet：桥接视觉和语言模型以增强表格结构识别</title>
      <link>https://arxiv.org/abs/2409.13148</link>
      <description><![CDATA[arXiv:2409.13148v1 公告类型：新
摘要：在数字时代，表格结构识别技术是处理和分析大量表格数据的重要工具。以前的方法主要关注表格结构恢复的视觉方面，但往往无法有效理解表格中的文本语义，尤其是描述性文本单元。在本文中，我们介绍了UniTabNet，这是一种基于图像到文本模型的表格结构解析新框架。UniTabNet采用“分而治之”的策略，利用图像到文本模型解耦表格单元，并集成物理和逻辑解码器来重建完整的表格结构。我们通过Vision Guider进一步增强了我们的框架，它将模型的注意力引导到相关区域，从而提高预测准确性。此外，我们引入了Language Guider来改进模型理解表格图像中文本语义的能力。在 PubTabNet、PubTables1M、WTW 和 iFLYTAB 等著名表格结构数据集上进行评估后，UniTabNet 取得了新的最佳性能，证明了我们方法的有效性。代码也将公开发布。]]></description>
      <guid>https://arxiv.org/abs/2409.13148</guid>
      <pubDate>Mon, 23 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>