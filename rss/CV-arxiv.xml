<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Mon, 19 Aug 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>通过选择性对抗扰动实现视觉友好概念保护</title>
      <link>https://arxiv.org/abs/2408.08518</link>
      <description><![CDATA[arXiv:2408.08518v1 公告类型：新
摘要：通过使用少量图像调整扩散模型来生成个性化概念，这引发了有关隐私和知识产权的潜在法律和道德问题。研究人员试图使用对抗性扰动来防止恶意个性化。然而，以前的努力主要集中在保护的有效性上，而忽略了扰动的可见性。他们利用全局对抗性扰动，这会对原始图像造成明显的改变并显著降低视觉质量。在这项工作中，我们提出了视觉友好概念保护 (VCPro) 框架，该框架通过可感知性较低的对抗性扰动优先保护图像所有者选择的关键概念。为了确保这些扰动尽可能不显眼，我们引入了一个宽松的优化目标来识别最不可感知但有效的对抗性扰动，并使用拉格朗日乘数法解决。定性和定量实验验证了 VCPro 在扰动的可见性和保护有效性之间实现了更好的权衡，有效地优先保护扰动较少的图像中的目标概念。]]></description>
      <guid>https://arxiv.org/abs/2408.08518</guid>
      <pubDate>Mon, 19 Aug 2024 06:18:26 GMT</pubDate>
    </item>
    <item>
      <title>GS-ID：通过扩散先验和参数光源优化实现高斯散射光照分解</title>
      <link>https://arxiv.org/abs/2408.08524</link>
      <description><![CDATA[arXiv:2408.08524v1 公告类型：新
摘要：我们提出了 GS-ID，一种用于高斯溅射照明分解的新型框架，实现了逼真的新型视图合成和直观的光线编辑。照明分解是一个不适定问题，面临三个主要挑战：1) 几何和材料的先验通常缺乏；2) 复杂的照明条件涉及多个未知光源；3) 计算具有大量光源的表面着色计算成本高昂。为了应对这些挑战，我们首先引入内在扩散先验来估计基于物理的渲染的属性。然后我们将照明分为环境和直接分量以进行联合优化。最后，我们采用延迟渲染来减少计算负荷。我们的框架使用可学习的环境图和球面高斯 (SG) 以参数化方式表示光源，从而实现高斯溅射上的可控和逼真的重新照明。大量实验和应用表明，GS-ID 产生最先进的光照分解结果，同时实现更好的几何重建和渲染性能。]]></description>
      <guid>https://arxiv.org/abs/2408.08524</guid>
      <pubDate>Mon, 19 Aug 2024 06:18:26 GMT</pubDate>
    </item>
    <item>
      <title>通过扩散模型的功能聚合实现复杂的图像编辑</title>
      <link>https://arxiv.org/abs/2408.08495</link>
      <description><![CDATA[arXiv:2408.08495v1 公告类型：新
摘要：扩散模型在生成任务中表现出色，使其成为图像编辑的理想候选者。最近的研究强调了它们通过遵循文本指令有效地应用所需编辑的能力，但仍存在两个关键挑战。首先，这些模型难以同时应用多个编辑，由于它们依赖于顺序处理，导致计算效率低下。其次，依靠文本提示来确定编辑区域可能会导致图像其他部分的意外更改。在这项工作中，我们引入了 FunEditor，这是一种高效的扩散模型，旨在学习原子编辑功能并通过聚合更简单的功能来执行复杂的编辑。这种方法通过聚合多个函数并将它们同时应用于特定区域来实现复杂的编辑任务，例如对象移动。在对象移动等复杂任务上，FunEditor 的推理速度比现有方法快 5 到 24 倍。我们的实验表明，FunEditor 在图像质量评估 (IQA) 和对象背景一致性等各项指标上都明显优于最近的基线，包括推理时间优化方法和微调模型。]]></description>
      <guid>https://arxiv.org/abs/2408.08495</guid>
      <pubDate>Mon, 19 Aug 2024 06:18:25 GMT</pubDate>
    </item>
    <item>
      <title>CoSEC：用于自动驾驶的同轴立体事件摄像机数据集</title>
      <link>https://arxiv.org/abs/2408.08500</link>
      <description><![CDATA[arXiv:2408.08500v1 公告类型：新 
摘要：传统帧相机是自动驾驶场景感知的主流传感器，但在弱光等不利条件下性能受限。高动态范围的事件相机已被用于辅助帧相机进行多模态融合，这在很大程度上依赖于各种模态之间的像素级空间对齐。通常，现有的多模态数据集主要将事件和帧相机平行放置，并通过扭曲操作直接在空间上对齐它们。然而，这种并行策略对于多模态融合效果较差，因为由于事件帧基线较大，较大的视差加剧了空间错位。我们认为基线最小化可以减少事件和帧相机之间的对齐误差。在这项工作中，我们引入混合同轴事件帧设备来构建多模态系统，并提出了一种用于自动驾驶的同轴立体事件相机 (CoSEC) 数据集。对于多模态系统，我们首先利用微控制器实现时间同步，然后对不同的传感器进行空间校准，其中我们对立体同轴设备进行内部和外部校准。对于多模态数据集，我们利用参考深度过滤 LiDAR 点云以生成深度和光流标签，并通过在夜间条件下融合对齐的事件和帧数据进一步改进。借助同轴设备，所提出的数据集可以促进全天候像素级多模态融合。此外，我们还进行了实验，以证明所提出的数据集可以提高多模态融合的性能和泛化能力。]]></description>
      <guid>https://arxiv.org/abs/2408.08500</guid>
      <pubDate>Mon, 19 Aug 2024 06:18:25 GMT</pubDate>
    </item>
    <item>
      <title>具有对抗鲁棒性的高效图像到图像扩散分类器</title>
      <link>https://arxiv.org/abs/2408.08502</link>
      <description><![CDATA[arXiv:2408.08502v1 公告类型：新
摘要：扩散模型 (DM) 在对抗鲁棒性领域表现出巨大潜力，基于 DM 的防御方法无需对抗训练即可实现卓越的防御能力。然而，由于使用大规模预训练的 DM，它们都需要巨大的计算成本，因此很难在强攻击下进行全面评估并与传统的基于 CNN 的方法进行比较。简单地减少 DM 中的网络大小和时间步长可能会严重损害图像生成质量，从而使以前的框架失效。为了缓解这个问题，我们重新设计了扩散框架，从生成高质量图像到预测可区分的图像标签。具体来说，我们采用图像转换框架来学习从输入样本到设计的正交图像标签的多对一映射。基于这个框架，我们引入了一个高效的图像到图像扩散分类器，它具有修剪的 U-Net 结构和减少的扩散时间步长。除了框架之外，我们还重新设计了 DM 的优化目标以适应图像分类的目标，其中在基于 DM 的图像转换框架中加入了新的分类损失，以区分生成的标签与其他类别的标签。我们在流行基准的各种攻击下对所提出的分类器进行了充分的评估。大量实验表明，与基于 DM 和基于 CNN 的方法相比，我们的方法以更少的计算成本实现了更好的对抗鲁棒性。代码可在 https://github.com/hfmei/IDC 获得。]]></description>
      <guid>https://arxiv.org/abs/2408.08502</guid>
      <pubDate>Mon, 19 Aug 2024 06:18:25 GMT</pubDate>
    </item>
    <item>
      <title>超越统一查询分布：键驱动的分组查询注意</title>
      <link>https://arxiv.org/abs/2408.08454</link>
      <description><![CDATA[arXiv:2408.08454v1 公告类型：新
摘要：Transformer 架构通过其自注意力机制彻底改变了深度学习，该机制可以有效地捕获上下文信息。然而，自注意力的内存占用对长序列任务提出了重大挑战。分组查询注意 (GQA) 通过对查询进行分组并对相应的键值头进行均值池化来解决此问题 - 以灵活的方式减少总体参数数量和内存需求，而不会对模型准确性造成不利影响。在这项工作中，我们引入了对 GQA 的增强，重点关注两种偏离分组静态性质的新方法：键分布式 GQA (KDGQA) 和动态键分布式 GQA (DGQA)，它们利用来自键头规范的信息来通知查询分配。具体而言，KDGQA 在每次前向传递过程中查看关键头的范数比率，而 DGQA 检查范数在训练过程中演变时的比率。此外，我们以扰动 GQA (PGQA) 为例进行案例研究，它通过从注意力图中减去噪声来引入 (静态) 组形成的可变性。我们使用经过训练的 Vision Transformers 在 CIFAR-10、CIFAR-100、Food101 和 Tiny ImageNet 等数据集上进行图像分类的实验证明了这些变体有望通过更明智和自适应的分组机制改进原始 GQA：具体来说，与 GQA 和其他变体相比，ViT-L 在使用 DGQA 时，准确率提高了 8%。我们进一步分析了键值头的数量对性能的影响，强调了利用查询键亲和性的重要性。]]></description>
      <guid>https://arxiv.org/abs/2408.08454</guid>
      <pubDate>Mon, 19 Aug 2024 06:18:24 GMT</pubDate>
    </item>
    <item>
      <title>TEXTOC：文本驱动的以对象为中心的风格转换</title>
      <link>https://arxiv.org/abs/2408.08461</link>
      <description><![CDATA[arXiv:2408.08461v1 公告类型：新
摘要：我们提出了文本驱动的以对象为中心的风格转移 (TEXTOC)，这是一种使用文本输入在以对象为中心的级别引导风格转​​移的新方法。TEXTOC 的核心是我们的 Patch-wise Co-Directional (PCD) 损失，它经过精心设计，可实现与输入文本紧密一致的精确以对象为中心的转换。此损失结合了用于文本引导风格方向的补丁方向性损失和用于跨对象区域均匀 CLIP 嵌入分布的补丁分布一致性损失。它确保跨对象区域的无缝和谐风格转移。我们方法的关键是文本匹配补丁选择 (TMPS) 和预固定区域选择 (PRS) 模块，用于通过文本识别对象位置，从而无需分割蒙版。最后，我们引入了自适应背景保留 (ABP) 损失来保持图像背景的原始风格和结构本质。此损失应用于动态识别的背景区域。大量实验证明了我们的方法在创建视觉连贯和文本对齐的风格转换方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2408.08461</guid>
      <pubDate>Mon, 19 Aug 2024 06:18:24 GMT</pubDate>
    </item>
    <item>
      <title>PQV-Mobile：用于优化移动应用视觉变换器的组合剪枝和量化工具包</title>
      <link>https://arxiv.org/abs/2408.08437</link>
      <description><![CDATA[arXiv:2408.08437v1 公告类型：新
摘要：虽然视觉变换器 (ViT) 在计算机视觉任务中非常有效，并且正在取代卷积神经网络成为新的最先进技术，但它们是复杂且内存密集型的模型。为了在资源受限的移动/边缘系统上有效地运行这些模型，不仅需要压缩这些模型，还需要优化它们并将它们转换为易于部署的格式。为此，本文提出了一种组合修剪和量化工具，称为 PQV-Mobile，用于优化移动应用程序的视觉变换器。该工具能够支持基于量级重要性、泰勒重要性和 Hessian 重要性的不同类型的结构化修剪。它还支持从 FP32 到 FP16 和 int8 的量化，针对不同的移动硬件后端。我们展示了我们工具的功能，并展示了使用 Facebook 数据高效图像转换器 (DeiT) 模型进行不同程度的修剪和 int8 量化时重要的延迟-内存-准确度权衡。我们的结果表明，即使将 DeiT 模型修剪 9.375% 并将其从 FP32 量化为 int8，然后针对移动应用程序进行优化，我们也发现延迟减少了 7.18 倍，准确度损失了 2.24%。该工具是开源的。]]></description>
      <guid>https://arxiv.org/abs/2408.08437</guid>
      <pubDate>Mon, 19 Aug 2024 06:18:23 GMT</pubDate>
    </item>
    <item>
      <title>SpectralEarth：大规模训练高光谱基础模型</title>
      <link>https://arxiv.org/abs/2408.08447</link>
      <description><![CDATA[arXiv:2408.08447v1 公告类型：新
摘要：基础模型引发了计算机视觉领域的范式转变，并越来越多地被应用于遥感，尤其是多光谱图像。然而，由于缺乏全面且具有全球代表性的高光谱数据集，它们在高光谱成像 (HSI) 中的潜力仍未得到开发。为了弥补这一差距，我们引入了 SpectralEarth，这是一个大规模多时间数据集，旨在利用环境测绘和分析计划 (EnMAP) 的数据预训练高光谱基础模型。SpectralEarth 包含 538,974 个图像块，覆盖 415,153 个独特位置，这些位置来自 11,636 多个全球分布的 EnMAP 场景，跨越两年的存档。此外，这些位置中有 17.5% 包含多个时间戳，从而实现多时间 HSI 分析。利用最先进的自监督学习 (SSL) 算法，我们在 SpectralEarth 上预训练了一系列基础模型。我们将光谱适配器集成到经典视觉主干中，以适应 HSI 的独特特征。同时，我们构建了四个下游数据集，用于土地覆盖和作物类型制图，为模型评估提供基准。实验结果支持我们模型的多功能性，展示了它们在不同任务和传感器中的通用性。我们还强调了模型微调过程中的计算效率。数据集、模型和源代码将公开提供。]]></description>
      <guid>https://arxiv.org/abs/2408.08447</guid>
      <pubDate>Mon, 19 Aug 2024 06:18:23 GMT</pubDate>
    </item>
    <item>
      <title>提升教程质量：用于游戏教程质量评估的 VLM</title>
      <link>https://arxiv.org/abs/2408.08396</link>
      <description><![CDATA[arXiv:2408.08396v1 公告类型：新
摘要：设计有效的游戏教程对于新玩家的平滑学习曲线至关重要，尤其是在具有许多规则和复杂核心机制的游戏中。评估这些教程的有效性通常需要与对游戏没有任何先验知识的测试人员进行多次迭代。最近的视觉语言模型 (VLM) 已展示出理解和解释视觉内容的强大能力。VLM 可以分析图像、提供详细的见解并回答有关其内容的问题。它们可以识别视觉数据中的对象、动作和上下文，使其成为各种应用（包括自动游戏测试）的宝贵工具。在这项工作中，我们提出了一种自动化游戏测试解决方案来评估游戏教程的质量。我们的方法利用 VLM 分析视频游戏教程中的帧，回答相关问题以模拟人类感知，并提供反馈。将此反馈与预期结果进行比较，以识别令人困惑或有问题的场景并为开发人员突出显示潜在错误。此外，我们发布了测试中使用的不同游戏版本的完整教程视频和带注释的帧。此解决方案减少了大量手动测试的需要，尤其是通过加快和简化教程的初始开发阶段来改善最终的游戏体验。]]></description>
      <guid>https://arxiv.org/abs/2408.08396</guid>
      <pubDate>Mon, 19 Aug 2024 06:18:22 GMT</pubDate>
    </item>
    <item>
      <title>在 Deepfake 检测中因小失大</title>
      <link>https://arxiv.org/abs/2408.08412</link>
      <description><![CDATA[arXiv:2408.08412v1 公告类型：新
摘要：深度伪造技术的传播引发了人们对其在各个领域可能被滥用的严重担忧，这促使人们迫切需要强大的检测方法。尽管取得了进展，但许多当前的方法优先考虑短期收益而牺牲了长期有效性。本文批评了在单个深度伪造数据集上仅使用小事一桩的目标对预训练模型进行微调的过于专业化的方法，而忽略了泛化和知识保留的磅数平衡。为了解决这个“小事一桩愚蠢”的问题，我们提出了一种新颖的学习框架 (PoundNet)，用于在预训练的视觉语言模型上泛化深度伪造检测。PoundNet 结合了可学习的提示设计和平衡的目标，以保留上游任务（对象分类）的广泛知识，同时增强下游任务（深度伪造检测）的泛化。我们按照文献中的常见做法，在标准的单个深度伪造数据集上训练 PoundNet。然后，我们利用 5 个主要评估指标在 10 个公开的大型深度伪造数据集上评估其性能，据我们所知，这构成了评估深度伪造检测模型泛化能力的最大基准测试集。全面的基准评估表明，所提出的 PoundNet 明显减少了“小事聪明大事糊涂”，与最先进的方法相比，深度伪造检测性能显著提高了 19%，同时在对象分类任务上保持了 63% 的强劲性能，而其他深度伪造检测模型往往无效。代码和数据在 https://github.com/iamwangyabin/PoundNet 上开源。]]></description>
      <guid>https://arxiv.org/abs/2408.08412</guid>
      <pubDate>Mon, 19 Aug 2024 06:18:22 GMT</pubDate>
    </item>
    <item>
      <title>预处理和压缩：通过内在维度理解跨成像域的隐藏表示细化</title>
      <link>https://arxiv.org/abs/2408.08381</link>
      <description><![CDATA[arXiv:2408.08381v1 公告类型：新
摘要：近年来，人们对神经网络隐藏表示的几何属性（例如内在维度 (ID)）如何在其层中演变感兴趣，以及这些属性如何预测重要的模型行为（例如泛化能力）。然而，有证据表明，这种行为可能会根据网络训练数据的领域（例如自然图像与医学图像）而发生显着变化。在这里，我们通过探索网络学习表示的 ID 如何在其层中演变来进一步探究，本质上是描述网络如何连续细化用于预测的输入数据的信息内容。通过分析六种网络架构中的 11 个自然和医学图像数据集，我们发现自然图像模型和医学图像模型之间的 ID 演化曲线形状明显不同：医学图像模型在网络中的表示 ID 更早达到峰值，这意味着通常用于这些领域下游任务的图像特征及其抽象性存在差异。此外，我们发现峰值表示 ID 与其输入空间中数据的 ID 之间存在很强的相关性，这意味着模型学习到的表示的内在信息内容受其训练数据的信息内容引导。总体而言，我们的研究结果强调了自然和非自然成像领域在隐藏表示信息内容方面的网络行为存在显著差异，并进一步深入了解了网络学习到的特征如何由其训练数据塑造。]]></description>
      <guid>https://arxiv.org/abs/2408.08381</guid>
      <pubDate>Mon, 19 Aug 2024 06:18:21 GMT</pubDate>
    </item>
    <item>
      <title>TurboEdit：即时基于文本的图像编辑</title>
      <link>https://arxiv.org/abs/2408.08332</link>
      <description><![CDATA[arXiv:2408.08332v1 公告类型：新
摘要：我们在几步扩散模型的背景下解决了精确图像反转和解缠结图像编辑的挑战。我们引入了一种基于编码器的迭代反演技术。反演网络以输入图像和上一步的重建图像为条件，允许对输入图像进行下一步重建校正。我们证明，通过以（自动生成的）详细文本提示为条件，可以在几步扩散模型中轻松实现解缠结控制。为了操纵反转图像，我们冻结噪声图并修改文本提示中的一个属性（手动或通过 LLM 驱动的基于指令的编辑），从而生成与输入图像相似的新图像，但只有一个属性发生了变化。它可以进一步控制编辑强度并接受指导性文本提示。我们的方法可以实时实现逼真的文本引导图像编辑，反转时仅需 8 次函数评估 (NFE)（一次性成本），每次编辑仅需 4 次 NFE。我们的方法不仅速度快，而且性能远超最先进的多步扩散编辑技术。]]></description>
      <guid>https://arxiv.org/abs/2408.08332</guid>
      <pubDate>Mon, 19 Aug 2024 06:18:20 GMT</pubDate>
    </item>
    <item>
      <title>5%>100%：打破视觉识别任务全面微调的性能桎梏</title>
      <link>https://arxiv.org/abs/2408.08345</link>
      <description><![CDATA[arXiv:2408.08345v1 公告类型：新
摘要：预训练和微调可以提高视觉任务的传输效率和性能。最近的增量调整方法为视觉分类任务提供了更多选择。尽管取得了成功，但现有的视觉增量调整技术未能超越完全微调在物体检测和分割等具有挑战性的任务上的上限。为了找到完全微调的竞争替代方案，我们提出了一种基于适配器的新型调整方法，即多认知视觉适配器 (Mona) 调整。首先，我们在适配器中引入了多个视觉友好的过滤器，以增强其处理视觉信号的能力，而以前的方法主要依赖于语言友好的线性过滤器。其次，我们在适配器中添加了缩放规范化层来调节视觉过滤器的输入特征分布。为了充分展示 Mona 的实用性和通用性，我们在多个代表性视觉任务上进行了实验，包括 COCO 上的实例分割、ADE20K 上的语义分割、Pascal VOC 上的对象检测、DOTA/STAR 上的定向对象检测以及三个常见数据集上的图像分类。令人兴奋的结果表明，Mona 在所有这些任务上都超越了完全微调，并且是唯一一种在上述各种任务上表现优于完全微调的增量微调方法。例如，与完全微调相比，Mona 在 COCO 数据集上实现了 1% 的性能提升。综合结果表明，与完全微调相比，Mona 微调更适合保留和利用预训练模型的功能。我们将公开提供代码。]]></description>
      <guid>https://arxiv.org/abs/2408.08345</guid>
      <pubDate>Mon, 19 Aug 2024 06:18:20 GMT</pubDate>
    </item>
    <item>
      <title>对视频进行任何分段：系统调查</title>
      <link>https://arxiv.org/abs/2408.08315</link>
      <description><![CDATA[arXiv:2408.08315v1 公告类型：新
摘要：最近一波基础模型在计算机视觉 (CV) 及其他领域取得了巨大成功，其中分割任何模型 (SAM) 激发了人们对探索任务无关的视觉基础模型的热情。凭借其卓越的零样本泛化能力，SAM 目前正在挑战 CV 中的许多传统范式，不仅在各种图像分割和多模态分割（例如，文本到蒙版）任务中，而且在视频领域也表现出色。此外，最新发布的 SAM 2 再次激发了图像和视频可提示视觉分割领域的研究热情。然而，现有的调查主要集中在各种图像处理任务中的 SAM，对视频领域的全面深入的审查明显不足。为了弥补这一差距，这项工作对基础模型时代的视频 SAM 进行了系统回顾。作为首次回顾视频 SAM 进展的研究，本文重点关注其在各种任务中的应用，讨论其最新进展以及在广泛应用上开发基础模型的创新机会。我们首先简要介绍 SAM 和视频相关研究领域的背景。随后，我们提出一个系统的分类法，将现有方法分为三个关键领域：视频理解、视频生成和视频编辑，分析和总结它们的优点和局限性。此外，我们还提供了基于 SAM 和当前最先进方法在代表性基准上的比较结果以及富有洞察力的分析。最后，我们讨论了当前研究面临的挑战，并展望了视频 SAM 领域及其他领域未来的几个研究方向。]]></description>
      <guid>https://arxiv.org/abs/2408.08315</guid>
      <pubDate>Mon, 19 Aug 2024 06:18:19 GMT</pubDate>
    </item>
    </channel>
</rss>