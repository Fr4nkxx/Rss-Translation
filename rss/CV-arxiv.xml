<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>CS.CV更新arxiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.cv更新arxiv.org e-print存档。</description>
    <lastBuildDate>Wed, 09 Apr 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>GARF：学习可概括的3D重新组装现实世界裂缝</title>
      <link>https://arxiv.org/abs/2504.05400</link>
      <description><![CDATA[ARXIV：2504.05400V1公告类型：新 
摘要：3D重新组装是一项具有挑战性的空间智能任务，具有跨科学领域的广泛应用。尽管大规模合成数据集助长了有希望的基于学习的方法，但它们对不同领域的推广性是有限的。至关重要的是，在合成数据集中训练的模型是否可以推广到破裂模式更为复杂的现实裂缝。为了弥合这一差距，我们提出了Garf，这是一个可推广的3D重组框架，用于现实世界中的骨折。 GARF利用裂缝意识预处理以从单个碎片中学习断裂特征，流动匹配使精确的6-DOF比对。在推理时，我们引入了一步的预组件，改善了对看不见的物体和不同裂缝数量的稳健性。我们与考古学家，古人类学家和鸟类学家合作，我们策划了Fractura，这是一个多元化的视觉和学习社区数据集，以跨陶瓷，骨头，蛋壳和岩性的现实世界裂缝类型为特色。全面的实验表明，我们的方法始终优于合成数据集和现实世界数据集的最先进方法，达到82.87 \％降低旋转误差和25.15 \％的零件精度。这阐明了培训合成数据，以推动现实世界中的3D拼图解决，这表明了其在看不见的物体形状和各种断裂类型中的强烈概括。]]></description>
      <guid>https://arxiv.org/abs/2504.05400</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于残差扩散的时间自适应视频框架插值</title>
      <link>https://arxiv.org/abs/2504.05402</link>
      <description><![CDATA[ARXIV：2504.05402V1公告类型：新 
摘要：在这项工作中，我们在传统的手工动画的背景下，提出了一种基于基于扩散的视频框架插值方法（VFI）。我们介绍了三个主要贡献：第一个是我们在模型中明确处理插值时间，在训练过程中我们也重新估算了与自然视频相比，在动画域中观察到的特别差异；第二是，我们适应并概括了最近在超分辨率社区中提出的称为Resshift的扩散方案，该方案使我们能够执行非常少数的扩散步骤（以10顺序）来产生我们的估计值；第三个是我们利用扩散过程的随机性质来提供插值框架上不确定性的像素估计，这对于预测模型可能是错误的位置可能很有用。我们提供了与最先进模型的广泛比较，并表明我们的模型在动画视频上的表现优于这些模型。]]></description>
      <guid>https://arxiv.org/abs/2504.05402</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>EP-DIFFUSER：通过多项式表示的流量场景生成和预测的有效扩散模型</title>
      <link>https://arxiv.org/abs/2504.05422</link>
      <description><![CDATA[ARXIV：2504.05422V1公告类型：新 
摘要：随着预测范围的增加，由于代理运动的多模式性质，预测交通场景的未来演变变得越来越困难。大多数最先进的（SOTA）预测模型主要集中于预测最可能的未来。但是，对于自动驾驶汽车的安全运行，涵盖合理运动替代方案的分布同样重要。为了解决这个问题，我们介绍了EP-Diffuser，这是一种新型的基于参数效率扩散的生成模型，旨在捕获可能的流量场景演变的分布。以道路布局和代理历史记录为条件，我们的模型充当预测指标，并产生了多样的，合理的场景连续性。我们根据Argoverse 2数据集对预测的准确性和合理性，对两个SOTA模型进行基准测试。尽管模型大小明显较小，但我们的方法既可以实现高度准确又合理的交通现场预测。我们进一步评估了使用Waymo打开数据集的分布（OOD）测试设置中的模型泛化能力，并显示出我们方法的较高鲁棒性。代码和模型检查点可以在此处找到：https：//github.com/continental/ep-diffuser。]]></description>
      <guid>https://arxiv.org/abs/2504.05422</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>深入学习图像注册中的生物力学约束同化：应用于滑动和局部刚性变形</title>
      <link>https://arxiv.org/abs/2504.05444</link>
      <description><![CDATA[ARXIV：2504.05444V1公告类型：新 
摘要：医疗图像注册中的正则化策略通常通过在整个图像域中施加统一的约束来采用一定大小的方法。然而，生物结构并非规律。缺乏结构意识，这些策略可能未能考虑到众多空间不均匀变形的特性，这将忠实地解释软组织和硬组织的生物力学，尤其是在不良对比的结构中。
  为了弥合这一差距，我们提出了一种基于学习的图像注册方法，其中推断的变形属性可以在本地适应训练有素的生物力学特征。具体而言，我们首先在训练过程中强制执行局部刚性位移，剪切运动或伪弹性变形，使用固体力学领域启发的正则化损失。然后，我们在合成和真实的3D胸部和腹部图像上表明，当推断新图像对之间的变形时，不同性质的这些机械性能得到了充分的普遍性。我们的方法使神经网络能够直接从输入图像中推断组织特异性变形模式，从而确保机械合理的运动。这些网络可在硬组织中保留刚性，同时允许在组织自然分离，更忠实地捕获生理运动的区域中受控滑动。该代码可在https://github.com/kheil-z/biomechanical_dlir上公开获得。]]></description>
      <guid>https://arxiv.org/abs/2504.05444</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过课程知识蒸馏，在极端观点下的学习活动视图不变变化</title>
      <link>https://arxiv.org/abs/2504.05451</link>
      <description><![CDATA[ARXIV：2504.05451V1公告类型：新 
摘要：从视频中进行视频学习的传统方法依赖于以最小的场景混乱为由的多视图设置。但是，他们在野外视频中挣扎，这些视频表现出极端的观点差异，几乎没有视觉内容。我们介绍了一种在存在如此严重的观看式估计的情况下学习丰富视频表示的方法。我们首先定义一个基于几何的度量标准，该指标将其可能的闭塞水平以细粒度的时间尺度排名。然后，使用这些排名，我们制定了一个知识蒸馏目标，该目标可以通过新颖的课程学习程序来保留以动作为中心的语义，随着时间的流逝，将逐渐具有挑战性的观点逐渐具有挑战性，从而使平稳适应极端的观点差异。我们在两项任务上评估了我们的方法，在时间密钥步骤接地和细粒度的密钥识别基准上都优于SOTA模型，尤其是在表现出严重遮挡的观点上。]]></description>
      <guid>https://arxiv.org/abs/2504.05451</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有有限数据的生成对抗网络：调查和基准测试</title>
      <link>https://arxiv.org/abs/2504.05456</link>
      <description><![CDATA[ARXIV：2504.05456V1公告类型：新 
摘要：生成对抗网络（GAN）在各种图像综合任务中显示出令人印象深刻的结果。大量的研究表明，与其他生成模型及其潜在空间相比，GAN在功能和表达学习方面更强大，可以编码丰富的语义信息。但是，甘恩斯的巨大性能在很大程度上依赖于获得大规模训练数据的访问，并且在数据量受到限制时会迅速恶化。本文旨在概述gan，其在各种视觉任务中的变体和应用程序，重点是解决有限的数据问题。我们通过设计的实验在有限的数据制度中分析了最先进的gan，并提出各种方法试图从不同的角度解决此问题。最后，我们进一步阐述了未来研究的剩余挑战和趋势。]]></description>
      <guid>https://arxiv.org/abs/2504.05456</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>视觉模型的分类学评估</title>
      <link>https://arxiv.org/abs/2504.05457</link>
      <description><![CDATA[ARXIV：2504.05457V1公告类型：新 
摘要：当提示视觉模型（VLM）识别图像中描绘的实体时，它可能会回答“我看到针叶树”，而不是特定的标签“挪威云杉”。这提出了两个问题以进行评估：首先，不受约束的生成的文本需要映射到评估标签空间（即&#39;Conifer&#39;）。其次，有用的分类措施应为答案提供部分信用（“挪威云杉”为一种“针叶树”）。为了满足这些要求，我们提出了一个框架，以评估针对分类法产生的不受约束的文本预测，例如由视觉模型产生的预测。具体而言，我们建议使用层次精度和召回措施来评估有关分类学预测的正确性和特异性的水平。在实验上，我们首先表明现有的文本相似性度量不能很好地捕获分类学相似性。然后，我们开发并比较不同的方法将文本VLM预测映射到分类法上。这使我们能够计算生成的文本和地面真相标签之间的层次相似性度量。最后，我们根据我们提出的分类评估方案分析了现代VLM对细粒的视觉分类任务。]]></description>
      <guid>https://arxiv.org/abs/2504.05457</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从单景观图像中优化4D高斯人的动态场景视频</title>
      <link>https://arxiv.org/abs/2504.05458</link>
      <description><![CDATA[ARXIV：2504.05458V1公告类型：新 
摘要：为了实现现实的沉浸在景观图像中，水和云等流体需要在图像中移动，同时从各种相机的角度揭示新场景。最近，出现了一个名为“动态场景”视频的领域，该视频将单个图像动画与3D摄影结合在一起。这些方法使用伪3D空间，用分层深度图像（LDIS）隐式表示。 LDIS将单个图像分为基于深度的图层，这使水和云等元素可以在图像中移动，同时从不同的相机角度揭示新场景。但是，由于景观通常由连续元素组成，包括流体，3D空间的表示将景观图像分为离散的层，并且会导致深度感知的减小和根据摄像机的运动而导致潜在的变形。此外，由于其对3D空间的隐式建模，输出可能仅限于2D域中的视频，从而有可能降低其多功能性。在本文中，我们通过从单个图像中对明确表示（特别是4D高斯人）进行建模，以代表一个完整的3D空间，用于动态场景视频。该框架的重点是通过从单个图像中生成多视图图像并创建3D运动来优化4D高斯人，以优化3D高斯。拟议框架的最重要部分是一致的3D运动估计，该估计多视图图像之间的共同运动使运动在3D空间中更接近实际动作。据我们所知，这是第一次考虑动画的尝试，同时代表单个景观图像的完整3D空间。我们的模型证明了通过各种实验和指标能够在各种景观图像中置入现实的能力。广泛的实验结果是https://cvsp-lab.github.io/iclr2025_3d-mom/。]]></description>
      <guid>https://arxiv.org/abs/2504.05458</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>揭示：基于关系的视频表示学习，用于视频问题 - 纠正</title>
      <link>https://arxiv.org/abs/2504.05463</link>
      <description><![CDATA[ARXIV：2504.05463V1公告类型：新 
摘要：视频问题 - 撤职（VideoQA）包括随时间变化的复杂视觉关系的捕获，即使是高级视频语言模型（VLM），I.A.仍然存在挑战，因为需要将视觉内容代表到这些模型合理尺寸的输入。为了解决这个问题，我们建议
  基于关系的视频表示学习（揭示），该框架旨在通过将其编码为结构化的，分解的表示形式来捕获视觉关系信息。具体来说，受时空场景图的启发，我们建议通过其语言嵌入时间嵌入（\ textit {object-predicate-object}）将视频序列作为（\ textIt {object-predeDicate-object}）的形式编码。为此，我们从视频字幕中提取明确的关系，并引入多到多的噪声对比估计（MM-NCE）以及Q形式架构，以使一组无序的视频衍生的查询与相应的基于文本的关系描述相结合。在推断时，由此产生的Q形式产生有效的令牌表示，可以作为VIDEOQA的VLM的输入。
  我们在五个具有挑战性的基准上评估了拟议的框架：Next-QA，Intent-QA，Star，Vlep和TVQA。它表明，由此产生的基于查询的视频表示形式能够超过基于全球的CLS或补丁令牌表示形式，并针对最新模型实现竞争成果，尤其是在需要时间推理和关系理解的任务上。代码和模型将公开发布。]]></description>
      <guid>https://arxiv.org/abs/2504.05463</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>研究零摄像视频对象细分的图像扩散特征</title>
      <link>https://arxiv.org/abs/2504.05468</link>
      <description><![CDATA[ARXIV：2504.05468V1公告类型：新 
摘要：本文研究了用于零量扩散模型的零量扩散模型，以零摄像视频对象细分（ZS-VOS），而无需对视频数据进行微调或对任何图像分割数据进行培训。尽管扩散模型在各种任务中都表现出强烈的视觉表示，但它们在ZS-VOS上的直接应用仍未被忽略。我们的目标是通过识别最合适的时间步长和提取特征的最佳特征提取过程来找到ZS-VOS的最佳特征提取过程。我们进一步分析了这些特征的亲和力，并观察到与点对应关系有很强的相关性。通过对Davis-17和Mose的广泛实验，我们发现在ImageNet上训练的扩散模型优于在ZS-VOS的较大，更多样化的数据集中训练的扩散模型。此外，我们强调了点对应关系在实现高分子精度中的重要性，并且我们产生的最新结果在ZS-VOS中产生。最后，我们的方法与在昂贵的图像分割数据集中训练的模型相同。]]></description>
      <guid>https://arxiv.org/abs/2504.05468</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>安全诊断：对抗性鲁棒性符合临床解释性</title>
      <link>https://arxiv.org/abs/2504.05483</link>
      <description><![CDATA[ARXIV：2504.05483V1公告类型：新 
摘要：由于违反I.I.D.假设和不透明的决策。本文通过评估针对对抗性攻击的模型性能，并将可解释性方法与骨科外科医生注释的断裂区域进行比较，从而研究了对骨折检测的深度神经网络中的可解释性。我们的发现证明，鲁棒模型产生的解释与临床上有意义的区域更加一致，这表明鲁棒性鼓励了解剖学上相关的特征优先级。我们强调了可解释性对促进人类协作的价值，在该合作中，模型在人类的范围内充当助手：临床上合理的解释促进了信任，促进误差纠正，并劝阻对AI的高风险决策的依赖。本文研究了鲁棒性和可解释性，作为弥合基准性能与安全，可行的临床部署之间差距的互补基准。]]></description>
      <guid>https://arxiv.org/abs/2504.05483</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>礁石：相关性和高效的LLM适配器用于视频理解</title>
      <link>https://arxiv.org/abs/2504.05491</link>
      <description><![CDATA[ARXIV：2504.05491V1公告类型：新 
摘要：将视觉模型集成到大型语言模型（LLM）中引发了人们对创建视觉基础模型的重大兴趣，尤其是为了了解视频的理解。最近的方法经常利用内存库来处理未修剪的视频，以了解视频级别的理解。但是，它们通常使用基于相似性的贪婪方法来压缩视觉记忆，这些方法可以忽略单个令牌的上下文重要性。为了解决这个问题，我们介绍了一个高效的LLM适配器，旨在视频级别对未修剪的视频的理解，该视频优先考虑时尚代币的上下文相关性。我们的框架利用得分手网络有选择地根据相关性来选择性地压缩视觉记忆库和过滤空间令牌，并使用可区分的TOP-K运算符进行端到端培训。在三个关键的视频级别中，了解任务$ \ unicode {x2013} $未修剪的视频分类，视频询问和视频字幕$ \ unicode {x2013} $我们的方法在四个大型数据集中获得了竞争性或优越的结果，同时最大减少了计算上的计算超过34％。该代码将很快在GitHub上提供。]]></description>
      <guid>https://arxiv.org/abs/2504.05491</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>几个个性化的个性化扫描预测</title>
      <link>https://arxiv.org/abs/2504.05499</link>
      <description><![CDATA[ARXIV：2504.05499V1公告类型：新 
摘要：一种个性化扫描预测模型提供了对单个受试者的视觉偏好和注意力模式的见解。但是，现有的培训扫描预测模型的方法是数据密集型的，不能有效地对只有几个可用示例的新个人进行个性化。在本文中，我们提出了一些个性化的个性化扫描预测任务（FS-PSP），并提出了一种解决它的新方法，该方法旨在使用该主题的扫描路径行为的最小支持数据来预测一个看不见的主题的扫描路径。我们方法适应性的关键是主题 - 安装网络（SE-NET），该网络专为捕获每个受试者扫描路径的独特，个性化表示形式而设计。 SE-NET生成受试者的嵌入，这些嵌入有效地区分受试者的同时，同时将扫描路径之间的变异性与同一个体的变异性最小化。然后将个性化的扫描预测模型在这些主题嵌入中进行条件，以产生准确的个性化结果。在多个引人注目的数据集上进行的实验表明，我们的方法在FS-PSP设置中表现出色，并且在测试时不需要任何微调步骤。代码可在以下网址找到：https：//github.com/cvlab-stonybrook/few-shot-scanpath]]></description>
      <guid>https://arxiv.org/abs/2504.05499</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>selfmad：通过自学学习增强变形攻击检测中的概括和鲁棒性</title>
      <link>https://arxiv.org/abs/2504.05504</link>
      <description><![CDATA[ARXIV：2504.05504V1公告类型：新 
摘要：随着生成模型的持续发展，由于其潜在使用在身份欺诈和其他恶意活动中，面部变形攻击已成为现有面部验证系统的重大挑战。当代的变形攻击检测（MAD）方法经常取决于受到监督的，歧视性的模型，该模型接受了真正的图像和变形图像的例子。这些模型通常会通过在训练过程中看到的技术产生的形态表现良好，但经过新颖的不见变形技术，通常会导致次优性能。尽管已证明无监督的模型在概括性方面的性能更好，但它们通常会导致较高的错误率，因为它们难以有效捕获微妙的人工制品的特征。为了解决这些缺点，我们提出了一种新颖的自我监督方法，可以模拟一般的变形攻击伪像，使分类器能够学习通用和健壮的决策界限，而不会过分适合特定面部变形方法引起的特定伪像。通过对广泛使用的数据集进行的广泛实验，我们证明，与最强大的无人看待的竞争对手相比，SelfMAD的表现显着胜过当前的最新疯狂，而与最强大的无人监督的竞争对手相比，EER的检测错误将超过64％，并且与最佳性能歧视性疯狂模型相比，在交叉界面上进行了鉴别。 SelfMad的源代码可在https://github.com/leontodorov/selfmad上获得。]]></description>
      <guid>https://arxiv.org/abs/2504.05504</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Partstickers：生成对象的部分进行快速原型制作</title>
      <link>https://arxiv.org/abs/2504.05508</link>
      <description><![CDATA[ARXIV：2504.05508V1公告类型：新 
摘要：设计原型制作涉及创建产品或概念的模型，以收集反馈并迭代思想。虽然原型制作通常需要对象的特定部分，例如为视频游戏构造新颖的生物时，现有的文本对图像方法往往只生成整个对象。为了解决这个问题，我们提出了一种新颖的任务和方法``一部分贴纸的生成&#39;&#39;，它需要在中性背景下产生一个孤立的对象的一部分。实验表明，我们的方法优于现实主义和文本对齐的最先进的基线，同时保留对象级别的能力，以保护我们的代码和模型。我们公开地共享社区范围的进步，以在新的任务上进行新的任务范围，这是在新任务上进行的，这是新的任务范围。 https://partsticker.github.io。]]></description>
      <guid>https://arxiv.org/abs/2504.05508</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>