<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Fri, 06 Dec 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>HunyuanVideo：大型视频生成模型的系统框架</title>
      <link>https://arxiv.org/abs/2412.03603</link>
      <description><![CDATA[arXiv:2412.03603v1 公告类型：新
摘要：视频生成领域的最新进展对个人和行业的日常生活产生了重大影响。然而，领先的视频生成模型仍然是闭源的，导致行业能力与公众可用的能力之间存在明显的性能差距。在本报告中，我们介绍了HunyuanVideo，这是一种创新的开源视频基础模型，其视频生成性能可与领先的闭源模型相媲美甚至超越。HunyuanVideo包含一个全面的框架，该框架集成了几个关键要素，包括数据管理、先进的架构设计、渐进式模型扩展和训练，以及为大规模模型训练和推理量身定制的高效基础设施。因此，我们成功训练了一个具有超过130亿个参数的视频生成模型，使其成为所有开源模型中最大的。我们进行了广泛的实验并实施了一系列有针对性的设计，以确保高视觉质量、运动动态、文本-视频对齐和先进的拍摄技术。根据专家的评估，HunyuanVideo 的表现优于之前的先进模型，包括 Runway Gen-3、Luma 1.6 和三个表现最好的中文视频生成模型。通过发布基础模型及其应用程序的代码，我们旨在弥合闭源社区和开源社区之间的差距。此举将使社区中的个人能够尝试他们的想法，从而培育一个更具活力和生机的视频生成生态系统。代码已在 https://github.com/Tencent/HunyuanVideo 上公开发布。]]></description>
      <guid>https://arxiv.org/abs/2412.03603</guid>
      <pubDate>Fri, 06 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>评估深度神经网络中用于语义分割的单事件扰动：嵌入式系统视角</title>
      <link>https://arxiv.org/abs/2412.03630</link>
      <description><![CDATA[arXiv:2412.03630v1 公告类型：新
摘要：随着人工智能 (AI) 算法在边缘设备上的部署越来越普遍，增强基于 AI 的自主感知和决策系统的稳健性和可靠性变得与精度和性能一样重要，尤其是在自动驾驶和航空航天等被视为安全关键的应用领域。本文深入研究了嵌入式深度神经网络 (DNN) 中的稳健性评估，特别关注单事件扰动 (SEU) 产生的参数扰动对用于图像语义分割的卷积神经网络 (CNN) 的影响。通过逐层和逐位检查各种编码器-解码器模型对软错误的敏感性，本研究彻底调查了分割 DNN 对 SEU 的脆弱性，并评估了模型修剪和参数量化等技术对针对嵌入式实现的压缩模型稳健性的影响。这些发现为了解 SEU 引发故障的机制提供了宝贵的见解，有助于评估预先训练的 DNN 的稳健性。此外，基于收集的数据，我们提出了一套实用的轻量级错误缓解技术，无需内存或计算成本，适合资源受限的部署。用于执行故障注入 (FI) 活动的代码可在 https://github.com/jonGuti13/TensorFI2 获得，而用于实施所提技术的代码可在 https://github.com/jonGuti13/parameterProtection 获得。]]></description>
      <guid>https://arxiv.org/abs/2412.03630</guid>
      <pubDate>Fri, 06 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MV-Adapter：轻松实现多视图一致图像生成</title>
      <link>https://arxiv.org/abs/2412.03632</link>
      <description><![CDATA[arXiv:2412.03632v1 公告类型：新
摘要：现有的多视图图像生成方法通常会对预训练的文本到图像 (T2I) 模型进行侵入性修改，并需要进行全面微调，导致 (1) 计算成本高，尤其是对于大型基础模型和高分辨率图像，以及 (2) 由于优化困难和高质量 3D 数据稀缺导致图像质量下降。在本文中，我们提出了第一个基于适配器的多视图图像生成解决方案，并介绍了 MV-Adapter，这是一种多功能即插即用适配器，可在不改变原始网络结构或特征空间的情况下增强 T2I 模型及其衍生产品。通过更新更少的参数，MV-Adapter 可以实现高效训练并保留嵌入在预训练模型中的先验知识，从而降低过度拟合风险。为了有效地在适配器内建模 3D 几何知识，我们引入了创新设计，包括重复的自注意力层和并行注意力架构，使适配器能够继承预训练模型的强大先验来建模新颖的 3D 知识。此外，我们提出了一个统一的条件编码器，无缝集成了相机参数和几何信息，促进了基于文本和图像的 3D 生成和纹理等应用。MV-Adapter 在稳定扩散 XL (SDXL) 上实现了 768 分辨率的多视图生成，并展示了适应性和多功能性。它还可以扩展到任意视图生成，从而实现更广泛的应用。我们证明 MV-Adapter 为多视图图像生成设定了新的质量标准，并因其效率、适应性和多功能性开辟了新的可能性。]]></description>
      <guid>https://arxiv.org/abs/2412.03632</guid>
      <pubDate>Fri, 06 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于图像字幕的个性化多模态大型语言模型：实验分析</title>
      <link>https://arxiv.org/abs/2412.03665</link>
      <description><![CDATA[arXiv:2412.03665v1 公告类型：新
摘要：图像字幕任务需要一种算法来生成视觉输入的自然语言描述。最近的进展见证了图像字幕研究与大型语言模型 (LLM) 和多模态 LLM（如 GPT-4V 和 Gemini）的开发之间的融合，这些 LLM 将纯文本 LLM 的功能扩展到多种模态。本文通过评估多模态 LLM 在各种图像描述基准上的表现，研究了多模态 LLM 是否可以取代传统的图像字幕网络。我们通过微调方法（包括快速学习、前缀调整和低秩自适应）探索这些模型的零样本能力及其对不同语义域的适应性。我们的结果表明，虽然多模态 LLM 实现了令人印象深刻的零样本性能，但在保持其泛化能力不变的情况下对特定领域进行微调仍然具有挑战性。我们讨论了这些发现对未来图像字幕研究和开发更具适应性的多模态 LLM 的影响。]]></description>
      <guid>https://arxiv.org/abs/2412.03665</guid>
      <pubDate>Fri, 06 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用视觉价值模型扩展推理时间搜索，提高视觉理解能力</title>
      <link>https://arxiv.org/abs/2412.03704</link>
      <description><![CDATA[arXiv:2412.03704v1 公告类型：新
摘要：尽管视觉语言模型 (VLM) 取得了重大进展，但缺乏通过扩展推理时间计算来提高响应质量的有效方法。在最近的大型语言模型研究中，这种能力被认为是迈向自我改进模型的核心一步。在本文中，我们提出了视觉价值模型 (VisVM)，它可以指导 VLM 推理时间搜索以生成具有更好视觉理解的响应。具体来说，VisVM 不仅评估当前搜索步骤中生成的句子质量，而且还预测当前步骤可能产生的后续句子的质量，从而提供长期价值。通过这种方式，VisVM 引导 VLM 避免生成容易产生幻觉或细节不足的句子，从而产生更高质量的响应。实验结果表明，与贪婪解码和使用其他视觉奖励信号的搜索方法相比，VisVM 引导的搜索显著增强了 VLM 生成具有更丰富视觉细节和更少幻觉的描述性字幕的能力。此外，我们发现使用 VisVM 引导的字幕对模型进行自我训练可以提高 VLM 在各种多模态基准测试中的表现，这表明开发自我改进的 VLM 具有潜力。我们的价值模型和代码可在 https://github.com/si0wang/VisVM 上找到。]]></description>
      <guid>https://arxiv.org/abs/2412.03704</guid>
      <pubDate>Fri, 06 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>VidHalluc：评估多模态大型语言模型中的时间幻觉以进行视频理解</title>
      <link>https://arxiv.org/abs/2412.03735</link>
      <description><![CDATA[arXiv:2412.03735v1 公告类型：新
摘要：多模态大型语言模型 (MLLM) 最近在视频理解方面取得了重大进展，在内容推理和指令遵循任务中表现出色。然而，幻觉问题，即模型生成不准确或误导性的内容，在视频领域仍未得到充分探索。基于 MLLM 的视觉编码器通常难以区分视觉上不同但语义上相似的视频对的观察，我们引入了 VidHalluc，这是旨在检查 MLLM 中用于视频理解任务的幻觉的最大基准。VidHalluc 从三个关键维度评估幻觉：(1) 动作、(2) 时间序列和 (3) 场景转换。VidHalluc 由 5,002 个视频组成，根据语义相似性和视觉差异配对，重点关注最有可能出现幻觉的情况。通过全面的测试，我们的实验表明，大多数 MLLM 在这些维度上都容易产生幻觉。此外，我们提出了 DINO-HEAL，这是一种无需训练的方法，通过结合来自 DINOv2 的空间显着性信息在推理过程中重新加权视觉特征来减少幻觉。我们的结果表明，DINO-HEAL 持续提高了 VidHalluc 的性能，在所有任务中减轻幻觉的平均改善率为 3.02%。VidHalluc 基准和 DINO-HEAL 代码都可以通过 $\href{https://vid-halluc.github.io/}//text{此链接}}$ 访问。]]></description>
      <guid>https://arxiv.org/abs/2412.03735</guid>
      <pubDate>Fri, 06 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>雾霾退化过程的深度变分贝叶斯建模</title>
      <link>https://arxiv.org/abs/2412.03745</link>
      <description><![CDATA[arXiv:2412.03745v1 公告类型：新
摘要：依靠神经网络的表示能力，最近的大多数研究往往忽略了雾霾退化所涉及的几个因素，例如透射（从远处场景到达观察者的光量）和大气光。这些因素通常是未知的，使得去雾问题变得不适定并产生固有的不确定性。为了解释这些不确定性和雾霾退化所涉及的因素，我们引入了一个用于单幅图像去雾的变分贝叶斯框架。我们建议不仅将干净图像而且将传输图作为潜在变量，其后验分布分别由相应的神经网络参数化：去雾和传输网络。基于雾霾退化的物理模型，我们的变分贝叶斯框架产生了一个新的目标函数，鼓励它们之间的合作，促进联合训练并从而提高彼此的性能。在我们的框架中，去雾网络可以在推理过程中独立于传输图估计来估计干净图像，不会引入任何开销。此外，我们的与模型无关的框架可以与其他现有的去雾网络无缝结合，大大提高跨数据集和模型的一致性能。]]></description>
      <guid>https://arxiv.org/abs/2412.03745</guid>
      <pubDate>Fri, 06 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>HIIF：基于分层编码的连续超分辨率隐式图像函数</title>
      <link>https://arxiv.org/abs/2412.03748</link>
      <description><![CDATA[arXiv:2412.03748v1 公告类型：新
摘要：隐式神经表征 (INR) 的最新进展已显示出在为各种低视力任务（包括图像超分辨率 (ISR)）建模视觉信号方面具有巨大潜力。基于 INR 的 ISR 方法通常学习连续表征，从而可以灵活地从低分辨率对应图像生成任意所需比例的高分辨率图像。然而，现有的基于 INR 的 ISR 方法利用多层感知器在网络中进行参数化；这没有考虑到局部采样点中存在的层次结构，因此限制了表征能力。在本文中，我们提出了一种新的基于 \textbf{H} 分层编码的 \textbf{I} 隐式 \textbf{I} 图像 \textbf{F} 函数，用于连续图像超分辨率，即 \textbf{HIIF}，它利用一种新颖的分层位置编码来增强局部隐式表示，使其能够捕捉多个尺度的精细细节。我们的方法还通过考虑额外的非局部信息，在隐式注意网络中嵌入了一个多头线性注意机制。我们的实验表明，当与不同的骨干编码器集成时，HIIF 在 PSNR 方面的表现比最先进的连续图像超分辨率方法高出 0.17dB。HIIF 的源代码将在 \url{www.github.com} 上公开发布。]]></description>
      <guid>https://arxiv.org/abs/2412.03748</guid>
      <pubDate>Fri, 06 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过坐标噪声和傅里叶注意进行多视角图像扩散</title>
      <link>https://arxiv.org/abs/2412.03756</link>
      <description><![CDATA[arXiv:2412.03756v1 公告类型：新
摘要：最近，与以前的基线相比，使用扩散模型的文本到图像生成在更高保真度和泛化能力方面取得了显着进步。然而，从提示中生成整体多视图一致图像仍然是一项重要且具有挑战性的任务。为了应对这一挑战，我们提出了一种扩散过程，该过程通过新颖的注意机制以及新颖的噪声初始化技术和交叉注意损失来关注特征的时间相关空间频率。这个基于傅里叶的注意力模块专注于生成场景中非重叠区域的特征，以更好地对齐全局外观。我们的噪声初始化技术结合了从像素坐标和深度图得出的共享噪声和低空间频率信息，以在视图之间引起噪声相关性。交叉注意损失进一步对齐了整个场景中共享相同提示的特征。与其他最先进的多视图一致性方法相比，我们的技术在几个定量指标上改进了 SOTA，并在质量上获得了更好的结果。]]></description>
      <guid>https://arxiv.org/abs/2412.03756</guid>
      <pubDate>Fri, 06 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>推进视频帧的自回归连续性</title>
      <link>https://arxiv.org/abs/2412.03758</link>
      <description><![CDATA[arXiv:2412.03758v1 公告类型：新
摘要：自回归大型语言模型 (LLM) 的最新进展已显示出其在生成高质量文本方面的潜力，激发了研究人员将其应用于图像和视频生成。本文探讨了 LLM 在视频连续性中的应用，这是构建世界模型和预测未来帧的一项必不可少的任务。在本文中，我们解决了包括防止长期帧生成中的退化和提高生成图像质量在内的挑战。我们设计了一个名为 ARCON 的方案，该方案涉及训练我们的模型以交替生成语义标记和 RGB 标记，使 LLM 能够明确学习和预测视频的高级结构信息。我们发现生成的 RGB 图像和语义图具有高度一致性，而无需特殊设计。此外，我们采用基于光流的纹理拼接方法来增强生成的视频的视觉质量。自动驾驶场景中的定量和定性实验表明我们的模型可以持续生成长视频。]]></description>
      <guid>https://arxiv.org/abs/2412.03758</guid>
      <pubDate>Fri, 06 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>EditScout：使用多模态 LLM 从基于扩散的编辑图像中定位伪造区域</title>
      <link>https://arxiv.org/abs/2412.03809</link>
      <description><![CDATA[arXiv:2412.03809v1 公告类型：新
摘要：图像编辑技术是用于转换、调整、删除或以其他方式更改图像的工具。最近的研究显著提高了图像编辑工具的功能，使得创建与真实图像几乎无法区分的逼真且语义上知情的伪造区域成为可能，这对数字取证和媒体可信度提出了新的挑战。虽然当前的图像取证技术擅长定位由传统图像处理方法产生的伪造区域，但当前的能力难以定位由基于扩散的技术创建的区域。为了弥补这一差距，我们提出了一个新颖的框架，该框架集成了多模态大语言模型 (LLM)，以增强推理能力，以定位由基于扩散模型的编辑方法生成的图像中的篡改区域。通过利用 LLM 的上下文和语义优势，我们的框架在 MagicBrush、AutoSplice 和 PerfBrush（新型基于扩散的数据集）数据集上取得了令人鼓舞的结果，在 mIoU 和 F1 分数指标方面优于以前的方法。值得注意的是，我们的方法在 PerfBrush 数据集上表现出色，这是一个自建的测试集，具有以前从未见过的编辑类型。在这里，传统方法通常会失败，得分明显较低，而我们的方法表现出色。]]></description>
      <guid>https://arxiv.org/abs/2412.03809</guid>
      <pubDate>Fri, 06 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>I$^2$OL-Net：用于点监督 X 射线违禁物品检测的内部-外部对象学习网络</title>
      <link>https://arxiv.org/abs/2412.03811</link>
      <description><![CDATA[arXiv:2412.03811v1 公告类型：新 
摘要：X射线图像中违禁物品的自动检测在公共安全中起着至关重要的作用。然而，现有的方法严重依赖于劳动密集型的框注释。为了解决这个问题，我们研究了劳动效率高的点监督下的X射线违禁物品检测，并开发了一个模态内-模态间物体性学习网络（I$^2$OL-Net）。I$^2$OL-Net由两个关键模块组成：模态内物体性学习（intra-OL）模块和模态间物体性学习（inter-OL）模块。intra-OL模块设计了局部聚焦高斯掩模块和全局随机高斯掩模块来协同学习X射线图像中的物体性。同时，inter-OL模块引入了基于小波分解的对抗学习块和物体性块，有效地减少了模态差异，并将从带有框注释的自然图像中学习到的物体性知识转移到X射线图像中。基于此，I$^2$OL-Net 极大地缓解了 X 射线图像中严重的类内变异所导致的局部支配问题。在四个 X 射线数据集上的实验结果表明，I$^2$OL-Net 可以在显著降低标注成本的情况下取得优异的性能，从而提升了其可及性和实用性。]]></description>
      <guid>https://arxiv.org/abs/2412.03811</guid>
      <pubDate>Fri, 06 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Pinco：前景条件修复中扩散变换器的位置诱导一致性适配器</title>
      <link>https://arxiv.org/abs/2412.03812</link>
      <description><![CDATA[arXiv:2412.03812v1 公告类型：新
摘要：前景条件修复旨在利用提供的前景主体和文本描述无缝填充图像的背景区域。虽然现有的基于 T2I 的图像修复方法可以应用于此任务，但它们存在主体形状膨胀、扭曲或与文本描述对齐能力受损的问题，导致视觉元素与文本描述不一致。为了应对这些挑战，我们提出了 Pinco，这是一种即插即用的前景条件修复适配器，可生成具有良好文本对齐的高质量背景，同时有效保留前景主体的形状。首先，我们设计了一个自洽适配器，将前景主体特征集成到与布局相关的自注意层中，这有助于缓解文本和主体特征之间的冲突，确保模型在处理整体图像布局时能够有效地考虑前景主体的特征。其次，我们设计了一种解耦图像特征提取方法，该方法采用不同的架构分别提取语义和形状特征，显著改善了主题特征提取并确保了主题形状的高质量保存。第三，为了确保精确利用提取的特征并将注意力集中在主题区域上，我们引入了共享位置嵌入锚点，大大提高了模型对主题特征的理解并提高了训练效率。大量实验表明，我们的方法在前景条件修复中实现了卓越的性能和效率。]]></description>
      <guid>https://arxiv.org/abs/2412.03812</guid>
      <pubDate>Fri, 06 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>探索图像恢复中的真实和合成数据集以及线性注意力</title>
      <link>https://arxiv.org/abs/2412.03814</link>
      <description><![CDATA[arXiv:2412.03814v1 公告类型：新
摘要：图像恢复旨在恢复退化的图像，深度学习，尤其是 CNN 和 Transformers，可以提高性能。然而，IR 缺乏统一的训练基准。我们发现训练和测试数据集之间的图像复杂度存在偏差，影响恢复质量。为了解决这个问题，我们创建了 ReSyn，一个具有平衡复杂度的大规模 IR 数据集，包括真实图像和合成图像。我们还为 IR 模型建立了统一的训练标准。我们的 RWKV-IR 模型将线性复杂度 RWKV 集成到 Transformers 中，用于全局和局部感受野。它用深度卷积代替 Q-Shift 来实现局部依赖，并结合双向注意力来实现全局-局部意识。Cross-Bi-WKV 模块平衡了水平和垂直注意力。实验证明了 RWKV-IR 在图像恢复方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2412.03814</guid>
      <pubDate>Fri, 06 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CLIP-FSAC++：基于 CLIP 的异常描述符小样本异常分类</title>
      <link>https://arxiv.org/abs/2412.03829</link>
      <description><![CDATA[arXiv:2412.03829v1 公告类型：新
摘要：工业异常分类（AC）是工业制造中不可或缺的任务，可保证各种产品的质量和安全。为了解决工业场景中数据稀缺的问题，最近出现了许多少样本异常检测方法。在本文中，我们提出了一种有效的单阶段训练少样本异常分类（FSAC）框架，称为 CLIP-FSAC++。具体而言，我们在图像和文本编码器之后引入了一个名为异常描述符的跨模态交互模块，它增强了视觉和文本嵌入的相关性，并将 CLIP 的表示从预训练数据调整为目标数据。在异常描述符中，图像到文本交叉注意模块用于获得特定于图像的文本嵌入，文本到图像交叉注意模块用于获得特定于文本的视觉嵌入。然后，这些特定于模态的嵌入用于增强 CLIP 的原始表示，以获得更好的匹配能力。提供了全面的实验结果，用于评估我们在 VisA 和 MVTEC-AD 上针对 1、2、4 和 8 镜头设置的少数正态镜头异常分类中的方法。源代码位于 https://github.com/Jay-zzcoder/clip-fsac-pp]]></description>
      <guid>https://arxiv.org/abs/2412.03829</guid>
      <pubDate>Fri, 06 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>