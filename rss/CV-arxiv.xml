<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Tue, 14 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>MCMC 的路径空间划分和引导图像采样</title>
      <link>https://arxiv.org/abs/2501.06214</link>
      <description><![CDATA[arXiv:2501.06214v1 公告类型：新
摘要：渲染算法通常在路径空间上集成光路。但是，在这个统一的空间上进行集成不一定是最有效的方法，我们表明，对路径空间进行分区并使用单独的估计器集成每个分区空间可以带来优势。我们提出了一种基于分析标准蒙特卡罗估计器的路径并使用马尔可夫链蒙特卡罗 (MCMC) 估计器集成这些分区路径空间的路径空间分区方法。这也意味着集成发生在路径空间的稀疏子集内，因此我们建议在图像空间中使用引导提议分布来提高效率。我们表明，与其他 MCMC 集成方法相比，我们的方法在相同数量的样本下提高了图像质量。]]></description>
      <guid>https://arxiv.org/abs/2501.06214</guid>
      <pubDate>Tue, 14 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>拟合不同交互信息：情绪与意图的联合分类</title>
      <link>https://arxiv.org/abs/2501.06215</link>
      <description><![CDATA[arXiv:2501.06215v1 Announce Type: new 
摘要：本文为ICASSP MEIJU@2025 Track I的一等奖解决方案，主要针对低资源多模态情感与意图识别。如何有效利用大量未标注数据，同时保证交互阶段不同难度任务的相互促进，这两点成为比赛的关键。本文对标注数据训练的模型进行伪标签标注，选取置信度较高的样本及其标签，缓解资源不足的问题。同时利用实验中发现的意图识别易表征的特点，在不同Attention Head下与情感识别相互促进，通过融合实现更高的意图识别性能。最终在精炼处理数据下，在测试集上取得0.5532的成绩，夺得该赛道冠军。]]></description>
      <guid>https://arxiv.org/abs/2501.06215</guid>
      <pubDate>Tue, 14 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>了解 Dufaycolor 的颜色：我们可以使用历史比色和光谱数据恢复它们吗？</title>
      <link>https://arxiv.org/abs/2501.06216</link>
      <description><![CDATA[arXiv:2501.06216v1 公告类型：新
摘要：Dufaycolor 是一种从 1935 年到 1950 年代末产生的加色摄影工艺，代表了该技术最先进的迭代之一。本文介绍了一种开源 Color-Screen 工具的持续研究和开发，该工具旨在重建加色照片的原始颜色。我们讨论了在生产彩色屏幕滤镜 (r\&#39;eseau) 时使用的染料的历史测量值，以实现准确的颜色恢复。]]></description>
      <guid>https://arxiv.org/abs/2501.06216</guid>
      <pubDate>Tue, 14 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>解析量化视觉生成模型中的位级缩放定律</title>
      <link>https://arxiv.org/abs/2501.06218</link>
      <description><![CDATA[arXiv:2501.06218v1 公告类型：新
摘要：视觉生成模型最近在两个主要范式上取得了重大进展：扩散风格和语言风格，两者都表现出了出色的缩放定律。量化对于有效部署这些模型至关重要，因为它可以降低内存和计算成本。在这项工作中，我们系统地研究了量化对这两个范式的影响。令人惊讶的是，尽管在完全精度下实现了可比的性能，但语言风格模型在各种量化设置中始终优于扩散风格模型。这一观察结果表明，语言风格模型具有优越的位级缩放定律，在模型质量和总位之间提供了更好的权衡。为了剖析这一现象，我们进行了大量实验，发现主要原因是语言风格模型的离散表示空间，它对量化过程中的信息丢失更具容忍度。此外，我们的分析表明，改进量化视觉生成模型的位级缩放定律具有挑战性，模型蒸馏被认为是一种非常有效的方法。具体来说，我们提出 TopKLD 通过在蒸馏过程中平衡“隐性知识”和“显性知识”来优化蒸馏知识的传递。这种方法在整数和浮点量化设置中将位级缩放定律提升了一个级别。]]></description>
      <guid>https://arxiv.org/abs/2501.06218</guid>
      <pubDate>Tue, 14 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>WhACC：具有专家级人类水平性能的 Whisker 自动接触分类器</title>
      <link>https://arxiv.org/abs/2501.06219</link>
      <description><![CDATA[arXiv:2501.06219v1 公告类型：新
摘要：啮齿动物触须系统在推进神经科学研究方面至关重要，特别是对于皮质可塑性、学习、决策、感觉编码和感觉运动整合的研究。尽管有这些优势，但策划触摸事件是一项劳动密集型工作，即使利用 Janelia Whisker Tracker 等自动化工具，每百万视频帧通常也需要 3 小时以上。我们通过引入 Whisker 自动接触分类器 (WhACC) 来解决这一限制，这是一个 Python 包，旨在以人类水平的性能从头部固定行为啮齿动物的高速视频中识别触摸周期。WhACC 利用 ResNet50V2 进行特征提取，结合 LightGBM 进行分类。在超过一百万帧上，性能通过三位专家人类策展人进行评估。99.5% 的视频帧的成对触摸分类一致性，相当于人与人之间的一致性。最后，我们提供了一个自定义的再训练界面，允许在一小部分数据上定制模型，该界面已在 16 个单元电生理记录的 400 万帧上进行了验证。包括这个再训练步骤在内，我们将整理 1 亿帧数据集所需的人工时间从约 333 小时减少到约 6 小时。]]></description>
      <guid>https://arxiv.org/abs/2501.06219</guid>
      <pubDate>Tue, 14 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>检测、检索和解释的统一：基于知识图谱和 GAT 的暴力检测系统</title>
      <link>https://arxiv.org/abs/2501.06224</link>
      <description><![CDATA[arXiv:2501.06224v1 公告类型：新
摘要：最近，使用统一多模态模型开发的暴力检测系统取得了重大成功并引起了广泛关注。然而，这些系统中的大多数面临两个关键挑战：作为黑盒模型缺乏可解释性，功能有限，仅提供分类或检索功能。为了应对这些挑战，本文提出了一种新型可解释的暴力检测系统，称为三合一（TIO）系统。TIO 系统集成知识图谱（KG）和图注意网络（GAT）以提供三个核心功能：检测、检索和解释。具体来说，该系统处理每个视频帧以及由大型语言模型（LLM）为包含潜在暴力行为的视频生成的文本描述。它使用 ImageBind 生成高维嵌入以构建知识图谱，使用 GAT 进行推理，并应用轻量级时间序列模块提取视频嵌入特征。最后一步连接分类器和检索器以获得多功能输出。 KG 的可解释性使系统能够验证每个输出背后的推理过程。此外，本文还介绍了几种轻量级方法来减少 TIO 系统的资源消耗并提高其效率。在 XD-Violence 和 UCF-Crime 数据集上进行的大量实验验证了所提系统的有效性。案例研究进一步揭示了一个有趣的现象：随着旁观者数量的增加，暴力行为的发生趋于减少。]]></description>
      <guid>https://arxiv.org/abs/2501.06224</guid>
      <pubDate>Tue, 14 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于医学图像分类的分布式混合量子卷积神经网络</title>
      <link>https://arxiv.org/abs/2501.06225</link>
      <description><![CDATA[arXiv:2501.06225v1 Announce Type: new 
摘要：医学图像具有错综复杂的特征，需要具有医学知识和经验的医生进行解读。经典神经网络可以减少医生的工作量，但只能在有限的范围内处理这些复杂特征。量子计算理论上可以用更少的参数探索更广阔的参数空间，但目前受到量子硬件的限制。考虑到这些因素，我们提出了一种基于量子电路分裂的分布式混合量子卷积神经网络。该模型利用量子计算的优势，有效捕捉医学图像的复杂特征，即使在资源受限的环境中也能实现高效的分类。我们的模型采用量子卷积神经网络（QCNN）从医学图像中提取高维特征，从而增强模型的表达能力。通过集成基于量子电路分裂的分布式技术，仅使用 5 个量子位即可重建 8 量子位 QCNN。实验结果表明，我们的模型在 3 个数据集的二分类和多分类任务中均表现出色。此外，与最近的技术相比，我们的模型以更少的参数实现了卓越的性能，实验结果验证了我们模型的有效性。]]></description>
      <guid>https://arxiv.org/abs/2501.06225</guid>
      <pubDate>Tue, 14 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用深度学习从 3D MRI 自动分割的开源手动注释声道数据库：对 2D 和 3D 卷积和变换器网络进行基准测试</title>
      <link>https://arxiv.org/abs/2501.06229</link>
      <description><![CDATA[arXiv:2501.06229v1 公告类型：新
摘要：从磁共振成像 (MRI) 数据中准确分割声道对于各种语音和语音应用至关重要。手动分割耗时且容易出错。本研究旨在评估深度学习算法对从 3D MRI 自动分割声道的有效性。]]></description>
      <guid>https://arxiv.org/abs/2501.06229</guid>
      <pubDate>Tue, 14 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>BEN：使用置信引导抠图进行二分图像分割</title>
      <link>https://arxiv.org/abs/2501.06230</link>
      <description><![CDATA[arXiv:2501.06230v1 公告类型：新
摘要：当前的二分图像分割 (DIS) 方法将图像抠图和对象分割视为根本不同的任务。随着图像分割的改进变得越来越具有挑战性，将图像抠图和灰度分割技术结合起来为架构创新提供了有希望的新方向。受到将这两个模型任务结合起来的可能性的启发，我们提出了一种称为置信引导抠图 (CGM) 的 DIS 新架构方法。我们创建了第一个 CGM 模型，称为背景擦除网络 (BEN)。BEN 由两个组件组成：用于初始分割的 BEN Base 和用于置信度细化的 BEN Refiner。我们的方法在 DIS5K 验证数据集上实现了比当前最先进的方法有显著改进，表明基于抠图的细化可以显著提高分割质量。这项工作为计算机视觉中抠图和分割技术的交叉融合开辟了新的可能性。]]></description>
      <guid>https://arxiv.org/abs/2501.06230</guid>
      <pubDate>Tue, 14 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>NextStop：一种改进的全景激光雷达分割数据跟踪器</title>
      <link>https://arxiv.org/abs/2501.06235</link>
      <description><![CDATA[arXiv:2501.06235v1 公告类型：新
摘要：4D 全景 LiDAR 分割对于自动驾驶和机器人中的场景理解至关重要，它将语义和实例分割与时间一致性相结合。当前的方法，如 4D-PLS 和 4D-STOP，使用跟踪检测方法，采用深度学习网络对每帧执行语义和实例分割。为了保持时间一致性，将在当前帧中检测到的大尺寸实例与包含当前帧和前一帧的时间窗口内的实例进行比较和关联。然而，它们对短期实例检测的依赖、缺乏运动估计和排除小尺寸实例导致频繁的身份切换和跟踪性能下降。我们使用 NextStop1 跟踪器解决了这些问题，它集成了基于卡尔曼滤波器的运动估计、数据关联和生命周期管理，以及用于改进优先级的轨迹状态概念。使用 SemanticKITTI 验证集上的 LiDAR 分割和跟踪质量 (LSTQ) 指标进行评估，NextStop 表现出增强的跟踪性能，特别是对于人和自行车等小型物体，ID 切换更少、跟踪启动更早，并且在复杂环境中的可靠性更高。源代码可在 https://github.com/AIROTAU/NextStopTracker 上找到]]></description>
      <guid>https://arxiv.org/abs/2501.06235</guid>
      <pubDate>Tue, 14 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 FMI 实现云无服务器计算的可扩展宇宙 AI 推理</title>
      <link>https://arxiv.org/abs/2501.06249</link>
      <description><![CDATA[arXiv:2501.06249v1 公告类型：新
摘要：大规模天文图像数据处理和预测对于天文学家至关重要，可为了解天体、宇宙历史及其演化提供关键见解。虽然现代深度学习模型具有很高的预测准确性，但它们通常需要大量的计算资源，因此资源密集型且限制了可访问性。我们引入了基于云的天文学推理 (CAI) 框架来应对这些挑战。此可扩展解决方案通过函数即服务 (FaaS) 消息接口 (FMI) 将预先训练的基础模型与无服务器云基础设施集成在一起。CAI 无需大量硬件即可对天文图像进行高效且可扩展的推理。以红移预测的基础模型为例，我们的大量实验涵盖了用户设备、HPC（高性能计算）服务器和云。CAI 在大数据量上的显著可扩展性改进为天文学界提供了一种可访问且有效的工具。代码可在 https://github.com/UVA-MLSys/AI-for-Astronomy 上访问。]]></description>
      <guid>https://arxiv.org/abs/2501.06249</guid>
      <pubDate>Tue, 14 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>动画的生成式人工智能：一项调查</title>
      <link>https://arxiv.org/abs/2501.06250</link>
      <description><![CDATA[arXiv:2501.06250v1 公告类型：新
摘要：传统的赛璐珞 (Cel) 动画制作流程包含多个基本步骤，包括故事板、布局设计、关键帧动画、中间帧和着色，这些步骤需要大量的手动工作、技术专业知识和大量时间投入。这些挑战历来阻碍了 Cel-Animation 制作的效率和可扩展性。生成人工智能 (GenAI) 的兴起，包括大型语言模型、多模态模型和扩散模型，通过自动执行诸如中间帧生成、着色和故事板创建等任务提供了创新的解决方案。本调查探讨了 GenAI 集成如何通过降低技术壁垒、通过 AniDoc、ToonCrafter 和 AniSora 等工具为更广泛的创作者扩大可访问性以及使艺术家能够更多地专注于创意表达和艺术创新来彻底改变传统动画工作流程。尽管它具有潜力，但保持视觉一致性、确保风格连贯性和解决道德问题等问题仍然带来挑战。此外，本文还讨论了未来的发展方向，并探索了 AI 辅助动画的潜在进步。如需进一步探索和获取资源，请访问我们的 GitHub 存储库：https://github.com/yunlong10/Awesome-AI4Animation]]></description>
      <guid>https://arxiv.org/abs/2501.06250</guid>
      <pubDate>Tue, 14 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>图像处理的后局部 XAI 技术现状：挑战与动机</title>
      <link>https://arxiv.org/abs/2501.06253</link>
      <description><![CDATA[arXiv:2501.06253v1 公告类型：新
摘要：随着复杂的人工智能系统进一步被证明是我们生活中不可或缺的一部分，一个持续而关键的问题是此类产品和系统潜在的黑箱性质。在追求生产力提高的过程中，我们不能忘记需要各种技术来提高此类人工智能系统的整体可信度。本文广泛研究的一个例子是可解释人工智能 (XAI) 领域。该范围内的研究工作围绕着使人工智能系统更加透明和可解释的目标，以进一步提高使用它们的可靠性和信任度。在本文中，我们讨论了 XAI 及其方法的各种动机、XAI 面临的潜在挑战以及我们认为值得进一步研究的一些未解决的问题。我们还简要讨论了用于图像处理的各种 XAI 方法，最后讨论了一些未来的方向，希望能够表达和激励 XAI 研究领域的积极发展。]]></description>
      <guid>https://arxiv.org/abs/2501.06253</guid>
      <pubDate>Tue, 14 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于变分自动编码器的量子下采样滤波器</title>
      <link>https://arxiv.org/abs/2501.06259</link>
      <description><![CDATA[arXiv:2501.06259v1 公告类型：新
摘要：变分自动编码器 (VAE) 是生成建模和图像重建中必不可少的工具，其性能在很大程度上受到编码器-解码器架构的影响。本研究旨在通过提高分辨率和保留更精细的细节来提高重建图像的质量，特别是在处理低分辨率输入（16x16 像素）时，传统 VAE 通常会产生模糊或不准确的结果。为了解决这个问题，我们提出了一种混合模型，将 VAE 编码器中的量子计算技术与解码器中的卷积神经网络 (CNN) 相结合。通过在编码过程中将分辨率从 16x16 升级到 32x32，我们的方法评估了模型如何在保持关键特征和结构的同时以增强的分辨率重建图像。该方法测试了模型在处理图像重建方面的稳健性以及它在低分辨率数据上训练时保留基本细节的能力。我们在 MNIST 和 USPS 数据集上评估了我们提出的用于量子 VAE (Q-VAE) 的下采样滤波器，并将其与经典 VAE 和称为经典直接传递 VAE (CDP-VAE) 的变体进行了比较，后者在编码过程中使用窗口池化滤波器。使用诸如 Fr\&#39;echet 初始距离 (FID) 和均方误差 (MSE) 等指标来评估性能，这些指标衡量重建图像的保真度。我们的结果表明，Q-VAE 的表现始终优于经典 VAE 和 CDP-VAE，实现了明显更低的 FID 和 MSE 分数。此外，CDP-VAE 的性能优于 C-VAE。这些发现凸显了量子增强 VAE 通过提高分辨率和保留基本特征来改善图像重建质量的潜力，为未来在计算机视觉和合成数据生成中的应用提供了一个有希望的方向。]]></description>
      <guid>https://arxiv.org/abs/2501.06259</guid>
      <pubDate>Tue, 14 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CAM 作为基于 Shapley 值的解释器</title>
      <link>https://arxiv.org/abs/2501.06261</link>
      <description><![CDATA[arXiv:2501.06261v1 公告类型：新
摘要：类激活映射 (CAM) 方法被广泛用于可视化神经网络决策，但其底层机制仍未完全理解。为了增强对 CAM 方法的理解并提高其可解释性，我们引入了内容保留博弈论 (CRG) 解释器。该理论框架通过将神经网络预测过程建模为合作博弈，阐明了 GradCAM 和 HiResCAM 的理论基础。在这个框架内，我们开发了 ShapleyCAM，这是一种利用梯度和 Hessian 矩阵提供更精确、理论依据的视觉解释的新方法。由于精确的 Shapley 值计算不可行，ShapleyCAM 采用合作博弈效用函数的二阶泰勒展开来推导出闭式表达式。此外，我们提出了残差 Softmax 目标类 (ReST) 效用函数来解决前 softmax 和后 softmax 分数的局限性。在 ImageNet 验证集上对 12 个流行网络进行的大量实验证明了 ShapleyCAM 及其变体的有效性。我们的发现不仅提高了 CAM 的可解释性，而且还弥合了启发式驱动的 CAM 方法与计算密集型 Shapley 值方法之间的差距。代码可在 \url{https://github.com/caihuaiguang/pytorch-shapley-cam} 获得。]]></description>
      <guid>https://arxiv.org/abs/2501.06261</guid>
      <pubDate>Tue, 14 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>