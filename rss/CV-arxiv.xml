<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Mon, 20 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>VideoWorld：探索从未标记的视频中学习知识</title>
      <link>https://arxiv.org/abs/2501.09781</link>
      <description><![CDATA[arXiv:2501.09781v1 公告类型：新
摘要：这项工作探讨了深度生成模型是否可以仅从视觉输入中学习复杂知识，这与普遍关注的基于文本的模型（如大型语言模型 (LLM)）形成鲜明对比。我们开发了 VideoWorld，这是一种在未标记视频数据上训练的自回归视频生成模型，并在基于视频的围棋和机器人控制任务中测试其知识获取能力。我们的实验揭示了两个关键发现：（1）纯视频训练为学习知识提供了足够的信息，包括规则、推理和规划能力，（2）视觉变化的表示对于知识获取至关重要。为了提高这一过程的效率和功效，我们引入了潜在动力学模型 (LDM) 作为 VideoWorld 的关键组成部分。值得注意的是，VideoWorld 仅使用 3 亿参数模型就达到了 Video-GoBench 中的 5 段专业水平，而无需依赖强化学习中典型的搜索算法或奖励机制。在机器人任务中，VideoWorld 有效地学习了各种控制操作并跨环境进行推广，接近 CALVIN 和 RLBench 中的 oracle 模型的性能。这项研究为从视觉数据中获取知识开辟了新途径，所有代码、数据和模型均开源以供进一步研究。]]></description>
      <guid>https://arxiv.org/abs/2501.09781</guid>
      <pubDate>Mon, 20 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SMPLest-X：富有表现力的人体姿势和形状估计的终极缩放</title>
      <link>https://arxiv.org/abs/2501.09782</link>
      <description><![CDATA[arXiv:2501.09782v1 公告类型：新
摘要：富有表现力的人体姿势和形状估计 (EHPS) 将身体、手部和面部动作捕捉与众多应用统一起来。尽管取得了令人鼓舞的进展，但目前最先进的方法仍侧重于在有限的数据集上训练创新的建筑设计。在这项工作中，我们研究了将 EHPS 扩展到一系列通用基础模型的影响。1) 对于数据扩展，我们对 40 个 EHPS 数据集进行了系统调查，涵盖了在任何单个数据集上训练的模型无法处理的广泛场景。更重要的是，利用从广泛的基准测试过程中获得的见解，我们优化了我们的训练方案并选择了可显著提高 EHPS 能力的数据集。最终，我们在来自不同数据源的 10M 个训练实例中实现了收益递减。2) 对于模型扩展，我们利用视觉变换器（最高 ViT-Huge 作为主干）来研究 EHPS 中模型大小的缩放规律。为了排除算法设计的影响，我们的实验基于两个极简架构：SMPLer-X，由手部和面部定位的中间步骤组成；SMPLest-X，一个更简单的版本，将网络精简到基本要素，并突出了在捕捉关节手部方面的重大进步。借助大数据和大型模型，基础模型在各种测试基准中表现出色，甚至在看不见的环境中也表现出色。此外，我们的微调策略将通才模型变成了专才模型，使它们能够进一步提高性能。值得注意的是，我们的基础模型在七个基准测试（如 AGORA、UBody、EgoBody 和我们提出的用于全面手部评估的 SynHand 数据集）上始终提供最先进的结果。（代码可在以下位置获得：https://github.com/wqyin/SMPLest-X）。]]></description>
      <guid>https://arxiv.org/abs/2501.09782</guid>
      <pubDate>Mon, 20 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用预训练扩散模型进行有损压缩</title>
      <link>https://arxiv.org/abs/2501.09815</link>
      <description><![CDATA[arXiv:2501.09815v1 公告类型：新 
摘要：我们将 DiffC 算法（Theis 等人，2022 年）应用于 Stable Diffusion 1.5、2.1、XL 和 Flux-dev，并证明这些预训练模型是非常出色的有损图像压缩器。自 Ho 等人 2020 年以来，人们已经了解了使用预训练扩散模型进行有损压缩的原理算法，但反向通道编码中的挑战阻碍了此类算法的完全实现。我们介绍了一些简单的解决方法，从而首次完整实现了 DiffC，它能够在 10 秒内使用 Stable Diffusion 压缩和解压缩图像。尽管不需要额外的训练，但我们的方法在低超低比特率下与其他最先进的生成压缩方法具有竞争力。]]></description>
      <guid>https://arxiv.org/abs/2501.09815</guid>
      <pubDate>Mon, 20 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 Vision Transformer 的深度表示进行基于单图像的广义变形攻击检测</title>
      <link>https://arxiv.org/abs/2501.09817</link>
      <description><![CDATA[arXiv:2501.09817v1 公告类型：新
摘要：人脸变形攻击对边境管制和护照签发用例中运行的人脸识别系统 (FRS) 构成了严重威胁。相应地，需要变形攻击检测算法 (MAD) 来防御此类攻击。MAD 方法必须足够强大，以处理开放场景中的未知攻击，其中攻击可能源自各种变形生成算法、后处理和打印机/扫描仪的多样性。当必须对单个可疑图像进行检测时，泛化问题更加明显。在本文中，我们通过学习 Vision Transformer (ViT) 架构的编码，提出了一种基于单图像的通用 MAD (S-MAD) 算法。与基于 CNN 的架构相比，ViT 模型在整合局部和全局信息方面具有优势，因此适合检测广泛分布在人脸区域中的变形痕迹。在使用公开可用的 FRGC 人脸数据集生成的脸部变形数据集上进行了大量实验。我们选择了几种最先进的 (SOTA) MAD 算法，包括已公开评估的代表性算法，并使用基于 ViT 的方法进行了基准测试。获得的结果表明，所提出的 S-MAD 方法在数据集间测试（当使用不同的数据进行训练和测试时）中的检测性能有所提高，并且在数据集内测试（当使用相同的数据进行训练和测试时）实验协议中的性能相当。]]></description>
      <guid>https://arxiv.org/abs/2501.09817</guid>
      <pubDate>Mon, 20 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PIXELS：基于 Xemplar 的渐进式图像编辑与潜伏性手术</title>
      <link>https://arxiv.org/abs/2501.09826</link>
      <description><![CDATA[arXiv:2501.09826v1 公告类型：新
摘要：用于图像编辑的语言引导扩散模型的最新进展通常受到繁琐的提示工程的瓶颈限制，无法精确表达所需的更改。一种直观的替代方法是利用野生图像样本的指导来帮助用户将他们想象的编辑变为现实。当代基于样本的编辑方法回避利用预先存在的大型文本到图像 (TTI) 模型学习到的丰富潜在空间，而是依靠精心策划的目标函数进行训练来完成任务。虽然这有点有效，但它需要大量的计算资源，并且与各种基础模型和任意样本计数缺乏兼容性。经过进一步研究，我们还发现这些技术将用户控制限制为仅对整个编辑区域应用统一的全局更改。在本文中，我们引入了一种使用现成的扩散模型进行渐进式样例驱动编辑的新框架，称为 PIXELS，通过提供对编辑的精细控制来实现定制，允许在像素或区域级别进行调整。我们的方法仅在推理过程中运行，以促进模仿编辑，使用户能够从动态数量的参考图像或多模态提示中汲取灵感，并逐步整合所有所需的更改，而无需重新训练或微调现有的 TTI 模型。这种细粒度控制能力开辟了一系列新的可能性，包括选择性修改单个对象和指定渐进的空间变化。我们证明 PIXELS 可以高效地提供高质量的编辑，从而显着改善定量指标和人工评估。通过使高质量图像编辑更容易获得，PIXELS 有可能让更广泛的受众轻松使用任何开源图像生成模型进行专业级编辑。]]></description>
      <guid>https://arxiv.org/abs/2501.09826</guid>
      <pubDate>Mon, 20 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>EraseBench：了解概念擦除技术的连锁反应</title>
      <link>https://arxiv.org/abs/2501.09833</link>
      <description><![CDATA[arXiv:2501.09833v1 公告类型：新
摘要：概念擦除技术最近因其从文本到图像模型中删除不需要的概念的潜力而受到广泛关注。虽然这些方法通常在受控场景中取得成功，但它们在实际应用中的稳健性和部署准备情况仍不确定。在这项工作中，我们发现了评估清理模型的一个关键差距，特别是在它们在各个概念维度上的表现方面。我们系统地研究了当前概念擦除技术的失败模式，重点关注视觉相似、二项式和语义相关的概念。我们认为这些相互关联的关系会引起概念纠缠现象，从而导致连锁反应和图像质量下降。为了促进更全面的评估，我们引入了 EraseBENCH，这是一个多维基准，旨在更深入地评估概念擦除方法。我们的数据集包括 100 多个不同的概念和 1,000 多个定制提示，并配有一套全面的指标，共同提供擦除效果的整体视图。我们的研究结果表明，即使是最先进的技术也难以维持擦除后的质量，这表明这些方法尚未准备好在现实世界中部署。这凸显了概念擦除技术在可靠性方面的差距。]]></description>
      <guid>https://arxiv.org/abs/2501.09833</guid>
      <pubDate>Mon, 20 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CrossModalityDiffusion：具有统一中间表示的多模态新视图合成</title>
      <link>https://arxiv.org/abs/2501.09838</link>
      <description><![CDATA[arXiv:2501.09838v1 公告类型：新
摘要：地理空间成像利用来自各种传感模式的数据 - 例如 EO、SAR 和 LiDAR，从地面无人机到卫星视图。这些异构输入为场景理解提供了重要的机会，但在准确解释几何方面也带来了挑战，特别是在缺乏精确的地面真实数据的情况下。为了解决这个问题，我们提出了 CrossModalityDiffusion，这是一个模块化框架，旨在生成跨不同模态和视点的图像，而无需事先了解场景几何。CrossModalityDiffusion 采用特定于模态的编码器，可获取多个输入图像并生成几何感知特征体积，这些特征体积相对于其输入相机位置对场景结构进行编码。放置特征体积的空间充当统一输入模态的共同基础。这些特征体积被重叠并使用体积渲染技术从新视角渲染成特征图像。渲染的特征图像用作特定模态扩散模型的条件输入，从而能够为所需的输出模态合成新图像。在本文中，我们展示了联合训练不同的模块可确保框架内所有模态的几何理解一致。我们在合成的 ShapeNet 汽车数据集上验证了 CrossModalityDiffusion 的功能，证明了其在跨多种成像模态和视角生成准确且一致的新视图方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2501.09838</guid>
      <pubDate>Mon, 20 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ASTRA：基于场景感知 TRAnsformer 的轨迹预测模型</title>
      <link>https://arxiv.org/abs/2501.09878</link>
      <description><![CDATA[arXiv:2501.09878v1 公告类型：新
摘要：我们提出了 ASTRA（一种基于场景感知 TRANSformer 的轨迹预测模型），这是一种轻量级行人轨迹预测模型，它集成了场景背景、空间动态、社交代理间交互和时间进展，以实现精确预测。我们利用基于 U-Net 的特征提取器通过其潜在向量表示来捕获场景表示，并利用图形感知 Transformer 编码器来捕获社交互动。这些组件集成在一起以学习代理场景感知嵌入，使模型能够学习空间动态并预测行人的未来轨迹。该模型旨在产生确定性和随机性结果，其中随机预测是通过结合条件变分自动编码器 (CVAE) 生成的。 ASTRA 还提出了一种简单但有效的加权惩罚损失函数，这有助于产生优于各种最先进的确定性和生成性模型的预测。ASTRA 在 ETH-UCY 数据集的确定性/随机设置中分别平均提高了 27%/10%，在 PIE 数据集上提高了 26%，同时参数数量比现有的最先进模型少了七倍（见图 1）。此外，该模型的多功能性使其能够跨不同视角进行推广，例如鸟瞰图 (BEV) 和自车视图 (EVV)。]]></description>
      <guid>https://arxiv.org/abs/2501.09878</guid>
      <pubDate>Mon, 20 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>半监督的基于图像的叙述提取：以历史摄影记录为例</title>
      <link>https://arxiv.org/abs/2501.09884</link>
      <description><![CDATA[arXiv:2501.09884v1 公告类型：新
摘要：本文介绍了一种半监督方法，使用叙事地图算法的改编版从历史摄影记录中提取叙事。我们扩展了原始的无监督文本方法以处理图像数据，利用深度学习技术进行视觉特征提取和相似度计算。我们的方法应用于 ROGER 数据集，该数据集是罗伯特·格斯特曼 (Robert Gerstmann) 拍摄的 1928 年玻利维亚萨坎巴亚探险队的照片集。我们将算法提取的视觉叙事与专家策划的不同长度的时间线（5 到 30 张图像）进行比较，以评估我们方法的有效性。特别是，我们使用动态时间扭曲 (DTW) 算法将提取的叙述与专家策划的基线进行匹配。此外，我们请该主题的专家对所得叙述的代表性示例进行定性评估。我们的研究结果表明，叙事地图方法在较长时间范围内的表现通常优于随机抽样（10 张以上图像，p &lt; 0.05），专家评估证实了提取的叙事的历史准确性和连贯性。这项研究为视觉文化遗产的计算分析领域做出了贡献，为历史学家、档案保管员和数字人文学者提供了探索和理解大规模图像集的新工具。该方法能够从视觉数据中生成有意义的叙事，为通过摄影证据研究和解释历史事件开辟了新的可能性。]]></description>
      <guid>https://arxiv.org/abs/2501.09884</guid>
      <pubDate>Mon, 20 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FLORA：形式语言模型可实现稳健的无训练零样本对象引用分析</title>
      <link>https://arxiv.org/abs/2501.09887</link>
      <description><![CDATA[arXiv:2501.09887v1 公告类型：新
摘要：对象指称分析 (ORA)，通常称为指称表达理解，需要基于自然描述识别和定位图像中的特定对象。与通用对象检测不同，ORA 需要准确的语言理解和精确的视觉定位，这使得它本质上更加复杂。尽管最近经过预训练的大型视觉接地检测器取得了重大进展，但它们严重依赖大量标记的数据和耗时的学习。为了解决这些问题，我们引入了一个新颖的、无需训练的零样本 ORA 框架，称为 FLORA（用于对象指称和分析的形式语言）。FLORA 利用大型语言模型 (LLM) 固有的推理能力并集成形式语言模型（一种在结构化、基于规则的描述中规范语言的逻辑框架）来提供有效的零样本 ORA。更具体地说，我们的形式语言模型 (FLM) 无需任何训练过程即可实现对对象描述的有效、逻辑驱动的解释。基于 FLM 调节的 LLM 输出，我们进一步设计了一个贝叶斯推理框架，并采用适当的现成解释模型来完成推理，从而以无需训练的方式提供对 LLM 幻觉的良好鲁棒性以及令人信服的 ORA 性能。在实践中，我们的 FLORA 将现有预训练接地检测器的零样本性能提高了约 45%。我们对不同具有挑战性的数据集的全面评估也证实，FLORA 在与零样本 ORA 相关的检测和分割任务中始终超越当前最先进的零样本方法。我们相信我们对 LLM 输出的概率解析和推理提高了零样本 ORA 的可靠性和可解释性。我们将在发布后发布代码。]]></description>
      <guid>https://arxiv.org/abs/2501.09887</guid>
      <pubDate>Mon, 20 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FoundationStereo：零样本立体匹配</title>
      <link>https://arxiv.org/abs/2501.09898</link>
      <description><![CDATA[arXiv:2501.09898v1 公告类型：新
摘要：通过对每个域进行微调，深度立体匹配在基准数据集上取得了巨大进展。然而，实现强大的零样本泛化（这是其他计算机视觉任务中基础模型的标志）对于立体匹配来说仍然具有挑战性。我们引入了 FoundationStereo，这是一个立体深度估计的基础模型，旨在实现强大的零样本泛化。为此，我们首先构建一个大规模（1M 立体对）合成训练数据集，具有高多样性和高照片真实感，然后是自动自我管理管道以删除模糊样本。然后，我们设计了许多网络架构组件来增强可扩展性，包括一个侧调特征主干，它可以调整视觉基础模型中丰富的单目先验以减轻模拟与现实之间的差距，以及用于有效成本体积过滤的远程上下文推理。这些组件共同带来了跨领域的强大稳健性和准确性，为零样本立体深度估计建立了新标准。]]></description>
      <guid>https://arxiv.org/abs/2501.09898</guid>
      <pubDate>Mon, 20 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>TalkingEyes：多元化语音驱动的 3D 视线动画</title>
      <link>https://arxiv.org/abs/2501.09921</link>
      <description><![CDATA[arXiv:2501.09921v1 公告类型：新
摘要：尽管最近在语音驱动的 3D 面部动画领域取得了重大进展，但最近的研究却忽视了面部不可或缺的一部分——目光注视的语音驱动动画。这主要是因为语音和目光注视之间的相关性较弱，以及音频注视数据的稀缺，使得仅从语音生成 3D 目光注视运动非常具有挑战性。在本文中，我们提出了一种新颖的数据驱动方法，可以生成与语音协调的多种 3D 目光注视运动。为此，我们首先构建了一个音频注视数据集，其中包含约 14 小时的音频网格序列，同时具有高质量的目光注视运动、头部运动和面部运动。通过对现有视听数据集中的视频进行轻量级的目光注视拟合和面部重建来获取运动数据。然后，我们定制了一个新颖的语音到运动转换框架，其中头部运动和眼球注视运动由语音联合生成，但在两个独立的潜在空间中建模。这种设计源于眼球旋转范围小于头部的生理知识。通过将语音嵌入映射到两个潜在空间，可以降低对语音和非语言运动之间弱相关性进行建模的难度。最后，我们的 TalkingEyes 与语音驱动的 3D 面部运动生成器集成，可以从语音中集体合成眼球注视运动、眨眼、头部运动和面部运动。广泛的定量和定性评估证明了所提出的方法在从语音生成多样化和自然的 3D 眼球注视运动方面的优越性。本文的项目页面是：https://lkjkjoiuiu.github.io/TalkingEyes_Home/]]></description>
      <guid>https://arxiv.org/abs/2501.09921</guid>
      <pubDate>Mon, 20 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>IE-Bench：推进文本驱动图像编辑的测量以实现人类感知对齐</title>
      <link>https://arxiv.org/abs/2501.09927</link>
      <description><![CDATA[arXiv:2501.09927v1 公告类型：新
摘要：文本驱动的图像编辑最近取得了重大进展，但准确评估这些编辑图像的任务仍然是一个相当大的挑战。与文本驱动的图像生成的评估不同，文本驱动的图像编辑的特点是同时对文本和源图像进行调节。编辑后的图像通常保留与原始图像的内在联系，这些联系会随着文本的语义而动态变化。然而，以前的方法往往只关注文本-图像对齐，或者与人类感知不一致。在这项工作中，我们引入了文本驱动的图像编辑基准套件 (IE-Bench) 来增强对文本驱动的编辑图像的评估。IE-Bench 包括一个数据库，其中包含各种源图像、各种编辑提示和不同编辑方法的相应结果，以及 25 名人类受试者提供的总共 3,010 个平均意见分数 (MOS)。此外，我们介绍了 IE-QA，一种用于文本驱动图像编辑的多模态源感知质量评估方法。据我们所知，IE-Bench 提供了第一个专为文本驱动的图像编辑量身定制的 IQA 数据集和模型。大量实验表明，与之前的指标相比，IE-QA 在文本驱动的图像编辑任务上具有更出色的主观对齐效果。我们将向公众提供所有相关数据和代码。]]></description>
      <guid>https://arxiv.org/abs/2501.09927</guid>
      <pubDate>Mon, 20 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一种用于小麦病害分类的多尺度特征提取与融合深度学习方法</title>
      <link>https://arxiv.org/abs/2501.09938</link>
      <description><![CDATA[arXiv:2501.09938v1 公告类型：新
摘要：小麦是膳食纤维和蛋白质的重要​​来源，但其生长受到多种风险的负面影响。本文讨论了识别和分类小麦病害的难度，重点讨论了小麦散黑穗病、叶锈病以及冠腐病和根腐病。针对冠腐病和根腐病等情况，本研究引入了一种创新方法，将多尺度特征提取与先进的图像分割技术相结合，以提高分类准确性。所提出的方法使用神经网络模型 Xception、Inception V3 和 ResNet 50 在大型小麦病害分类数据集 2020 上进行训练，并结合一组机器视觉分类器，包括投票和堆叠。研究表明，与目前最先进的方法相比，所提出的方法在小麦病害分类方面具有 99.75% 的卓越准确率。深度学习集成模型 Xception 显示出最高的准确率。]]></description>
      <guid>https://arxiv.org/abs/2501.09938</guid>
      <pubDate>Mon, 20 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Surface-SOS：通过神经表面表征进行自监督对象分割</title>
      <link>https://arxiv.org/abs/2501.09947</link>
      <description><![CDATA[arXiv:2501.09947v1 公告类型：新
摘要：自监督对象分割 (SOS) 旨在在没有任何注释的情况下分割对象。在多摄像机输入的情况下，可以利用每个视图之间的结构、纹理和几何一致性来实现细粒度的对象分割。为了更好地利用上述信息，我们提出了基于表面表示的自监督对象分割 (Surface-SOS)，这是一种新的框架，通过场景多视图图像中的 3D 表面表示为每个视图分割对象。为了为复杂场景建模高质量的几何表面，我们设计了一种新颖的场景表示方案，该方案分别使用有符号距离函数 (SDF) 将场景分解为两个互补的神经表示模块。此外，通过引入粗分割掩码作为附加输入，Surface-SOS 能够使用多视图未标记图像来细化单视图分割。据我们所知，Surface-SOS 是第一个利用神经表面表征来打破对大量注释数据和强约束的依赖的自监督方法。这些约束通常涉及在静态背景下观察目标对象或依赖视频中的时间监督。在包括 LLFF、CO3D、BlendedMVS、TUM 和几个真实场景在内的标准基准上进行的大量实验表明，Surface-SOS 总是比基于 NeRF 的同类方法产生更精细的对象蒙版，并且显著超越了监督单视图基线。代码可在以下位置获得：https://github.com/zhengxyun/Surface-SOS。]]></description>
      <guid>https://arxiv.org/abs/2501.09947</guid>
      <pubDate>Mon, 20 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>