<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>CS.CV更新arxiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.cv更新arxiv.org e-print存档。</description>
    <lastBuildDate>Mon, 24 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>KKA：通过大型语言模型的异常知识改善视力异常检测</title>
      <link>https://arxiv.org/abs/2502.14880</link>
      <description><![CDATA[ARXIV：2502.14880V1公告类型：新 
摘要：视力异常检测，尤其是在无监督的环境中，通常由于异常的差异而努力区分正常样品和异常。最近，越来越多的研究集中在产生异常，以帮助探测器学习正常样本和异常之间的更有效界限。但是，由于产生的异常通常来自随机因素，因此它们经常缺乏现实主义。此外，随机生成的异常通常在构建有效边界方面提供有限的支持，因为大多数与正常样本大大差异，并且远离边界。为了应对这些挑战，我们提出了关键知识增强（KKA），该方法从大型语言模型（LLMS）中提取异常知识。更具体地说，KKA利用LLM的广泛的先验知识来基于正常样本产生有意义的异常。然后，KKA根据与正常样本的相似性将生成的异常分类为简单的异常和硬异常。易于异常与正常样品显示出显着差异，而硬异常与正常样品非常相似。 KKA迭代更新了生成的异常，并逐渐增加了硬异常的比例，以使探测器能够学习更有效的边界。实验结果表明，所提出的方法可显着提高各种视力异常检测器的性能，同时保持低发电成本。可以在https://github.com/anfeather/kka上找到CMG的代码。]]></description>
      <guid>https://arxiv.org/abs/2502.14880</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从16位到1位：用于存储效率多模式模型的视觉KV缓存量化</title>
      <link>https://arxiv.org/abs/2502.14882</link>
      <description><![CDATA[ARXIV：2502.14882V1公告类型：新 
摘要：多模式的大语言模型（MLLM）在各种应用程序中都取得了巨大的成功，但是部署期间的计算开销仍然是一个至关重要的挑战。尽管密钥值（KV）缓存通过将存储器用于计算来提高推理效率，但存储的内存足迹不断增长，而存储广泛的KV caches可减少吞吐量，并限制了具有约束GPU内存的设备上的长期执行。现有方法主要集中于降低不重要的令牌以减少KV缓存大小，从而以潜在的信息损失为代价减轻内存约束。相比之下，我们提出了一种简单而有效的视觉量化策略，该策略可保留所有视觉令牌，同时大大减少记忆消耗。为了实现极端量化比，即1位量化，我们提出了以KV缓存的固有模式进行的，我们提出了特定于小组的量化和基于分位数的量化方法。我们的方法是插件，使无缝集成到各种MLLM中，以提高内存效率而无需进行体系结构修改。广泛的实验表明，我们的方法有效地减少了内存开销，同时保持计算效率并保留多模式性能。]]></description>
      <guid>https://arxiv.org/abs/2502.14882</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LVLM和自动指标能否捕获盲人和低视频个人的潜在偏好？</title>
      <link>https://arxiv.org/abs/2502.14883</link>
      <description><![CDATA[ARXIV：2502.14883V1公告类型：新 
摘要：视觉是人类如何看待环境的主要手段，但是盲人和低视觉（BLV）人们需要帮助了解周围环境，尤其是在陌生的环境中。基于语义的系统作为BLV用户的辅助工具的出现已经激发了许多研究人员探索大型视觉语言模型（LVLMS）的响应。但是，尚未研究BLV用户对LVLM的各种响应的偏好，特别是用于导航辅助。为了填补这一空白，我们首先构建了由人类验证的1.1K策划的室外/室内场景组成的数据集，每个场景的相关请求为5-10个。然后，我们对八个BLV用户进行了深入的用户研究，以从五个角度从六个LVLM上评估他们的偏好：害怕，不可行，充分性和简洁性。最后，我们介绍了Eye4b基准测试，以评估广泛使用的基于模型的图像文本指标与我们收集的BLV偏好之间的对齐。我们的工作可以作为将BLV感知的LVLMS开发到无屏障AI系统的指南。]]></description>
      <guid>https://arxiv.org/abs/2502.14883</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SEM-CLIP：扫描电子显微镜图像中纳米级缺陷检测的精确少数学习</title>
      <link>https://arxiv.org/abs/2502.14884</link>
      <description><![CDATA[ARXIV：2502.14884V1公告类型：新 
摘要：在综合电路制造领域，纳米级晶片缺陷的检测和分类对于随后的根本原因分析和提高产量至关重要。在扫描电子显微镜（SEM）图像中观察到的复杂背景模式和缺陷的各种纹理构成了重大挑战。传统方法通常遭受数据，标签和可传递性差的不足。在本文中，我们提出了一种新颖的几次学习方法，即SEM-CLIP，以进行准确的缺陷分类和分割。 Sem-CLIP自定义了对比度语言图像预处理（剪辑）模型，以更好地专注于缺陷区域并最大程度地减少背景干扰，从而提高了细分精度。我们采用文本提示，丰富了域知识作为先验信息，以协助精确分析。此外，我们的方法还将功能工程与文本指导结合在一起，以更有效地对缺陷进行分类。 SEM-CLIP几乎不需要带注释的数据，从而大大减少了半导体行业的劳动力需求。广泛的实验验证表明，我们的模型在很少的学习方案下实现了令人印象深刻的分类和分割结果。]]></description>
      <guid>https://arxiv.org/abs/2502.14884</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基础AI模型时代的手术场景理解：全面评论</title>
      <link>https://arxiv.org/abs/2502.14886</link>
      <description><![CDATA[ARXIV：2502.14886V1公告类型：新 
摘要：机器学习（ML）和深度学习（DL）的最新进展，尤其是通过引入基础模型（FMS），在微创手术（MIS）中具有显着增强的手术现场理解。本文调查了最先进的ML和DL技术的整合，包括卷积神经网络（CNN），视觉变压器（VITS）和基础模型，例如任何段（SAM）（SAM），即手术工作流程。这些技术在手术内窥镜视频分析中提高了分割精度，仪器跟踪和相位识别。本文探讨了这些技术所面临的挑战，例如数据可变性和计算需求，并讨论了临床环境中的道德考虑和整合障碍。我们强调了FMS的作用，我们桥接了技术能力，并阐述了临床需求，并概述了未来的研究方向，以增强AI在手术中应用中AI应用的适应性，效率和道德一致性。我们的发现表明已经取得了重大进展。但是，需要更加集中的努力来实现这些技术无缝整合到临床工作流程中，从而确保它们通过提高精度，降低风险并优化患者结果来补充手术实践。]]></description>
      <guid>https://arxiv.org/abs/2502.14886</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过潜在扩散模型预测视觉增强的时间序列</title>
      <link>https://arxiv.org/abs/2502.14887</link>
      <description><![CDATA[ARXIV：2502.14887V1公告类型：新 
摘要：扩散模型最近已成为生成高质量图像的强大框架。尽管最近的研究探讨了他们对时间序列预测的应用，但这些方法在跨模式建模和有效转换视觉信息以捕获时间模式方面面临着巨大的挑战。在本文中，我们提出了LDM4TS，这是一个新颖的框架，利用潜在扩散模型的强大图像重建功能来预测视觉增强时间序列。我们不是引入外部视觉数据，而是第一个使用互补转换技术将时间序列转换为多视觉视觉表示的人，从而使模型可以利用预训练的视觉编码器的丰富特征提取能力。随后，这些表示是使用具有跨模式调节机制和融合模块的潜扩散模型重建的。实验结果表明，LDM4TS在时间序列预测任务方面的表现优于各种专业预测模型。]]></description>
      <guid>https://arxiv.org/abs/2502.14887</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多模式表示中的多方面的单体气质</title>
      <link>https://arxiv.org/abs/2502.14888</link>
      <description><![CDATA[ARXIV：2502.14888V1公告类型：新 
摘要：在本文中，我们利用特征单体性的最新进步来从深层多模型中提取可解释的特征，从而提供了对模态差距的数据驱动的理解。具体而言，我们研究了剪辑（对比性语言图像预处理），这是一种在广泛的图像文本对中训练的突出的视觉表示模型。在为单模模型开发的可解释性工具的基础上，我们扩展了这些方法，以评估夹子特征的多模式可解释性。此外，我们将模式优势得分（MDS）介绍为将每个特征的解释性归因于其各自的模态。接下来，我们将剪辑功能转换为更容易解释的空间，使我们能够将它们分为三个不同的类：视觉功能（单模式），语言特征（单模式）和视觉语言特征（交叉模式）。我们的发现表明，这种分类与人类对不同方式的认知理解紧密相符。我们还展示了这种特定于模态特征的重要用例，包括检测性别偏见，对抗性攻击防御和文本对图像模型编辑。这些结果表明，配备了任务不可解释性工具的大规模多模型模型为关键连接和不同模态之间的区别提供了宝贵的见解。]]></description>
      <guid>https://arxiv.org/abs/2502.14888</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>狭窄的信息瓶颈理论用于多模式图像文本表示可解释性</title>
      <link>https://arxiv.org/abs/2502.14889</link>
      <description><![CDATA[ARXIV：2502.14889V1公告类型：新 
摘要：识别多模式图像文本表示的任务引起了人们的关注，尤其是诸如剪辑（对比性语言图像预审计）之类的模型，这些模型在学习图像和文本之间的学习复杂关联时表现出了出色的表现。尽管取得了这些进步，但确保此类模型的可解释性对于它们在医疗保健等实际应用中的安全部署至关重要。尽管已经为单峰任务开发了许多可解释性方法，但由于表示结构的固有差异，这些方法通常无法有效地转移到多模式上下文。在信息理论中建立良好的瓶颈方法已应用于增强剪辑的解释性。但是，通常会受到强烈的假设或固有的随机性的阻碍。为了克服这些挑战，我们提出了狭窄的信息瓶颈理论，这是一个从根本上重新定义传统瓶颈方法的新颖框架。该理论专门设计用于满足当代归因公理，为改善多模型的可解释性提供了一种更健壮和可靠的解决方案。在我们的实验中，与最先进的方法相比，我们的方法平均可以增强图像可解释性9％，文本可解释性平均提高58.83％，并使处理速度的加速速度升高63.95％。我们的代码可在https://github.com/lmbtough/nib上公开访问。]]></description>
      <guid>https://arxiv.org/abs/2502.14889</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>杂草：使用迪特尔和视网膜进行精确农业的多阶段生长和杂草分类</title>
      <link>https://arxiv.org/abs/2502.14890</link>
      <description><![CDATA[ARXIV：2502.14890V1公告类型：新 
摘要：杂草管理仍然是农业的关键挑战，在农业中，杂草与农作物争夺基本资源，从而造成了巨大的收益率损失。在各个生长阶段的杂草的准确检测对于有效的管理但对农民来说具有挑战性至关重要，因为它需要在多个生长阶段识别不同的物种。这项研究通过利用高级对象检测模型，特别是带有Resnet50骨干和带有Resnext101骨架的视网膜的检测变压器（DETR）来解决这些挑战，以识别和分类174个班级的16种经济问题，跨越了11周的增长从幼苗到成熟的阶段。开发了一个包含203,567张图像的强大数据集，并通过物种和生长阶段精心贴上标记。对这些模型进行了严格的训练和评估，视网膜表现出卓越的性能，与Detr的地图分别为0.854和0.840相比，在训练集上的平均平均精度（MAP）为0.907和0.904。视网膜还优于7.28 fps的回忆和推理速度的表现，使其更适合实时应用。随着植物的成熟，这两种模型均表现出提高的精度。这项研究为开发精确，可持续和自动化的杂草管理策略提供了关键见解，为实时物种特定的检测系统铺平了道路，并通过在模型开发和早期检测准确性方面的持续创新来推动AI-AI-Assiscrist农业。]]></description>
      <guid>https://arxiv.org/abs/2502.14890</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Codiff：协作3D对象检测的条件扩散模型</title>
      <link>https://arxiv.org/abs/2502.14891</link>
      <description><![CDATA[ARXIV：2502.14891V1公告类型：新 
摘要：协作3D对象检测在自主驾驶领域中非常重要，因为它通过促进多个代理之间的信息交换来大大增强了每个人的感知能力。但是，实际上，由于构成估计错误和时间延迟，跨代理的信息的融合通常会导致具有空间和时间噪声的特征表示，从而导致检测错误。扩散模型自然能够将嘈杂的样本定义为理想数据，这促使我们探索使用扩散模型来解决多代理系统之间的噪声问题。在这项工作中，我们提出了Codiff，这是一个新颖的强大协作感知框架，它利用扩散模型的潜力生成更全面和更清晰的特征表示。据我们所知，这是第一项将扩散模型应用于多代理协作感知的工作。具体而言，我们将高维功能图投影到功能强大的预训练自动编码器的潜在空间中。在这个空间中，单个代理信息是指导扩散模型采样的条件。此过程将粗糙的特征图降低，并逐步完善了融合功能。对模拟和现实世界数据集的实验研究表明，所提出的框架CODIFF始终优于合作对象检测性能的现有相关方法，并且当代理的姿势和延迟信息具有高级噪声时，表现出高度期望的鲁棒性。]]></description>
      <guid>https://arxiv.org/abs/2502.14891</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Egoskeak：学习何时为野外以自我为中心的对话代理人说话</title>
      <link>https://arxiv.org/abs/2502.14892</link>
      <description><![CDATA[ARXIV：2502.14892V1公告类型：新 
摘要：预测何时在现实世界环境中发起语音仍然是对会话代理的基本挑战。我们介绍了EgoSeak，这是一个新颖的框架，用于以自我为中心的流媒体视频中实时语音启动预测。通过从说话者的第一人称观点中对对话进行建模，Egoskeak是针对类似人类的互动而定制的，在这种互动中，对话代理必须不断观察其环境并动态决定何时进行交谈。我们的方法通过整合四个关键功能来弥合简化的实验设置与复杂的自然对话之间的差距：（1）第一人称视角，（2）RGB处理，（3）在线处理和（4）未修剪的视频处理。我们还提出了YT转换，这是来自YouTube的野外对话视频的多样化集合，作为大规模预处理的资源。对EasyCom和EGO4D的实验表明，Egoseak实时超过了随机和沉默的基准。我们的结果还强调了多模式输入和上下文长度在有效决定何时讲话时的重要性。]]></description>
      <guid>https://arxiv.org/abs/2502.14892</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>NOTA：视觉大语言模型的多模式音乐符号理解</title>
      <link>https://arxiv.org/abs/2502.14893</link>
      <description><![CDATA[ARXIV：2502.14893V1公告类型：新 
摘要：符号音乐以两种不同的形式表示：二维，视觉直观的分数图像和一维标准化的文本注释序列。尽管大型语言模型在音乐中表现出极大的潜力，但当前的研究主要集中在单峰符号序列文本上。现有的通用视觉语言模型仍然缺乏音乐符号理解的能力。认识到这一差距，我们提出了Nota，这是第一个大型综合多模式符号数据集。它由来自世界3个地区的1,019,237个记录组成，并包含3个任务。基于数据集，我们训练了宣布，这是一种音乐符号视觉大语言模型。具体而言，我们涉及一个预先对准训练阶段，以在音乐得分图像中描述的音符与ABC符号中的文字表示之间进行跨模式对齐。随后的培训阶段着重于基础音乐信息提取，然后进行音乐符号分析培训。实验结果表明，我们的宣传-7B在音乐理解上取得了重大改进，展示了NOTA和训练管道的有效性。我们的数据集在https://huggingface.co/datasets/myth-lab/nota-dataset上进行开源。]]></description>
      <guid>https://arxiv.org/abs/2502.14893</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>专注于污染：地理空间深度学习框架，具有噪音吸引的地表水PFAS预测</title>
      <link>https://arxiv.org/abs/2502.14894</link>
      <description><![CDATA[ARXIV：2502.14894V1公告类型：新 
摘要：在不粘炊具等产品中发现的化学物质（PFAS）和多氟烷基物质（PFA）是不幸的是具有严重健康风险的持续存在的环境污染物。准确地映射PFAS污染对于指导有针对性的补救工作和保护公共和环境健康至关重要，但是由于测试成本和模拟其传播的困难，大型地区的检测仍然具有挑战性。在这项工作中，我们引入了焦点，这是一个具有标签噪声损失函数的地理空间深度学习框架，以预测大型地区地表水中的PFAS污染。通过整合水文流数据，土地覆盖信息以及与已知的PFA源的邻近性，我们的方法利用了空间和环境环境来提高预测准确性。我们通过广泛的消融研究和针对基线的比较分析（如稀疏分割）以及现有的科学方法（包括Kriging和污染物运输模拟）来评估方法的性能。结果突出了我们的框架进行可扩展的PFA监测的潜力。]]></description>
      <guid>https://arxiv.org/abs/2502.14894</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用时空连贯的高斯表示天气的高动力雷达序列预测</title>
      <link>https://arxiv.org/abs/2502.14895</link>
      <description><![CDATA[ARXIV：2502.14895V1公告类型：新 
摘要：天气现象是一项重要任务，涉及根据当前观察结果来预测未来的雷达回声序列，为灾难管理，运输和城市规划带来重大好处。当前的预测方法受培训和存储效率的限制，主要集中在特定高度的2D空间预测上。同时，每个时间戳上的3D体积预测在很大程度上尚未探索。为了应对这一挑战，我们使用新提出的时空相干高斯脱落（STC-GS）为3D雷达序列预测引入了一个综合框架，以进行动态雷达表示，并进行高效和准确的预测。具体而言，STC-GS不是依靠4D高斯进行动态场景重建，而是通过采用一组高斯人，同时有效地在连续帧中捕获其运动，从而在每个帧中优化了3D场景。它可以确保随着时间的推移对每个高斯的一致跟踪，从而使其在预测任务中特别有效。随着建立时间相关的高斯组，我们利用它们来训练Gaumamba，将记忆机制集成到MAMBA框架中。这使模型可以学习高斯基团的时间演变，同时有效地处理大量高斯令牌。结果，它可以在预测广泛的动态气象雷达信号方面达到效率和准确性。实验结果表明，与现有的3D表示方法相比，我们的STC-GS可以有效地表示具有超过$ 16的空间分辨率的3D雷达序列，而Gaumamba在预测广泛的高频谱中的先进方法均优于先进方法。 - 动态天气条件。]]></description>
      <guid>https://arxiv.org/abs/2502.14895</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>关于文本到图像扩散模型中概念擦除的综合调查</title>
      <link>https://arxiv.org/abs/2502.14896</link>
      <description><![CDATA[ARXIV：2502.14896V1公告类型：新 
摘要：文本对图像（T2I）模型在产生自然语言提示的高质量，不同的视觉内容方面取得了显着进步。但是，它们具有重现受版权保护的样式，敏感图像和有害内容的能力引起了重大的道德和法律关注。概念擦除通过修改T2I模型来防止产生不希望的内容，为外部过滤提供了主动替代方案。在这项调查中，我们提供了概念擦除的结构化概述，并根据其优化策略和它们修改的建筑组件对现有方法进行分类。我们将概念擦除方法分为微调，以进行参数更新，有效编辑的封闭式解决方案以及用于内容限制的推理时间干预措施，而无需修改重量。此外，我们探索绕过擦除技术并讨论新兴防御的对抗性攻击。为了支持进一步的研究，我们巩固了关键数据集，评估指标和基准，以评估擦除有效性和模型鲁棒性。这项调查是一种全面的资源，提供了有关概念擦除，挑战和未来方向不断发展的景观的见解。]]></description>
      <guid>https://arxiv.org/abs/2502.14896</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>