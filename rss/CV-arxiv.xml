<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Fri, 18 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>解码情绪：通过对比注意的声学感知揭示面部表情</title>
      <link>https://arxiv.org/abs/2410.12811</link>
      <description><![CDATA[arXiv:2410.12811v1 公告类型：新
摘要：表情识别通过准确检测用户的情绪状态，为内容推荐和心理健康等应用带来了巨大的希望。传统方法通常依赖于摄像头或可穿戴传感器，这引发了隐私问题并增加了额外的设备负担。此外，当训练数据集和推理数据集之间的分布发生变化时，现有的基于声学的方法难以保持令人满意的性能。在本文中，我们介绍了一种主动声学面部表情识别系统 FacER+，它消除了对外部麦克风阵列的需求。FacER+ 通过分析智能手机上 3D 面部轮廓和耳机扬声器之间发出的近超声信号的回声来提取面部表情特征。这种方法不仅可以降低背景噪音，而且还可以用最少的训练数据识别来自不同用户的不同表情。我们开发了一个基于对比外部注意力的模型，以一致地学习不同用户的表情特征，从而减少分布差异。经过 20 名志愿者（佩戴和不佩戴口罩）的大量实验，结果表明 FacER+ 可以在各种独立于用户的现实生活场景中准确识别六种常见面部表情，准确率超过 90%，比领先的声学传感方法的性能高出 10%。FacER+ 为面部表情识别提供了强大而实用的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2410.12811</guid>
      <pubDate>Fri, 18 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用生成模型来描述图像分类器的失败情况</title>
      <link>https://arxiv.org/abs/2410.12814</link>
      <description><![CDATA[arXiv:2410.12814v1 公告类型：新
摘要：我们在本研究中解决了识别给定图像分类器的故障条件的问题。为此，我们利用最近的生成对抗网络 (StyleGAN2) 提供的生成高质量图像数据的可控分布的能力：故障条件表示为生成模型潜在空间中性能下降的方向。这种分析策略用于发现结合多种损坏源的极端情况，并更详细地比较不同分类器的行为。还可以通过生成数据以更好的可解释性直观地呈现退化的方向。一些退化（例如图像质量）会影响所有类别，而其他退化（例如形状）则更具类别特异性。该方法在由两个损坏源（噪声和模糊）完成的 MNIST 数据集上进行了演示，并展示了一种有希望的方法来更好地理解和控制利用人工智能组件进行安全关键应用的风险。]]></description>
      <guid>https://arxiv.org/abs/2410.12814</guid>
      <pubDate>Fri, 18 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从因果视角重新思考视觉-语言模型适应中的错位</title>
      <link>https://arxiv.org/abs/2410.12816</link>
      <description><![CDATA[arXiv:2410.12816v1 公告类型：新
摘要：基础视觉语言模型（例如 CLIP）在下游任务中表现出了令人印象深刻的泛化能力。然而，CLIP 在适应特定任务时存在两级错位问题，即任务错位和数据错位。软提示调整减轻了任务错位，但数据错位仍然是一个挑战。为了分析数据错位的影响，我们重新审视了 CLIP 的预训练和适应过程，并开发了一个结构因果模型。我们发现，虽然我们希望准确捕获下游任务的任务相关信息，但与任务无关的知识会影响预测结果并妨碍对图像和预测类别之间真实关系的建模。由于任务无关知识不可观测，我们利用前门调整并提出因果关系引导的语义解耦和分类 (CDC) 来减轻任务无关知识的干扰。具体来说，我们将下游任务数据中包含的语义解耦并根据每个语义进行分类。此外，我们使用 Dempster-Shafer 证据理论来评估由不同语义生成的每个预测的不确定性。在多个不同设置中进行的实验一致证明了 CDC 的有效性。]]></description>
      <guid>https://arxiv.org/abs/2410.12816</guid>
      <pubDate>Fri, 18 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>面向工业环境的交互式可解释异常检测</title>
      <link>https://arxiv.org/abs/2410.12817</link>
      <description><![CDATA[arXiv:2410.12817v1 公告类型：新
摘要：能够识别工业物体中的缺陷是生产线质量保证的关键要素。我们的研究重点是 RGB 图像中的视觉异常检测。尽管卷积神经网络 (CNN) 在该任务中实现了高精度，但工业环境中的最终用户无需额外解释即可获得模型的决策。因此，有必要通过进一步的解释来丰富模型的输出，以增加对模型的信心并加快异常检测速度。在我们的工作中，我们专注于 (1) 基于 CNN 的分类模型和 (2) 进一步开发与模型无关的黑盒分类器解​​释算法。此外，(3) 我们演示了如何建立一个交互式界面，让用户进一步纠正模型的输出。我们介绍了我们的 NearCAIPI 交互框架，该框架通过用户交互来改进 AI，并展示了这种方法如何提高系统的可信度。我们还说明了 NearCAIPI 如何将人工反馈整合到交互式流程链中。]]></description>
      <guid>https://arxiv.org/abs/2410.12817</guid>
      <pubDate>Fri, 18 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AVID：将视频传播模型调整为世界模型</title>
      <link>https://arxiv.org/abs/2410.12822</link>
      <description><![CDATA[arXiv:2410.12822v1 公告类型：新
摘要：大规模生成模型在许多领域取得了显著的成功。然而，对于机器人等顺序决策问题，动作标记数据往往很少，因此扩大决策基础模型仍然是一个挑战。一个潜在的解决方案是利用广泛可用的未标记视频来训练模拟动作后果的世界模型。如果世界模型准确，它可以用于优化下游任务中的决策。图像到视频的扩散模型已经能够生成高度逼真的合成视频。然而，这些模型不是动作条件的，最强大的模型是闭源的，这意味着它们无法进行微调。在这项工作中，我们建议将预训练的视频扩散模型调整为动作条件的世界模型，而无需访问预训练模型的参数。我们的方法 AVID 在一个小型特定领域的动作标记视频数据集上训练一个适配器。 AVID 使用学习掩码来修改预训练模型的中间输出并生成准确的动作条件视频。我们在视频游戏和现实世界的机器人数据上评估了 AVID，并表明它在扩散模型适应方面优于现有的基线。1 我们的结果表明，如果使用得当，预训练视频模型有可能成为具身人工智能的强大工具。]]></description>
      <guid>https://arxiv.org/abs/2410.12822</guid>
      <pubDate>Fri, 18 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GCM-Net：基于元启发式驱动网络的图形增强跨模态融合，用于视频情绪和情感分析</title>
      <link>https://arxiv.org/abs/2410.12828</link>
      <description><![CDATA[arXiv:2410.12828v1 公告类型：新
摘要：鉴于不同模态传达的信息的多样性和复杂性，视频中的情感分析和情感识别是一项具有挑战性的任务。开发一个能够有效解决各种模态之间不同特征的高效框架是该领域的主要关注点。以前关于多模态情感和情感分析的研究经常忽视模态整合的有效融合、模态间上下文一致性、优化连接特征空间，导致架构不理想。本文提出了一个新颖的框架，该框架利用来自话语的多模态上下文信息，并应用元启发式算法来学习话语级情感和情感预测的贡献特征。我们的图形增强跨模态融合与元启发式驱动网络 (GCM-Net) 集成了图形采样和聚合，以重新校准视频情感和情感预测的模态特征。GCM-Net 包括一个跨模态注意模块，用于确定模态间交互和话语相关性。采用元启发式算法的谐波优化模块结合了关注特征，可以处理单声道和多声道输入。为了证明我们方法的有效性，我们对三个著名的多模态基准数据集 CMU MOSI、CMU MOSEI 和 IEMOCAP 进行了广泛的评估。实验结果证明了我们提出的方法的有效性，在 MOSI 和 MOSEI 数据集上情绪分析的准确率分别为 91.56% 和 86.95%。我们对 IEMOCAP 数据集进行了情感分析，准确率达到 85.66%，这意味着与现有方法相比，性能有了显著的提升。]]></description>
      <guid>https://arxiv.org/abs/2410.12828</guid>
      <pubDate>Fri, 18 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DEeR：隐私保护联合低秩自适应的偏差消除和噪声调节</title>
      <link>https://arxiv.org/abs/2410.12926</link>
      <description><![CDATA[arXiv:2410.12926v1 公告类型：新 
摘要：将低秩自适应（LoRA）与联邦学习（FL）相结合最近受到了广泛关注，旨在通过隐私保护的分散训练将预训练的基础模型（FM）适应下游医疗任务。然而，由于LoRA和FL的直接结合，当前的方法通常存在两个问题，即聚合偏差和差分隐私（DP）噪声放大效应。为了解决这些问题，我们提出了一种新颖的隐私保护联邦微调框架，称为\ underline{D}eviation \ underline{E} liminating和Nois \ underline{e} \ underline{R} regulating（DEeR）。具体而言，我们首先从理论上证明消除聚合偏差的必要条件是保证客户端的LoRA参数之间的等价性。基于理论洞察，设计偏差消除器，利用交替最小化算法迭代优化LoRA的零初始化和非零初始化参数矩阵，确保训练期间聚合偏差始终为零。此外，我们还对噪声放大效应进行了深入分析，发现该问题主要是由DP噪声和LoRA参数之间的“线性关系”引起的。为了抑制噪声放大效应，我们提出了一种噪声调节器，利用两个调节因子来解耦DP和LoRA之间的关系，从而实现强大的隐私保护和出色的微调性能。此外，我们进行了全面的消融实验以验证偏差消除器和噪声调节器的有效性。与最先进的方法相比，DEeR在公共医疗数据集上表现出更好的性能。代码可在https://github.com/CUHK-AIM-Group/DEeR获得。]]></description>
      <guid>https://arxiv.org/abs/2410.12926</guid>
      <pubDate>Fri, 18 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DreamCraft3D++：利用多平面重建模型实现高效的分层 3D 生成</title>
      <link>https://arxiv.org/abs/2410.12928</link>
      <description><![CDATA[arXiv:2410.12928v1 公告类型：新
摘要：我们介绍了 DreamCraft3D++，这是 DreamCraft3D 的扩展，可以高效高质量地生成复杂的 3D 资源。DreamCraft3D++ 继承了 DreamCraft3D 的多阶段生成过程，但用基于前馈多平面的重建模型取代了耗时的几何雕刻优化，将过程加快了 1000 倍。对于纹理细化，我们提出了一个无需训练的 IP 适配器模块，该模块以增强的多视图图像为条件，以增强纹理和几何一致性，为 DreamCraft3D 的 DreamBooth 微调提供了 4 倍的替代方案。在不同数据集上进行的实验表明，DreamCraft3D++ 能够生成具有复杂几何形状和逼真的 360{\deg} 纹理的创意 3D 资源，在质量和速度方面优于最先进的图像到 3D 方法。整个实施过程将是开源的，从而为 3D 内容创作提供新的可能性。]]></description>
      <guid>https://arxiv.org/abs/2410.12928</guid>
      <pubDate>Fri, 18 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>UMambaAdj：使用 UMamba 和 nnU-Net ResEnc Planner 推进 MRI 引导 RT 中头颈癌的 GTV 分割</title>
      <link>https://arxiv.org/abs/2410.12940</link>
      <description><![CDATA[arXiv:2410.12940v1 公告类型：新
摘要：磁共振成像 (MRI) 因其优越的软组织对比度而在 MRI 引导的头颈癌 (HNC) 自适应放射治疗中起着至关重要的作用。然而，准确分割包括原发肿瘤 (GTVp) 和淋巴结 (GTVn) 的大肿瘤体积 (GTV) 仍然具有挑战性。最近，两项深度学习分割创新显示出巨大的前景：UMamba，可有效捕获长距离依赖关系，以及 nnU-Net 残差编码器 (ResEnc)，可通过多级残差块增强特征提取。在本研究中，我们将这些优势整合到一种称为“UMambaAdj”的新方法中。我们提出的方法在 HNTS-MRG 2024 挑战测试集上使用放疗前 T2 加权 MRI 图像进行了评估，GTVp 的聚合 Dice 相似系数 (DSCagg) 为 0.751，GTVn 的聚合 Dice 相似系数为 0.842，平均 DSCagg 为 0.796。这种方法表明，在 MRI 引导的自适应放疗中，肿瘤描绘更精确，最终改善 HNC 患者的治疗效果。团队：DCPT-Stine 团队。]]></description>
      <guid>https://arxiv.org/abs/2410.12940</guid>
      <pubDate>Fri, 18 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>梯度图辅助头颈部肿瘤分割：MRI 引导放射治疗中的放疗前至放疗中方法</title>
      <link>https://arxiv.org/abs/2410.12941</link>
      <description><![CDATA[arXiv:2410.12941v1 公告类型：新
摘要：放射治疗 (RT) 是头颈癌治疗的重要组成部分，其中准确分割大体肿瘤体积 (GTV) 对于有效的治疗计划至关重要。本研究探讨了在 MRI 引导的自适应放射治疗中使用放射前肿瘤区域和局部梯度图来增强头颈癌的放射中期肿瘤分割。通过利用放射前图像及其分割作为先验知识，我们解决了放射中期分割中肿瘤定位的挑战。计算放射前图像中的肿瘤区域梯度图并将其应用于放射中期图像以改善肿瘤边界描绘。我们的方法证明了对原发性 GTV (GTVp) 和淋巴结 GTV (GTVn) 的分割精度有所提高，尽管性能受到数据限制的限制。挑战赛测试集评估的最终 DSCagg 得分为 GTVp 0.534、GTVn 0.867，平均得分为 0.70。该方法显示出增强自适应放射治疗中的分割和治疗计划的潜力。团队：DCPT-Stine 团队。]]></description>
      <guid>https://arxiv.org/abs/2410.12941</guid>
      <pubDate>Fri, 18 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>超分辨率真实世界图像照明增强：新数据集和条件扩散模型</title>
      <link>https://arxiv.org/abs/2410.12961</link>
      <description><![CDATA[arXiv:2410.12961v1 公告类型：新
摘要：大多数现有的超分辨率方法和数据集都是为了在光照充足的情况下提高图像质量而开发的。然而，这些方法在现实世界的低光照条件下效果不佳，因为在这种条件下拍摄的图像会丢失最重要的信息并包含大量未知噪声。为了解决这个问题，我们提出了一个基于高效条件扩散概率模型方法的 SRRIIE 数据集。所提出的数据集包含 4800 对低质量-高质量图像。为了确保数据集能够模拟低照度环境下的真实图像退化，我们使用 ILDC 相机和光学变焦镜头捕捉图像，曝光水平范围从 -6 EV 到 0 EV，ISO 水平范围从 50 到 12800。我们综合评估了各种重建和感知指标，并证明了 SRRIIE 数据集对于基于深度学习的方法的实用性。我们表明，大多数现有方法在从复杂噪声中保留恢复图像的结构和清晰度方面效果较差。为了解决这个问题，我们修改了原始传感器数据的条件，并提出了一种新的扩散概率模型时间融合条件。在真实世界基准数据集上进行的全面定量和定性实验结果证明了所提出的条件扩散概率模型在原始传感器数据上的可行性和有效性。代码和数据集将在 https://github.com/Yaofang-Liu/Super-Resolving 上提供]]></description>
      <guid>https://arxiv.org/abs/2410.12961</guid>
      <pubDate>Fri, 18 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>可分离形状集合的可解释二元分类</title>
      <link>https://arxiv.org/abs/2410.12994</link>
      <description><![CDATA[arXiv:2410.12994v1 公告类型：新
摘要：材料科学家利用显微照片的图像分割来创建表示材料微结构晶粒边界的大型曲线集合。对这些形状集合的观察可以促进对材料特性和制造工艺的推断。我们寻求使用新颖的模式识别形式和对大量分段曲线的推断来支持此应用以及相关的工程/科学任务——即促进量化形状分布差异的原则性评估。为此，我们应用复合积分算子来激发矩阵流形上离散平面曲线的准确和有效的数值表示。主要结果是将曲线分量函数刚性不变正交分解为可分离的尺度变化形式和波动的互补特征。我们展示了这些可分离形状张量（给定一个集合中的数千条曲线）如何通过利用乘积最大平均差异来区分形状分布，从而为分割图像的可解释二元分类提供信息；在没有标记数据的情况下，无需高性能计算即可在几秒钟内构建可解释的特征空间，并在粗略的目视检查下检测出差异。]]></description>
      <guid>https://arxiv.org/abs/2410.12994</guid>
      <pubDate>Fri, 18 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过相互知识解释和分析 CLIP 的零样本图像分类</title>
      <link>https://arxiv.org/abs/2410.13016</link>
      <description><![CDATA[arXiv:2410.13016v1 公告类型：新
摘要：对比语言-图像预训练 (CLIP) 通过将图像和文本类表示映射到共享嵌入空间，然后检索最接近图像的类来执行零样本图像分类。这项工作提供了一种从两种模态之间的相互知识角度解释 CLIP 模型进行图像分类的新方法。具体来说，我们问：视觉和语言 CLIP 编码器共同学习了哪些概念，这些概念会影响联合嵌入空间，导致点更近或更远？我们通过基于文本概念的解释方法来回答这个问题，展示它们的有效性，并对 13 个 CLIP 模型池进行分析，这些模型在架构、大小和预训练数据集方面各不相同。我们探索与相互知识相关的不同方面，并分析零样本预测。我们的方法展示了一种有效且人性化的方式来理解使用 CLIP 的零样本分类决策。]]></description>
      <guid>https://arxiv.org/abs/2410.13016</guid>
      <pubDate>Fri, 18 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>几何轨迹扩散模型</title>
      <link>https://arxiv.org/abs/2410.13027</link>
      <description><![CDATA[arXiv:2410.13027v1 公告类型：新
摘要：生成模型在生成 3D 几何系统方面显示出巨大的潜力，这是许多自然科学领域（例如分子和蛋白质设计）的基本问题。然而，现有的方法只对静态结构进行操作，忽略了物理系统本质上总是动态的事实。在这项工作中，我们提出了几何轨迹扩散模型 (GeoTDM)，这是第一个用于建模 3D 几何轨迹时间分布的扩散模型。建模这种分布具有挑战性，因为它需要捕捉具有物理对称性的复杂空间相互作用和动态中封装的时间对应关系。我们从理论上证明了具有等变时间核的扩散模型可以产生具有所需对称性的密度，并开发了一种利用 SE(3)-等变空间卷积和时间注意力的新型过渡核。此外，为了为条件生成诱导出富有表现力的轨迹分布，我们在前向扩散过程中引入了一个广义可学习的几何先验来增强时间调节。我们对各种场景中的无条件和条件生成进行了广泛的实验，包括物理模拟、分子动力学和行人运动。对一系列指标的实证结果表明，GeoTDM 可以生成质量显著提高的逼真几何轨迹。]]></description>
      <guid>https://arxiv.org/abs/2410.13027</guid>
      <pubDate>Fri, 18 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>生成式 VLM 对语义和词汇改变提示的敏感性</title>
      <link>https://arxiv.org/abs/2410.13030</link>
      <description><![CDATA[arXiv:2410.13030v1 公告类型：新
摘要：尽管生成视觉语言模型 (VLM) 的提示调整技术大量涌入，但这些模型对提示中的词汇和语义变化的敏感度仍不清楚。在本文中，我们使用 SugarCrepe++ 数据集评估生成 VLM 理解文本中词汇和语义变化的能力。我们分析了 VLM 对提示中词汇变化的敏感性，而没有相应的语义变化。我们的研究结果表明，生成 VLM 对此类变化高度敏感。此外，我们表明这种脆弱性会影响旨在实现输出一致性的技术的性能。]]></description>
      <guid>https://arxiv.org/abs/2410.13030</guid>
      <pubDate>Fri, 18 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>