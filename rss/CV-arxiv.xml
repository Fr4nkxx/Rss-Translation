<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Thu, 03 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>面向内窥镜相机深度估计的全参数和参数高效自学习</title>
      <link>https://arxiv.org/abs/2410.00979</link>
      <description><![CDATA[arXiv:2410.00979v1 公告类型：新
摘要：最近开发了适应深度基础模型以适应内窥镜深度估计的自适应方法。然而，这种方法通常训练效果不佳，因为它们将参数搜索限制在低秩子空间并改变了训练动态。因此，我们提出了一个用于内窥镜深度估计的全参数和参数高效的学习框架。在第一阶段，注意力、卷积和多层感知的子空间在不同的子空间内同时适应。在第二阶段，提出了一种内存高效的子空间组合优化方法，并在联合子空间中进一步提高性能。在 SCARED 数据集上的初步实验表明，与最先进的模型相比，第一阶段的结果将 Sq Rel、Abs Rel、RMSE 和 RMSE log 的性能从 10.2% 提高到 4.1%。]]></description>
      <guid>https://arxiv.org/abs/2410.00979</guid>
      <pubDate>Thu, 03 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ScVLM：用于理解驾驶安全关键事件的视觉语言模型</title>
      <link>https://arxiv.org/abs/2410.00982</link>
      <description><![CDATA[arXiv:2410.00982v1 公告类型：新 
摘要：准确识别、理解和描述驾驶安全关键事件 (SCE)，包括碰撞和近乎碰撞，对于交通安全、自动驾驶系统和高级驾驶辅助系统的研究和应用至关重要。由于 SCE 是罕见事件，大多数通用视觉语言模型 (VLM) 尚未经过足够的训练以将 SCE 视频和叙述联系起来，这可能会导致幻觉和缺少关键的安全特性。为了应对这些挑战，我们提出了 ScVLM，这是一种结合监督学习和对比学习的混合方法，以提高 VLM 对驾驶视频的理解和事件描述的合理性。所提出的方法在第二战略公路研究计划自然驾驶研究数据集中的 8,600 多个 SCE 上进行了训练和评估，该数据集是最大的可公开访问的驾驶数据集，其中包含视频和 SCE 注释。结果证明了所提出的方法在生成上下文准确的事件描述和减轻 VLM 幻觉方面的优势。]]></description>
      <guid>https://arxiv.org/abs/2410.00982</guid>
      <pubDate>Thu, 03 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LaDTalk：用于合成具有高频细节的说话头部视频的潜在去噪</title>
      <link>https://arxiv.org/abs/2410.00990</link>
      <description><![CDATA[arXiv:2410.00990v1 公告类型：新
摘要：音频驱动的说话头部生成是电影制作和虚拟现实中的关键领域。尽管现有方法在端到端范式之后取得了重大进展，但由于其在此领域的表达能力有限，它们在制作具有高频细节的视频方面仍然面临挑战。这种限制促使我们探索一种有效的后处理方法来合成照片般逼真的说话头部视频。具体来说，我们采用预训练的 Wav2Lip 模型作为基础模型，利用其强大的音频嘴唇对齐功能。借鉴 Lipschitz 连续性理论，我们从理论上建立了矢量量化自动编码器 (VQAE) 的噪声鲁棒性。我们的实验进一步表明，我们引入的空间优化矢量量化自动编码器 (SOVQAE) 可以在时间上一致地恢复基础模型的高频纹理缺陷，从而促进逼真的说话头部视频的创作。我们对传统数据集和我们整理的高频说话头 (HFTK) 数据集进行了实验。结果表明，我们的方法 LaDTalk 实现了新的最先进的视频质量和域外唇形同步性能。]]></description>
      <guid>https://arxiv.org/abs/2410.00990</guid>
      <pubDate>Thu, 03 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Y-CA-Net：基于卷积注意力的体积医学图像分割网络</title>
      <link>https://arxiv.org/abs/2410.01003</link>
      <description><![CDATA[arXiv:2410.01003v1 公告类型：新
摘要：最近基于注意力的体积分割 (VS) 方法在专注于建模长距离依赖关系的医学领域取得了显著的表现。然而，对于体素预测任务，判别性局部特征是 VS 模型性能的关键组成部分，而基于注意力的 VS 方法则缺少这一点。为了解决这个问题，我们特意将卷积编码器分支与 Transformer 主干结合起来，以并行方式提取局部和全局特征，并将它们聚合在交叉特征混合器模块 (CFMM) 中，以更好地预测分割掩模。因此，我们观察到派生模型 Y-CT-Net 在多个医学分割任务上取得了有竞争力的表现。例如，在多器官分割中，Y-CT-Net 实现了 82.4% 的骰子分数，超过了经过良好调整的 VS Transformer/CNN 类基线 UNETR/ResNet-3D 2.9%/1.4%。随着 Y-CT-Net 的成功，我们用混合注意力模型扩展了这一概念，即衍生出 Y-CH-Net 模型，该模型在相同分割任务中将 HD95 得分提高了 3%。Y-CT-Net 和 Y-CH-Net 两种模型的有效性验证了我们的假设，并促使我们提出 Y-CA-Net 的概念，这是一种基于任意两个编码器和一个解码器主干的通用通用架构，可以充分利用卷积和注意力机制的互补优势。根据实验结果，我们认为 Y-CA-Net 是实现体积分割卓越结果的关键因素。]]></description>
      <guid>https://arxiv.org/abs/2410.01003</guid>
      <pubDate>Thu, 03 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>对包括负面音频在内的视觉声源定位模型的严格评估</title>
      <link>https://arxiv.org/abs/2410.01020</link>
      <description><![CDATA[arXiv:2410.01020v1 公告类型：新
摘要：视觉声源定位 (VSSL) 的任务涉及识别视觉场景中声源的位置，整合视听数据以增强对场景的理解。尽管最先进的 (SOTA) 模型取得了进步，但我们观察到三个关键缺陷：i) 模型的评估主要集中在图像中可见物体产生的声音上，ii) 评估通常假设事先知道发声物体的大小，iii) 没有建立现实世界场景中定位的通用阈值，因为以前的方法只考虑正面的例子，而不考虑正面和负面的情况。在本文中，我们引入了一个新颖的测试集和指标，旨在通过在图像中没有任何对象与音频输入对应的场景（即负面音频）中测试它们来完成 VSSL 模型的当前标准评估。我们考虑三种类型的负面音频：静音、噪音和屏幕外。我们的分析表明，许多 SOTA 模型未能根据音频输入适当调整其预测，这表明这些模型可能没有按预期利用音频信息。此外，我们还对估计的视听相似性图中的最大值范围进行了全面分析，包括正音频和负音频情况，并表明大多数模型的判别能力不够强，因此不适合选择适合在没有任何发声物体的先验信息（即物体大小和可见性）的情况下执行声音定位的通用阈值。]]></description>
      <guid>https://arxiv.org/abs/2410.01020</guid>
      <pubDate>Thu, 03 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>视觉语言模型能否利用视觉线索解决文本歧义问题？让视觉双关语告诉你！</title>
      <link>https://arxiv.org/abs/2410.01023</link>
      <description><![CDATA[arXiv:2410.01023v1 公告类型：新
摘要：人类拥有多模态识字能力，使他们能够主动整合来自各种模态的信息以形成推理。面对文本中的词汇歧义等挑战，我们用其他模态来补充这一点，例如缩略图或教科书插图。机器有可能实现类似的多模态理解能力吗？作为回应，我们提出了通过图像解释理解双关语 (UNPIE)，这是一种新颖的基准，旨在评估多模态输入在解决词汇歧义方面的影响。双关语因其内在的歧义性而成为这种评估的理想对象。我们的数据集包括 1,000 个双关语，每个都附有一张解释两种含义的图像。我们提出了三个多模态挑战和注释来评估多模态识字的不同方面；双关语基础、消歧和重构。结果表明，当给定视觉环境时，各种苏格拉底模型和视觉语言模型比纯文本模型有所改进，尤其是在任务复杂性增加时。]]></description>
      <guid>https://arxiv.org/abs/2410.01023</guid>
      <pubDate>Thu, 03 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FCE-YOLOv8：带有特征上下文激励模块的 YOLOv8，用于儿科腕部 X 射线图像中的骨折检测</title>
      <link>https://arxiv.org/abs/2410.01031</link>
      <description><![CDATA[arXiv:2410.01031v1 公告类型：新
摘要：儿童在日常生活中经常遭受腕部创伤，而他们通常需要放射科医生在外科医生进行手术治疗之前分析和解释X射线图像。深度学习的发展使神经网络可以作为计算机辅助诊断（CAD）工具，帮助医生和医学图像诊断专家。由于You Only Look Once Version-8（YOLOv8）模型在物体检测任务中取得了令人满意的成功，它已被应用于各种骨折检测。这项工作引入了特征上下文激励-YO​​LOv8（FCE-YOLOv8）模型的四种变体，每种变体都包含不同的FCE模块（即挤压和激励（SE）、全局上下文（GC）、聚集-激励（GE）和高斯上下文变换器（GCT）模块）以增强模型性能。在 GRAZPEDWRI-DX 数据集上的实验结果表明，我们提出的 YOLOv8+GC-M3 模型将 mAP@50 值从 65.78% 提高到 66.32%，在缩短推理时间的同时，性能超越了最先进 (SOTA) 模型。此外，我们提出的 YOLOv8+SE-M3 模型实现了最高的 mAP@50 值 67.07%，超过了 SOTA 性能。这项工作的实现可在 https://github.com/RuiyangJu/FCE-YOLOv8 上找到。]]></description>
      <guid>https://arxiv.org/abs/2410.01031</guid>
      <pubDate>Thu, 03 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ARPOV：通过全景拼接扩展 AR 中物体检测的可视化</title>
      <link>https://arxiv.org/abs/2410.01055</link>
      <description><![CDATA[arXiv:2410.01055v1 公告类型：新
摘要：随着增强现实 (AR) 的用途变得越来越复杂和广泛，AR 应用程序将越来越多地融入智能功能，这些功能要求开发人员了解用户的行为和周围环境（例如智能助手）。此类应用程序依赖于 AR 耳机捕获的视频，这些视频通常包含脱节的摄像头运动和有限的视野，无法捕捉用户在任何给定时间看到的全部范围。此外，可视化对象检测模型输出的标准方法仅限于在单个帧和时间步内捕获对象，因此无法捕获各种领域应用通常需要的时间和空间上下文。我们提出了 ARPOV，这是一种交互式可视化分析工具，用于分析针对 AR 耳机捕获的视频量身定制的对象检测模型输出，可最大限度地提高用户对模型性能的理解。所提出的工具利用全景拼接来扩展环境视图，同时自动过滤不需要的帧，并包括便于对象检测模型调试的交互式功能。 ARPOV 是可视化研究人员与机器学习和 AR 专家合作设计的一部分；我们通过采访 5 位领域专家来验证我们的设计选择。]]></description>
      <guid>https://arxiv.org/abs/2410.01055</guid>
      <pubDate>Thu, 03 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 3D 视觉深度学习模型对深海埋藏物体进行姿态估计</title>
      <link>https://arxiv.org/abs/2410.01061</link>
      <description><![CDATA[arXiv:2410.01061v1 公告类型：新
摘要：我们提出了一种估计在南加州圣佩德罗盆地海床上发现的垃圾场桶的姿势和埋藏分数的方法。我们的计算工作流程利用基础模型的最新进展进行分割，并利用基于视觉变换器的方法来估计定义桶几何形状的点云。我们提出使用 BarrelNet 从桶点云作为输入来估计埋藏桶的 6-DOF 姿势和半径。我们使用合成生成的桶点云训练 BarrelNet，并使用在历史垃圾场发现的桶的遥控车辆 (ROV) 视频片段定性地展示我们的方法的潜力。我们将我们的方法与传统的最小二乘拟合方法进行了比较，并根据我们定义的基准显示出显着的改进。]]></description>
      <guid>https://arxiv.org/abs/2410.01061</guid>
      <pubDate>Thu, 03 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>带有子采样层的深度网络在测试时不知不觉地丢弃了有用的激活</title>
      <link>https://arxiv.org/abs/2410.01083</link>
      <description><![CDATA[arXiv:2410.01083v1 公告类型：新
摘要：子采样层在深度网络中起着至关重要的作用，它通过丢弃激活图的一部分来减少其空间维度。这鼓励深度网络学习更高级的表示。与此动机相反，我们假设丢弃的激活是有用的，并且可以动态合并以改进模型的预测。为了验证我们的假设，我们提出了一种搜索和聚合方法来查找在测试时要使用的有用激活图。我们将我们的方法应用于图像分类和语义分割任务。在多个数据集上对九种不同架构进行的大量实验表明，我们的方法可以持续提高模型测试时性能，补充现有的测试时增强技术。我们的代码可在 https://github.com/ca-joe-yang/discard-in-subsampling 上找到。]]></description>
      <guid>https://arxiv.org/abs/2410.01083</guid>
      <pubDate>Thu, 03 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FMBench：在医疗任务中对多模态大型语言模型的公平性进行基准测试</title>
      <link>https://arxiv.org/abs/2410.01089</link>
      <description><![CDATA[arXiv:2410.01089v1 公告类型：新 
摘要：多模态大型语言模型 (MLLM) 的进步显著提高了医疗任务的性能，例如视觉问答 (VQA) 和报告生成 (RG)。然而，尽管这些模型在医疗保健中具有重要意义，但它们在不同人口群体中的公平性仍未得到充分探索。这种疏忽部分是由于现有医疗多模态数据集缺乏人口多样性，这使得公平性的评估变得复杂。作为回应，我们提出了 FMBench，这是第一个旨在评估 MLLM 在不同人口统计属性中的表现公平性的基准。FMBench 具有以下主要特点：1：它包括四个人口统计属性：种族、民族、语言和性别，涉及两个任务 VQA 和 RG，在零样本设置下。2：我们的 VQA 任务是自由形式的，增强了现实世界的适用性并减轻了与预定义选择相关的偏见。 3：我们利用词汇指标和基于 LLM 的指标，结合临床评估，不仅从语言准确性，而且从临床角度评估模型。此外，我们引入了一个新的指标，即公平性感知性能 (FAP)，以评估 MLLM 在各种人口统计属性中的表现如何。我们全面评估了八个最先进的开源 MLLM 的性能和公平性，包括通用和医学 MLLM，在拟议的基准上参数范围从 7B 到 26B。我们的目标是让 FMBench 协助研究界改进模型评估并推动该领域的未来进步。所有数据和代码将在接受后发布。]]></description>
      <guid>https://arxiv.org/abs/2410.01089</guid>
      <pubDate>Thu, 03 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 SegFormer 对无人机遥感图像进行语义分割</title>
      <link>https://arxiv.org/abs/2410.01092</link>
      <description><![CDATA[arXiv:2410.01092v1 公告类型：新
摘要：无人机 (UAV) 作为遥感平台的使用日益增多，引起了广泛关注，对地面物体识别具有不可估量的价值。虽然卫星遥感图像在分辨率和天气敏感性方面受到限制，但采用低速无人机的无人机遥感可以提高物体分辨率和灵活性。先进的机器学习技术的出现推动了图像分析的重大进步，特别是在无人机遥感图像的语义分割方面。本文评估了语义分割框架 SegFormer 对无人机图像语义分割的有效性和效率。使用为语义分割任务量身定制的 UAVid 数据集评估了从实时 (B0) 到高性能 (B5) 模型的 SegFormer 变体。该研究详细介绍了无人机语义分割背景下 SegFormer 特定的架构和训练程序。实验结果展示了该模型在基准数据集上的性能，突出了其在不同无人机场景中准确描绘物体和土地覆盖特征的能力，从而实现了高效率和性能。]]></description>
      <guid>https://arxiv.org/abs/2410.01092</guid>
      <pubDate>Thu, 03 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>RobustEMD：跨域小样本医学图像分割的域稳健匹配</title>
      <link>https://arxiv.org/abs/2410.01110</link>
      <description><![CDATA[arXiv:2410.01110v1 公告类型：新
摘要：少样本医学图像分割（FSMIS）旨在在医学图像分析范围内执行有限的注释数据学习。尽管已经取得了进展，但当前的FSMIS模型都是在同一个数据域上训练和部署的，这与医学影像数据总是跨不同数据域（例如成像模式、机构和设备序列）的临床现实不一致。如何增强FSMIS模型以在不同的特定医学影像领域中很好地推广？在本文中，我们重点研究了少样本语义分割模型的匹配机制，并针对跨域场景引入了一种基于地球移动距离（EMD）计算的域稳健匹配机制。具体来说，我们制定了前景支持查询特征之间的EMD传输过程，在EMD匹配流程中引入了纹理结构感知权重生成方法，该方法提出在节点上执行基于sobel的图像梯度计算，以约束域相关节点。此外，引入点集级距离度量指标来计算从支持集节点到查询集节点的传输成本。为了评估我们模型的性能，我们在三种场景（即跨模式、跨序列和跨机构）上进行了实验，其中包括八个医疗数据集并涉及三个身体区域，结果表明我们的模型相对于比较模型实现了 SoTA 性能。]]></description>
      <guid>https://arxiv.org/abs/2410.01110</guid>
      <pubDate>Thu, 03 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于模糊物体检测的合成图像：一项比较研究</title>
      <link>https://arxiv.org/abs/2410.01124</link>
      <description><![CDATA[arXiv:2410.01124v1 公告类型：新
摘要：模糊物体检测是计算机视觉 (CV) 中一个具有挑战性的研究领域。区分 CV 中的模糊和非模糊物体检测非常重要。与树木和汽车等非模糊物体相比，火、烟、雾和蒸汽等模糊物体在视觉特征、模糊边缘、不同形状、不透明度和体积方面呈现出更大的复杂性。收集平衡多样的数据集和准确的注释对于实现更好的模糊物体 ML 模型至关重要，然而，收集和注释的任务仍然是高度手动的。在这项研究中，我们提出并利用一种基于 3D 模型生成和自动注释完全合成火灾图像的替代方法来训练物体检测模型。此外，还将训练后的 ML 模型在合成图像上的性能和效率与在真实图像和混合图像上训练的 ML 模型进行了比较。研究结果证明了合成数据在火灾探测中的有效性，而随着测试数据集涵盖更广泛的真实火灾，性能也会提高。我们的研究结果表明，当在混合训练集中使用合成图像和真实图像时，生成的 ML 模型在检测各种火灾方面的表现优于在真实图像上训练的模型以及在合成图像上训练的模型。所提出的自动注释合成模糊物体图像的方法对于减少创建专门用于检测模糊物体的计算机视觉模型的时间和成本具有重大意义。]]></description>
      <guid>https://arxiv.org/abs/2410.01124</guid>
      <pubDate>Thu, 03 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用交错集成反学习来阻止后门程序对 Vision Transformer 进行微调</title>
      <link>https://arxiv.org/abs/2410.01128</link>
      <description><![CDATA[arXiv:2410.01128v1 公告类型：新
摘要：视觉变换器 (ViT) 在计算机视觉任务中变得流行起来。后门攻击会在推理过程中触发模型中的不良行为，威胁 ViT 的性能，尤其是在安全敏感任务中。尽管已经为卷积神经网络 (CNN) 开发了后门防御，但它们对 ViT 的效果较差，而且针对 ViT 量身定制的防御很少。为了解决这个问题，我们提出了交错集成反学习 (IEU)，这是一种在后门数据集上微调干净 ViT 的方法。在第 1 阶段，对浅层 ViT 进行微调以对后门数据具有高置信度，对干净数据具有低置信度。在第 2 阶段，浅层 ViT 充当“门”，阻止潜在的中毒数据进入受保护的 ViT。此数据被添加到反学习集中并通过梯度上升异步反学习。我们在三个数据集上证明了 IEU 对抗 11 种最先进的后门攻击的有效性，并通过将其应用于不同的模型架构展示了其多功能性。]]></description>
      <guid>https://arxiv.org/abs/2410.01128</guid>
      <pubDate>Thu, 03 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>