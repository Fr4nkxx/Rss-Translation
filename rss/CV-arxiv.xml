<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Sat, 07 Sep 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>视觉语言模型的多模式适配器</title>
      <link>https://arxiv.org/abs/2409.02958</link>
      <description><![CDATA[arXiv:2409.02958v1 公告类型：新
摘要：大型预训练视觉语言模型（例如 CLIP）已在广泛的图像分类任务中展示了最先进的性能，而无需重新训练。小样本 CLIP 与在下游任务上训练的现有专门架构具有竞争力。最近的研究表明，使用轻量级自适应方法可以进一步提高 CLIP 的性能。然而，以前的方法分别适应 CLIP 模型的不同模态，忽略了视觉和文本表示之间的相互作用和关系。在这项工作中，我们提出了多模态适配器，一种用于 CLIP 的多模态自适应的方法。具体来说，我们添加了一个可训练的多头注意力层，该层结合了文本和图像特征以产生两者的附加自适应。与现有的自适应方法相比，多模态适配器基于其在看不见的类别上的表现，表现出更好的通用性。我们进行了额外的消融和调查，以验证和解释所提出的方法。]]></description>
      <guid>https://arxiv.org/abs/2409.02958</guid>
      <pubDate>Sat, 07 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Vec2Face：使用松散约束向量扩展人脸数据集生成</title>
      <link>https://arxiv.org/abs/2409.02979</link>
      <description><![CDATA[arXiv:2409.02979v1 公告类型：新
摘要：本文研究如何合成不存在的人的脸部图像，以创建一个允许有效训练人脸识别 (FR) 模型的数据集。两个重要目标是 (1) 能够生成大量不同的身份（类间分离）和 (2) 每个身份的外观变化很大（类内变化）。然而，现有的工作 1) 通常限制可以生成多少个分离良好的身份，2) 忽略或使用单独的编辑模型进行属性增强。我们提出了 Vec2Face，这是一个整体模型，它仅使用采样向量作为输入，可以灵活地生成和控制人脸图像及其属性。Vec2Face 由特征掩蔽自动编码器和解码器组成，由人脸图像重建监督，可以方便地用于推理。使用彼此相似性较低的向量作为输入，Vec2Face 可以生成分离良好的身份。在小范围内随机扰动输入身份向量，Vec2Face 可以生成具有相同身份但人脸属性变化较大的人脸。还可以通过使用梯度下降法调整向量值来生成具有指定属性的图像。Vec2Face 已高效合成了多达 30 万个身份，总共 1500 万张图像，而 6 万是之前研究中创建的身份数量最多的。使用生成的 HSFace 数据集（从 1 万到 30 万个身份）训练的 FR 模型在五个真实世界测试集上实现了从 92% 到 93.52% 的最佳准确率。我们使用合成训练集创建的模型首次实现了比使用相同规模的真实人脸图像训练集创建的模型更高的准确率（在 CALFW 测试集上）。]]></description>
      <guid>https://arxiv.org/abs/2409.02979</guid>
      <pubDate>Sat, 07 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>无界：生成逼真的合成数据用于城市街景中的物体检测</title>
      <link>https://arxiv.org/abs/2409.03022</link>
      <description><![CDATA[arXiv:2409.03022v1 公告类型：新
摘要：我们介绍了 Boundless，这是一种照片级逼真的合成数据生成系统，可在密集的城市街景中实现高精度的物体检测。Boundless 可以用自动化和可配置的过程取代大量的现实世界数据收集和手动地面实况物体注释（标记）。Boundless 基于虚幻引擎 5 (UE5) 城市示例项目，并进行了改进，能够在不同的光照和场景变化条件下准确收集 3D 边界框。
我们评估了在 Boundless 生成的数据集上训练的物体检测模型在对从中空摄像机获取的现实世界数据集进行推理时的性能。我们将 Boundless 训练模型的性能与 CARLA 训练模型进行了比较，并观察到 ​​7.8 mAP 的改进。我们取得的结果支持了这样一个前提：合成数据生成是一种可靠的方法，可用于训练/微调可扩展的城市场景物体检测模型。]]></description>
      <guid>https://arxiv.org/abs/2409.03022</guid>
      <pubDate>Sat, 07 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>不遗漏任何细节：重新审视细粒度图像字幕的自我检索</title>
      <link>https://arxiv.org/abs/2409.03025</link>
      <description><![CDATA[arXiv:2409.03025v1 公告类型：新 
摘要：图像字幕系统无法生成细粒度字幕，因为它们是在嘈杂的数据（替代文本）或通用数据（人工注释）上进行训练的。最大似然训练鼓励生成频繁出现的短语，这进一步加剧了这种情况。以前的研究试图通过使用自我检索 (SR) 奖励对字幕制作者进行微调来解决这一限制。然而，我们发现 SR 微调有降低字幕忠实度甚至产生幻觉的趋势。在这项工作中，我们通过改进字幕系统的 MLE 初始化和设计 SR 微调过程的课程来绕过这个瓶颈。为此，我们提出了 (1) Visual Caption Boosting，这是一个新颖的框架，用于在通用图像字幕数据集中灌输细粒度，同时保持锚定在人工注释中； (2) BagCurri，一种精心设计的训练课程，可以更优化地利用自我检索奖励的对比性质。它们共同使字幕制作者能够描述图像中的细粒度方面，同时保持对真实字幕的忠实。我们的方法在对 99 个随机干扰项 (RD100) 的 SR 上比以前的工作高出 +8.9% (Dessi et al., 2023)；在 ImageCoDe 上高出 +7.6%。
此外，现有的评估字幕系统的指标未能奖励多样性或评估模型的细粒度理解能力。我们的第三个贡献通过从评估的角度提出自我检索来解决这个问题。我们引入了 TrueMatch，这是一个由高度相似的图像包组成的基准，它使用 SR 来评估字幕制作者捕捉细微视觉区别的能力。我们在 TrueMatch 上评估和比较了几种最先进的开源 MLLM，发现我们的 SR 方法以显著的优势胜过它们（例如比 Cambrian 高出 +4.8% - 7.1%），同时参数减少了 1-2 个数量级。]]></description>
      <guid>https://arxiv.org/abs/2409.03025</guid>
      <pubDate>Sat, 07 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过逆向渲染实现航空摄影测量图像反照率恢复的通用方法</title>
      <link>https://arxiv.org/abs/2409.03032</link>
      <description><![CDATA[arXiv:2409.03032v1 公告类型：新
摘要：为合成 3D 环境建模室外场景需要从原始图像中恢复反射率/反照率信息，这是一个不适定问题，因为此过程中存在复杂的未建模物理（例如间接照明、体积散射、镜面反射）。该问题在实际情况下仍未解决。恢复的反照率可以促进模型重新照明和着色，从而进一步增强渲染模型的真实感和数字孪生的应用。通常，摄影测量 3D 模型只是将源图像作为纹理材料，这固有地将不必要的照明伪影（在捕获时）嵌入到纹理中。因此，这些受污染的纹理对于合成环境而言并非最佳选择，无法实现逼真的渲染。此外，这些嵌入的环境照明进一步给不同图像之间的照片一致性带来挑战，从而导致图像匹配的不确定性。本文提出了一种通用图像形成模型，用于在自然光照下从典型的航空摄影测量图像中恢复反照率，并通过逆向渲染固有图像分解推导出解决反照率信息的逆模型。我们的方法建立在这样一个事实之上：在航空摄影测量中，太阳光照和场景几何都是可估计的，因此它们可以为这个不适定问题提供直接输入。这种基于物理的方法不需要除通过典型的无人机摄影测量收集获得的数据之外的其他输入，并且已被证明比现有方法表现更好。我们还证明，恢复的反照率图像反过来可以改进摄影测量中的典型图像处理任务，例如特征和密集匹配、边缘和线条提取。]]></description>
      <guid>https://arxiv.org/abs/2409.03032</guid>
      <pubDate>Sat, 07 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MDNF：网格上神经场的多扩散网络</title>
      <link>https://arxiv.org/abs/2409.03034</link>
      <description><![CDATA[arXiv:2409.03034v1 公告类型：新
摘要：我们提出了一种新颖的框架，用于在三角网格上表示神经场，该框架在空间和频域上都具有多分辨率。受神经傅里叶滤波器组 (NFFB) 的启发，我们的架构通过将更精细的空间分辨率级别与更高的频带相关联来分解空间和频域，而将较粗的分辨率映射到较低的频率。为了实现几何感知的空间分解，我们利用多个 DiffusionNet 组件，每个组件都与不同的空间分辨率级别相关联。随后，我们应用傅里叶特征映射来鼓励更精细的分辨率级别与更高的频率相关联。最终信号以小波启发的方式使用正弦激活 MLP 组合而成，将高频信号聚合在低频信号之上。我们的架构在学习复杂神经场方面实现了高精度，并且对不连续性、目标场的指数尺度变化和网格修改具有鲁棒性。我们通过将我们的方法应用于不同的神经领域（例如合成 RGB 函数、UV 纹理坐标和顶点法线）来证明其有效性，并说明不同的挑战。为了验证我们的方法，我们将其性能与两种替代方案进行了比较，展示了我们的多分辨率架构的优势。]]></description>
      <guid>https://arxiv.org/abs/2409.03034</guid>
      <pubDate>Sat, 07 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>你的生成模型可以检测分布外的协变量偏移吗？</title>
      <link>https://arxiv.org/abs/2409.03043</link>
      <description><![CDATA[arXiv:2409.03043v1 公告类型：新
摘要：检测分布外（OOD）传感数据和协变量分布偏移旨在识别与捕获的、正常的和分布内（ID）集具有不同高级图像统计数据的新测试示例。现有的OOD检测文献主要关注语义偏移，对协变量偏移几乎没有共识。生成模型以无监督的方式捕获ID数据，使它们能够有效地识别与该学习分布有显着偏差的样本，而不管下游任务如何。在这项工作中，我们通过涉及各种模型的广泛分析阐明了生成模型检测和量化领域特定协变量偏移的能力。为此，我们推测仅通过对高频信号相关和独立细节进行建模就足以检测大多数发生的传感故障（全局信号统计中的异常和偏差）。我们提出了一种用于 OOD 检测的新方法 CovariateFlow，该方法专门针对使用条件正则化流 (cNF) 对异方差高频图像分量进行协变量化。我们在 CIFAR10 vs. CIFAR10-C 和 ImageNet200 vs. ImageNet200-C 上的结果证明了该方法的有效性，因为它可以准确检测 OOD 协变量偏移。这项工作有助于提高成像系统的保真度，并在存在协变量偏移的情况下帮助机器学习模型进行 OOD 检测。]]></description>
      <guid>https://arxiv.org/abs/2409.03043</guid>
      <pubDate>Sat, 07 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>将密集度量深度纳入神经 3D 表征，用于视图合成和重新照明</title>
      <link>https://arxiv.org/abs/2409.03061</link>
      <description><![CDATA[arXiv:2409.03061v1 公告类型：新
摘要：合成小场景的精确几何形状和照片般逼真的外观是一个活跃的研究领域，在游戏、虚拟现实、机器人操控、自动驾驶、便捷的产品捕捉和消费级摄影方面都有引人注目的用例。当将场景几何和外观估计技术应用于机器人技术时，我们发现，由于机器人运动范围有限和场景混乱，可能的视点范围狭窄，导致当前的估计技术产生质量较差的估计甚至失败。另一方面，在机器人应用中，通常可以使用立体声直接测量密集度量深度，并且可以控制照明。深度可以提供物体几何形状的良好初始估计以改进重建，而多照明图像可以促进重新照明。在这项工作中，我们展示了一种将密集度量深度纳入神经 3D 表示训练的方法，并通过消除纹理和几何边缘之间的歧义来解决在联合细化几何和外观时观察到的伪影。我们还讨论了一种多闪光立体摄像系统，该系统旨在捕获管道所需的数据，并通过一些训练视图展示重新照明和视图合成的结果。]]></description>
      <guid>https://arxiv.org/abs/2409.03061</guid>
      <pubDate>Sat, 07 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MobileUNETR：用于高效医学图像分割的轻量级端到端混合视觉转换器</title>
      <link>https://arxiv.org/abs/2409.03062</link>
      <description><![CDATA[arXiv:2409.03062v1 公告类型：新 
摘要：皮肤癌分割对医学图像分析提出了重大挑战。许多现有的解决方案，主要是基于 CNN 的，都面临着缺乏全局上下文理解的问题。或者，一些方法采用大规模 Transformer 模型来弥合全局上下文差距，但代价是模型大小和计算复杂度。最后，许多基于 Transformer 的方法主要依赖于基于 CNN 的解码器，而忽略了基于 Transformer 的解码模型的好处。认识到这些限制，我们通过引入 MobileUNETR 来满足对高效轻量级解决方案的需求，其旨在克服与 CNN 和 Transformer 相关的性能限制，同时最小化模型大小，为高效图像分割迈出有希望的一步。MobileUNETR 有 3 个主要特点。1) MobileUNETR 由一个轻量级混合 CNN-Transformer 编码器组成，以帮助以有效的方式平衡局部和全局上下文特征提取； 2) 一种新型混合解码器，在解码阶段同时利用不同分辨率的低级和全局特征来生成准确的掩码；3) MobileUNETR 超越了大型复杂架构，以 300 万个参数和 1.3 GFLOP 的计算复杂度实现了卓越的性能，分别将参数和 FLOPS 减少了 10 倍和 23 倍。我们在四个公开可用的皮肤病变分割数据集上进行了广泛的实验来验证我们提出的方法的有效性，包括 ISIC 2016、ISIC 2017、ISIC 2018 和 PH2 数据集。代码将在以下网址公开：https://github.com/OSUPCVLab/MobileUNETR.git]]></description>
      <guid>https://arxiv.org/abs/2409.03062</guid>
      <pubDate>Sat, 07 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>空间扩散用于单元布局生成</title>
      <link>https://arxiv.org/abs/2409.03106</link>
      <description><![CDATA[arXiv:2409.03106v1 公告类型：新
摘要：生成模型（例如 GAN 和扩散模型）已用于增强训练集并提高不同任务的性能。我们专注于用于细胞检测的生成模型，即在给定的病理图像中定位和分类细胞。一个被忽视的重要信息是细胞的空间模式。在本文中，我们提出了一种用于细胞布局生成的空间模式引导生成模型。具体而言，提出了一种由空间特征引导并生成逼真的细胞布局的新型扩散模型。我们探索了不同的密度模型作为扩散模型的空间特征。在下游任务中，我们表明生成的细胞布局可用于指导高质量病理图像的生成。使用这些图像进行增强可以显着提高 SOTA 细胞检测方法的性能。代码可在 https://github.com/superlc1995/Diffusion-cell 获得。]]></description>
      <guid>https://arxiv.org/abs/2409.03106</guid>
      <pubDate>Sat, 07 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FIDAVL：使用视觉语言模型进行假图像检测和归因</title>
      <link>https://arxiv.org/abs/2409.03109</link>
      <description><![CDATA[arXiv:2409.03109v1 公告类型：新 
摘要：我们介绍了 FIDAVL：使用视觉语言模型进行假图像检测和归因。FIDAVL 是一种新颖而高效的多任务方法，其灵感来自视觉和语言处理之间的协同作用。利用零样本学习的优势，FIDAVL 利用视觉和语言之间的互补性以及软提示调整策略来检测假图像并准确将其归因于其原始源模型。我们对一个综合数据集进行了广泛的实验，该数据集包含由各种最先进模型生成的合成图像。我们的结果表明，FIDAVL 实现了令人鼓舞的 95.42% 的平均检测准确率和 95.47% 的 F1 分数，同时还获得了值得注意的性能指标，平均 F1 得分为 92.64%，ROUGE-L 得分为 96.50%，用于将合成图像归因于各自的源生成模型。该作品的源代码将在https://github.com/Mamadou-Keita/FIDAVL公开发布。]]></description>
      <guid>https://arxiv.org/abs/2409.03109</guid>
      <pubDate>Sat, 07 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从单个显微镜图像进行乳突切除术多视图合成</title>
      <link>https://arxiv.org/abs/2409.03190</link>
      <description><![CDATA[arXiv:2409.03190v1 公告类型：新
摘要：人工耳蜗 (CI) 手术涉及进行侵入性乳突切除术以将电极阵列插入耳蜗。在本文中，我们介绍了一种新颖的管道，该管道能够从单个 CI 显微镜图像生成合成的多视图视频。在我们的方法中，我们使用患者术前 CT 扫描来预测乳突切除术后表面，使用为此目的设计的方法。我们手动将表面与选定的显微镜框架对齐，以获得重建的 CT 网格相对于显微镜的准确初始姿势。然后，我们执行 UV 投影以将颜色从框架转移到表面纹理。纹理表面的新视图可用于生成具有地面真实姿势的大量合成帧数据集。我们评估了使用 Pytorch3D 和 PyVista 渲染的合成视图的质量。我们发现，与地面实况相比，这两种渲染引擎都能产生同样高质量的合成新视图帧，两种方法的结构相似性指数平均约为 0.86。大量具有已知姿势的新视图数据集对于持续训练一种方法至关重要，该方法可以自动估计显微镜姿势，以便与术前 CT 进行 2D 到 3D 配准，从而促进增强现实手术。该数据集将支持各种下游任务，例如在手术室中集成增强现实 (AR)、跟踪手术工具以及支持其他视频分析研究。]]></description>
      <guid>https://arxiv.org/abs/2409.03190</guid>
      <pubDate>Sat, 07 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PEPL：半监督学习中用于细粒度图像分类的精度增强伪标签</title>
      <link>https://arxiv.org/abs/2409.03192</link>
      <description><![CDATA[arXiv:2409.03192v1 公告类型：新
摘要：随着深度学习和计算机视觉技术的出现，细粒度图像分类取得了重大进展。然而，详细注释的稀缺仍然是一个重大挑战，特别是在获取高质量标记数据成本高昂或耗时的情况下。为了解决这一限制，我们引入了专门为半监督学习框架内的细粒度图像分类设计的精确增强伪标记 (PEPL) 方法。我们的方法利用大量未标记数据，通过两个关键阶段逐步细化高质量伪标签：初始伪标签生成和语义混合伪标签生成。这些阶段利用类激活图 (CAM) 准确估计语义内容并生成精细的标签，以捕获细粒度分类所需的基本细节。通过关注语义级信息，我们的方法有效地解决了标准数据增强和图像混合技术在保留关键细粒度特征方面的局限性。我们在基准数据集上实现了最先进的性能，与现有的半监督策​​略相比有显著的改进，准确率和鲁棒性都有显著提高。我们的代码已在 https://github.com/TianSuya/SemiFG 开源。]]></description>
      <guid>https://arxiv.org/abs/2409.03192</guid>
      <pubDate>Sat, 07 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>RoomDiffusion：室内设计行业的专业扩散模型</title>
      <link>https://arxiv.org/abs/2409.03198</link>
      <description><![CDATA[arXiv:2409.03198v1 公告类型：新摘要：文本到图像扩散模型的最新进展极大地改变了视觉内容生成，但它们在室内设计等专业领域的应用仍未得到充分探索。在本文中，我们介绍了 RoomDiffusion，这是一种为室内设计行业精心定制的开创性扩散模型。首先，我们从头开始构建整个数据管道来更新和评估数据以进行迭代模型优化。随后，应用多方面训练、多阶段微调和模型融合等技术来增强生成结果的视觉吸引力和精度。最后，利用潜在一致性蒸馏方法，我们提炼和加速模型以获得最佳效率。与针对一般场景优化的现有模型不同，RoomDiffusion 解决了室内设计中的特定挑战，例如缺乏时尚、家具重复率高和风格不准确。通过我们拥有超过 20 名专业人工评估员的整体人工评估协议，RoomDiffusion 在美观性、准确性和效率方面表现出业界领先的性能，超越了所有现有的开源模型，例如稳定扩散和 SDXL。]]></description>
      <guid>https://arxiv.org/abs/2409.03198</guid>
      <pubDate>Sat, 07 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>主动造假：DeepFake 伪装</title>
      <link>https://arxiv.org/abs/2409.03200</link>
      <description><![CDATA[arXiv:2409.03200v1 公告类型：新
摘要：DeepFake 技术因其能够高度逼真地操纵面部特征而受到广泛关注，引起了严重的社会担忧。在这些技术中，Face-Swap DeepFake 是最有害的，它通过将原始面部与合成面部交换来伪造行为。现有的取证方法主要基于深度神经网络 (DNN)，可以有效地揭露这些操纵，并已成为重要的真实性指标。然而，这些方法主要集中在捕捉 DeepFake 人脸中的混合不一致性，当个人故意在真实视频中制造混合不一致性以逃避责任时，就会出现一个新的安全问题，称为主动伪造。这种策略称为 DeepFake 伪装。为了实现这一点，我们引入了一个用于创建 DeepFake 伪装的新框架，该框架可生成混合不一致性，同时确保不可察觉、有效性和可转移性。该框架通过对抗性学习策略进行了优化，可以制造难以察觉但有效的不一致性来误导取证检测人员。大量实验证明了我们方法的有效性和稳健性，凸显了在主动伪造检测方面需要进一步研究。]]></description>
      <guid>https://arxiv.org/abs/2409.03200</guid>
      <pubDate>Sat, 07 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>