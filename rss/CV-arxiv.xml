<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>CS.CV更新arxiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.cv更新arxiv.org e-print存档。</description>
    <lastBuildDate>Mon, 07 Apr 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>计算机视觉和深度学习4D增强现实</title>
      <link>https://arxiv.org/abs/2504.02860</link>
      <description><![CDATA[ARXIV：2504.02860V1公告类型：新 
摘要：扩展现实平台（XR）平台中4D视频的前景巨大而令人兴奋，它为人类计算机互动的全新方式和我们感知现实和消耗多媒体的方式打开了全新的方式。在本文中，我们已经表明，在Microsoft混合现实平台中渲染4D视频的可行性。这使我们能够将任何3D性能捕获从CVSSP移植到XR产品中，例如相对轻松的Hololens设备。但是，如果3D模型太复杂并且由数百万个顶点组成，则使用当前的硬件和通信系统，该模型所需的数据带宽是一个严重的限制。因此，在这个项目中，我们还使用深度学习模型对4D视频序列的形状和外观进行了紧凑的表示，以有效地学习4D视频序列的紧凑表示并重建它，而不会影响视频序列的形状和外观。]]></description>
      <guid>https://arxiv.org/abs/2504.02860</guid>
      <pubDate>Mon, 07 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>要了解知识如何在大型视觉模型中发展</title>
      <link>https://arxiv.org/abs/2504.02862</link>
      <description><![CDATA[ARXIV：2504.02862V1公告类型：新 
摘要：大型视觉模型（LVLM）逐渐成为许多人工智能应用的基础。但是，了解他们的内部工作机制一直使研究人员感到困惑，这反过来又限制了其能力的进一步增强。在本文中，我们试图研究多模式知识如何发展并最终诱导LVLMS的自然语言。我们设计了一系列新颖的策略，用于分析LVLM中的内部知识，并从三个层面（包括单个令牌概率，令牌概率分布和特征编码）中深入研究多模式知识的演变。在此过程中，我们确定了知识进化中的两个关键节点：临界层和突变层，将演化过程分为三个阶段：快速进化，稳定和突变。我们的研究是第一个揭示LVLM中知识进化轨迹的研究，为理解其潜在机制提供了新的观点。我们的代码可从https://github.com/xiao4579/vlm-interpretability获得。]]></description>
      <guid>https://arxiv.org/abs/2504.02862</guid>
      <pubDate>Mon, 07 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenFacades：通过街道视图图像的建筑标题和属性数据丰富的开放框架</title>
      <link>https://arxiv.org/abs/2504.02866</link>
      <description><![CDATA[ARXIV：2504.02866V1公告类型：新 
摘要：高度，用法和材料组成之类的建筑特性在空间数据基础架构中起着至关重要的作用，支持应用程序，例如能量模拟，风险评估和环境建模。尽管它们的重要性，但在许多城市地区，全面和高质量的建筑属性数据仍然稀缺。最近的进步使使用遥感和街道级图像可以提取和标记目标建筑属性。但是，建立一种整合多种开放数据集，大规模获取整体建筑物图像并渗透到全面的建筑物属性的方法和管道仍然是一个重大挑战。在第一项研究中，这项研究通过引入OpenFacades来弥合差距，OpenFacades是一个开放式框架，该框架利用多模式众包数据通过多模式的大型语言模型来丰富具有客观属性和语义描述符的建筑概况。我们的方法分为三个主要步骤进行。首先，我们通过ISOVIST分析将Mapillary的街道级图像元数据与OpenStreetMap几何形状集成在一起，从而有效地识别了为观察目标建筑物提供合适的有利位置的图像。其次，我们将检测到全景图像中的建筑外墙的检测自动化，并量身定制一种对物体转换为整体透视视图，以近似现实世界的观察。第三，我们介绍了一种创新方法，该方法利用并系统地研究了开源大型视觉模型（VLMS）的功能，用于多属性预测和开放式墨西哥银行分析，并利用了30,180个标记的图像的全球级别数据集中的建筑级别分析中的功能。评估表明，多属性推理中的微调VLM Excel，表现优于单属性计算机视觉模型和零拍的ChatGpt-4O。]]></description>
      <guid>https://arxiv.org/abs/2504.02866</guid>
      <pubDate>Mon, 07 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多模式参考视觉接地</title>
      <link>https://arxiv.org/abs/2504.02876</link>
      <description><![CDATA[ARXIV：2504.02876V1公告类型：新 
摘要：视觉接地重点是根据语言表达式从图像中检测对象。最近的大型视觉模型（LVLM）通过使用大型数据集训练大型模型具有显着高级的视觉接地性能。但是，问题仍然具有挑战性，尤其是当输入图像中出现类似对象时。例如，LVLM可能无法在图像中区分饮食可乐和常规可乐。在这种情况下，如果可以使用Diet Coke和常规焦炭的其他参考图像，则可以帮助类似物体的视觉接地。
  在这项工作中，我们介绍了一个名为多模式参考视觉接地（MRVG）的新任务。在此任务中，模型可以访问数据库中对象的一组参考图像。基于这些参考图像和语言表达式，需要模型从查询图像检测目标对象。我们首先引入一个新数据集来研究MRVG问题。然后，我们介绍了一种名为MRVG-NET的新颖方法，以解决此视觉接地问题。我们表明，通过有效地使用几乎没有拍摄对象检测的参考图像并使用大型语言模型（LLMS）进行对象匹配，我们的方法与QWEN2.5-VL-7B等最先进的LVLM相比实现了出色的视觉接地性能。我们的方法弥合了几个射击检测和视觉接地之间的差距，从而解开了新的能力以进行视觉理解。带有代码和数据集的项目页面：https：//irvlutd.github.io/multigrounding]]></description>
      <guid>https://arxiv.org/abs/2504.02876</guid>
      <pubDate>Mon, 07 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>探索LLMS基于IMU的细粒度活动理解的能力</title>
      <link>https://arxiv.org/abs/2504.02878</link>
      <description><![CDATA[ARXIV：2504.02878V1公告类型：新 
摘要：使用惯性测量单元（IMU）越来越多地利用大型语言模型（LLMS）的人类活动识别（HAR），但现有的方法集中于步行或跑步等粗糙活动。我们的初步研究表明，经过预估计的LLMS在诸如空气写的字母识别之类的细粒度HAR任务上发生灾难性的失败，仅实现了几乎随机的猜测准确性。在这项工作中，我们首先弥合了这个差距，以进行平面写作方案：通过使用自收集的数据集和几乎没有射击学习的微调LLM，我们实现了2D数据的129倍改进。为了将其扩展到3D方案，我们设计了一个基于编码器的管道，该管道将3D数据映射到2D等效物中，从而保留了时空信息以进行稳健的字母预测。我们的端到端管道在单词识别方面具有78％的准确性，在空中写作方案中最多5个字母，将LLMS确立为可行的细粒度HAR工具。]]></description>
      <guid>https://arxiv.org/abs/2504.02878</guid>
      <pubDate>Mon, 07 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于Yolov8的性能增强交通标志识别</title>
      <link>https://arxiv.org/abs/2504.02884</link>
      <description><![CDATA[ARXIV：2504.02884V1公告类型：新 
摘要：本文交通标志识别在自动驾驶汽车和高级驾驶员辅助系统（ADAS）的开发中起着至关重要的作用。尽管深度学习和对象检测方面取得了重大进展，但由于其小尺寸，可变的环境条件，遮挡和阶级失衡，准确检测和分类的交通标志仍然具有挑战性。本论文提出了一个增强的基于Yolov8的检测系统，该系统集成了高级数据增强技术，新颖的建筑增强功能，包括协调关注（CA），双向功能特征金字塔网络（BIFPN）以及ODCONV和LSKA等动态模块，以及精致的损失功能（EIOU和WIOU损失）。在包括GTSRB，TT100K和GTSDB在内的数据集上进行的广泛实验表明，检测准确性，不良条件下的鲁棒性以及对边缘设备的实时推断的提高。这些发现为在现实世界自动驾驶方案中部署可靠的交通标志识别系统提供了可行的见解。]]></description>
      <guid>https://arxiv.org/abs/2504.02884</guid>
      <pubDate>Mon, 07 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>UAC：神经网络的不确定性感知校准用于手势检测</title>
      <link>https://arxiv.org/abs/2504.02895</link>
      <description><![CDATA[ARXIV：2504.02895V1公告类型：新 
摘要：人工智能有可能影响安全 - 关键领域（例如建筑，制造和医疗保健）的安全性和效率。例如，使用来自可穿戴设备的传感器数据，例如惯性测量单元（IMU），可以在保持隐私时检测到人的手势，从而确保遵循安全协议。但是，这些域中的严格安全要求限制了AI的采用，因为必须准确校准针对离分布（OOD）数据的预测概率和鲁棒性。
  本文提出了UAC（不确定性感知校准），这是一种新型的两步方法，可在基于IMU的手势识别中解决这些挑战。首先，我们提出了一种不确定性感知的手势网络体系结构，该架构可以预测IMU数据中的手势概率及其相关的不确定性。然后使用这种不确定性来校准每个潜在手势的概率。其次，对多个IMU数据窗口上预测的熵加权期望用于提高准确性，同时保持正确的校准。
  使用三个可公开可用的IMU数据集评估我们的方法，并将其与三种用于神经网络的最新校准方法进行比较：温度缩放，熵最大化和拉普拉斯近似。 UAC的表现优于现有方法，在OOD和分布场景中都提高了准确性和校准。此外，我们发现，与我们的方法不同，最先进的方法都没有显着改善基于IMU的手势识别模型的校准。总之，我们的工作突出了神经网络不确定性校准的优势，证明了使用IMU数据的校准和准确性的提高。]]></description>
      <guid>https://arxiv.org/abs/2504.02895</guid>
      <pubDate>Mon, 07 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>深层检测模型的比较分析：新方法和观点</title>
      <link>https://arxiv.org/abs/2504.02900</link>
      <description><![CDATA[ARXIV：2504.02900V1公告类型：新 
摘要：Deepfake视频构成的日益严重的威胁，能够操纵现实和传播错误信息，迫使人们迫切需要有效的检测方法。这项工作研究并比较了识别深击的不同方法，重点是Genconvit模型及其性能相对于DeepFakeBench Markch中存在的其他体系结构。为了使研究与之相关，解决了深层蛋糕的社会和法律影响，以及它们创建和检测的技术基础，包括数字图像处理，机器学习和人工神经网络，重点是卷积神经网络（CNNS），生成性的对抗性网络（GANS）和变形金刚。模型的性能评估是使用文献中建立的相关指标和新数据集进行的，例如Wilddeep-Fake和DeepSpeak，旨在确定反对错误信息和媒体操纵的战斗中最有效的工具。获得的结果表明，在微调后，Genconvit在准确性（93.82％）和概括能力方面表现出卓越的性能，超过了DeepSpeak DataSet上DeepFakebenchmark中的其他体系结构。这项研究有助于进步深泡检测技术，为开发更强大和有效的解决方案提供了贡献，以防止传播错误信息。]]></description>
      <guid>https://arxiv.org/abs/2504.02900</guid>
      <pubDate>Mon, 07 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>随意输入作为在线学习中的图像</title>
      <link>https://arxiv.org/abs/2504.02912</link>
      <description><![CDATA[ARXIV：2504.02912V1公告类型：新 
摘要：在线学习设置中不同特征空间的领域，也称为Hanphazard Inputs，由于其在各个领域的适用性，如今非常突出。但是，当前对随意输入的解决方案是模型依赖性的，不能从现有的先进深度学习方法中受益，这需要固定维度的输入。因此，我们建议将在线学习环境中的不同特征空间转换为即时的固定维数图像表示。这种简单但新颖的方法是模型不平衡，允许任何基于视觉的模型适用于随意输入，如使用Resnet和Vit所证明的那样。图像表示可以无缝地处理不一致的输入数据，从而使我们提出的方法可扩展且健壮。我们在四个公开可用的数据集上显示了我们方法的功效。该代码可从https://github.com/rohit102497/haphazardinputsasimages获得。]]></description>
      <guid>https://arxiv.org/abs/2504.02912</guid>
      <pubDate>Mon, 07 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>morpheus：通过实体实验的基准测试视频生成模型的物理推理</title>
      <link>https://arxiv.org/abs/2504.02918</link>
      <description><![CDATA[ARXIV：2504.02918V1公告类型：新 
摘要：图像和视频产生的最新进展提高了希望，这些模型具有世界建模功能，即产生现实，物理上合理的视频的能力。这可能会彻底改变机器人技术，自动驾驶和科学模拟的应用。但是，在将这些模型视为世界模型之前，我们必须问：它们是否遵守物理保护法？为了回答这个问题，我们介绍了Morpheus，这是评估视频生产模型的基准。它具有80个现实世界的视频，这些视频捕获了在保护法律的指导下，以捕获身体现象。由于人工世代缺乏地面真理，因此我们使用针对无可靠的保护定律评估的物理知识指标来评估身体的合理性，从而利用了物理知识的神经网络和视觉语言基础模型的进步。我们的发现表明，即使采用高级提示和视频调理，当前的模型尽管产生了美学上令人愉悦的视频，但仍很难编码物理原理。所有数据，排行榜和代码都在我们的项目页面开源。]]></description>
      <guid>https://arxiv.org/abs/2504.02918</guid>
      <pubDate>Mon, 07 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>带有实时语音规格的基于激光雷达的对象检测</title>
      <link>https://arxiv.org/abs/2504.02920</link>
      <description><![CDATA[ARXIV：2504.02920V1公告类型：新 
摘要：本文介绍了一个基于激光雷达的对象检测系统，具有实时语音规格，通过多模式点网框架集成了Kitti的3D点云和RGB图像。它在3000个样本子集上实现了87.0％的验证精度，通过将空间和视觉数据结合，解决阶级失衡与加权损失以及通过自适应技术进行精炼培训，超过了200个样本基线为67.5％。 Tkinter原型使用边缘TT（启动）提供天然的印度男性语音输出，以及3D可视化和实时反馈，增强自主导航，辅助技术及其他地区的可访问性和安全性。该研究提供了详细的方法，全面的实验分析以及对应用和挑战的广泛审查，将这项工作确立为人类计算机互动和环境感知的可扩展性进步，与当前的研究趋势保持一致。]]></description>
      <guid>https://arxiv.org/abs/2504.02920</guid>
      <pubDate>Mon, 07 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>VARGPT-V1.1：通过迭代指导调整和增强学习来改善视觉自动回归大型统一模型</title>
      <link>https://arxiv.org/abs/2504.02949</link>
      <description><![CDATA[ARXIV：2504.02949V1公告类型：新 
摘要：在这项工作中，我们提出了VARGPT-V1.1，这是一种基于我们以前的框架Vargpt的高级统一视觉自动回归模型。该模型保留了视觉理解的下一个预测的双重范式和图像合成的临时生成。具体而言，VARGPT-V1.1整合：（1）一种新型的培训策略，通过直接优先优化（DPO）结合了迭代视觉教学调谐与增强学习，（2）一个扩展的培训语料库，该培训语料库包含83m的视觉传播指导对，（3）使用QWEN2，（4）增强图像的图像的backbone，（3）使用QWEN 2的升级功能，（3），（3）修改。这些进步使VARGPT-V1.1能够在多模式理解和文本图像指导遵循任务中实现最先进的表现，从而证明了理解和发电指标的显着改善。值得注意的是，通过视觉指导调整，该模型在与其前身保持建筑一致性的同时，获得了图像编辑功能，从而揭示了统一的视觉理解，生成和编辑的潜力。我们的发现表明，精心设计的统一视觉自回归模型可以有效地采用大型语言模型（LLM）的灵活培训策略，表现出有希望的可扩展性。代码库和模型权重可在https://github.com/vargpt-family/vargpt-v1.1上公开获得。]]></description>
      <guid>https://arxiv.org/abs/2504.02949</guid>
      <pubDate>Mon, 07 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>QID：在数据筛选方面有效的查询信息，以了解无OCR的视觉文档理解</title>
      <link>https://arxiv.org/abs/2504.02971</link>
      <description><![CDATA[ARXIV：2504.02971V1公告类型：新 
摘要：在视觉文档理解（VDU）任务中，使用新数据集对预训练的视觉模型（VLM）进行微调，通常在优化视觉编码器以识别文本丰富的文档图像中的特定特定区域时通常会出现。通过修改网络体系结构将查询直接注入模型层的现有方法通常难以适应有限的注释的新数据集。为了解决这个问题，我们介绍了一种新颖的，简化的，架构的方法，将查询嵌入到视觉编码器中，从而导致了显着的性能增长，尤其是在数据筛选的微调方案中。具体而言，我们的方法引入了双模型框架：一个查询感知的模块，该模块生成独特的查询向量以精确指导模型的焦点，以及一个查询敏锐的模块，可捕获标记之间的位置关系，确保可靠的空间理解。值得注意的是，这两个模块都独立于视力注意障碍物，促进了对查询嵌入的目标学习并增强视觉语义识别。在多个数据集中使用无OCR VLM的实验表明，使用我们的方法，尤其是在处理数据筛选环境中文本丰富的文档时的性能改进。]]></description>
      <guid>https://arxiv.org/abs/2504.02971</guid>
      <pubDate>Mon, 07 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>板床：基于扩散的SIM到现实转移框架，用于内部的人网恢复</title>
      <link>https://arxiv.org/abs/2504.03006</link>
      <description><![CDATA[ARXIV：2504.03006V1公告类型：新 
摘要：内部网状网状恢复可能至关重要，并且可以为多种医疗保健应用，包括睡眠模式监测，康复支持和预防压力溃疡。但是，很难在该域中收集大型现实世界可视数据集，部分原因是隐私和费用约束，这反过来又对培训和部署深度学习模型面临重大挑战。现有的床内人类网格估计方法通常很大程度上依赖于现实世界数据，从而限制了它们在不同的床内场景中概括的能力，例如不同的覆盖范围和环境环境。为了解决这个问题，我们提出了一个SIM到现实的转移框架，用于从间接头顶深度图像中恢复床内的网状网格，该图像利用大规模合成数据以及有限的或没有现实世界的样本。我们介绍了一个扩散模型，该模型弥合了合成数据和实际数据之间的差距，以支持现实世界内的姿势和身体推理情景中的概括。广泛的实验和消融研究证明了我们框架的有效性，证明了各种医疗保健方案的鲁棒性和适应性的显着改善。]]></description>
      <guid>https://arxiv.org/abs/2504.03006</guid>
      <pubDate>Mon, 07 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用卷积神经网络识别情绪</title>
      <link>https://arxiv.org/abs/2504.03010</link>
      <description><![CDATA[ARXIV：2504.03010V1公告类型：新 
摘要：情感在日常生活中起着重要的作用，因为它可以帮助人们更好地与彼此沟通和互相理解。面部表情可以分为7个类别：愤怒，厌恶，恐惧，快乐，中立，悲伤和惊喜。在过去十年中，如何检测和认识这七个情绪已成为一个流行的话题。在本文中，我们开发了一种情感识别系统，可以通过使用深度学习在静止图像和实时视频上应用情感识别。
  我们从头开始构建自己的情感识别分类和回归系统，其中包括数据集收集，数据预处理，模型培训和测试。给定特定图像或实时视频，我们的系统能够显示所有7个情绪的分类和回归结果。在2个不同的数据集上测试了所提出的系统，并实现了超过80 \％的精度。此外，从实时测试获得的结果证明了实时实施卷积神经网络以准确有效地检测情绪的可行性。]]></description>
      <guid>https://arxiv.org/abs/2504.03010</guid>
      <pubDate>Mon, 07 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>