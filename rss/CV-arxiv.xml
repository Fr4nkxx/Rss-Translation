<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Thu, 31 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>加速增强不变性预训练</title>
      <link>https://arxiv.org/abs/2410.22364</link>
      <description><![CDATA[arXiv:2410.22364v1 公告类型：新
摘要：我们的工作解决了对比学习方法的计算挑战，特别是对于 Vision Transformers (ViTs) 的预训练。尽管对比学习很有效，但训练所需的大量计算资源往往会阻碍它们的实际应用。为了缓解这个问题，我们提出了一个加速框架，利用 ViT 独特的能力来概括不同序列长度的输入。我们的方法采用了多种序列压缩策略，包括随机标记丢弃和灵活的补丁缩放，以降低梯度估计的成本并加速收敛。我们进一步深入分析了各种加速策略的梯度估计误差及其对下游任务的影响，为加速和性能之间的权衡提供了宝贵的见解。
我们还提出了一种新颖的程序来确定最佳加速计划，以根据训练进度调整序列压缩比，确保高效训练而不牺牲下游性能。我们的方法显着降低了大规模数据集上各种自监督学习算法的计算开销。在 ImageNet 中，我们的方法在 MoCo 中实现了 4$\times$ 的加速，在 SimCLR 中实现了 3.3$\times$ 的加速，在 DINO 中实现了 2.5$\times$ 的加速，表明效率显著提升。]]></description>
      <guid>https://arxiv.org/abs/2410.22364</guid>
      <pubDate>Thu, 31 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用语义场景重建来估计建筑围护结构特征</title>
      <link>https://arxiv.org/abs/2410.22383</link>
      <description><![CDATA[arXiv:2410.22383v1 公告类型：新
摘要：实现欧盟的气候中和目标需要改造现有建筑以减少能源使用和排放。此过程中的关键步骤是精确评估几何建筑围护结构特征，以指导改造决策。以前用于估计建筑物特征（例如窗墙比、建筑物占地面积和建筑元素的位置）的方法主要依赖于在 2D 图像上应用基于深度学习的检测或分割技术。然而，这些方法往往侧重于平面立面属性，在分析 3D 中的完整建筑围护结构时限制了它们的准确性和全面性。
虽然神经场景表示在室内场景重建中表现出色，但它们在外部建筑围护结构分析中仍未得到充分探索。这项工作通过利用基于有符号距离函数 (SDF) 表示的尖端神经表面重建技术进行 3D 建筑分析来解决这一差距。我们提出了 BuildNet3D，这是一个新颖的框架，用于从 2D 图像输入估计几何建筑特征。通过将基于 SDF 的表示与语义模态相结合，BuildNet3D 可以恢复建筑外围结构的细粒度 3D 几何和语义，然后用于自动提取建筑特征。我们的框架在一系列复杂的建筑结构上进行了评估，在估计窗墙比和建筑占地面积方面表现出很高的准确性和通用性。结果强调了 BuildNet3D 在建筑分析和改造实际应用中的有效性。]]></description>
      <guid>https://arxiv.org/abs/2410.22383</guid>
      <pubDate>Thu, 31 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>梯度距离函数</title>
      <link>https://arxiv.org/abs/2410.22422</link>
      <description><![CDATA[arXiv:2410.22422v1 公告类型：新
摘要：无符号距离函数 (UDF) 可用于表示深度学习框架中的非水密表面。然而，UDF 往往很脆弱且难以学习，部分原因是表面恰好位于 UDF 不可微分的位置。在这项工作中，我们表明梯度距离函数 (GDF) 可以通过在表面上可微分来解决此问题，同时仍然能够表示开放表面。这是通过将每个 3D 点与一个 3D 向量相关联来实现的，该向量的范数被视为到表面的无符号距离，其方向被视为朝向最近表面点的方向。我们展示了 GDF 在 ShapeNet Car、Multi-Garment 和 3D-Scene 数据集上的有效性，包括单形状重建网络或分类自动解码器。]]></description>
      <guid>https://arxiv.org/abs/2410.22422</guid>
      <pubDate>Thu, 31 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在扩散过程中嵌入水印以实现模型知识产权保护</title>
      <link>https://arxiv.org/abs/2410.22445</link>
      <description><![CDATA[arXiv:2410.22445v1 公告类型：新
摘要：在实际应用中，扩散模型的广泛部署通常需要大量的培训投入。随着扩散模型的应用越来越广泛，对潜在滥用的担忧凸显了对强大知识产权保护的必要性。当前的保护策略要么采用基于后门的方法，将水印任务作为更简单的训练目标与主要模型任务相结合，要么将水印直接嵌入到最终输出样本中。然而，与现有的后门防御技术相比，前一种方法很脆弱，而后者从根本上改变了预期的输出。在这项工作中，我们通过将水印嵌入整个扩散过程来引入一种新颖的水印框架，并从理论上确保我们的最终输出样本不包含任何额外信息。此外，我们利用统计算法从内部生成的模型样本中验证水印，而无需触发器作为条件。详细的理论分析和实验验证证明了我们提出的方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2410.22445</guid>
      <pubDate>Thu, 31 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>解决视频对象分割中的工作记忆问题</title>
      <link>https://arxiv.org/abs/2410.22451</link>
      <description><![CDATA[arXiv:2410.22451v1 公告类型：新
摘要：当代最先进的视频对象分割 (VOS) 模型通过亲和力或交叉注意将传入的未注释图像与图像掩码关系历史进行比较，以预测对象掩码。我们将初始图像掩码对和过去图像掩​​码的内部记忆状态称为工作记忆缓冲区。虽然当前最先进的模型在干净的视频数据上表现非常好，但它们对前几帧的工作记忆的依赖留下了误差的空间。基于亲和力的算法包括归纳偏差，即连续帧之间存在时间连续性。为了解释所需对象的不一致摄像机视图，工作记忆模型需要进行算法修改，以调节内存更新并避免将不相关的帧写入工作记忆。提出了一种简单的算法更改，可以应用于任何现有的基于工作记忆的 VOS 模型，以提高不一致视图的性能，例如突然的摄像机切换、帧插入和极端上下文变化。相比没有添加算法的同一模型，使用这些帧插入，最终模型性能在视频数据上表现出显著改善。我们的贡献是一个简单的决策函数，它根据对突然、极端变化的检测以及对象不再在帧中的假设来确定是否应该更新工作记忆。通过实施诸如此类的算法更改，我们可以提高当前 VOS 模型的实际适用性。]]></description>
      <guid>https://arxiv.org/abs/2410.22451</guid>
      <pubDate>Thu, 31 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过扩散磁共振成像识别大脑年龄可协同预测神经退行性疾病</title>
      <link>https://arxiv.org/abs/2410.22454</link>
      <description><![CDATA[arXiv:2410.22454v1 公告类型：新
摘要：根据磁共振图像 (MRI) 估计的大脑年龄及其与实际年龄的偏差可以提供对潜在神经退行性疾病的早期洞察，支持早期发现和实施预防策略。扩散磁共振成像 (dMRI) 是一种广泛使用的大脑年龄估计方式，它为建立神经退行性疾病预测的早期生物标记物提供了机会，因为它可以捕捉到先于更明显的宏观结构变化的细微微观结构变化。然而，dMRI 中宏观和微观结构信息的共存引发了一个问题：当前基于 dMRI 的大脑年龄估计模型是否利用了预期的微观结构信息，或者它们是否无意中依赖了宏观结构信息。为了开发特定于微观结构的大脑年龄，我们提出了一种从 dMRI 识别大脑年龄的方法，通过将所有图像非刚性地配准到标准模板来最大限度地减少模型对宏观结构信息的使用。来自 12 个数据集的 13,398 名参与者的影像数据用于训练和评估。我们将我们的大脑年龄模型（在最小化和未最小化宏观结构信息的情况下训练）与架构相似的基于 T1 加权 (T1w) MRI 的大脑年龄模型和两个主要使用宏观结构信息的最先进的基于 T1w MRI 的大脑年龄模型进行了比较。我们观察到基于 dMRI 的大脑年龄和基于 T1w MRI 的大脑年龄在神经退行性的各个阶段存在差异，在从认知正常 (CN) 过渡到轻度认知障碍 (MCI) 的参与者中，基于 dMRI 的大脑年龄比基于 T1w MRI 的大脑年龄更大，但在已经被诊断患有阿尔茨海默病 (AD) 的参与者中则更年轻。在诊断为 MCI 之前约 4 年，基于 dMRI 的大脑年龄在预测从 CN 到 MCI 的转变方面比基于 T1w MRI 的大脑年龄表现更好。]]></description>
      <guid>https://arxiv.org/abs/2410.22454</guid>
      <pubDate>Thu, 31 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Image2Struct：视觉语言模型的基准结构提取</title>
      <link>https://arxiv.org/abs/2410.22456</link>
      <description><![CDATA[arXiv:2410.22456v1 公告类型：新
摘要：我们引入了 Image2Struct，这是一个用于评估视觉语言模型 (VLM) 从图像中提取结构的基准。我们的基准 1) 捕获现实世界的用例，2) 是全自动的，不需要人工判断，3) 基于可更新的新数据流。在 Image2Struct 中，VLM 被提示从输入图像（例如网页屏幕截图）生成底层结构（例如 LaTeX 代码或 HTML）。然后渲染该结构以生成输出图像（例如渲染的网页），将其与输入图像进行比较以产生相似度分数。这种往返评估使我们能够在具有多个有效结构的任务上定量评估 VLM。我们创建了一个管道，在执行时从活跃的在线社区下载新数据并在没有人工干预的情况下评估 VLM。我们引入了三个领域（网页、LaTeX 和乐谱），并使用了五个图像指标（像素相似度、Inception 向量之间的余弦相似度、学习到的感知图像块相似度、结构相似度指数测量和地球移动相似度），以便对图像对进行高效且自动的比较。我们在 14 个著名的 VLM 上评估了 Image2Struct，发现得分差异很大，这表明 Image2Struct 可以区分不同 VLM 的性能。此外，最佳得分在不同领域之间差异很大（例如，乐谱得分为 0.402，而 LaTeX 方程得分为 0.830），这表明 Image2Struct 包含不同难度的任务。为了透明起见，我们在 https://crfm.stanford.edu/helm/image2struct/v1.0.1/ 上发布了完整结果。]]></description>
      <guid>https://arxiv.org/abs/2410.22456</guid>
      <pubDate>Thu, 31 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多视图 3D 物体检测的统一域泛化与自适应</title>
      <link>https://arxiv.org/abs/2410.22461</link>
      <description><![CDATA[arXiv:2410.22461v1 公告类型：新
摘要：利用多视角相机进行 3D 物体检测的最新进展已在各种具有挑战性的视觉任务中证明了其实用和经济价值。然而，由于源域和目标域之间不可避免的几何错位，典型的监督学习方法在实现对未见和未标记的目标数据集的令人满意的适应（即直接传输）方面面临挑战。在实践中，我们还遇到了训练模型和收集注释以成功部署 3D 物体检测器的资源限制。在本文中，我们提出了统一域泛化和自适应 (UDGA)，这是一种缓解这些缺点的实用解决方案。我们首先提出多视图重叠深度约束，利用多视图之间的强关联，显着缓解由于透视图变化而导致的几何间隙。然后，我们提出了一种标签高效域自适应方法，以处理标签数量明显较少的不熟悉目标（即 1% 和 5%），同时保留明确定义的源知识以提高训练效率。总体而言，UDGA 框架可在源域和目标域中实现稳定的检测性能，有效地弥补不可避免的域差距，同时减少注释需求。我们通过大规模基准测试证明了 UDGA 的稳健性：nuScenes、Lyft 和 Waymo，其中我们的框架优于当前最先进的方法。]]></description>
      <guid>https://arxiv.org/abs/2410.22461</guid>
      <pubDate>Thu, 31 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多模态助力小样本 3D 点云语义分割</title>
      <link>https://arxiv.org/abs/2410.22489</link>
      <description><![CDATA[arXiv:2410.22489v1 公告类型：新
摘要：少样本 3D 点云分割 (FS-PCS) 旨在将模型推广到使用最少的带注释支持样本来分割新类别。虽然现有的 FS-PCS 方法已显示出良好的前景，但它们主要关注单模态点云输入，而忽略了利用多模态信息的潜在好处。在本文中，我们通过引入免费的多模态 FS-PCS 设置来解决这一问题，该设置利用文本标签和潜在可用的 2D 图像模态。在这种易于实现的设置下，我们提出了多模态少样本 SegNet (MM-FSS)，这是一种有效利用多种模态互补信息的模型。MM-FSS 采用具有两个头的共享主干来提取多模态和单模态视觉特征，并使用预训练的文本编码器来生成文本嵌入。为了充分利用多模态信息，我们提出了一个多模态相关融合 (MCF) 模块来生成多模态相关，以及一个多模态语义融合 (MSF) 模块来使用文本感知语义指导来细化相关。此外，我们提出了一种简单但有效的测试时自适应跨模态校准 (TACC) 技术来减轻训练偏差，进一步提高泛化能力。在 S3DIS 和 ScanNet 数据集上的实验结果表明，我们的方法显著提高了性能。我们方法的有效性表明了利用常被忽略的免费模态进行 FS-PCS 的好处，为未来的研究提供了宝贵的见解。代码可在 https://github.com/ZhaochongAn/Multimodality-3D-Few-Shot 获得。]]></description>
      <guid>https://arxiv.org/abs/2410.22489</guid>
      <pubDate>Thu, 31 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PV-ALE 数据集：通过卷积神经网络的迁移学习增强苹果叶片病害分类</title>
      <link>https://arxiv.org/abs/2410.22490</link>
      <description><![CDATA[arXiv:2410.22490v1 公告类型：新
摘要：随着全球粮食安全形势的不断发展，对准确可靠的农作物病害诊断的需求从未如此迫切。为了解决全球粮食安全问题，我们扩展了广泛使用的 PlantVillage 数据集，增加了苹果叶病类别，增强了多样性和复杂性。对原始数据集和扩展数据集的实验评估表明，现有模型难以应对新增内容，这凸显了对更强大和更通用的计算机视觉模型的需求。在原始数据集和扩展数据集上分别获得了 99.63% 和 97.87% 的测试 F1 分数。我们的研究提供了更具挑战性和多样性的基准，为开发在不同成像条件下识别苹果叶病的准确可靠模型铺平了道路。扩展的数据集可在 https://www.kaggle.com/datasets/akinyemijoseph/apple-leaf-disease-dataset-6-classes-v2 上获得，使未来的研究能够以我们的研究成果为基础。]]></description>
      <guid>https://arxiv.org/abs/2410.22490</guid>
      <pubDate>Thu, 31 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AffectNet+：使用软标签增强面部表情识别的数据库</title>
      <link>https://arxiv.org/abs/2410.22506</link>
      <description><![CDATA[arXiv:2410.22506v1 公告类型：新
摘要：由于类内差异和类间相似性，自动面部表情识别 (FER) 具有挑战性。当面部表情反映出各种情绪的混合（又称复合表情）时，FER 会特别困难。现有的 FER 数据集（例如 AffectNet）提供离散情绪标签（硬标签），其中将单一情绪类别分配给一个表情。为了缓解类间和类内挑战，并提供更好的面部表情描述符，我们提出了一种通过标记方法创建 FER 数据集的新方法，其中图像被标记为多种情绪（称为软标签），每个标签具有不同的置信度。具体而言，我们引入了面部表情数据集的软标签概念，这是一种情感计算的新方法，可以更真实地识别面部表情。为了实现这一目标，我们提出了一种新方法来准确计算软标签：一个向量，表示多个情绪类别在单个面部表情中同时出现的程度。找到更平滑的决策边界、实现多标签以及减轻偏见和不平衡数据是我们提出的方法的一些优势。在 AffectNet 的基础上，我们推出了下一代面部表情数据集 AffectNet+。该数据集包含软标签、三类数据复杂性子集以及年龄、性别、种族、头部姿势、面部特征、效价和唤醒等其他元数据。AffectNet+ 将向研究人员公开。]]></description>
      <guid>https://arxiv.org/abs/2410.22506</guid>
      <pubDate>Thu, 31 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FairSkin：用于生成皮肤病图像的公平扩散</title>
      <link>https://arxiv.org/abs/2410.22551</link>
      <description><![CDATA[arXiv:2410.22551v1 公告类型：新
摘要：图像生成是一种流行的临床数据增强技术，可提高诊断准确性并减少医疗保健差异。扩散模型 (DM) 已成为生成合成医学图像的主要方法，但它存在严重的双重偏见：(1) 以 Frechet 初始距离 (FID) 衡量，为白种人生成的图像质量明显更高。(2) 下游任务学习者从疾病图像中学习关键特征的能力因肤色而异。这些偏见带来了重大风险，特别是在皮肤病检测中，某些肤色的代表性不足可能导致误诊或忽视特定疾病。为了应对这些挑战，我们提出了 FairSkin，这是一种新颖的 DM 框架，它通过三级重采样机制减轻这些偏见，确保在种族和疾病类别中更公平地表示。我们的方法显着提高了生成图像的多样性和质量，有助于在临床环境中更公平地检测皮肤病。]]></description>
      <guid>https://arxiv.org/abs/2410.22551</guid>
      <pubDate>Thu, 31 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>遥感技术用于杂草检测和控制</title>
      <link>https://arxiv.org/abs/2410.22554</link>
      <description><![CDATA[arXiv:2410.22554v1 公告类型：新
摘要：意大利黑麦草是一种常见于冬小麦田的草类杂草，与冬小麦争夺水分和养分。如果不使用除草剂进行适当控制，黑麦草会导致产量和谷物质量大幅下降。为了控制成本和环境影响，我们在无人机和卫星图像中检测杂草。卫星图像太粗糙，无法用于精确喷洒，但可以帮助规划无人机飞行和治疗。另一方面，无人机图像具有足够好的分辨率，可以进行精确喷洒。然而，黑麦草很难与作物区分开来，注释需要专业知识。我们使用 Python 分割模型库测试了 600 多种不同的神经网络架构，用于无人机图像中的杂草分割，并将这些模型预测的准确性与成本进行了映射。我们最好的系统将除草剂应用于 99% 以上的杂草，而喷洒的面积只比注释的杂草面积大 30%。如果杂草只覆盖了田地的一小部分，这些模型就能带来很大的节省。]]></description>
      <guid>https://arxiv.org/abs/2410.22554</guid>
      <pubDate>Thu, 31 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>预训练视觉模型作为自动驾驶安全过滤器的感知骨干</title>
      <link>https://arxiv.org/abs/2410.22585</link>
      <description><![CDATA[arXiv:2410.22585v1 公告类型：新
摘要：端到端基于视觉的自动驾驶取得了令人瞩目的成功，但安全仍然是一个主要问题。安全控制问题已在低维设置中使用安全过滤器（例如基于控制屏障函数的安全过滤器）得到解决。在自动驾驶的高维设置中为基于视觉的控制器设计安全过滤器可以同样缓解安全问题，但挑战性要大得多。在本文中，我们通过使用冻结的预训练视觉表示模型作为感知主干来设计基于视觉的安全过滤器来应对这一挑战，灵感来自这些模型作为机器人控制策略主干的成功。我们在此背景下实证评估了四种常见预训练视觉模型的离线性能。我们尝试了三种现有的方法来训练黑盒动力学的安全过滤器，因为表示空间上的动力学是未知的。我们使用 DeepAccident 数据集，该数据集由 CARLA 中来自车辆上多个摄像头的动作注释视频组成，模拟真实的事故场景。我们的结果表明，我们的方法产生的过滤器与给定自我车辆及其环境的基本真实状态的过滤器具有竞争力。]]></description>
      <guid>https://arxiv.org/abs/2410.22585</guid>
      <pubDate>Thu, 31 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GRADE：量化文本到图像模型中的样本多样性</title>
      <link>https://arxiv.org/abs/2410.22592</link>
      <description><![CDATA[arXiv:2410.22592v1 公告类型：新
摘要：文本到图像 (T2I) 模型在根据文本描述生成逼真的图像方面非常出色。但是，文本提示本质上是未充分指定的：它们没有指定所需图像的所有可能属性。这引发了两个关键问题：T2I 模型是否会在未充分指定的提示上生成不同的输出？我们如何自动测量多样性？我们提出了 GRADE：粒度属性多样性评估，这是一种量化样本多样性的自动方法。GRADE 利用嵌入在大型语言模型和视觉问答系统中的世界知识来识别相关的概念特定多样性轴（例如，概念“cookie”的“形状”和“颜色”）。然后，它估计概念及其属性的频率分布，并使用（标准化）熵量化多样性。GRADE 实现了超过 90% 的人类一致性，同时与常用的多样性指标表现出弱相关性。我们使用 GRADE 测量了 12 个 T2I 模型的整体多样性，这些模型使用了 400 个概念-属性对，结果表明所有模型都表现出有限的变化。此外，我们发现这些模型经常表现出默认行为，即模型始终生成具有相同属性的概念（例如，98% 的饼干是圆形的）。最后，我们证明了多样性低的一个关键原因是训练数据中的标题指定不足。我们的工作提出了一种现代的、语义驱动的方法来测量样本多样性，并强调了 T2I 模型输出的惊人同质性。]]></description>
      <guid>https://arxiv.org/abs/2410.22592</guid>
      <pubDate>Thu, 31 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>