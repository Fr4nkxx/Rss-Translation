<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Mon, 30 Dec 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>农业边缘人工智能：资源有限环境下的疾病检测轻量级视觉模型</title>
      <link>https://arxiv.org/abs/2412.18635</link>
      <description><![CDATA[arXiv:2412.18635v1 公告类型：新
摘要：本研究论文介绍了一种轻量级、高效的计算机视觉流程的开发，旨在帮助农民使用最少的资源检测橙子疾病。所提出的系统集成了先进的对象检测、分类和分割模型，针对边缘设备的部署进行了优化，确保在资源有限的环境中能够正常运作。该研究评估了各种最先进模型的性能，重点关注它们的准确性、计算效率和泛化能力。值得注意的发现包括 Vision Transformer 在橙子物种分类中实现了 96 的准确率，轻量级 YOLOv8-S 模型以最小的计算开销展示了出色的对象检测性能。该研究强调了现代深度学习架构应对关键农业挑战的潜力，强调了模型复杂性与实用性的重要性。未来的工作将探索扩展数据集、模型压缩技术和联合学习，以增强这些系统在不同农业环境中的适用性，最终为更可持续的农业实践做出贡献。]]></description>
      <guid>https://arxiv.org/abs/2412.18635</guid>
      <pubDate>Mon, 30 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ZenSVI：面向可扩展城市科学的街景图像集成采集、处理和分析开源软件</title>
      <link>https://arxiv.org/abs/2412.18641</link>
      <description><![CDATA[arXiv:2412.18641v1 公告类型：新
摘要：在过去十年中，街景图像 (SVI) 在许多研究中发挥了重要作用，有助于了解和描述街道特征和建筑环境。交通、健康、建筑、人类感知和基础设施等各个领域的研究人员都采用了不同的方法来分析 SVI。然而，这些应用程序和图像处理程序尚未标准化，解决方案是孤立实施的，这通常使其他人难以重现现有工作并开展新的研究。使用 SVI 进行研究需要多个技术步骤：访问可扩展数据收集的 API、预处理图像以标准化格式、实施计算机视觉模型以提取特征以及进行空间分析。这些技术要求为城市研究人员，尤其是那些没有丰富编程经验的研究人员设置了障碍。我们开发了 ZenSVI，这是一个免费的开源 Python 包，它集成并实现了 SVI 分析的整个过程，支持广泛的用例。其端到端流程包括高效地从多个平台（例如 Mapillary 和 KartaView）下载 SVI、分析 SVI 的元数据、应用计算机视觉模型提取目标特征、将 SVI 转换为不同的投影（例如鱼眼和透视）和不同的格式（例如深度图和点云）、使用地图和图表可视化分析以及将输出导出到其他软件工具。我们通过数据质量评估和聚类分析的案例研究，以简化的方式展示了其在新加坡的使用情况。我们的软件提高了依赖 SVI 的研究的透明度、可重复性和可扩展性，并支持研究人员高效地进行城市分析。其模块化设计有利于扩展和解锁新的用例。]]></description>
      <guid>https://arxiv.org/abs/2412.18641</guid>
      <pubDate>Mon, 30 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>剖析 CLIP：使用基于 Schur 补的方法进行分解</title>
      <link>https://arxiv.org/abs/2412.18645</link>
      <description><![CDATA[arXiv:2412.18645v1 公告类型：新
摘要：文献中广泛探讨了使用 CLIP 嵌入来评估文本到图像生成模型生成的样本的对齐方式。虽然广泛采用的 CLIPScore（源自文本和图像嵌入的余弦相似度）可以有效地衡量生成图像的相关性，但它并不能量化文本到图像模型生成的图像的多样性。在这项工作中，我们扩展了 CLIP 嵌入的应用，以量化和解释文本到图像模型的内在多样性，该模型负责从相似的文本提示中生成不同的图像。为了实现这一点，我们提出将基于 CLIP 的图像数据核协方差矩阵分解为基于文本和非基于文本的组件。使用联合图像文本核协方差矩阵的 Schur 补，我们执行此分解并将分解成分的基于矩阵的熵定义为 \textit{Schur 补熵 (SCE)} 分数，这是基于使用不同文本提示收集的数据来衡量文本到图像模型的内在多样性的指标。此外，我们演示了如何使用基于 Schur 补的分解来消除图像 CLIP 嵌入中给定提示的影响，从而实现嵌入对下游任务的特定对象或属性的聚焦或散焦。我们展示了几个数值结果，这些结果应用了我们基于 Schur 补的方法来评估文本到图像模型并修改 CLIP 图像嵌入。代码库可在 https://github.com/aziksh-ospanov/CLIP-DISSECTION 获得]]></description>
      <guid>https://arxiv.org/abs/2412.18645</guid>
      <pubDate>Mon, 30 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>1.58 位通量</title>
      <link>https://arxiv.org/abs/2412.18653</link>
      <description><![CDATA[arXiv:2412.18653v1 公告类型：新
摘要：我们提出了 1.58 位 FLUX，这是第一个成功量化最先进的文本到图像生成模型 FLUX.1-dev 的方法，使用 1.58 位权重（即 {-1, 0, +1} 中的值），同时保持生成 1024 x 1024 图像的可比性能。值得注意的是，我们的量化方法无需访问图像数据，仅依靠 FLUX.1-dev 模型的自我监督即可运行。此外，我们开发了一个针对 1.58 位操作优化的自定义内核，实现了模型存储减少 7.7 倍，推理内存减少 5.1 倍，并改善了推理延迟。对 GenEval 和 T2I Compbench 基准的广泛评估证明了 1.58 位 FLUX 在保持生成质量的同时显着提高计算效率的有效性。]]></description>
      <guid>https://arxiv.org/abs/2412.18653</guid>
      <pubDate>Mon, 30 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>TAB：Transformer Attention Bottlenecks 可实现视觉语言模型中的用户干预和调试</title>
      <link>https://arxiv.org/abs/2412.18675</link>
      <description><![CDATA[arXiv:2412.18675v1 公告类型：新
摘要：多头自注意力 (MHSA) 是 Transformers 的一个关键组件，Transformers 是一种在语言和视觉领域广受欢迎的架构。多个头直观地支持对同一输入进行不同的并行处理。然而，它们也掩盖了每个输入补丁对模型输出的归因。我们提出了一种新颖的 1 头 Transformer 注意力瓶颈 (TAB) 层，插入传统的 MHSA 架构之后，作为可解释性和干预的注意力瓶颈。与标准自注意力不同，TAB 将所有补丁的总注意力限制在 [0, 1] 之内。也就是说，当总注意力为 0 时，没有视觉信息进一步传播到网络中，视觉语言模型 (VLM) 将默认为通用的、与图像无关的响应。为了展示 TAB 的优势，我们使用 TAB 训练 VLM 来执行图像差异字幕。在三个数据集上，我们的模型在字幕制作方面的表现与基线 VLM 类似，但在定位变化和识别未发生变化时，瓶颈更胜一筹。TAB 是第一个允许用户通过编辑注意力进行干预的架构，这通常会产生 VLM 的预期输出。]]></description>
      <guid>https://arxiv.org/abs/2412.18675</guid>
      <pubDate>Mon, 30 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>视频胜过千张图片：探索长视频生成的最新趋势</title>
      <link>https://arxiv.org/abs/2412.18688</link>
      <description><![CDATA[arXiv:2412.18688v1 公告类型：新
摘要：一张图片可能传达一千个单词，但由数百或数千个图像帧组成的视频讲述了一个更复杂的故事。尽管多模态大型语言模型 (MLLM) 取得了重大进展，但生成扩展视频仍然是一项艰巨的挑战。截至撰写本文时，OpenAI 的 Sora（当前最先进的系统）仍然仅限于制作长达一分钟的视频。这种限制源于长视频生成的复杂性，它需要的不仅仅是用于近似密度函数的生成 AI 技术，规划、故事发展和保持空间和时间一致性等基本方面也带来了额外的障碍。将生成 AI 与分而治之的方法相结合可以提高长视频的可扩展性，同时提供更好的控制。在本综述中，我们研究了长视频生成的现状，涵盖了 GAN 和扩散模型等基础技术、视频生成策略、大规模训练数据集、评估长视频的质量指标以及解决现有视频生成能力局限性的未来研究领域。我们相信它将成为一个全面的基础，提供广泛的信息来指导长视频生成领域的未来发展和研究。]]></description>
      <guid>https://arxiv.org/abs/2412.18688</guid>
      <pubDate>Mon, 30 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>STITCH：使用具有拓扑约束和持久同源性的隐式神经表征进行表面重建</title>
      <link>https://arxiv.org/abs/2412.18696</link>
      <description><![CDATA[arXiv:2412.18696v1 公告类型：新
摘要：我们提出了 STITCH，这是一种用于稀疏和不规则间隔点云的神经隐式曲面重建的新方法，同时强制拓扑约束（例如具有单个连通分量）。我们开发了一个基于持久同源性的新可微框架，以制定拓扑损失项，以强制单个 2 流形对象的先验。我们的方法在保留复杂 3D 几何的拓扑方面表现出色，这通过视觉和经验比较都很明显。我们对此进行了理论分析，并可证明使用随机（次）梯度下降优化损失会导致收敛并能够用单个连通分量重建形状。我们的方法展示了可微分拓扑数据分析工具在隐式曲面重建中的集成。]]></description>
      <guid>https://arxiv.org/abs/2412.18696</guid>
      <pubDate>Mon, 30 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>立体匹配中的不确定性量化</title>
      <link>https://arxiv.org/abs/2412.18703</link>
      <description><![CDATA[arXiv:2412.18703v1 公告类型：新
摘要：立体匹配在各种应用中起着至关重要的作用，了解不确定性可以提高安全性和可靠性。尽管如此，立体匹配中不确定性的估计和分析在很大程度上被忽视了。以前的研究通常对不确定性的解释有限，并且难以有效地将其分为数据（随机）和模型（认知）部分。这种解开是至关重要的，因为它可以更清楚地了解错误的根本来源，增强预测信心和决策过程。在本文中，我们提出了一种立体匹配及其不确定性量化的新框架。我们采用贝叶斯风险作为不确定性的度量，分别估计数据和模型不确定性。在四个立体基准上进行了实验，结果表明我们的方法可以准确有效地估计不确定性。此外，我们应用我们的不确定性方法通过选择不确定性较小的数据点来提高预测精度，这反映了我们估计的不确定性的准确性。代码可在 https://github.com/RussRobin/Uncertainty 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2412.18703</guid>
      <pubDate>Mon, 30 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>评估检测变压器的对抗鲁棒性</title>
      <link>https://arxiv.org/abs/2412.18718</link>
      <description><![CDATA[arXiv:2412.18718v1 公告类型：新 
摘要：稳健的物体检测对于自动驾驶和移动机器人至关重要，准确检测车辆、行人和障碍物对于确保安全至关重要。尽管物体检测变换器 (DETR) 取得了进步，但它们对对抗攻击的稳健性仍未得到充分探索。本文使用 MS-COCO 和 KITTI 数据集对白盒和黑盒对抗攻击下的 DETR 模型及其变体进行了全面评估，以涵盖一般和自动驾驶场景。我们扩展了著名的白盒攻击方法 (FGSM、PGD 和 CW) 来评估 DETR 漏洞，表明 DETR 模型很容易受到对抗攻击，类似于传统的基于 CNN 的检测器。我们广泛的可迁移性分析表明，DETR 变体之间的网络内可迁移性很高，但跨网络可迁移性有限，无法迁移到基于 CNN 的模型。此外，我们提出了一种专为 DETR 设计的新型无针对性攻击，利用其中间损失函数以最小的扰动引起错误分类。自注意力特征图的可视化提供了对抗性攻击如何影响 DETR 模型内部表示的见解。这些发现揭示了在标准对抗性攻击下检测变压器的严重漏洞，强调了未来研究的必要性，以增强基于变压器的物体检测器在安全关键应用中的稳健性。]]></description>
      <guid>https://arxiv.org/abs/2412.18718</guid>
      <pubDate>Mon, 30 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>HELPNet：分层扰动一致性和熵引导集成，用于涂鸦监督医学图像分割</title>
      <link>https://arxiv.org/abs/2412.18738</link>
      <description><![CDATA[arXiv:2412.18738v1 公告类型：新
摘要：为医学图像分割创建完全注释的标签非常耗时且成本高昂，这强调了创新方法的必要性，以尽量减少对详细注释的依赖。涂鸦注释提供了一种更具成本效益的替代方案，大大降低了与完整注释相关的费用。然而，涂鸦注释提供的信息有限且不精确，无法捕捉准确描绘器官所需的详细结构和边界特征。为了应对这些挑战，我们提出了 HELPNet，这是一种新颖的基于涂鸦的弱监督分割框架，旨在弥合注释效率和分割性能之间的差距。HELPNet 集成了三个模块。分层扰动一致性 (HPC) 模块通过在全局、局部和焦点视图中采用密度控制的拼图扰动来增强特征学习，从而实现多尺度结构表示的稳健建模。在此基础上，熵引导伪标签 (EGPL) 模块使用熵来评估分割预测的置信度，从而生成高质量的伪标签。最后，结构先验细化 (SPR) 模块结合了连通性和有界先验来提高伪标签的精度和可靠性。在三个公共数据集 ACDC、MSCMRseg 和 CHAOS 上的实验结果表明，HELPNet 明显优于基于涂鸦的弱监督分割的最新方法，并实现了与全监督方法相当的性能。代码可在 https://github.com/IPMI-NWU/HELPNet 上找到。]]></description>
      <guid>https://arxiv.org/abs/2412.18738</guid>
      <pubDate>Mon, 30 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>对象中心模型在组合泛化方面的成功与局限性</title>
      <link>https://arxiv.org/abs/2412.18743</link>
      <description><![CDATA[arXiv:2412.18743v1 公告类型：新
摘要：近年来，经验表明，标准的解缠潜变量模型不支持视觉领域的稳健组合学习。事实上，尽管解缠模型的设计目标是将数据集分解为其变化的组成因素，但它表现出极其有限的组合泛化能力。另一方面，以对象为中心的架构已经显示出有希望的组合技能，尽管这些技能 1) 尚未经过广泛测试，2) 实验仅限于场景组合——其中模型必须推广到视觉场景中的新对象组合，而不是新对象属性组合。在这项工作中，我们表明这些组合泛化技能可以扩展到后来的设置。此外，我们提供了指向这些技能来源的证据，以及如何通过仔细训练来改进它们。最后，我们指出了一个仍然存在的重要限制，这表明了新的研究方向。]]></description>
      <guid>https://arxiv.org/abs/2412.18743</guid>
      <pubDate>Mon, 30 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于稳健群组重新识别的分层多图学习</title>
      <link>https://arxiv.org/abs/2412.18766</link>
      <description><![CDATA[arXiv:2412.18766v1 公告类型：新
摘要：由于相互遮挡、动态成员交互和不断发展的群体结构等挑战，群体重新识别 (G-ReID) 面临的复杂性高于个体重新识别 (ReID)。先前基于图的方法旨在通过将群体建模为单一拓扑结构来捕捉这些动态。然而，这些方法难以推广到不同的群体组成，因为它们无法完全代表群体内的多方面关系。
在本研究中，我们引入了一个分层多图学习 (HMGL) 框架来应对这些挑战。我们的方法将群体建模为多关系图的集合，利用显式特征（例如遮挡、外观和前景信息）和成员之间的隐式依赖关系。这种通过多图神经网络 (MGNN) 编码的分层表示使我们能够解决成员关系中的歧义，特别是在复杂、人口密集的场景中。为了进一步提高匹配准确率，我们提出了一种多尺度匹配 (MSM) 算法，该算法可缓解成员信息模糊性和对硬样本的敏感性问题，从而提高在具有挑战性的场景中的鲁棒性。
我们的方法在两个标准基准 CSG 和 RoadGroup 上实现了最先进的性能，Rank-1/mAP 得分分别为 95.3%/94.4% 和 93.9%/95.4%。这些结果标志着 Rank-1 准确率比现有方法显著提高了 1.7% 和 2.5%。]]></description>
      <guid>https://arxiv.org/abs/2412.18766</guid>
      <pubDate>Mon, 30 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>机器人智能的具体图像质量评估</title>
      <link>https://arxiv.org/abs/2412.18774</link>
      <description><![CDATA[arXiv:2412.18774v1 Announce Type: new 
摘要：用户生成内容（UGC）的图像质量评估（IQA）是人类体验质量（QoE）的关键技术。然而，对于机器人生成内容（RGC），其图像质量是否会符合莫拉维克悖论并违背人类的常识？人类的主观评分更多是基于图像的吸引力。具身智能体需要在环境中进行交互和感知，并最终执行特定任务。视觉图像作为输入直接影响下游任务。在本文中，我们首先提出了一个具身图像质量评估（EIQA）框架。我们根据机器人的下游任务为输入图像建立评估指标。此外，我们构建了一个包含5,000个参考和扭曲图像注释的具身偏好数据库（EPD）。最后验证了主流IQA算法在EPD数据集上的性能。实验表明，具身图像的质量评估与人类不同。我们真诚希望 EPD 能够通过专注于图像质量评估为具身人工智能的发展做出贡献。基准测试可在 https://github.com/Jianbo-maker/EPD_benchmark 上找到。]]></description>
      <guid>https://arxiv.org/abs/2412.18774</guid>
      <pubDate>Mon, 30 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ObitoNet：多模态高分辨率点云重建</title>
      <link>https://arxiv.org/abs/2412.18775</link>
      <description><![CDATA[arXiv:2412.18775v1 公告类型：新
摘要：ObitoNet 采用交叉注意机制来集成多模态输入，其中视觉变换器 (ViT) 从图像中提取语义特征，点云标记器使用最远点采样 (FPS) 和 K 最近邻 (KNN) 处理几何信息以进行空间结构捕获。学习到的多模态特征被输入到基于变换器的解码器中进行高分辨率点云重建。这种方法利用了两种模态丰富的图像特征和精确的几何细节的互补优势，即使在稀疏或嘈杂数据等具有挑战性的条件下也能确保稳健的点云生成。]]></description>
      <guid>https://arxiv.org/abs/2412.18775</guid>
      <pubDate>Mon, 30 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>视觉变换器的统一局部和全局注意力交互模型</title>
      <link>https://arxiv.org/abs/2412.18778</link>
      <description><![CDATA[arXiv:2412.18778v1 公告类型：新
摘要：我们提出了一种新方法，该方法扩展了视觉变换器 (ViT) 的自注意力机制，以便在不同的数据集上更准确地检测对象。ViT 在图像理解任务（例如对象检测、分割和分类）方面表现出强大的能力。这在一定程度上归功于它们能够利用视觉标记之间的交互产生的全局信息。然而，ViT 中的自注意力机制是有限的，因为它们不允许视觉标记在计算全局注意力之前与相邻特征交换局部或全局信息。这是有问题的，因为在关注（匹配）其他标记时，标记会被孤立地处理，而有价值的空间关系会被忽略。这种孤立因点积相似性操作而进一步加剧，这些操作使来自不同语义类别的标记在视觉上看起来相似。为了解决这些限制，我们对传统的自注意力框架进行了两项修改；一种用于局部特征混合的新型积极卷积池化策略，以及一种促进语义概念之间交互和特征交换的新概念注意力转换。实验结果表明，在自我注意之前，视觉特征之间的局部和全局信息交换显著提高了具有挑战性的物体检测任务的性能，并适用于多个基准数据集和具有挑战性的医疗数据集。我们发布了源代码和一个新的癌性肿瘤数据集（嵌合细胞簇）。]]></description>
      <guid>https://arxiv.org/abs/2412.18778</guid>
      <pubDate>Mon, 30 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>