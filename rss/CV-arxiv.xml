<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>CS.CV更新arxiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.cv更新arxiv.org e-print存档。</description>
    <lastBuildDate>Wed, 19 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>哈巴狗：零射的物理理解与高斯碎片</title>
      <link>https://arxiv.org/abs/2502.12231</link>
      <description><![CDATA[ARXIV：2502.12231V1公告类型：新 
摘要：当前的机器人系统可以很好地了解对象的类别和姿势。但是，在野外了解质量，摩擦和硬度等物理特性仍然具有挑战性。我们提出了一种新方法，该方法使用高斯分裂表示来重建3D对象，并以零拍的方式预测各种物理属性。我们在重建阶段提出了两种技术：几何学意识的正则化损失函数，以提高形状质量和区域感知的特征对比损失函数，以促进区域亲和力。在推断期间设计了另外两种新技术：基于特征的属性传播模块和一个针对高斯表示的体积集成模块。我们的框架被称为零拍物的物理理解，高斯分裂或哈巴狗。 PUGS以ABO-500质量预测的标准基准取得了新的最新结果。我们提供广泛的定量消融和定性可视化，以证明我们的设计机制。我们表明，所提出的方法可以帮助解决挑战性的现实掌握任务。我们的代码，数据和模型可在https://github.com/evernorif/pugs上找到]]></description>
      <guid>https://arxiv.org/abs/2502.12231</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Smokenet：有效的烟雾分段利用多尺度卷积和多视图注意机制</title>
      <link>https://arxiv.org/abs/2502.12258</link>
      <description><![CDATA[ARXIV：2502.12258V1公告类型：新 
摘要：有效的烟羽分段对于环境监测和工业安全至关重要，从而可以检测和缓解来自采石场爆炸和野火等活动的有害排放。准确的细分促进了环境影响评估，及时的干预措施以及遵守安全标准。但是，现有模型通常面临高计算需求，并且对各种烟雾外观的适应性有限，从而限制了它们在资源受限环境中的部署。为了解决这些问题，我们介绍了Smokenet，这是一种新颖的深度学习体系结构，利用多尺度卷积和多视线线性注意机制结合了特定层的损失功能，以处理各种烟雾羽流的复杂动态，从而确保各种环境之间有效且准确的段。此外，我们使用四个数据集评估了Smokenet的性能和多功能性，包括我们提供给社区的采石场爆炸烟数据集。结果表明，Smokenet在计算效率和细分精度之间保持了良好的平衡，使其适合在环境监测和安全管理系统中部署。通过贡献一个新的数据集并提供有效的细分模型，Smokenet在多样化和挑战性的环境中提高了烟分段能力。]]></description>
      <guid>https://arxiv.org/abs/2502.12258</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用深度先验和正则化的数据效率限量角CT</title>
      <link>https://arxiv.org/abs/2502.12293</link>
      <description><![CDATA[arxiv：2502.12293v1公告类型：新 
摘要：从其ra ronversion中重建图像是在X射线扫描等应用中产生的基本计算机断层扫描（CT）任务。在许多实际情况下，全面的180度扫描是不可行的，或者有减少辐射暴露的愿望。在这些有限的角度设置中，问题变得不足，专为全视图数据而设计的方法通常会留下重要的人工制品。我们提出了一种非常低的DATA方法，可以在严重的角度限制下从其ra换变换中重建原始图像。由于逆问题是错误的，因此我们结合了多种正则化方法，包括总变化，曲官滤波器，深层图像先验和贴片级自动编码器。我们使用ra变换的可区分实现，这使我们能够使用基于梯度的技术来解决逆问题。我们的方法在2022年赫尔辛基层析成像挑战赛的数据集上进行了评估，该数据集的目标是从其有限角度的正式图中重建二进制磁盘。我们只使用总共12个数据点 - 学习先前和四个用于选择超参数的数据 - 并获得与最佳合成数据驱动方法相当的结果。]]></description>
      <guid>https://arxiv.org/abs/2502.12293</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>二人流媒体：流媒体识别框架</title>
      <link>https://arxiv.org/abs/2502.12297</link>
      <description><![CDATA[ARXIV：2502.12297V1公告类型：新 
摘要：在资源受限的方案中的手势识别在实现高准确性和低潜伏期方面面临重大挑战。本文提出的二重奏流媒体二重奏流媒体通过三阶段的稀疏识别机制，具有外部隐藏状态的RNN-LITE模型以及专门的培训和后处理管道来解决这些挑战，从而实现了创新的进度，从而实现了创新的进度在实时性能和轻量级设计中。实验结果表明，二人流媒体与精度指标的主流方法匹配，同时将实时因子降低了约92.3％，即提供了近13倍的速度。此外，与主流模型相比，该框架将参数计数缩小为1/38（空闲状态）和1/9（繁忙状态）。总而言之，二人流媒体不仅提供了一种有效且实用的解决方案，用于在资源约束设备中流式识别识别手势，而且还为在多模式和多样化场景中扩展应用程序奠定了坚实的基础。]]></description>
      <guid>https://arxiv.org/abs/2502.12297</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从游戏到研究：用于机器人技术的合成数据生成的GTA V</title>
      <link>https://arxiv.org/abs/2502.12303</link>
      <description><![CDATA[ARXIV：2502.12303V1公告类型：新 
摘要：在计算机视觉中，能够在现实世界中有效概括的鲁棒算法的开发越来越多地需要在各种环境条件下收集的大规模数据集。但是，获取此类数据集是耗时的，昂贵的，有时是不可行的。为了解决这些局限性，合成数据的使用已成为可行的替代方案，使研究人员能够生成大量数据，同时在受控的环境中模拟各种环境环境。在这项研究中，我们研究了合成数据在机器人技术和导航中的使用，特别关注同时定位和映射（SLAM）和Visual Plote识别（VPR）。特别是，我们介绍了一个使用视频游戏Grand Theft Auto V（GTA V）的虚拟环境创建的合成数据集，以及一个算法，旨在生成VPR数据集的算法，而无需人为监督。通过以SLAM和VPR为中心的一系列实验，我们证明了从GTA V得出的合成数据在质量上与现实世界中的数据相当。此外，这些合成数据可以在这些应用程序中补充甚至替代现实世界数据。这项研究为创建大规模合成数据集的创建奠定了基础，为未来的研究和开发提供了一种经济高效且可扩展的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2502.12303</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LANP：重新思考语言先验在大型视觉模型中的影响</title>
      <link>https://arxiv.org/abs/2502.12359</link>
      <description><![CDATA[ARXIV：2502.12359V1公告类型：新 
摘要：大型视觉模型（LVLM）在各种任务中表现出令人印象深刻的表现。但是，LVLM遭受了幻觉的困扰，这阻碍了他们在现实世界中的收养。现有研究强调，LVLM的强大语言先验可以压倒视觉信息，从而导致幻觉。但是，语言先验的积极作用是强大的LVLM的关键。如果语言先验太弱，LVLM将难以利用丰富的参数知识和教学理解能力，以在挑战视觉场景中完成任务，而单独视觉信息不足。因此，我们提出了一个名为LANP的基准，以重新考虑LVLMS语言先验的影响。它旨在调查当前LVLM中语言先验的强度。 LANP由170张图像和340个相应的精心设计的问题组成。在25个流行的LVLM上进行了广泛的实验表明，许多LVLM的语言先验不足以有效地帮助问题回答何时部分隐藏。在这种情况下，包括GPT-4涡轮在内的许多型号（包括GPT-4涡轮增压）的精度低于0.5。]]></description>
      <guid>https://arxiv.org/abs/2502.12359</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>沿预定义的人为理解的维度检测视觉模型中的系统弱点</title>
      <link>https://arxiv.org/abs/2502.12360</link>
      <description><![CDATA[ARXIV：2502.12360V1公告类型：新 
摘要：在过去的几年中，研究DNNS的系统弱点在建立安全AI系统的关注度上不断增强。切片发现方法（SDMS）是发现此类系统弱点的突出算法方法。他们识别出最高的语义相干切片/数据的子集，其中DNN的测试表现较低。为了直接有用，例如，作为安全论证中的证据，应将切片与人类理解（安全相关的）维度保持一致，例如，安全和域专家将其定义为操作设计领域的一部分（奇数（奇数） ）。尽管对于结构化数据而言，缺乏语义元数据使这些调查对非结构化数据挑战。因此，我们提出了一个完整的工作流程，该工作流将当代基础模型与用于组合搜索的算法相结合，该算法考虑了结构化数据和DNN错误，以发现图像中的系统弱点。与现有方法相反，我们的方法确定了与预定义的人类理解的维度一致的弱切片。由于工作流程包括基础模型，因此其中间和最终结果可能并不总是准确的。因此，我们在工作流程中建立了一种解决嘈杂元数据影响的方法。我们评估我们的方法W.R.T.它在四个流行的计算机视觉数据集上的质量，包括CityScapes，BDD100K和RailSem19等自主驾驶数据集，同时将多种最先进的模型用作DNNS省的DNNS。]]></description>
      <guid>https://arxiv.org/abs/2502.12360</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>对齐和对抗性鲁棒性：更类似人类的模型是否更安全？</title>
      <link>https://arxiv.org/abs/2502.12377</link>
      <description><![CDATA[ARXIV：2502.12377V1公告类型：新 
摘要：代表性对齐是指模型的内部表示反映生物学视觉的程度，从而提供了对神经相似性和功能对应关系的见解。最近，一些更一致的模型证明了对对抗性例子的弹性更高，这提出了一个问题，即更多的人类安装模型本质上更安全。在这项工作中，我们进行了大规模的经验分析，以系统地研究代表性一致性和对抗性鲁棒性之间的关系。我们评估了118个型号，涵盖了各种体系结构和训练范例，测量了他们在106个基准测试中的神经和行为一致性和工程任务性能以及通过自动攻击的对抗性鲁棒性。我们的发现表明，尽管平均比对和鲁棒性表现出较弱的总体相关性，但特定的比对基准是对抗性鲁棒性的有力预测指标，尤其是那些衡量对纹理或形状选择性的那些。这些结果表明，不同形式的一致性在模型鲁棒性中起着不同的作用，激发了对如何利用对齐驱动的方法的进一步调查，以构建更安全和更感知的视觉模型。]]></description>
      <guid>https://arxiv.org/abs/2502.12377</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>OCT数据就是您所需要的：视力变压器如何进行或没有预训练益处成像</title>
      <link>https://arxiv.org/abs/2502.12379</link>
      <description><![CDATA[ARXIV：2502.12379V1公告类型：新 
摘要：光学连贯性层析成像（OCT）提供了可用于诊断各种疾病的高分辨率横截面图像，但是它们与自然图像的独特特征有关在像Imagenet这样的数据集中进行大规模预训练的问题引起了问题。在本文中，我们研究了基于Imagenet的预训练对视觉变压器（VIT）性能对不同数据集大小的OCT图像分类的影响。我们的实验涵盖了四类视网膜病理（CNV，DME，DRUSEN，正常）。结果表明，尽管预训练可以加速收敛并有可能在较小的数据集中提供更好的性能，但是当有足够的OCT数据可用时，从头开始培训可能会达到可比甚至更高的精度。我们的发现强调了匹配域特征在预训练中的重要性，并呼吁对大规模的OCT特异性预训练进行进一步研究。]]></description>
      <guid>https://arxiv.org/abs/2502.12379</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>气体对象检测</title>
      <link>https://arxiv.org/abs/2502.12415</link>
      <description><![CDATA[ARXIV：2502.12415V1公告类型：新 
摘要：对象检测是计算机视觉中的基本和挑战性问题，由于深度学习的有效性而经历了快速发展。要检测到的当前对象主要是具有明显和不同视觉特征的刚性固体物质。在本文中，我们努力努力探讨一个名为“气体对象检测”的任务（上帝），该任务是为了探索是否可以将物体检测技术从固体物质扩展到气态物质。然而，气体具有明显不同的视觉特征：1）显着性缺乏，2）任意和不断变化的形状，3）缺乏不同的边界。为了促进这项具有挑战性的任务的研究，我们构建了一个由600个视频（141,017帧）组成的God-Video数据集，该视频涵盖了具有多种气体的各种属性。基于此数据集建立了全面的基准测试，从而可以对框架级别和视频级检测器进行严格的评估。从高斯分散模型中推导的物理风格的体素偏移场（VSF）旨在模拟潜在3D空间中的几何不规则和不断变化的形状。通过将VSF整合到更快的RCNN中，VSF RCNN是气态对象检测的简单但强的基线。我们的工作旨在吸引对这个有价值的领域的进一步研究。]]></description>
      <guid>https://arxiv.org/abs/2502.12415</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过增强亮度鲁棒性来增强深色恒定的照明估计</title>
      <link>https://arxiv.org/abs/2502.12418</link>
      <description><![CDATA[ARXIV：2502.12418V1公告类型：新 
摘要：颜色稳定性估计以校正颜色偏见的图像的发光色。最近，深度神经网络驱动的色彩恒定（DNNCC）模型已取得了重大进步。然而，由于深神经网络的脆弱性，尚未探索DNNCC的潜在风险。在本文中，我们从稳健性的角度进行了首次研究，对颜色恒定性的关键因素在dnncc中的影响。我们的评估表明，尽管重点是色度估计，但几种主流DNNCC模型对亮度表现出较高的敏感性。这阐明了现有DNNCC模型的潜在局限性：考虑到现实世界中数据集的广泛亮度变化，它们对亮度的敏感性可能会阻碍性能。从分析的见解中，我们提出了一种简单而有效的亮度鲁棒性增强策略，称为BRE。 BRE的核心建立在自适应的阶梯尺寸的对抗亮度增强技术的基础上，该技术可以识别高风险的亮度变化，并通过显式亮度调节生成增强图像。随后，BRE开发了一种亮度 - 耐心感知的模型优化策略，该策略整合了对抗性亮度训练和亮度对比损失，从而显着增强了DNNCC模型的亮度鲁棒性。 BRE不含高参数，可以集成到现有的DNNCC模型中，而不会在测试阶段产生其他开销。在两个公共颜色恒定数据集 - 彩色检查器和立方体+的实验表明，拟议的BRE始终提高现有DNNCC模型的照明估计性能，使估计误差平均减少了5.04％的六个主流DNNCC模型，并强大的角色增强了重要的作用。这些模型中的亮度稳健性。]]></description>
      <guid>https://arxiv.org/abs/2502.12418</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>强大的分解反事实学习，用于物理视觉常识性推理</title>
      <link>https://arxiv.org/abs/2502.12425</link>
      <description><![CDATA[ARXIV：2502.12425V1公告类型：新 
摘要：在本文中，我们提出了一种新的鲁棒分解的反事实学习（RDCL）方法，用于物理视听常识性推理。该任务旨在根据视频和音频输入来推断对象的物理通心，主要挑战是如何模仿人类的推理能力，即使在缺失模态的情况下也是如此。当前的大多数方法都无法在多模式数据中充分利用不同的特征，并且在模型中缺乏因果推理能力阻碍了隐式物理知识推断的进步。为了解决这些问题，我们提出的RDCL方法将视频分解为潜在空间中静态（时间不变）和动态（时变）因素，该因素是通过分离的顺序编码器将视频解散为潜在的空间中的视频，从而采用了各种自动编码器（VAE），以最大程度地提高互助信息。对比损失函数。此外，我们引入了反事实学习模块，以通过对反事实干预下的不同对象之间的物理知识关系进行建模，以增强模型的推理能力。为了减轻不完整的模式数据问题，我们引入了一种强大的多模式学习方法，通过分解共享功能和特定于模型的功能来恢复缺失的数据。我们提出的方法是一个插件模块，可以将其合并到包括VLM在内的任何基线中。在实验中，我们表明我们提出的方法提高了基线方法的推理准确性和鲁棒性，并实现了最新的性能。]]></description>
      <guid>https://arxiv.org/abs/2502.12425</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>地球系统模型的多图像超级分辨率建模</title>
      <link>https://arxiv.org/abs/2502.12427</link>
      <description><![CDATA[ARXIV：2502.12427V1公告类型：新 
摘要：超分辨率（SR）技术对于改善地球系统模型（ESM）数据的空间分辨率至关重要，这有助于更好地理解复杂的环境过程。本文提出了一种新的算法Vifor，该算法结合了视觉变压器（VIT）和隐式神经表示网络（INRS），以从低分辨率（LR）输入中生成高分辨率（HR）图像。 Vifor在视觉变压器体系结构中引入了基于傅立叶的激活功能的新颖集成，使其能够有效捕获全球上下文和高频细节，对于准确的SR重建至关重要。结果表明，基于峰值信噪比（PSNR）等指标，VIT，正弦表示网络（Siren）和SR生成对抗网络（SRGAN）（SRGANS）均优于最先进的方法，例如VIT，SINUSOIDAL代表网络（Siren）和SR生成对抗网络（SRGANS）全局和本地图像的错误（MSE）。 VIFOR在VIT，短波和Longwave Flux的完整图像上提高了高达4.18 dB，1.56 dB和1.73 dB的PSNR。]]></description>
      <guid>https://arxiv.org/abs/2502.12427</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Yunet：改进的Yolov11网络用于天际线检测</title>
      <link>https://arxiv.org/abs/2502.12449</link>
      <description><![CDATA[ARXIV：2502.12449V1公告类型：新 
摘要：天际线检测在地球层，飞行控制，视觉导航，端口安全等中起着重要作用。由于天气或照明环境不同，天空和非天空区域的外观是可变的，这给天际线发现带来了挑战。在这项研究中，我们提出了Yunet算法，该算法改善了Yolov11架构，以分割天空区域并在复杂且可变的环境中提取天际线。为了提高多尺度和大范围上下文特征融合的能力，Yolov11体系结构被扩展为一个类似于Unet的体系结构，由编码器，颈部和解码器子模块组成。编码器从给定图像中提取多尺度功能。脖子融合了这些多尺度功能。解码器应用融合功能来完成预测重建。为了验证提出的方法，在Skyfinder和CH1数据集上测试了Yunet，分别进行了分割和天际线检测。我们的测试表明，Yunet分割的IOU可以达到0.9858，而Yunet Skyline检测的平均误差仅为1.36像素。该实施发表在https://github.com/kuazhangxiaoai/skylinedet-yolov11seg.git上。]]></description>
      <guid>https://arxiv.org/abs/2502.12449</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用大语言模型进行基准测试零镜头的面部情感注释：Dailylife中的多级和多框架方法</title>
      <link>https://arxiv.org/abs/2502.12454</link>
      <description><![CDATA[ARXIV：2502.12454V1公告类型：新 
摘要：本研究调查了使用大语言模型（LLM）在日常情况下自动注释人类情绪的可行性和性能。我们采用了GPT-4O-MINI模型，对从视频段中提取的关键帧进行快速，零拍标记，对公开可用的Ferv39K数据集进行了实验。在七级情感分类法（“愤怒”，“厌恶”，“恐惧”，“快乐”，“中立”，“悲伤”，“惊喜”）下，LLM的平均精度约为50％。相反，如果仅限于三元情绪分类（负/中性/阳性）时，平均精度增加到约64％。此外，我们探索了一种策略，该策略在1-2秒的视频片段中集成了多个帧，以提高标签性能并降低成本。结果表明，这种方法可以稍微提高注释精度。总体而言，我们的初步发现突出了零摄影LLM在人面部情感注释任务中的潜在应用，这为降低标签成本并扩大了LLM在复杂的多模式环境中的适用性提供了新的途径。]]></description>
      <guid>https://arxiv.org/abs/2502.12454</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>