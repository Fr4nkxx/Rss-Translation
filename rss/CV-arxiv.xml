<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Tue, 20 Aug 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>GoodSAM++：通过全景语义分割的“任意分割”模型弥合领域和容量差距</title>
      <link>https://arxiv.org/abs/2408.09115</link>
      <description><![CDATA[arXiv:2408.09115v1 公告类型：新
摘要：本文介绍了 GoodSAM++，这是一个新颖的框架，利用 SAM（即老师）强大的零样本实例分割能力来学习紧凑的全景语义分割模型，即学生，而无需任何标记数据。GoodSAM++ 解决了两个关键挑战：1）SAM 无法提供语义标签和全景图像固有的失真问题；2）SAM 和学生之间的巨大容量差异。GoodSAM++ 的“开箱即用”洞察力是引入教师助理 (TA) 为 SAM 提供语义信息，与 SAM 集成以获得可靠的伪语义图，以弥合领域和容量差距。为了实现这一点，我们首先提出了一个失真感知校正 (DARv2) 模块来解决领域差距。它有效地缓解了全景图像中的物体变形和失真问题，以获得伪语义图。然后，我们引入了一个多级知识自适应 (MKA) 模块，以有效地将语义信息从 TA 和伪语义图传输到我们的紧凑型学生模型，从而解决巨大的容量差距。我们在室外和室内基准数据集上进行了广泛的实验，结果表明我们的 GoodSAM++ 比最先进的 (SOTA) 域自适应方法实现了显着的性能提升。此外，多样化的开放世界场景展示了我们 GoodSAM++ 的泛化能力。最后但并非最不重要的是，我们最轻量级的学生模型仅使用 370 万个参数就实现了与 SOTA 模型相当的性能。]]></description>
      <guid>https://arxiv.org/abs/2408.09115</guid>
      <pubDate>Wed, 21 Aug 2024 03:14:09 GMT</pubDate>
    </item>
    <item>
      <title>用于图像语义分割的深度引导纹理扩散</title>
      <link>https://arxiv.org/abs/2408.09097</link>
      <description><![CDATA[arXiv:2408.09097v1 公告类型：新
摘要：深度信息为 3D 结构（尤其是物体的轮廓）提供了宝贵的见解，可用于改进语义分割任务。然而，由于深度和视觉之间的模态差距，深度信息的简单融合可能会破坏特征并损害准确性。在这项工作中，我们引入了一种深度引导的纹理扩散方法，可以有效地解决轮廓挑战。我们的方法从边缘和纹理中提取低级特征以创建纹理图像。然后有选择地将该图像扩散到深度图上，增强了精确提取物体轮廓所必需的结构信息。通过将这个丰富的深度图与原始 RGB 图像集成到联合特征嵌入中，我们的方法有效地弥合了深度图和图像之间的差异，从而实现了更准确的语义分割。我们对各种常用数据集进行了全面的实验，涵盖了各种语义分割任务，包括伪装物体检测 (COD)、显著物体检测 (SOD) 和室内语义分割。借助无源估计深度或深度摄像头捕获的深度，我们的方法始终优于现有基线并取得了新的最佳结果，证明了我们的深度引导纹理扩散对图像语义分割的有效性。]]></description>
      <guid>https://arxiv.org/abs/2408.09097</guid>
      <pubDate>Wed, 21 Aug 2024 03:14:08 GMT</pubDate>
    </item>
    <item>
      <title>HybridOcc：基于 NeRF 增强 Transformer 的多摄像头 3D 占用预测</title>
      <link>https://arxiv.org/abs/2408.09104</link>
      <description><![CDATA[arXiv:2408.09104v1 公告类型：新
摘要：基于视觉的 3D 语义场景完成 (SSC) 通过 3D 体积表示描述自动驾驶场景。然而，场景表面对不可见体素的遮挡对当前 SSC 方法在幻觉精细的 3D 几何方面提出了挑战。本文提出了 HybridOcc，一种由 Transformer 框架和 NeRF 表示生成并在粗到细的 SSC 预测框架中进行细化的混合 3D 体积查询提议方法。HybridOcc 通过基于混合查询提议的 Transformer 范式聚合上下文特征，同时将其与 NeRF 表示相结合以获得深度监督。Transformer 分支包含多个尺度并使用空间交叉注意进行 2D 到 3D 的转换。新设计的 NeRF 分支通过体积渲染隐式推断场景占用情况，包括可见和不可见体素，并显式捕获场景深度而不是生成 RGB 颜色。此外，我们提出了一种创新的占用感知射线采样方法来定位 SSC 任务，而不是专注于场景表面，从而进一步提高整体性能。在 nuScenes 和 SemanticKITTI 数据集上进行的大量实验证明了我们的 HybridOcc 在 SSC 任务上的有效性。]]></description>
      <guid>https://arxiv.org/abs/2408.09104</guid>
      <pubDate>Wed, 21 Aug 2024 03:14:08 GMT</pubDate>
    </item>
    <item>
      <title>定位地球上的任何事物：推进遥感社区的开放词汇对象检测</title>
      <link>https://arxiv.org/abs/2408.09110</link>
      <description><![CDATA[arXiv:2408.09110v1 公告类型：新
摘要：物体检测，特别是开放词汇物体检测，在环境监测、自然灾害评估和土地利用规划等地球科学中起着至关重要的作用。然而，现有的开放词汇检测器主要在自然世界图像上进行训练，由于数据领域存在巨大差距，难以推广到遥感图像。因此，本文旨在推动遥感社区开放词汇物体检测的发展。为了实现这一目标，我们首先将任务重新表述为定位地球上的任何东西 (LAE)，目标是检测地球上的任何新概念。然后，我们开发了 LAE-Label Engine，它可以收集、自动注释和统一多达 10 个遥感数据集，从而创建 LAE-1M——第一个具有广泛类别覆盖范围的大规模遥感物体检测数据集。利用 LAE-1M，我们进一步提出并训练了新颖的 LAE-DINO 模型，这是 LAE 任务的第一个开放词汇基础对象检测器，具有动态词汇构建 (DVC) 和视觉引导文本提示学习 (VisGT) 模块。DVC 为每个训练批次动态构建词汇，而 VisGT 将视觉特征映射到语义空间，增强文本特征。我们在已建立的遥感基准 DIOR、DOTAv2.0 以及我们新推出的 80 类 LAE-80C 基准上进行了全面实验。结果证明了 LAE-1M 数据集的优势和 LAE-DINO 方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2408.09110</guid>
      <pubDate>Wed, 21 Aug 2024 03:14:08 GMT</pubDate>
    </item>
    <item>
      <title>ADen：稀疏视图相机姿态估计的自适应密度表示</title>
      <link>https://arxiv.org/abs/2408.09042</link>
      <description><![CDATA[arXiv:2408.09042v1 公告类型：新
摘要：从一组图像中恢复相机姿势是 3D 计算机视觉的一项基础任务，它为 3D 场景/物体重建等关键应用提供支持。经典方法通常依赖于特征对应，例如关键点，这要求输入图像具有较大的重叠和较小的视点变化。这样的要求在视图稀疏的场景中带来了相当大的挑战。最近的数据驱动方法旨在直接输出相机姿势，要么通过回归 6DoF 相机姿势，要么将旋转制定为概率分布。然而，每种方法都有其局限性。一方面，直接回归相机姿势可能是不适定的，因为它假设单一模式，这在对称性下是不正确的，并导致次优解。另一方面，概率方法能够对对称模糊性进行建模，但它们通过蛮力均匀地对整个旋转空间进行采样。这导致在高样本密度（可提高模型精度）和样本效率（决定运行时间）之间不可避免地存在权衡。在本文中，我们提出 ADen 通过使用生成器和鉴别器来统一这两个框架：生成器经过训练可输出 6DoF 相机姿势的多个假设以表示分布并处理多模式模糊性，鉴别器经过训练可识别最能解释数据的假设。这使得 ADen 能够结合两全其美，在实证评估中实现比以前的方法更高的精度和更低的运行时间。]]></description>
      <guid>https://arxiv.org/abs/2408.09042</guid>
      <pubDate>Wed, 21 Aug 2024 03:14:07 GMT</pubDate>
    </item>
    <item>
      <title>MoRA：LoRA 引导的多模态疾病诊断（缺失模态）</title>
      <link>https://arxiv.org/abs/2408.09064</link>
      <description><![CDATA[arXiv:2408.09064v1 公告类型：新
摘要：多模态预训练模型可以有效地提取和融合来自不同模态的特征，并且对微调的内存要求较低。尽管效率很高，但它们在疾病诊断中的应用尚未得到充分探索。一个重大挑战是经常出现缺失模态，这会损害性能。此外，微调整个预训练模型需要大量的计算资源。为了解决这些问题，我们引入了一种计算效率高的方法，即模态感知低秩自适应 (MoRA)。MoRA 将每个输入投影到低固有维度，但在模态缺失的情况下使用不同的模态感知上投影进行模态特定自适应。实际上，MoRA 集成到模型的第一个块中，当模态缺失时可显著提高性能。它需要的计算资源最少，与训练整个模型相比，所需的可训练参数不到 1.6%。实验结果表明，MoRA 在疾病诊断方面优于现有技术，表现出卓越的性能、稳健性和训练效率。]]></description>
      <guid>https://arxiv.org/abs/2408.09064</guid>
      <pubDate>Wed, 21 Aug 2024 03:14:07 GMT</pubDate>
    </item>
    <item>
      <title>连接鲁棒性和泛化性：视觉模型潜在空间中概念聚类的 k* 分布分析</title>
      <link>https://arxiv.org/abs/2408.09065</link>
      <description><![CDATA[arXiv:2408.09065v1 公告类型：新
摘要：大多数视觉模型评估都使用间接方法来评估潜在空间质量。这些方法通常涉及添加额外的层以将潜在空间投影到新的空间中。这种投影使得分析和比较原始潜在空间变得困难。本文使用局部邻域分析方法 k* 分布在单个概念层面上检查学习到的潜在空间，该方法可以扩展到检查整个潜在空间。我们引入了基于偏度的真实和近似指标来解释单个概念，以评估视觉模型潜在空间的整体质量。我们的研究结果表明，当前的视觉模型经常破坏潜在空间内单个概念的分布。然而，随着这些模型在多个数据集上的泛化能力不断提高，分裂程度逐渐减小。在稳健视觉模型中也观察到了类似的趋势，其中稳健性的提高与分裂的减少相关。最终，这种方法可以直接解释和比较不同视觉模型的潜在空间，并揭示模型的通用性和鲁棒性之间的关系。结果表明，随着模型变得更加通用和鲁棒，它倾向于学习能够更好地聚类概念的特征。项目网站可在线访问：https://shashankkotyan.github.io/k-Distribution/]]></description>
      <guid>https://arxiv.org/abs/2408.09065</guid>
      <pubDate>Wed, 21 Aug 2024 03:14:07 GMT</pubDate>
    </item>
    <item>
      <title>使用多种方式对任何事物进行细分</title>
      <link>https://arxiv.org/abs/2408.09085</link>
      <description><![CDATA[arXiv:2408.09085v1 公告类型：新
摘要：稳健而准确的场景分割已成为各种视觉识别和导航任务的核心功能之一。这启发了最近开发的 Segment Anything Model (SAM)，这是通用掩模分割的基础模型。然而，SAM 主要针对单模态 RGB 图像，限制了其对使用广泛采用的传感器套件捕获的多模态数据的适用性，例如 LiDAR 加 RGB、深度加 RGB、热加 RGB 等。我们开发了 MM-SAM，这是 SAM 的扩展和扩展，支持跨模态和多模态处理，可通过不同的传感器套件实现稳健和增强的分割。MM-SAM 具有两种关键设计，即无监督跨模态传输和弱监督多模态融合，可实现对各种传感器模态的标签高效和参数高效适应。它解决了三个主要挑战：1) 适应各种非 RGB 传感器以进行单模态处理，2) 通过传感器融合协同处理多模态数据，3) 针对不同的下游任务进行无掩码训练。大量实验表明，MM-SAM 的表现始终远胜于 SAM，证明了其在各种传感器和数据模态中的有效性和稳健性。]]></description>
      <guid>https://arxiv.org/abs/2408.09085</guid>
      <pubDate>Wed, 21 Aug 2024 03:14:07 GMT</pubDate>
    </item>
    <item>
      <title>火灾动态视觉：多尺度火灾和羽流行为的图像分割和跟踪</title>
      <link>https://arxiv.org/abs/2408.08984</link>
      <description><![CDATA[arXiv:2408.08984v1 公告类型：新
摘要：野火发生频率和严重程度的增加凸显了对精确的火灾和羽流蔓延模型的需求。我们介绍了一种方法，可以有效地隔离和跟踪各种空间和时间尺度以及图像类型的火灾和羽流行为，识别系统中的物理现象并提供有助于开发和验证模型的见解。我们的方法结合了图像分割和图论来描绘火灾前沿和羽流边界。我们证明该方法可以有效地将火灾和羽流与视觉上相似的物体区分开来。结果表明，成功地隔离和跟踪了各种图像源中的火灾和羽流动态，范围从天气尺度（$10^4$-$10^5$ m）卫星图像到在火灾环境附近拍摄的亚微尺度（$10^0$-$10^1$ m）图像。此外，该方法利用图像修复和时空数据集生成用于统计和机器学习模型。]]></description>
      <guid>https://arxiv.org/abs/2408.08984</guid>
      <pubDate>Wed, 21 Aug 2024 03:14:06 GMT</pubDate>
    </item>
    <item>
      <title>基于 Transformer 的预训练模型检测圆锥角膜疾病的比较性能分析</title>
      <link>https://arxiv.org/abs/2408.09005</link>
      <description><![CDATA[arXiv:2408.09005v1 公告类型：新
摘要：本研究比较了八种预先训练的 CNN，用于诊断退行性眼病圆锥角膜。使用了精心选择的圆锥角膜、正常和可疑病例数据集。测试的模型包括 DenseNet121、EfficientNetB0、InceptionResNetV2、InceptionV3、MobileNetV2、ResNet50、VGG16 和 VGG19。为了最大化模型训练，使用了坏样本删除、调整大小、重新缩放和增强。使用相似的参数、激活函数、分类函数和优化器对模型进行训练，以比较性能。为了确定类别分离效果，对每个模型的准确度、精确度、召回率和 F1 分数进行了评估。MobileNetV2 是识别圆锥角膜和正常病例的最佳准确模型，错误分类很少。 InceptionV3 和 DenseNet121 在圆锥角膜检测方面均表现良好，但在可疑病例方面却存在困难。相比之下，EfficientNetB0、ResNet50 和 VGG19 在区分可疑病例和正常病例方面更困难，这表明需要改进和开发模型。对用于自动圆锥角膜识别的最先进的 CNN 架构进行详细比较，揭示了每种模型的优点和缺点。这项研究表明，先进的深度学习模型可以增强圆锥角膜的诊断和治疗计划。未来的研究应探索混合模型并整合临床参数，以提高现实世界临床应用中的诊断准确性和稳健性，为更有效的 AI 驱动眼科工具铺平道路。]]></description>
      <guid>https://arxiv.org/abs/2408.09005</guid>
      <pubDate>Wed, 21 Aug 2024 03:14:06 GMT</pubDate>
    </item>
    <item>
      <title>用于多模态表情识别的多教师特权知识提炼</title>
      <link>https://arxiv.org/abs/2408.09035</link>
      <description><![CDATA[arXiv:2408.09035v1 公告类型：新
摘要：人类情感是一种复杂的现象，通过面部表情、声调、肢体语言和生理信号传达和感知。多模态情感识别系统之所以表现良好，是因为它们可以从不同的传感器学习互补和冗余的语义信息。在现实世界中，只有用于训练的模态子集可能在测试时可用。学习特权信息允许模型利用仅在训练期间可用的其他模态的数据。已经提出了用于 PKD 的 SOTA 方法，以将信息从教师模型（具有特权模态）提取到学生模型（没有特权模态）。然而，这种 PKD 方法利用点对点匹配，并没有明确捕获关系信息。最近，已经提出了提取结构信息的方法。然而，基于结构相似性的 PKD 方法主要局限于从单个联合教师表示中学习，这限制了它们的稳健性、准确性和从各种多模态源学习的能力。本文引入了一种具有自我提炼功能的多教师 PKD (MT-PKDOT) 方法，以在将不同的教师表示提炼给学生之前对其进行对齐。MT-PKDOT 采用基于正则化最优传输 (OT) 的结构相似性 KD 机制进行提炼。所提出的 MT-PKDOT 方法在 Affwild2 和 Biovid 数据集上得到了验证。结果表明，我们提出的方法可以胜过 SOTA PKD 方法。它将 Biovid 数据上的纯视觉基线提高了 5.5%。在 Affwild2 数据集上，所提出的方法分别将效价和唤醒度提高了 3% 和 5%。允许学生从多个不同的来源学习可以提高准确性，并隐性避免对学生模型的负面转移。]]></description>
      <guid>https://arxiv.org/abs/2408.09035</guid>
      <pubDate>Wed, 21 Aug 2024 03:14:06 GMT</pubDate>
    </item>
    <item>
      <title>SHARP-Net：用于涵洞和下水道管道缺陷分割的精细金字塔网络</title>
      <link>https://arxiv.org/abs/2408.08879</link>
      <description><![CDATA[arXiv:2408.08879v1 公告类型：新
摘要：本文介绍了一种用于语义分割的新型架构——语义 Haar 自适应精炼金字塔网络 (SHARP-Net)。SHARP-Net 集成了自下而上的路径，该路径具有类似 Inception 的块，具有不同的过滤器大小（3x3$ 和 5x5）、并行最大池化和附加空间检测层。这种设计可以捕获多尺度特征和精细的结构细节。在整个网络中，深度可分离卷积用于降低复杂性。SHARP-Net 的自上而下路径专注于通过使用 $1\times1$ 和 $3\times3$ 深度可分离卷积进行上采样和信息融合来生成高分辨率特征。我们使用我们开发的具有挑战性的 Culvert-Sewer Defects 数据集和基准 DeepGlobe Land Cover 数据集对我们的模型进行了评估。我们的实验评估证明了基础模型（不包括 Haar 类特征）在处理不规则缺陷形状、遮挡和类别不平衡方面的有效性。它的表现优于最先进的方法，包括 U-Net、CBAM U-Net、ASCU-Net、FPN 和 SegFormer，在涵洞下水道缺陷和 DeepGlobe 土地覆盖数据集上分别实现了 14.4% 和 12.1% 的平均改进，IoU 得分分别为 77.2% 和 70.6%。此外，训练时间也减少了。此外，精心挑选和微调的 Haar 类特征的集成将深度学习模型的性能提高了至少 20%。提出的 SHARP-Net 结合了 Haar 类特征，实现了令人印象深刻的 94.75% 的 IoU，比基础模型提高了 22.74%。这些特征也应用于其他深度学习模型，显示出 35.0% 的改进，证明了它们的多功能性和有效性。因此，SHARP-Net 为具有挑战性的现实场景中的准确语义分割提供了强大而有效的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2408.08879</guid>
      <pubDate>Wed, 21 Aug 2024 03:14:05 GMT</pubDate>
    </item>
    <item>
      <title>图像类别翻译距离：一种用于图像分类的新型可解释特征</title>
      <link>https://arxiv.org/abs/2408.08973</link>
      <description><![CDATA[arXiv:2408.08973v1 公告类型：新
摘要：我们提出了一种用于图像分类的图像翻译网络的新应用，并展示了它作为传统黑盒分类网络更具解释性的替代方案的潜力。我们训练一个网络在可能的类别之间翻译图像，然后量化翻译距离，即将图像归类为一个类别或另一个类别所需的改变程度。然后可以检查这些翻译距离的聚类和趋势，并将其直接输入到一个简单的分类器（例如支持向量机，SVM），与传统的端到端卷积神经网络分类器相比，提供相当的准确性。此外，对翻译图像的目视检查可以揭示训练集中特定于类别的特征和偏差，例如在某个类别中更频繁观察到的视觉伪影。我们在一个玩具 2 类场景（苹果与橘子）上演示了该方法，然后将其应用于两个医学成像任务：从色素病变照片中检测黑色素瘤，以及对骨髓活检涂片中的 6 种细胞类型进行分类。图像到图像网络的这种新颖应用表明，该技术有潜力超越想象不同的风格变化，并为图像分类和医学成像数据集提供更深入的见解。]]></description>
      <guid>https://arxiv.org/abs/2408.08973</guid>
      <pubDate>Wed, 21 Aug 2024 03:14:05 GMT</pubDate>
    </item>
    <item>
      <title>在制造环境中使用混合数据集增强对象检测：将联邦学习与传统技术进行比较</title>
      <link>https://arxiv.org/abs/2408.08974</link>
      <description><![CDATA[arXiv:2408.08974v1 公告类型：新
摘要：联邦学习 (FL) 因其强大的模型开发和隐私保护功能而在制造业引起了广泛关注。本文致力于研究 FL 模型在物体检测中的稳健性，并在此提出一项与使用混合数据集进行小物体检测的传统技术的比较研究。我们的研究结果表明，当在不同环境中记录的测试数据上进行测试时，FL 的性能优于集中式训练模型和不同的深度学习技术，这些测试数据具有各种物体视点、光照条件、杂乱背景等。这些结果凸显了 FL 在实现即使在看不见的环境中也能有效运行的稳健全局模型方面的潜力。该研究为在制造环境中部署弹性物体检测模型提供了宝贵的见解。]]></description>
      <guid>https://arxiv.org/abs/2408.08974</guid>
      <pubDate>Wed, 21 Aug 2024 03:14:05 GMT</pubDate>
    </item>
    <item>
      <title>血细胞形态的深度生成分类</title>
      <link>https://arxiv.org/abs/2408.08982</link>
      <description><![CDATA[arXiv:2408.08982v1 公告类型：新
摘要：准确分类血液细胞对于诊断血液疾病至关重要，但由于细胞形态的复杂性、生物、病理和成像特征的异质性以及细胞类型频率的不平衡，对机器自动化提出了重大挑战。我们引入了 CytoDiffusion，这是一种基于扩散的分类器，可以有效地模拟血细胞形态，将准确分类与强大的异常检测、抗分布偏移、可解释性、数据效率和超人不确定性量化相结合。我们的方法在异常检测（AUC 0.976 vs. 0.919）、抗域偏移（85.85% vs. 74.38% 平衡准确度）和低数据范围内的性能（95.88% vs. 94.95% 平衡准确度）方面优于最先进的判别模型。值得注意的是，我们的模型生成的合成血细胞图像几乎与真实图像没有区别，正如图灵测试所证明的那样，在该测试中，血液学专家的准确率仅为 52.3%（95% CI：[50.5%，54.2%]）。此外，我们通过生成可直接解释的反事实热图来增强模型的可解释性。我们的综合评估框架涵盖了这些多个性能维度，为血液学医学图像分析建立了新的基准，最终提高了临床诊断的准确性。我们的代码可在 https://github.com/Deltadahl/CytoDiffusion 上找到。]]></description>
      <guid>https://arxiv.org/abs/2408.08982</guid>
      <pubDate>Wed, 21 Aug 2024 03:14:05 GMT</pubDate>
    </item>
    </channel>
</rss>