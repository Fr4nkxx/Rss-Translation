<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Thu, 21 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>通过 ADMM 解耦无训练引导扩散</title>
      <link>https://arxiv.org/abs/2411.12773</link>
      <description><![CDATA[arXiv:2411.12773v1 公告类型：新
摘要：在本文中，我们通过以即插即用的方式引导具有可微分损失函数的现成无条件扩散模型来考虑条件生成问题。虽然以前的研究主要集中在通过调整权重超参数来平衡无条件扩散模型和引导损失，但我们提出了一个新颖的框架，可以明显地将这两个组件分离。具体来说，我们引入两个变量 ${x}$ 和 ${z}$，分别表示由无条件生成模型和引导函数控制的生成样本。这种解耦将条件生成重新表述为两个可管理的子问题，由约束 ${x} = {z}$ 统一。利用这种设置，我们开发了一种基于交替方向乘数法 (ADMM) 的新算法来自适应地平衡这些组件。此外，我们建立了扩散反向步骤和 ADMM 近端算子之间的等价性，并在某些温和假设下对我们的算法进行了详细的收敛分析。我们的实验表明，我们提出的方法 ADMMDiff 能够持续生成高质量样本，同时确保严格遵守条件标准。它在一系列条件生成任务中的表现优于现有方法，包括使用各种指导的图像生成和可控运动合成。]]></description>
      <guid>https://arxiv.org/abs/2411.12773</guid>
      <pubDate>Thu, 21 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 PPLL 实现更快的多 GPU 训练：利用本地学习的流水线并行框架</title>
      <link>https://arxiv.org/abs/2411.12780</link>
      <description><![CDATA[arXiv:2411.12780v1 公告类型：新
摘要：目前，大规模深度学习模型的训练通常通过跨多个 GPU 的并行训练来实现。然而，由于传统模型并行方法固有的通信开销和同步延迟，无法实现无缝并行训练，这在一定程度上影响了整体训练效率。为了解决这个问题，我们提出了 PPLL（基于局部学习的流水线并行），这是一种利用局部学习算法实现跨多个 GPU 的有效并行训练的新框架。PPLL 将模型划分为几个不同的块，每个块分配给单独的 GPU。通过利用队列来管理 GPU 之间的数据传输，PPLL 确保了无缝的跨 GPU 通信，允许多个块以流水线方式执行前向和后向传递。这种设计最大限度地减少了空闲时间并避免了通常由顺序梯度更新引起的瓶颈，从而加速了整个训练过程。我们通过在 CIFAR-10、SVHN 和 STL-10 数据集上使用 ResNet 和 Vision Transformer (ViT) 架构进行大量实验来验证 PPLL。我们的结果表明，PPLL 显著提高了局部学习方法的训练速度，同时实现了与传统流水线并行 (PP) 相当甚至更高的训练速度，且不牺牲模型性能。在 4-GPU 训练设置中，PPLL 分别将 ViT 和 ResNet 上的局部学习训练速度提高了 162% 和 33%，实现了传统流水线并行速度的 1.25 倍和 0.85 倍。]]></description>
      <guid>https://arxiv.org/abs/2411.12780</guid>
      <pubDate>Thu, 21 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FGP：特征梯度剪枝，实现高效的卷积层剪枝</title>
      <link>https://arxiv.org/abs/2411.12781</link>
      <description><![CDATA[arXiv:2411.12781v1 公告类型：新
摘要：为了在保持模型性能的同时减少计算开销，已经提出了模型修剪技术。其中，结构化修剪可以删除整个卷积通道或层，从而显着提高计算效率并与硬件加速兼容。然而，现有的仅依赖于图像特征或梯度的修剪方法往往会导致保留冗余通道，从而对推理效率产生负面影响。为了解决这个问题，本文介绍了一种称为特征梯度修剪（FGP）的新型修剪方法。该方法集成了基于特征和基于梯度的信息，以更有效地评估通道在不同目标类别中的重要性，从而能够更准确地识别对模型性能至关重要的通道。实验结果表明，所提出的方法在保持稳定性能的同时提高了模型的紧凑性和实用性。在多个任务和数据集上进行的实验表明，与现有方法相比，FGP 显着降低了计算成本并最大限度地减少了准确性损失，突出了其在优化修剪结果方面的有效性。源代码可在以下网址获取：https://github.com/FGP-code/FGP。]]></description>
      <guid>https://arxiv.org/abs/2411.12781</guid>
      <pubDate>Thu, 21 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Med-2E3：2D 增强型 3D 医学多模态大型语言模型</title>
      <link>https://arxiv.org/abs/2411.12783</link>
      <description><![CDATA[arXiv:2411.12783v1 公告类型：新
摘要：3D 医学图像分析对于现代医疗保健至关重要，但传统的任务特定模型由于在不同临床场景中的通用性有限而变得越来越不充分。多模态大型语言模型 (MLLM) 为这些挑战提供了一个有希望的解决方案。然而，现有的 MLLM 在充分利用 3D 医学图像中嵌入的丰富分层信息方面存在局限性。受临床实践的启发，放射科医生关注 3D 空间结构和 2D 平面内容，我们提出了 Med-2E3，一种用于 3D 医学图像分析的新型 MLLM，它集成了 3D 和 2D 编码器。为了更有效地聚合 2D 特征，我们设计了一个文本引导的切片间 (TG-IS) 评分模块，该模块根据切片内容和任务指令对每个 2D 切片的注意力进行评分。据我们所知，Med-2E3 是首个集成 3D 和 2D 特征进行 3D 医学图像分析的 MLLM。在大规模开源 3D 医学多模态基准上进行的实验表明，Med-2E3 表现出针对特定任务的注意力分布，并且显著优于当前最先进的模型，报告生成率提高了 14%，医学视觉问答 (VQA) 率提高了 5%，凸显了该模型在解决复杂多模态临床任务方面的潜力。代码将在接受后发布。]]></description>
      <guid>https://arxiv.org/abs/2411.12783</guid>
      <pubDate>Thu, 21 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>联合视觉-语言消除 CLIP 社会偏见</title>
      <link>https://arxiv.org/abs/2411.12785</link>
      <description><![CDATA[arXiv:2411.12785v1 公告类型：新
摘要：视觉语言 (V-L) 预训练模型（例如 CLIP）在各种下游任务中表现出卓越的能力。尽管前景光明，但 V-L 模型却因其固有的社会偏见而受到限制。一个典型的表现是，V-L 模型通常会对特定人群产生有偏见的预测，从而严重损害其现实世界的适用性。现有方法试图通过从模型嵌入中删除有偏见的属性信息来缓解 V-L 模型中的社会偏见问题。然而，在我们重新审视这些方法之后，我们发现它们的偏见消除通常伴随着 V-L 对齐能力的极大损害。然后我们发现，这种性能下降源于图像和文本嵌入中不平衡的去偏。为了解决这个问题，我们提出了一个新颖的 V-L 去偏框架来对齐图像和文本偏见，然后将它们从两种模态中移除。通过这样做，我们的方法实现了多模态偏差缓解，同时在去偏嵌入中保持 V-L 对齐。此外，我们提倡一种新的评估协议，它可以 1) 全面量化模型去偏和 V-L 对齐能力，以及 2) 评估社会偏见消除模型的泛化能力。我们相信这项工作将为未来解决 CLIP 中的社会偏见问题的研究提供新的见解和指导。]]></description>
      <guid>https://arxiv.org/abs/2411.12785</guid>
      <pubDate>Thu, 21 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>视觉提示增强和双低秩自适应实现高效的视觉指令微调</title>
      <link>https://arxiv.org/abs/2411.12787</link>
      <description><![CDATA[arXiv:2411.12787v1 公告类型：新
摘要：微调多模态大型语言模型 (MLLM) 面临重大挑战，包括依赖高级视觉特征限制细粒度细节理解，以及任务复杂性引起的数据冲突。为了解决这些问题，我们提出了一个有效的微调框架，其中包含两种新方法：视觉线索增强 (VCE) 和双低秩自适应 (Dual-LoRA)。VCE 通过集成多级视觉线索增强了视觉投影仪，提高了模型捕获细粒度视觉特征的能力。Dual-LoRA 引入了用于指令调整的双低秩结构，将学习分离为技能和任务空间，以实现跨不同任务的精确控制和有效适应。我们的方法简化了实施，增强了视觉理解，并提高了适应性。对下游任务和一般基准的实验证明了我们提出的方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2411.12787</guid>
      <pubDate>Thu, 21 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Mini-Splatting2：通过积极的高斯致密化在几分钟内构建 360 度场景</title>
      <link>https://arxiv.org/abs/2411.12788</link>
      <description><![CDATA[arXiv:2411.12788v1 公告类型：新
摘要：在本研究中，我们探讨了高斯溅射快速场景优化的基本挑战。通过对几何建模过程的彻底分析，我们发现可以通过高斯表示在优化早期有效地重建密集点云。这一见解促成了我们采用积极的高斯致密化方法，这为传统的渐进致密化方法提供了一种更有效的替代方案。通过显著增加关键高斯的数量，我们增强了模型在优化早期阶段捕捉密集场景几何图形的能力。该策略无缝集成到 Mini-Splatting 致密化和简化框架中，可在不影响质量的情况下实现快速收敛。此外，我们在高斯溅射中引入了可见性剔除，利用每个视图的高斯重要性作为预计算的可见性来加速优化过程。我们的 Mini-Splatting2 在优化时间、高斯数量和渲染质量之间实现了平衡，为未来基于高斯 Splatting 的工作奠定了坚实的基础。我们的工作为在实际应用中实现更高效、高质量的 3D 场景建模奠定了基础，无论是否被接受，代码都将可用。]]></description>
      <guid>https://arxiv.org/abs/2411.12788</guid>
      <pubDate>Thu, 21 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用高斯溅射技术实现开放世界场景的自动 3D 物理模拟</title>
      <link>https://arxiv.org/abs/2411.12789</link>
      <description><![CDATA[arXiv:2411.12789v1 公告类型：新
摘要：3D 生成模型的最新进展为模拟动态 3D 物体运动和自定义行为开辟了新的可能性，但创建此类内容仍然具有挑战性。当前的方法通常需要手动分配精确的物理属性以进行模拟，或者依靠视频生成模型来预测它们，这需要大量计算。在本文中，我们重新思考了多模态大型语言模型 (MLLM) 在基于物理的模拟中的使用，并提出了 Sim Anything，这是一种基于物理的方法，它赋予静态 3D 物体交互式动态。我们从详细的场景重建和对象级 3D 开放词汇分割开始，然后进行多视图图像修复。受人类视觉推理的启发，我们提出了基于 MLLM 的物理属性感知 (MLLM-P3)，以零样本方式预测物体的平均物理属性。然后，材料属性分布预测模型 (MPDP) 根据平均值和物体的几何形状估算完整分布，将问题重新表述为概率分布估算，以降低计算成本。最后，我们使用通过物理几何自适应采样 (PGAS) 策略采样的粒子模拟开放世界场景中的物体，有效捕捉复杂变形并显著降低计算成本。大量实验和用户研究表明，我们的 Sim Anything 在单个 GPU 上 2 分钟内实现比最先进方法更逼真的运动。]]></description>
      <guid>https://arxiv.org/abs/2411.12789</guid>
      <pubDate>Thu, 21 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>面向视觉的多模态大型语言模型的细粒度知识编辑</title>
      <link>https://arxiv.org/abs/2411.12790</link>
      <description><![CDATA[arXiv:2411.12790v1 公告类型：新
摘要：知识编辑旨在高效且经济地纠正不准确之处并更新过时的信息。最近，人们越来越有兴趣将知识编辑从大型语言模型 (LLM) 扩展到多模态大型语言模型 (MLLM)，后者集成了文本和视觉信息，从而引入了额外的编辑复杂性。现有的多模态知识编辑工作主要侧重于面向文本的粗粒度场景，未能解决多模态上下文带来的独特挑战。在本文中，我们提出了一种面向视觉的细粒度多模态知识编辑任务，旨在对具有多个交互实体的图像进行精确编辑。我们引入了细粒度视觉知识编辑 (FGVEdit) 基准来评估这项任务。此外，我们提出了一个基于多模态范围分类器的知识编辑器 (MSCKE) 框架。MSCKE 利用集成视觉和文本信息的多模态范围分类器来准确识别和更新与图像中特定实体相关的知识。这种方法确保了精确编辑，同时保留了无关信息，克服了传统纯文本编辑方法的局限性。在 FGVEdit 基准上进行的大量实验表明，MSCKE 的表现优于现有方法，展示了其在解决多模态知识编辑复杂挑战方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2411.12790</guid>
      <pubDate>Thu, 21 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>减轻感知偏差：一种无需训练即可增强 LMM 进行图像质量评估的方法</title>
      <link>https://arxiv.org/abs/2411.12791</link>
      <description><![CDATA[arXiv:2411.12791v1 公告类型：新
摘要：尽管大型多模态模型 (LMM) 在高级视觉任务中表现出色，但它们的图像质量评估 (IQA) 能力仍然有限。一个主要原因是 LMM 主要针对高级任务​​（例如，图像字幕）进行训练，强调在不同质量下统一的图像语义提取。当这些 LMM 被迫进行质量评级时，这种语义感知但不敏感的感知偏差不可避免地会导致对图像语义的严重依赖。在本文中，我们提出了一个无需训练的去偏框架，而不是花费大量成本重新训练或调整 LMM，其中通过减轻图像语义引起的偏差来纠正图像质量预测。具体而言，我们首先探索几种语义保留扭曲，这些扭曲可以显着降低图像质量，同时保持可识别的语义。通过将这些特定的扭曲应用于查询或测试图像，我们确保将降级图像识别为质量差，同时保留其语义。在质量推断过程中，查询图像及其对应的降级版本都会被输入到 LMM，同时还会给出一个提示，指示应在降级图像被视为质量差的条件下推断查询图像质量。此先验条件有效地与 LMM 的质量感知保持一致，因为所有降级图像都一致被评为质量差，无论它们的语义差异如何。最后，使用条件概率模型汇总在不同先验条件下推断的查询图像的质量分数（降级版本）。在各种 IQA 数据集上进行的大量实验表明，我们的去偏框架可以持续提高 LMM 性能，代码将公开。]]></description>
      <guid>https://arxiv.org/abs/2411.12791</guid>
      <pubDate>Thu, 21 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CLIC：无监督图像复杂性表示的对比学习框架</title>
      <link>https://arxiv.org/abs/2411.12792</link>
      <description><![CDATA[arXiv:2411.12792v1 Announce Type: new 
摘要：图像复杂度作为视觉的基本属性，影响着人类对图像的理解，直接影响着计算机视觉任务的性能。然而，准确评估和量化图像复杂度面临着巨大的挑战。前人的研究需要更强的泛化能力和标记良好的数据集来学习图像复杂度特征。然而，创建这样的数据集需要昂贵的人工标注成本，并且模型不可避免地会学习到人类的主观偏见。针对上述问题，我们提出了一种基于对比学习的无监督框架CLIC，用于学习图像复杂度表示。该方法在未标注数据上学习图像复杂度特征，避免了高昂的标注成本。具体而言，我们提出了一种独特的正负样本选择策略来强化复杂度特征的差异。同时，我们引入了一种基于图像先验的Complexity-Aware Loss来约束模型的学习过程。我们进行了大量实验验证，结果表明CLIC可以有效地学习图像复杂度表示。 CLIC 在 IC9600 上通过微调取得了与监督方法相当的结果，此外，CLIC 应用于下游任务也表现出显著的性能提升，展现出在各种实际场景中的应用潜力。\href{https://github.com/xauat-liushipeng/CLIC}{code}]]></description>
      <guid>https://arxiv.org/abs/2411.12792</guid>
      <pubDate>Thu, 21 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Stylecodes：对图像生成的风格信息进行编码</title>
      <link>https://arxiv.org/abs/2411.12811</link>
      <description><![CDATA[arXiv:2411.12811v1 公告类型：新
摘要：扩散模型在图像生成方面表现出色，但控制它们仍然是一个挑战。我们专注于风格条件图像生成的问题。虽然示例图像有效，但它们很麻烦：MidJourney 的 sref（样式参考代码）通过用短数字代码表达特定的图像样式解决了这个问题。由于它们易于共享，并且允许使用图像进行样式控制，而无需发布源图像本身，因此它们在整个社交媒体中得到了广泛采用。但是，用户无法从自己的图像生成 sref，底层训练过程也不公开。我们提出了 StyleCodes：一种开源和开放研究风格的编码器架构和训练程序，将图像风格表达为 20 个符号的 base64 代码。我们的实验表明，与传统的图像到样式技术相比，我们的编码质量损失最小。]]></description>
      <guid>https://arxiv.org/abs/2411.12811</guid>
      <pubDate>Thu, 21 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>交互式医学图像分割：基准数据集和基线</title>
      <link>https://arxiv.org/abs/2411.12814</link>
      <description><![CDATA[arXiv:2411.12814v1 公告类型：新
摘要：交互式医学图像分割 (IMIS) 长期以来一直受到大规模、多样化和密集注释数据集有限的限制，这阻碍了模型泛化和不同模型之间的一致评估。在本文中，我们介绍了 IMed-361M 基准数据集，这是一般 IMIS 研究的重大进步。首先，我们从多个数据源收集并标准化超过 640 万张医学图像及其相应的地面真实掩模。然后，利用视觉基础模型强大的对象识别功能，我们自动为每张图像生成密集的交互式掩模，并通过严格的质量控制和粒度管理确保其质量。与以前受特定模态或稀疏注释限制的数据集不同，IMed-361M 涵盖 14 种模态和 204 个分割目标，总计 3.61 亿个掩模，平均每张图像 56 个掩模。最后，我们在此数据集上开发了一个 IMIS 基线网络，该网络支持通过交互式输入（包括点击、边界框、文本提示及其组合）生成高质量的掩码。我们从多个角度评估了它在医学图像分割任务上的表现，与现有的交互式分割模型相比，它表现出卓越的准确性和可扩展性。为了促进医学计算机视觉基础模型的研究，我们在 https://github.com/uni-medical/IMIS-Bench 上发布了 IMed-361M 和模型。]]></description>
      <guid>https://arxiv.org/abs/2411.12814</guid>
      <pubDate>Thu, 21 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>什么构成了适合知识提炼的数据集？</title>
      <link>https://arxiv.org/abs/2411.12817</link>
      <description><![CDATA[arXiv:2411.12817v1 公告类型：新
摘要：知识蒸馏 (KD) 是一种流行且有效的模型压缩方法。KD 的一个重要假设是，在训练学生时，老师的原始数据集也将可用。然而，在持续学习和蒸馏在公司保留的数据集上训练的大型模型等情况下，可能并不总是能够访问原始数据。这导致从业者转向利用其他补充数据来源，这可能会产生混合结果。然后我们必须问：“什么是将知识从老师转移到学生的良好数据集？”许多人会认为只有真实的领域内图像才是可行的，但这是唯一的选择吗？在这项工作中，我们探索了多种可能的替代蒸馏数据集，并证明许多不同的数据集，甚至非自然的合成图像，都可以作为 KD 中的合适替代方案。通过检查这些替代数据集，我们确定并提出了描述什么是蒸馏良好数据集的各种标准。源代码将在未来提供。]]></description>
      <guid>https://arxiv.org/abs/2411.12817</guid>
      <pubDate>Thu, 21 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从视频传播模型看运动</title>
      <link>https://arxiv.org/abs/2411.12831</link>
      <description><![CDATA[arXiv:2411.12831v1 公告类型：新
摘要：文本条件视频扩散模型已成为视频生成和编辑领域的强大工具。但它们捕捉人类运动细微差别的能力仍未得到充分探索。事实上，这些模型忠实地模拟一系列文本提示的能力可以导致人类​​和角色动画中的广泛应用。在这项工作中，我们采取初步措施来研究这些模型是否可以有效地指导逼真的人体动画的合成。具体来说，我们建议通过使用视频扩散模型计算的分数蒸馏采样 (SDS) 引导变形 SMPL-X 身体表示来合成人体运动。通过分析生成的动画的保真度，我们可以深入了解使用 SDS 的公开文本到视频扩散模型可以获得运动的程度。我们的研究结果揭示了这些模型在生成多样化和合理的人体运动方面的潜力和局限性，为进一步研究这一令人兴奋的领域铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2411.12831</guid>
      <pubDate>Thu, 21 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>