<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Mon, 21 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>使用受不确定性影响的生成模型来解释图像分类器</title>
      <link>https://arxiv.org/abs/2410.13871</link>
      <description><![CDATA[arXiv:2410.13871v1 公告类型：新
摘要：我们建议通过给定的图像分类器不确定性来调节生成模型，以分析和解释其行为。对合成数据和损坏版本的 MNIST 数据集进行的初步实验说明了这个想法。]]></description>
      <guid>https://arxiv.org/abs/2410.13871</guid>
      <pubDate>Mon, 21 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Articulate-Anything：通过视觉语言基础模型自动建模铰接式物体</title>
      <link>https://arxiv.org/abs/2410.13882</link>
      <description><![CDATA[arXiv:2410.13882v1 公告类型：新
摘要：交互式 3D 模拟对象在 AR/VR、动画和机器人技术中至关重要，可推动沉浸式体验和高级自动化。然而，创建这些铰接式对象需要大量的人力和专业知识，限制了它们的广泛应用。为了克服这一挑战，我们提出了 Articulate-Anything，这是一个可以自动从许多输入模式（包括文本、图像和视频）中表达各种复杂对象的系统。Articulate-Anything 利用视觉语言模型 (VLM) 来生成代码，这些代码可以编译成可交互的数字孪生，用于标准 3D 模拟器。我们的系统通过网格检索机制利用现有的 3D 资产数据集，以及一个演员-评论家系统，该系统迭代地提出、评估和改进表达对象的解决方案，自我纠正错误以实现稳健的结果。定性评估表明 Articulate-Anything 能够利用丰富的基础输入来表达复杂甚至模糊的物体可供性。在对标准 PartNet-Mobility 数据集进行的大量定量实验中，Articulate-Anything 的表现大大优于之前的工作，成功率从 8.7-11.6% 提高到 75%，为最先进的性能树立了新的标杆。我们进一步展示了我们生成的资产的实用性，通过使用它们来训练机器人策略，以完成超出基本拾取和放置的细粒度操作任务。]]></description>
      <guid>https://arxiv.org/abs/2410.13882</guid>
      <pubDate>Mon, 21 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>变压器在图表理解中的应用：最新进展与未来趋势回顾</title>
      <link>https://arxiv.org/abs/2410.13883</link>
      <description><![CDATA[arXiv:2410.13883v1 公告类型：新
摘要：近年来，人们对视觉语言任务的兴趣日益增长，尤其是那些涉及图表交互的任务。这些任务本质上是多模态的，需要模型来处理图表图像、附带文本、底层数据表以及用户查询。传统上，图表理解 (CU) 依赖于启发式和基于规则的系统。然而，最近集成了变压器架构的进展显着提高了性能。本文回顾了 CU 中的著名研究，重点关注在端到端 (E2E) 解决方案中使用变压器的最先进的 (SoTA) 框架。分析了相关的基准数据集和评估技术。此外，本文还确定了关键挑战并概述了推进 CU 解决方案的有希望的未来方向。按照 PRISMA 指南，在 Google Scholar 上进行了全面的文献检索，重点关注 20 年 1 月至 24 年 6 月的出版物。经过严格的筛选和质量评估，选择了 32 项研究进行深入分析。根据所需的认知任务，CU 任务分为三层范式。还回顾了解决各种 CU 任务的框架的最新进展。根据 E2E 解决方案可解决的任务数量，框架分为单任务或多任务。在多任务框架中，探索了预训练和基于提示工程的技术。本综述概述了领先的架构、数据集和预训练任务。尽管取得了重大进展，但在 OCR 依赖性、处理低分辨率图像和增强视觉推理方面仍然存在挑战。未来的方向包括应对这些挑战、开发强大的基准和优化模型效率。此外，集成可解释的 AI 技术并探索真实数据和合成数据之间的平衡对于推进 CU 研究至关重要。]]></description>
      <guid>https://arxiv.org/abs/2410.13883</guid>
      <pubDate>Mon, 21 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GraspDiffusion：合成逼真的全身手部与物体的交互</title>
      <link>https://arxiv.org/abs/2410.13911</link>
      <description><![CDATA[arXiv:2410.13911v1 公告类型：新
摘要：最近的生成模型可以合成高质量的图像，但通常无法生成人类用手与物体交互的图像。这主要是由于模型对此类交互的误解，以及合成身体复杂部位的困难。在本文中，我们提出了 GraspDiffusion，这是一种新颖的生成方法，可以创建逼真的人与物体交互的场景。给定一个 3D 物体网格，GraspDiffusion 首先构建栩栩如生的全身姿势，并控制物体相对于人体的位置。这是通过分别利用 3D 身体和手部姿势的生成先验，将它们优化为关节抓握姿势来实现的。生成的姿势引导图像合成正确反映预期的交互，从而创建逼真且多样化的人与物体交互场景。我们证明 GraspDiffusion 可以成功解决生成完整人与物体交互这一相对未经研究的问题，同时优于以前的方法。代码和模型将在 https://webtoon.github.io/GraspDiffusion 上提供]]></description>
      <guid>https://arxiv.org/abs/2410.13911</guid>
      <pubDate>Mon, 21 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ARKit LabelMaker：室内 3D 场景理解的新尺度</title>
      <link>https://arxiv.org/abs/2410.13924</link>
      <description><![CDATA[arXiv:2410.13924v1 公告类型：新
摘要：神经网络的性能随其规模和训练数据量而变化。这在语言和图像生成中都有所体现。然而，这需要易于扩展的网络架构以及大规模数据集。尽管像 transformers 这样的易于扩展的架构已经出现在 3D 视觉任务中，但由于缺乏训练数据，3D 视觉的 GPT 时刻仍然遥不可及。在本文中，我们介绍了 ARKit LabelMaker，这是第一个具有密集语义注释的大规模真实世界 3D 数据集。具体来说，我们用自动大规模生成的密集语义注释补充了 ARKitScenes 数据集。为此，我们扩展了最近的自动注释管道 LabelMaker，以满足大规模预训练的需求。这涉及使用尖端分割模型扩展管道，并使其能够应对大规模处理的挑战。此外，我们利用流行的 3D 语义分割模型推动了 ScanNet 和 ScanNet200 数据集上的最佳性能，证明了我们生成的数据集的有效性。]]></description>
      <guid>https://arxiv.org/abs/2410.13924</guid>
      <pubDate>Mon, 21 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>卫星流媒体视频 QoE 预测：真实世界的主观数据库和网络级预测模型</title>
      <link>https://arxiv.org/abs/2410.13952</link>
      <description><![CDATA[arXiv:2410.13952v1 公告类型：新
摘要：对包括卫星在内的流媒体服务的需求继续呈现前所未有的增长。互联网服务提供商发现自己正处于技术进步和客户期望不断提高的十字路口。为了保持相关性和竞争力，这些 ISP 必须确保其网络提供最佳的视频流质量，这是用户满意度的关键决定因素。为此，建立准确的体验质量预测模型非常重要。然而，要通过这些模型实现稳健的性能，需要大量数据集，这些数据集由受各种播放中断影响的视频的主观意见分数标记。为了弥补这一数据差距，我们推出了 LIVE-Viasat 真实世界卫星 QoE 数据库。该数据库包含 179 个视频，这些视频来自受各种真实失真模式影响的真实世界流媒体服务。我们还进行了一项全面的主观研究，涉及 54 名参与者，他们贡献了连续时间意见分数和端点（回顾性）QoE 分数。我们的分析揭示了影响主观 QoE 的各种决定因素，例如停顿事件、空间分辨率、比特率和某些网络参数。我们通过评估流行的 QoE 预测模型的有效性来证明这一独特新资源的实用性。我们还创建了一个新模型，将网络参数映射到预测的人类感知分数，ISP 可以使用它来优化其网络的视频流质量。我们提出的模型称为 SatQA，它能够仅使用网络参数准确预测 QoE，而无需访问像素数据或特定于视频的元数据，这些元数据由 Spearman 的等级相关系数 (SROCC)、Pearson 线性相关系数 (PLCC) 和均方根误差 (RMSE) 估计，表明其具有很高的准确性和可靠性。]]></description>
      <guid>https://arxiv.org/abs/2410.13952</guid>
      <pubDate>Mon, 21 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过消除受保护的属性表示来消除大型视觉语言模型的偏差</title>
      <link>https://arxiv.org/abs/2410.13976</link>
      <description><![CDATA[arXiv:2410.13976v1 公告类型：新
摘要：大型视觉语言模型 (LVLM)（例如 LLaVA）已展示出令人印象深刻的功能，作为通用聊天机器人，可以就提供的输入图像进行对话。然而，他们的反应受到训练数据集中存在的社会偏见的影响，导致模型在呈现描绘不同人口统计数据的人的图像时的反应方式出现不良差异。在这项工作中，我们提出了一种新颖的 LVLM 去偏框架，通过在文本生成过程中直接消融有偏见的属性，以避免生成与受保护属性相关的文本，甚至在内部表示它们。我们的方法不需要训练，只需要相对少量的代表性偏差输出（约 1000 个样本）。我们的实验表明，我们不仅可以最大限度地减少 LVLM 生成与受保护属性相关的文本的倾向，而且我们甚至可以使用合成数据来通知消融，同时保留 COCO 等真实数据的字幕性能。此外，我们发现去偏 LVLM 产生的生成结果表现出与基线偏差模型相似的准确性，这表明可以在不牺牲模型性能的情况下实现去偏效果。]]></description>
      <guid>https://arxiv.org/abs/2410.13976</guid>
      <pubDate>Mon, 21 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>“LICO：具有语言-图像一致性的可解释模型”的可重复性研究</title>
      <link>https://arxiv.org/abs/2410.13989</link>
      <description><![CDATA[arXiv:2410.13989v1 公告类型：新
摘要：机器学习中日益严重的可重复性危机提出了仔细检查研究结果的必要性。本文研究了 Lei 等人 (2023) 关于他们提出的方法 LICO 的主张，该方法用于增强事后可解释性技术并提高图像分类性能。LICO 利用视觉语言模型中的自然语言监督来丰富特征表示并指导学习过程。我们进行了一项全面的可重复性研究，采用了 (Wide) ResNets 和已建立的可解释性方法，如 Grad-CAM 和 RISE。我们大多无法重现作者的结果。特别是，我们没有发现 LICO 能够持续提高分类性能或提高可解释性的定量和定性指标。因此，我们的研究结果强调了严格评估和透明报告在可解释性研究中的重要性。]]></description>
      <guid>https://arxiv.org/abs/2410.13989</guid>
      <pubDate>Mon, 21 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有 Kendall 形状空间的概率 U-Net，用于几何感知的图像分割</title>
      <link>https://arxiv.org/abs/2410.14017</link>
      <description><![CDATA[arXiv:2410.14017v1 公告类型：新
摘要：计算机视觉的基本问题之一是图像分割，即在给定图像中检测不同区域或对象的任务。深度神经网络 (DNN) 已被证明在分割具有挑战性的图像方面非常有效，可以产生令人信服的分割。还需要概率 DNN，它可以将输入图像和模型的不确定性反映到计算的分割中，换句话说，新的 DNN 可以根据输入或模型不确定性生成多个合理的分割及其分布。虽然存在现有的概率分割模型，但其中许多模型没有考虑到分割区域背后的几何形状。在本文中，我们提出了一种可以结合分割几何形状的概率图像分割模型。我们提出的模型建立在 \cite{kohl2018probabilistic} 的概率 U-Net 上，以生成概率分割，即。\！对输入图像进行多种可能的分割。我们的模型还采用了 Kendall 形状变分自动编码器 \cite{vadgama2023kendall}，在概率 U-Net 的先验和后验网络的潜在变量层中编码 Kendall 形状空间。以这种方式合并形状空间可实现具有空间连贯区域的更稳健的分割，同时尊重输入图像中的底层几何形状。]]></description>
      <guid>https://arxiv.org/abs/2410.14017</guid>
      <pubDate>Mon, 21 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>人类行为预期：一项调查</title>
      <link>https://arxiv.org/abs/2410.14045</link>
      <description><![CDATA[arXiv:2410.14045v1 公告类型：新
摘要：预测未来人类行为是计算机视觉领域一个越来越受欢迎的话题，其驱动力是人们对自动驾驶汽车、数字助理和人机交互等应用的兴趣。行为预测方面的文献涵盖各种任务，包括动作预测、活动预测、意图预测、目标预测等。我们的调查旨在将这些零散的文献联系在一起，涵盖最近的技术创新以及用于模型训练和评估的新大规模数据集的开发。我们还总结了不同任务的广泛使用的指标，并对 11 个动作预测数据集上的现有方法进行了全面的性能比较。这项调查不仅是当代动作预测方法的参考，也是这一不断发展的领域未来研究方向的指南。]]></description>
      <guid>https://arxiv.org/abs/2410.14045</guid>
      <pubDate>Mon, 21 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FaceSaliencyAug：通过基于显著性的数据增强减轻地理、性别和刻板偏见</title>
      <link>https://arxiv.org/abs/2410.14070</link>
      <description><![CDATA[arXiv:2410.14070v1 公告类型：新
摘要：计算机视觉模型中的地理、性别和刻板印象偏见对其性能和公平性构成了重大挑战。{在本研究中，我们提出了一种名为 FaceSaliencyAug 的方法，旨在解决卷积神经网络 (CNN) 和视觉转换器 (ViT) 中的性别偏见。利用显著性检测到的面部显著区域，该方法可以减轻数据集中的地理和刻板印象偏见。FaceSaliencyAug 从预定义的搜索空间中随机选择掩码并将其应用于面部图像的显著区域，随后使用掩码显著区域恢复原始图像。{提出的} 增强策略增强了数据多样性，从而提高了模型性能和去偏差效果。我们使用图像相似度得分 (ISS) 在五个数据集中量化数据集多样性，包括 Flickr Faces HQ (FFHQ)、WIKI、IMDB、Labelled Faces in the Wild (LFW)、UTK Faces 和 Diverse Dataset。经 ISS-intra 和 ISS-inter 算法评估，所提出的方法表现出卓越的多样性指标。此外，我们评估了我们的方法在减轻 CEO、工程师、护士和学校教师数据集上的性别偏见方面的有效性。我们使用图像-图像关联得分 (IIAS) 来衡量这些职业中的性别偏见。我们的实验表明，CNN 和 ViT 的性别偏见都有所减少，表明我们的方法在促进计算机视觉模型的公平性和包容性方面是有效的。]]></description>
      <guid>https://arxiv.org/abs/2410.14070</guid>
      <pubDate>Mon, 21 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过将视觉标记汇总到紧凑寄存器中来实现高效的视觉语言模型</title>
      <link>https://arxiv.org/abs/2410.14072</link>
      <description><![CDATA[arXiv:2410.14072v1 公告类型：新
摘要：视觉语言模型 (VLM) 的最新进展扩展了它们在现实世界中的应用潜力，使这些模型能够对图像执行复杂的推理。在广泛使用的完全自回归基于变换器的模型（如 LLaVA）中，投影的视觉标记被添加到文本标记之前。通常，视觉标记比提示标记多得多，导致训练和推理期间的计算开销增加。在本文中，我们提出了视觉紧凑标记寄存器 (Victor)，这是一种通过将视觉标记汇总为一组较小的寄存器标记来减少视觉标记数量的方法。Victor 在视觉标记之后添加了一些可学习的寄存器标记，并使用 VLM 语言塔中的前几层将视觉信息汇总到这些寄存器中。在这几层之后，所有视觉标记都被丢弃，从而显着提高训练和推理的计算效率。值得注意的是，我们的方法易于实现，只需要少量新的可训练参数，对模型性能的影响极小。在我们的实验中，仅使用 8 个视觉寄存器（约占原始标记的 1%），Victor 的准确率下降不到 4%，同时将总训练时间缩短了 43%，推理吞吐量提高了 3.3 倍。]]></description>
      <guid>https://arxiv.org/abs/2410.14072</guid>
      <pubDate>Mon, 21 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SAMReg：基于 ROI 对应关系的 SAM 图像配准</title>
      <link>https://arxiv.org/abs/2410.14083</link>
      <description><![CDATA[arXiv:2410.14083v1 公告类型：新
摘要：本文介绍了一种基于成对感兴趣区域 (ROI) 的新型空间对应表示，用于医学图像配准。与替​​代对应表示方法（例如基于采样位移和空间变换函数的方法）相比，本文讨论了所提出的基于 ROI 的对应关系的独特属性，并讨论了图像配准后在临床应用中的潜在优势。这些优势包括基于学习的图像配准和分割之间的明确联系，这反过来又激发了两种使用（预）训练分割网络的图像配准方法。基于分割任何模型 (SAM)（一种用于分割的视觉基础模型），我们开发了一种新的配准算法 SAMReg，它不需要任何训练（或训练数据）、基于梯度的微调或快速工程。所提出的 SAMReg 模型在五个实际应用中进行了评估，包括使用心脏 MR 和肺部 CT 的受试者内配准任务、使用前列腺 MR 和视网膜成像的具有挑战性的受试者间配准场景，以及使用非临床示例的空中图像配准的额外评估。所提出的方法在测试指标（包括解剖结构上的 Dice 和目标配准误差）方面优于基于强度的迭代算法和基于 DDF 预测学习的网络，并且与依赖完全分段训练数据的弱监督配准方法相比，进一步展示了具有竞争力的性能。开源代码和示例可在以下网址获取：https://github.com/sqhuang0103/SAMReg.git。]]></description>
      <guid>https://arxiv.org/abs/2410.14083</guid>
      <pubDate>Mon, 21 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>您的兴趣，您的摘要：以查询为中心的长视频摘要</title>
      <link>https://arxiv.org/abs/2410.14087</link>
      <description><![CDATA[arXiv:2410.14087v1 公告类型：新
摘要：从长视频中生成简洁且信息丰富的视频摘要很重要，但由于场景重要性不同，因此具有主观性。用户通过文本查询指定场景重要性的能力增强了此类摘要的相关性。本文介绍了一种以查询为中心的视频摘要方法，旨在将视频摘要与用户查询紧密结合。为此，我们提出了一种专为此任务设计的新型方法，即带注意力的全卷积序列网络 (FCSNA-QFVS)。利用时间卷积和注意力机制，我们的模型可以根据用户指定的查询有效地提取和突出显示相关内容。在以查询为中心的视频摘要基准数据集上的实验验证证明了我们方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2410.14087</guid>
      <pubDate>Mon, 21 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MMAD-Purify：一种针对高效、可扩展的多模态攻击的精确优化框架</title>
      <link>https://arxiv.org/abs/2410.14089</link>
      <description><![CDATA[arXiv:2410.14089v1 公告类型：新
摘要：神经网络在广泛的任务中取得了卓越的表现，但它们仍然容易受到对抗性干扰的影响，这对安全关键型应用构成了重大风险。随着多模态性的兴起，扩散模型已成为不仅用于生成任务而且用于图像编辑、修复和超分辨率等各种应用的强大工具。然而，这些模型仍然缺乏鲁棒性，因为对攻击它们以增强其弹性的研究有限。传统的攻击技术，例如基于梯度的对抗性攻击和基于扩散模型的方法，由于其迭代性质而受到计算效率低下和可扩展性问题的阻碍。为了应对这些挑战，我们引入了一个创新框架，该框架利用扩散模型的提炼主干并结合精度优化的噪声预测器来增强我们的攻击框架的有效性。这种方法不仅增强了攻击的效力，而且显着降低了计算成本。我们的框架为多模态对抗攻击提供了尖端解决方案，可确保减少延迟并生成具有出色成功率的高保真对抗示例。此外，我们证明我们的框架实现了出色的可迁移性和对净化防御的稳健性，在有效性和效率方面均优于现有的基于梯度的攻击模型。]]></description>
      <guid>https://arxiv.org/abs/2410.14089</guid>
      <pubDate>Mon, 21 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>