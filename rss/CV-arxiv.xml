<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Fri, 08 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>UnityGraph：多人运动预测的时空特征统一学习</title>
      <link>https://arxiv.org/abs/2411.04151</link>
      <description><![CDATA[arXiv:2411.04151v1 公告类型：新
摘要：多人运动预测是一个复杂而新兴的领域，具有重要的实际应用。当前最先进的方法通常采用双路径网络分别对空间特征和时间特征进行建模。然而，这两个网络的不确定性兼容性给时空特征融合带来了挑战，并且本质上违反了人体运动的时空连贯性和耦合性。为了解决这个问题，我们提出了一种新颖的图结构 UnityGraph，它将时空特征作为一个整体来处理，增强了模型的连贯性和耦合性。具体来说，UnityGraph 是一个基于超变量图的网络。超图的灵活性使我们能够将观察到的运动视为图节点。然后，我们利用超边来连接这些节点以探索时空特征。这种观点将时空动态统一起来，将多人运动预测重新表述为单个图上的问题。利用基于此超图的动态消息传递，我们的模型动态地从两种关系中学习，以生成反映节点间相关性的有针对性的消息。在多个数据集上进行的大量实验表明，我们的方法实现了最先进的性能，证实了其有效性和创新设计。]]></description>
      <guid>https://arxiv.org/abs/2411.04151</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DiMSUM：扩散曼巴——一种可扩展且统一的空间频率图像生成方法</title>
      <link>https://arxiv.org/abs/2411.04168</link>
      <description><![CDATA[arXiv:2411.04168v1 公告类型：新
摘要：我们为扩散模型引入了一种新颖的状态空间架构，有效地利用空间和频率信息来增强对输入图像中局部特征的归纳偏差，以完成图像生成任务。虽然状态空间网络（包括 Mamba，这是循环神经网络的革命性进步）通常从左到右扫描输入序列，但它们在设计有效的扫描策略方面面临困难，尤其是在处理图像数据时。我们的方法表明，将小波变换集成到 Mamba 中可以增强对视觉输入的局部结构感知，并通过将它们分解为代表低频和高频分量的小波子带来更好地捕捉频率的长程关系。然后通过交叉注意融合层处理这些基于小波的输出并将其与原始 Mamba 输出无缝融合，结合空间和频率信息来优化状态空间模型的顺序感知，这对于图像生成的细节和整体质量至关重要。此外，我们引入了全局共享的转换器来增强 Mamba 的性能，利用其卓越的能力来捕捉全局关系。通过在标准基准上进行大量实验，我们的方法与 DiT 和 DIFFUSSM 相比表现出色，实现了更快的训练收敛并提供高质量的输出。代码和预训练模型发布在 https://github.com/VinAIResearch/DiMSUM.git。]]></description>
      <guid>https://arxiv.org/abs/2411.04168</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>WiFlexFormer：基于 WiFi 的高效以人为中心的传感</title>
      <link>https://arxiv.org/abs/2411.04224</link>
      <description><![CDATA[arXiv:2411.04224v1 公告类型：新
摘要：我们提出了 WiFlexFormer，这是一种基于 Transformer 的高效架构，专为基于 WiFi 信道状态信息 (CSI) 的以人为中心的传感而设计。我们将 WiFlexFormer 与最先进的视觉和用于处理射频数据的专用架构进行对比，并证明它实现了可比的人类活动识别 (HAR) 性能，同时提供了显着更低的参数数量和更快的推理时间。WiFlexFormer 在 Nvidia Jetson Orin Nano 上的推理时间仅为 10 毫秒，针对实时推理进行了优化。此外，它的低参数数量有助于提高跨域泛化能力，在这方面它通常优于更大的模型。我们的全面评估表明，WiFlexFormer 是高效、可扩展的基于 WiFi 的传感应用的潜在解决方案。WiFlexFormer 的 PyTorch 实现可公开获取：https://github.com/StrohmayerJ/WiFlexFormer。]]></description>
      <guid>https://arxiv.org/abs/2411.04224</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PMPNet：用于动态场景中单目深度估计的像素运动预测网络</title>
      <link>https://arxiv.org/abs/2411.04227</link>
      <description><![CDATA[arXiv:2411.04227v1 公告类型：新
摘要：本文提出了一种动态场景中单目深度估计的新方法。我们首先从理论上探讨了动态场景中物体运动轨迹的任意性。为了克服这种任意性，我们假设点沿直线短距离移动，然后将其总结为二维欧几里得空间中的三角约束损失。为了克服边缘周围的深度不一致问题，我们提出了一个可变形的支持窗口模块，它可以从不同形状的物体中学习特征，使边缘区域周围的深度值更准确。所提出的模型在两个室外数据集 - KITTI 和 Make3D 以及室内数据集 - NYU Depth V2 上进行了训练和测试。这些数据集上报告的定量和定性结果表明，与其他方法相比，我们提出的模型是成功的。 KITTI 数据集上的消融研究结果也验证了所提出的像素运动预测模块以及可变形支持窗口模块的有效性。]]></description>
      <guid>https://arxiv.org/abs/2411.04227</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PocoLoco：宽松服装中人体形状的点云扩散模型</title>
      <link>https://arxiv.org/abs/2411.04249</link>
      <description><![CDATA[arXiv:2411.04249v1 公告类型：新
摘要：对可以合理变形为关节的人体化身进行建模是一个活跃的研究领域。我们提出了 PocoLoco——第一个无模板、基于点、姿势条件的宽松服装 3D 人体生成模型。我们通过注意到大多数方法都需要人体的参数模型来接地姿势相关的变形来激发我们的工作。因此，它们仅限于对拓扑上与裸体相似的服装进行建模，并且不能很好地扩展到宽松的服装。尝试对宽松服装进行建模的少数方法通常需要规范化或 UV 参数化，并且需要解决明确估计变形服装对应关系的挑战性问题。在这项工作中，我们将化身服装变形制定为去噪扩散框架内的条件点云生成任务。至关重要的是，我们的框架直接在无序点云上运行，无需参数模型或服装模板。这也使各种实际应用成为可能，例如点云补全和基于姿势的编辑——这是虚拟人体动画的重要功能。由于目前宽松服装中的人体化身数据集对于训练扩散模型来说太小，我们发布了一个数据集，其中包含两个穿着宽松服装摆出各种姿势的受试者，总共有 75K 个点云。通过解决有效建模宽松服装的挑战性任务并扩大可用于训练这些模型的数据，我们旨在为数字人类的进一步创新奠定基础。源代码可在 https://github.com/sidsunny/pocoloco 获得。]]></description>
      <guid>https://arxiv.org/abs/2411.04249</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于无监督行人重新识别的姿势变换和径向距离聚类</title>
      <link>https://arxiv.org/abs/2411.04255</link>
      <description><![CDATA[arXiv:2411.04255v1 公告类型：新
摘要：行人重新识别 (re-ID) 旨在解决跨非重叠相机匹配身份的问题。监督方法需要的身份信息可能难以获得，并且本质上偏向于它们所训练的数据集，使得它们无法跨领域扩展。为了克服这些挑战，我们提出了一种无监督的行人重新识别方法。由于对真实标签一无所知，我们提出的方法通过一种新颖的两阶段训练策略增强了学习特征的判别能力。第一阶段涉及在专业设计的姿势变换数据集上训练深度网络，该数据集是通过在姿势空间中为每个原​​始图像生成多个扰动获得的。接下来，网络学习使用所提出的判别聚类算法在特征空间中更接近地映射相似的特征。我们引入了一种新颖的径向距离损失，它关注特征学习的基本方面——紧凑的簇，簇内差异低，簇间差异高。在多个大规模 re-ID 数据集上进行的大量实验证明了我们的方法与最先进的方法相比的优越性。]]></description>
      <guid>https://arxiv.org/abs/2411.04255</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>提高 FPGA 实现的基于事件的视觉的图卷积的可扩展性</title>
      <link>https://arxiv.org/abs/2411.04269</link>
      <description><![CDATA[arXiv:2411.04269v1 公告类型：新
摘要：事件摄像机作为传统基于帧的视觉传感器的替代品，正变得越来越流行，尤其是在移动机器人领域。充分利用它们的高时间分辨率、高动态范围、低功耗和事件数据的稀疏性（仅反映观察到的场景的变化），需要高效的算法和专门的硬件平台。最近的趋势是使用在异构 SoC FPGA 上实现的图卷积神经网络 (GCNN)。在本文中，我们专注于优化图卷积的硬件模块，以便灵活选择 FPGA 资源（BlockRAM、DSP 和 LUT）来实现它们。我们提出了一种“两步卷积”方法，该方法利用额外的 BRAM 缓冲区来减少高达 94% 的乘法 LUT 使用量。该方法显着提高了 GCNN 的可扩展性，从而能够部署具有更多层、更大图大小的模型并将其应用于更动态的场景。]]></description>
      <guid>https://arxiv.org/abs/2411.04269</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>HandCraft：通过扩散生成的图像对畸形手进行解剖学正确修复</title>
      <link>https://arxiv.org/abs/2411.04332</link>
      <description><![CDATA[arXiv:2411.04332v1 公告类型：新
摘要：生成文本到图像模型（例如稳定扩散）已显示出生成多样化高质量图像的卓越能力。然而，在渲染人手时，它们却出奇地笨拙，因为人手在解剖学上往往不正确或处于“恐怖谷”状态。在本文中，我们提出了一种 HandCraft 方法来恢复这种畸形的手。这是通过使用参数模型自动构建手的掩模和深度图像作为调节信号来实现的，允许基于扩散的图像编辑器修复手的解剖结构并调整其姿势，同时将更改无缝集成到原始图像中，保留姿势、颜色和样式。我们的即插即用手部恢复解决方案与现有的预训练扩散模型兼容，并且恢复过程通过避免扩散模型的任何微调或训练要求来促进采用。我们还贡献了 MalHand 数据集，其中包含多种样式的畸形手部生成图像，用于手部检测器训练和手部恢复基准测试，并通过定性和定量评估证明 HandCraft 不仅恢复了解剖正确性，而且还保持了整体图像的完整性。]]></description>
      <guid>https://arxiv.org/abs/2411.04332</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GazeGen：凝视驱动的用户交互，用于生成视觉内容</title>
      <link>https://arxiv.org/abs/2411.04335</link>
      <description><![CDATA[arXiv:2411.04335v1 公告类型：新
摘要：我们介绍了 GazeGen，这是一种用户交互系统，可根据用户的目光所指位置生成视觉内容（图像和视频）。GazeGen 允许通过凝视目标区域直观地操纵视觉内容。使用对象检测和生成 AI 中的先进技术，GazeGen 可执行凝视控制的图像添加/删除、重新定位和图像对象的表面材料变化，并将静态图像转换为视频。GazeGen 的核心是 DFT Gaze（精炼和微调凝视）代理，这是一种只有 281K 个参数的超轻量级模型，可在小型边缘设备上根据个人用户的眼睛执行精确的实时凝视预测。GazeGen 是第一个将视觉内容生成与实时凝视估计相结合的系统，这完全由 DFT Gaze 实现。这种实时凝视估计可以实现各种视觉内容生成任务，所有任务均由用户的凝视控制。 DFT Gaze 的输入是用户的眼睛图像，而视觉内容生成的输入是用户的视图和 DFT Gaze 预测的注视点。为了实现有效的注视预测，我们通过新颖的知识提炼和个人适应技术从大模型（大 10 倍）中推导出小模型。我们将知识提炼与掩蔽自动编码器相结合，开发出紧凑但功能强大的注视估计模型。该模型通过适配器进一步微调，以最少的用户输入实现高度准确和个性化的注视预测。DFT Gaze 确保低延迟和精确的注视跟踪，支持广泛的注视驱动任务。我们在 AEA 和 OpenEDS2020 基准上验证了 DFT Gaze 的性能，证明了边缘设备（Raspberry Pi 4）上的低角度注视误差和低延迟。此外，我们描述了 GazeGen 的应用，说明了其在各种使用场景中的多功能性和有效性。]]></description>
      <guid>https://arxiv.org/abs/2411.04335</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>UEVAVD：用于开发无人机视角主动物体检测的数据集</title>
      <link>https://arxiv.org/abs/2411.04348</link>
      <description><![CDATA[arXiv:2411.04348v1 公告类型：新
摘要：遮挡是一个长期存在的难题，对基于无人机的物体检测提出了挑战。许多工作通过调整检测模型来解决此问题。然而，很少有人利用无人机可以通过改变视角从根本上提高检测性能。主动物体检测 (AOD) 提供了一种有效的方法来实现这一目的。通过深度强化学习 (DRL)，AOD 赋予无人机自主路径规划的能力，以搜索更有利于目标识别的观察结果。不幸的是，没有可用于开发无人机 AOD 方法的数据集。为了填补这一空白，我们发布了一个名为 UEVAVD 的无人机视角主动视觉数据集，并希望它能够促进对无人机 AOD 问题的研究。此外，我们通过在学习状态表示时加入归纳偏差来改进现有的基于 DRL 的 AOD 方法。首先，由于部分可观测性，我们使用门控循环单元从观测序列中提取状态表示，而不是从单视图观测中提取。其次，我们使用 Segment Anything 模型 (SAM) 预分解场景，并使用派生的掩码过滤掉不相关的信息。通过这些做法，代理可以学习具有更好泛化能力的主动观看策略。我们的创新的有效性已通过 UEVAVD 数据集上的实验得到验证。我们的数据集将很快在 https://github.com/Leo000ooo/UEVAVD_dataset 上提供。]]></description>
      <guid>https://arxiv.org/abs/2411.04348</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LidaRefer：使用 Transformer 实现自动驾驶的户外 3D 视觉基础</title>
      <link>https://arxiv.org/abs/2411.04351</link>
      <description><![CDATA[arXiv:2411.04351v1 公告类型：新摘要：3D 视觉定位 (VG) 旨在根据自然语言描述在 3D 场景中定位相关对象或区域。尽管最近的室内 3D VG 方法成功地利用基于 Transformer 的架构来捕获全局上下文信息并实现细粒度的跨模态融合，但由于室内和室外环境之间点云分布的差异，它们不适合室外环境。具体而言，首先，由于高维视觉特征，广泛的 LiDAR 点云在 Transformer 中需要不可接受的计算和内存资源。其次，稀疏 LiDAR 点云中的主要背景点和空白空间由于其不相关的视觉信息而使跨模态融合变得复杂。为了应对这些挑战，我们提出了 LidaRefer，这是一个基于 Transformer 的 3D VG 框架，专为大规模户外场景而设计。此外，在训练过程中，我们引入了一种简单有效的定位方法，该方法监督解码器的查询，不仅定位目标对象，还定位可能由于场景中出现类似属性或对语言描述的理解不正确而误认为目标的模糊对象。这种监督通过学习模糊对象和目标的空间关系和属性差异，增强了模型区分模糊对象和目标的能力。LidaRefer 在用于自动驾驶的 3D VG 数据集 Talk2Car-3D 上实现了最佳性能，并且在各种评估设置下都有显著改进。]]></description>
      <guid>https://arxiv.org/abs/2411.04351</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MegaPortrait：重新审视扩散控制以实现高保真肖像生成</title>
      <link>https://arxiv.org/abs/2411.04357</link>
      <description><![CDATA[arXiv:2411.04357v1 公告类型：新
摘要：我们提出了 MegaPortrait。这是一个在计算机视觉中创建个性化肖像图像的创新系统。它有三个模块：身份网络、阴影网络和协调网络。身份网络使用通过源图像微调的定制模型生成学习到的身份。阴影网络使用提取的表示重新渲染肖像。协调网络融合粘贴的面部和参考图像的身体以获得连贯的结果。我们使用现成的控制网的方法在身份保存和图像保真度方面优于最先进的 AI 肖像产品。MegaPortrait 有一个简单但有效的设计，我们将其与其他方法和产品进行比较以显示其优越性。]]></description>
      <guid>https://arxiv.org/abs/2411.04357</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ProGraph：用于 3D 人体重建的时间可对齐概率引导图拓扑建模</title>
      <link>https://arxiv.org/abs/2411.04399</link>
      <description><![CDATA[arXiv:2411.04399v1 公告类型：新
摘要：当前单目视频的 3D 人体运动重建方法依赖于当前重建窗口内的特征，导致在视频帧局部遮挡或模糊的情况下人体结构出现扭曲和变形。为了根据不完整的特征估计真实的 3D 人体网格序列，我们提出了用于 3D 人体重建的时间可对齐概率引导图拓扑建模 (ProGraph)。对于缺失部分的恢复，我们利用整个运动序列中的显式拓扑感知概率分布。为了恢复完整的人体，图拓扑建模 (GTM) 学习底层拓扑结构，重点关注各个部分固有的关系。接下来，为了生成模糊的运动部分，时间可对齐概率分布 (TPDist) 利用 GTM 根据分布预测特征。这种交互机制促进了运动一致性，从而可以恢复人体部位。此外，分层人类损失（HHLoss）限制了拓扑结构变化期间帧间特征的概率分布误差。我们的方法在解决 3DPW 上的遮挡和模糊方面取得了比其他 SOTA 方法更好的效果。]]></description>
      <guid>https://arxiv.org/abs/2411.04399</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>图像理解有助于生成良好的图像标记器</title>
      <link>https://arxiv.org/abs/2411.04406</link>
      <description><![CDATA[arXiv:2411.04406v1 公告类型：新
摘要：摘要 现代图像生成 (IG) 模型已被证明能够捕获对图像理解 (IU) 任务有价值的丰富语义。然而，IU 模型提高 IG 性能的潜力仍然未知。我们使用基于 token 的 IG 框架解决了这个问题，该框架依赖于有效的 tokenizer 将图像投影到 token 序列中。目前，像素重建（例如 VQGAN）主导了图像 tokenizer 的训练目标。相反，我们的方法采用特征重建目标，其中 tokenizer 通过从预训练的 IU 编码器中提取知识来训练。全面的比较表明，具有强大 IU 功能的 tokenizer 在各种指标、数据集、任务和提议网络中实现了卓越的 IG 性能。值得注意的是，VQ-KD CLIP 在 ImageNet-1k（IN-1k）上实现了 $4.10$ FID。可视化表明，VQ-KD 的优势部分归因于 VQ-KD 码本中的丰富语义。我们进一步引入了一个简单的管道，将 IU 编码器直接转换为标记器，证明了其对 IG 任务的卓越效果。这些发现可能会激发对图像标记器研究的进一步探索，并激励社区重新评估 IU 和 IG 之间的关系。代码发布于 https://github.com/magic-research/vector_quantization。]]></description>
      <guid>https://arxiv.org/abs/2411.04406</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>BendVLM：视觉语言嵌入的测试时去偏</title>
      <link>https://arxiv.org/abs/2411.04420</link>
      <description><![CDATA[arXiv:2411.04420v1 公告类型：新
摘要：视觉语言模型 (VLM) 嵌入已被证明会编码其训练数据中存在的偏见，例如将负面特征强加给各种种族和性别身份成员的社会偏见。VLM 正迅速被应用于从小样本分类到文本引导图像生成的各种任务，这使得消除 VLM 嵌入的偏见至关重要。微调 VLM 的消除偏见方法通常会遭受灾难性的遗忘。另一方面，无需微调的方法通常采用“一刀切”的方法，该方法假设可以使用所有可能输入的单一线性方向来解释与虚假属性的相关性。在这项工作中，我们提出了 Bend-VLM，这是一种非线性、无需微调的 VLM 嵌入消除偏见方法，可根据每个唯一输入定制消除偏见操作。这允许更灵活的去偏方法。此外，我们不需要事先知道推理时间的输入集，这使得我们的方法更适合在线、开放集任务，例如检索和文本引导图像生成。]]></description>
      <guid>https://arxiv.org/abs/2411.04420</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>