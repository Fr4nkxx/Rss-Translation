<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Thu, 01 Aug 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>点云语义分割的细粒度度量</title>
      <link>https://arxiv.org/abs/2407.21289</link>
      <description><![CDATA[arXiv:2407.21289v1 公告类型：新
摘要：在点云语义分割数据集中通常观察到两种形式的不平衡：（1）类别不平衡，某些对象比其他对象更普遍；（2）尺寸不平衡，某些对象比其他对象占据更多的点。因此，现有的评估指标倾向于大多数类别和大对象。为了解决这些问题，本文建议使用细粒度的 mIoU 和 mAcc 对点云分割算法进行更彻底的评估。这些细粒度指标为模型和数据集提供了更丰富的统计信息，这也减轻了当前语义分割指标对大对象的偏见。所提出的指标用于在三个不同的室内和室外语义分割数据集上训练和评估各种语义分割算法。]]></description>
      <guid>https://arxiv.org/abs/2407.21289</guid>
      <pubDate>Fri, 02 Aug 2024 03:15:13 GMT</pubDate>
    </item>
    <item>
      <title>DDU-Net：基于域分解的 CNN，用于多 GPU 上的高分辨率图像分割</title>
      <link>https://arxiv.org/abs/2407.21266</link>
      <description><![CDATA[arXiv:2407.21266v2 公告类型：新
摘要：超高分辨率图像的分割带来了诸如空间信息丢失或计算效率低下等挑战。在这项工作中，提出了一种将编码器-解码器架构与域分解策略相结合的新方法来应对这些挑战。具体而言，引入了一种基于域分解的 U-Net (DDU-Net) 架构，该架构将输入图像划分为不重叠的块，可以在单独的设备上独立处理。添加了通信网络以促进块间信息交换，从而增强对空间上下文的理解。在旨在衡量通信网络有效性的合成数据集上进行实验验证。然后，在 DeepGlobe 土地覆盖分类数据集上测试性能作为真实世界的基准数据集。结果表明，与没有图像块间通信的相同网络相比，该方法（包括将图像划分为 $16\times16$ 个不重叠子图像的块间通信）的交并比 (IoU) 得分高出 $2-3\,\%$。包含通信的网络的性能相当于在完整图像上训练的基线 U-Net，表明我们的模型为分割超高分辨率图像同时保留空间上下文提供了一种有效的解决方案。代码可在 https://github.com/corne00/HiRes-Seg-CNN 上找到。]]></description>
      <guid>https://arxiv.org/abs/2407.21266</guid>
      <pubDate>Fri, 02 Aug 2024 03:15:12 GMT</pubDate>
    </item>
    <item>
      <title>使用 MSU-Net 增强超声图像分割中的不确定性估计</title>
      <link>https://arxiv.org/abs/2407.21273</link>
      <description><![CDATA[arXiv:2407.21273v1 公告类型：新
摘要：创伤和重症监护中有效的血管内通路对患者的治疗效果有重大影响。然而，在严酷的环境中，熟练的医务人员往往是有限的。自主机器人超声系统可以帮助针头插入以输送药物，并支持非专家完成此类任务。尽管在自主针头插入方面取得了进展，但血管分割预测的不准确性会带来风险。了解超声成像中预测模型的不确定性对于评估其可靠性至关重要。我们引入了 MSU-Net，这是一种新颖的多阶段方法，用于训练一组 U-Net 以产生准确的超声图像分割图。我们展示了显着的改进，比单个 Monte Carlo U-Net 提高了 18.1%，增强了不确定性评估、模型透明度和可信度。通过突出显示模型确定性领域，MSU-Net 可以指导安全针头插入，使非专家能够完成此类任务。]]></description>
      <guid>https://arxiv.org/abs/2407.21273</guid>
      <pubDate>Fri, 02 Aug 2024 03:15:12 GMT</pubDate>
    </item>
    <item>
      <title>基于稳健框提示的 SAM 用于医学图像分割</title>
      <link>https://arxiv.org/abs/2407.21284</link>
      <description><![CDATA[arXiv:2407.21284v1 公告类型：新
摘要：分割任何模型 (SAM) 在高质量框提示下可以实现令人满意的分割性能。然而，SAM 的稳健性因框质量的下降而受到影响，限制了其在临床现实中的实用性。在本研究中，我们提出了一种基于稳健框提示的新型 SAM (\textbf{RoBox-SAM})，以确保 SAM 在不同质量提示下的分割性能。我们的贡献有三方面。首先，我们提出了一个提示细化模块来隐式感知潜在目标，并输出偏移量以直接将低质量框提示转换为高质量提示。然后，我们提供了一种在线迭代策略来进一步细化提示。其次，我们引入了一个提示增强模块来自动生成点提示，以有效地协助可框提示的分割。最后，我们构建了一个自信息提取器来对输入图像中的先验信息进行编码。这些特征可以优化图像嵌入和注意力计算，从而进一步增强 SAM 的鲁棒性。在包括 99,299 张图像、5 种模态和 25 个器官/目标的大型医学分割数据集上进行的大量实验验证了我们提出的 RoBox-SAM 的有效性。]]></description>
      <guid>https://arxiv.org/abs/2407.21284</guid>
      <pubDate>Fri, 02 Aug 2024 03:15:12 GMT</pubDate>
    </item>
    <item>
      <title>终身人员搜索</title>
      <link>https://arxiv.org/abs/2407.21252</link>
      <description><![CDATA[arXiv:2407.21252v1 公告类型：新
摘要：人物搜索是在场景图像的图库数据集中定位查询人物的任务。现有的方法主要是为了处理单个目标数据集而开发的，然而在人物搜索的实际应用中，不断给出不同的数据集。在这种情况下，当在新的数据集上进行训练时，他们会遭受旧数据集中灾难性的知识遗忘。在本文中，我们首先介绍了一个终身人物搜索 (LPS) 的新问题，其中模型在新的数据集上进行增量训练，同时保留在旧数据集中学习到的知识。我们提出了一个端到端的 LPS 框架，该框架利用前景人物的原型特征以及旧领域中的硬背景提议，促进知识提炼，以加强新旧模型之间的一致性学习。此外，我们还设计了基于排练的实例匹配，通过额外使用未标记的人员实例来进一步提高旧领域的辨别能力。实验结果表明，与现有方法相比，所提出的方法在检测和重新识别方面都取得了明显优越的性能，以保留在旧领域中学习到的知识。]]></description>
      <guid>https://arxiv.org/abs/2407.21252</guid>
      <pubDate>Fri, 02 Aug 2024 03:15:11 GMT</pubDate>
    </item>
    <item>
      <title>利用自适应隐式表示映射实现超高分辨率图像分割</title>
      <link>https://arxiv.org/abs/2407.21256</link>
      <description><![CDATA[arXiv:2407.21256v1 公告类型：新
摘要：隐式表示映射 (IRM) 可以将图像特征转换为任何连续分辨率，展示了其在超高分辨率图像分割细化方面的强大能力。当前基于 IRM 的超高分辨率图像分割细化方法通常依赖于基于 CNN 的编码器来提取图像特征，并应用共享隐式表示映射函数 (SIRMF) 将像素级特征转换为分割结果。因此，这些方法表现出两个关键的局限性。首先，基于 CNN 的编码器可能无法有效捕获长距离信息，导致像素级特征缺乏全局语义信息。其次，SIRM 在所有样本之间共享，这限制了其概括和处理不同输入的能力。为了解决这些限制，我们提出了一种新方法，利用新提出的自适应隐式表示映射 (AIRM) 进行超高分辨率图像分割。具体而言，所提出的方法包括两个部分：(1) 亲和力编码器 (AEE)，这是一种强大的特征提取器，它利用转换器架构和语义亲和力的优势有效地对长距离特征进行建模；(2) 自适应隐式表示映射函数 (AIRMF)，它自适应地转换逐像素特征而不忽略全局语义信息，从而实现灵活而精确的特征转换。我们在常用的超高分辨率分割细化数据集（即 BIG 和 PASCAL VOC 2012）上评估了我们的方法。大量实验表明，我们的方法远远优于竞争对手。代码在补充材料中提供。]]></description>
      <guid>https://arxiv.org/abs/2407.21256</guid>
      <pubDate>Fri, 02 Aug 2024 03:15:11 GMT</pubDate>
    </item>
    <item>
      <title>PLANesT-3D：用于分割 3D 植物点云的新型注释数据集</title>
      <link>https://arxiv.org/abs/2407.21150</link>
      <description><![CDATA[arXiv:2407.21150v1 公告类型：新
摘要：创建新的带注释的公共数据集对于帮助 3D 计算机视觉和机器学习的进步充分发挥其自动解释 3D 植物模型的潜力至关重要。在本文中，我们介绍了 PLANesT-3D；一种新的带注释的植物 3D 彩色点云数据集。PLANesT-3D 由 34 个点云模型组成，代表来自三个不同植物物种的 34 种真实植物：\textit{Capsicum annuum}、\textit{Rosa kordana} 和 \textit{Ribes rubrum}。对于完整点云，“叶”和“茎”方面的语义标签以及器官实例标签都是手动注释的。作为额外的贡献，SP-LSCnet 是一种新颖的语义分割方法，它结合了无监督超点提取和基于 3D 点的深度学习方法，并在新数据集上进行了评估。我们还对两种现有的深度神经网络架构 PointNet++ 和 RoseSegNet 在 PLANesT-3D 的点云上进行了语义分割测试。]]></description>
      <guid>https://arxiv.org/abs/2407.21150</guid>
      <pubDate>Fri, 02 Aug 2024 03:15:10 GMT</pubDate>
    </item>
    <item>
      <title>人工智能安全实践：增强多模态图像字幕的对抗鲁棒性</title>
      <link>https://arxiv.org/abs/2407.21174</link>
      <description><![CDATA[arXiv:2407.21174v1 公告类型：新
摘要：结合视觉和文本数据的多模态机器学习模型越来越多地被部署在关键应用中，由于它们容易受到对抗性攻击，因此引发了重大的安全问题。本文提出了一种有效的策略来增强多模态图像字幕模型对此类攻击的鲁棒性。通过利用快速梯度符号法 (FGSM) 生成对抗性示例并结合对抗性训练技术，我们在两个基准数据集上展示了改进的模型鲁棒性：Flickr8k 和 COCO。我们的研究结果表明，选择性地仅训练多模态架构的文本解码器表现出与完全对抗性训练相当的性能，同时提供更高的计算效率。这种有针对性的方法表明了鲁棒性和训练成本之间的平衡，促进了多模态 AI 系统在各个领域的合乎道德的部署。]]></description>
      <guid>https://arxiv.org/abs/2407.21174</guid>
      <pubDate>Fri, 02 Aug 2024 03:15:10 GMT</pubDate>
    </item>
    <item>
      <title>利用 Transformer 和卷积集成推进越南语视觉问答</title>
      <link>https://arxiv.org/abs/2407.21229</link>
      <description><![CDATA[arXiv:2407.21229v1 公告类型：新
摘要：视觉问答 (VQA) 最近成为一个潜在的研究领域，吸引了人工智能和计算机视觉领域许多人的兴趣。尽管英语方法很流行，但明显缺乏专门针对某些语言（尤其是越南语）开发的系统。本研究旨在通过对越南语视觉问答 (ViVQA) 数据集进行全面实验来弥补这一差距，证明我们提出的模型的有效性。为了响应社区的兴趣，我们开发了一个增强图像表示能力的模型，从而提高了 ViVQA 系统的整体性能。具体来说，我们的模型集成了 Bootstrapping 语言图像预训练与冻结单峰模型 (BLIP-2) 和卷积神经网络 EfficientNet，以从图像中提取和处理局部和全局特征。这种集成利用了基于 Transformer 的架构的优势来捕获全面的上下文信息，并利用卷积网络来获取详细的局部特征。通过冻结这些预训练模型的参数，我们显著降低了计算成本和训练时间，同时保持了高性能。这种方法显著改善了图像表示并提高了现有 VQA 系统的性能。然后，我们利用基于通用多模态基础模型 (BEiT-3) 的多模态融合模块来融合视觉和文本特征之间的信息。我们的实验结果表明，我们的模型超越了竞争基线，实现了令人鼓舞的性能。这在 ViVQA 数据集测试集上的准确率 $71.04\%$ 中尤为明显，标志着我们研究领域的重大进步。代码可在 https://github.com/nngocson2002/ViVQA 获得。]]></description>
      <guid>https://arxiv.org/abs/2407.21229</guid>
      <pubDate>Fri, 02 Aug 2024 03:15:10 GMT</pubDate>
    </item>
    <item>
      <title>自动驾驶的自监督多未来占用预测</title>
      <link>https://arxiv.org/abs/2407.21126</link>
      <description><![CDATA[arXiv:2407.21126v1 公告类型：新
摘要：环境预测框架对于动态环境中自动驾驶汽车 (AV) 的安全导航至关重要。LiDAR 生成的占用网格地图 (L-OGM) 为场景表示提供了强大的鸟瞰图，实现了自监督联合场景预测，同时表现出对部分可观测性和感知检测失败的弹性。先前的方法专注于网格单元空间内的确定性 L-OGM 预测架构。虽然这些方法取得了一些成功，但它们经常产生不切实际的预测并且无法捕捉环境的随机性。此外，它们不能有效地集成 AV 中存在的其他传感器模式。我们提出的框架在生成架构的潜在空间中执行随机 L-OGM 预测，并允许对 RGB 摄像机、地图和计划轨迹进行调节。我们使用单步解码器（可实时提供高质量预测）或基于扩散的批量解码器（可进一步优化解码后的帧以解决时间一致性问题并减少压缩损失）对预测进行解码。我们在 nuScenes 和 Waymo Open 数据集上进行的实验表明，我们方法的所有变体在质量和数量上均优于先前的方法。]]></description>
      <guid>https://arxiv.org/abs/2407.21126</guid>
      <pubDate>Fri, 02 Aug 2024 03:15:09 GMT</pubDate>
    </item>
    <item>
      <title>为全身人体运动生成添加多模式控制</title>
      <link>https://arxiv.org/abs/2407.21136</link>
      <description><![CDATA[arXiv:2407.21136v1 公告类型：新
摘要：全身多模态运动生成由文本、语音或音乐控制，具有多种应用，包括视频生成和角色动画。然而，使用统一模型完成具有不同条件模态的各种生成任务面临两个主要挑战：不同生成场景中的运动分布漂移以及具有不同粒度的混合条件的复杂优化。此外，现有数据集中不一致的运动格式进一步阻碍了有效的多模态运动生成。在本文中，我们提出了 ControlMM，这是一个统一的框架，用于以即插即用的方式控制全身多模态运动生成。为了有效地学习和传递不同运动分布之间的运动知识，我们提出了 ControlMM-Attn，用于静态和动态人体拓扑图的并行建模。为了处理粒度变化的情况，ControlMM 采用了由粗到细的训练策略，包括用于语义生成的第 1 阶段文本到运动预训练和用于低级粒度变化情况的第 2 阶段多模态控制自适应。为了解决现有基准测试的可变运动格式限制，我们推出了 ControlMM-Bench，这是第一个基于统一全身 SMPL-X 格式的公开可用的多模态全身人体运动生成基准测试。大量实验表明，ControlMM 在各种标准运动生成任务中均实现了最佳性能。我们的网站是 https://yxbian23.github.io/ControlMM。]]></description>
      <guid>https://arxiv.org/abs/2407.21136</guid>
      <pubDate>Fri, 02 Aug 2024 03:15:09 GMT</pubDate>
    </item>
    <item>
      <title>使用人类反馈反演保护文本到图像的传播模型</title>
      <link>https://arxiv.org/abs/2407.21032</link>
      <description><![CDATA[arXiv:2407.21032v1 公告类型：新
摘要：本文讨论了由大规模文本到图像传播模型引起的社会担忧，这些模型用于生成潜在有害或受版权保护的内容。现有模型严重依赖互联网爬取的数据，其中由于过滤过程不完整，有问题的概念仍然存在。虽然以前的方法在一定程度上缓解了这个问题，但它们通常依赖于文本指定的概念，这在准确捕捉细微差别的概念和将模型知识与人类理解相结合方面带来了挑战。作为回应，我们提出了一个名为人类反馈反转 (HFI) 的框架，其中人类对模型生成的图像的反馈被压缩为文本标记，指导缓解或删除有问题的图像。所提出的框架可以基于现有技术构建，以达到相同的目的，增强它们与人类判断的一致性。通过这样做，我们使用基于自我蒸馏的技术简化了训练目标，为概念删除提供了强大的基础。我们的实验结果表明，我们的框架在保持图像质量的同时显著减少了令人反感的内容生成，有助于在公共领域合乎道德地部署人工智能。]]></description>
      <guid>https://arxiv.org/abs/2407.21032</guid>
      <pubDate>Fri, 02 Aug 2024 03:15:08 GMT</pubDate>
    </item>
    <item>
      <title>针对稳健且安全的文本转图像模型的直接反学习优化</title>
      <link>https://arxiv.org/abs/2407.21035</link>
      <description><![CDATA[arXiv:2407.21035v1 公告类型：新
摘要：文本到图像 (T2I) 模型的最新进展极大地受益于大规模数据集，但它们也因可能生成不安全内容而带来重大风险。为了缓解这个问题，研究人员开发了反学习技术来消除模型生成潜在有害内容的能力。然而，这些方法很容易被对抗性攻击绕过，使得它们无法可靠地确保生成的图像的安全。在本文中，我们提出了直接反学习优化 (DUO)，这是一个新颖的框架，用于从 T2I 模型中删除不适合工作 (NSFW) 的内容，同时保留其在无关主题上的性能。DUO 采用偏好优化方法，使用精选的配对图像数据，确保模型学会删除不安全的视觉概念，同时保留不相关的特征。此外，我们引入了一个输出保留正则化项来保持模型在安全内容上的生成能力。大量实验表明，DUO 可以有效抵御各种最先进的红队攻击方法，而不会在无关主题上出现显著的性能下降（以 FID 和 CLIP 分数衡量）。我们的工作有助于开发更安全、更可靠的 T2I 模型，为在闭源和开源场景中负责任地部署它们铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2407.21035</guid>
      <pubDate>Fri, 02 Aug 2024 03:15:08 GMT</pubDate>
    </item>
    <item>
      <title>PAV：从非结构化视频集合中获取个性化头部头像</title>
      <link>https://arxiv.org/abs/2407.21047</link>
      <description><![CDATA[arXiv:2407.21047v1 公告类型：新 
摘要：我们提出了个性化头部头像 PAV，用于在任意视点和面部表情下合成人脸。PAV 引入了一种学习动态可变形神经辐射场 (NeRF) 的方法，特别是从同一角色在各种外观和形状变化下的单眼说话面部视频集合中学习。与现有的头部 NeRF 方法不同，这些方法仅限于基于每个外观对此类输入视频进行建模，我们的方法允许学习多外观 NeRF，通过附加到底层几何的可学习潜在神经特征为每个输入视频引入外观嵌入。此外，所提出的外观条件密度公式有助于在辐射场预测中对角色的形状变化（例如面部毛发和软组织）。据我们所知，我们的方法是第一个动态可变形 NeRF 框架，用于在单个统一网络中为同一主题的多种外观建模外观和形状变化。我们通过实验证明，在对各种主题的定量和定性研究中，PAV 在视觉渲染质量方面优于基线方法。]]></description>
      <guid>https://arxiv.org/abs/2407.21047</guid>
      <pubDate>Fri, 02 Aug 2024 03:15:08 GMT</pubDate>
    </item>
    <item>
      <title>多视角成像的成功概率</title>
      <link>https://arxiv.org/abs/2407.21027</link>
      <description><![CDATA[arXiv:2407.21027v1 公告类型：新
摘要：机器人、安全摄像头、无人机和卫星等平台用于多视图成像，通过立体视觉或断层扫描进行三维 (3D) 恢复。设置中的每个相机都有一个视场 (FOV)。多视图分析需要所有相机或其中很大一部分相机的视场重叠。然而，这种方法的成功并不能保证，因为视场可能没有足够的重叠。原因是由于平台控制不精确，从支架或平台指向相机具有一定的随机性 (噪声)，这是机械系统，尤其是卫星等移动系统的典型特征。所以，成功是概率性的。本文创建了一个框架来分析这方面。这对于设置成像系统功能的限制至关重要，例如分辨率 (像素占用空间)、视场、可捕获域的大小和效率。该框架利用了这样一个事实：不精确的指向可以通过自我校准来缓解——前提是视图对之间有足够的重叠，并且视图之间有足够的视觉相似性。我们展示了一个考虑设计一组纳米卫星的例子，这些卫星旨在对云进行 3D 重建。]]></description>
      <guid>https://arxiv.org/abs/2407.21027</guid>
      <pubDate>Fri, 02 Aug 2024 03:15:07 GMT</pubDate>
    </item>
    </channel>
</rss>