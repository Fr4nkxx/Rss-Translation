<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Thu, 07 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>解锁档案：使用大型语言模型转录手写历史文献</title>
      <link>https://arxiv.org/abs/2411.03340</link>
      <description><![CDATA[arXiv:2411.03340v1 公告类型：新
摘要：这项研究表明，大型语言模型 (LLM) 可以转录历史手写文档，其准确率明显高于专门的手写文本识别 (HTR) 软件，同时速度更快、更具成本效益。我们引入了一个名为 Transcription Pearl 的开源软件工具，该工具利用这些功能，使用来自 OpenAI、Anthropic 和 Google 的商用多模式 LLM 自动转录和更正批量手写文档。在对 18 世纪/19 世纪英语手写文档的多样化语料库进行的测试中，LLM 的字符错误率 (CER) 达到 5.7% 到 7%，单词错误率 (WER) 达到 8.9% 到 15.9%，与 Transkribus 等专业的最先进的 HTR 软件相比，分别提高了 14% 和 32%。最重要的是，当使用 LLM 校正这些转录以及传统 HTR 软件生成的文本时，它们的准确度接近人类水平，即 CER 低至 1.8%，WER 为 3.5%。LLM 完成这些任务的速度也比专有 HTR 程序快 50 倍，成本约为其 1/50。这些结果表明，当 LLM 被整合到 Transcription Pearl 等软件工具中时，它们提供了一种可访问、快速且高度准确的方法来批量转录历史手写文件，从而大大简化了数字化过程。]]></description>
      <guid>https://arxiv.org/abs/2411.03340</guid>
      <pubDate>Thu, 07 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于分布外检测的视觉语言模型的自校准调整</title>
      <link>https://arxiv.org/abs/2411.03359</link>
      <description><![CDATA[arXiv:2411.03359v1 公告类型：新
摘要：分布外 (OOD) 检测对于在开放世界应用中部署可靠的机器学习模型至关重要。基于 CLIP 的 OOD 检测的最新进展通过使用从 ID 数据中提取的 OOD 特征对即时调整进行正则化，显示出了良好的结果。然而，由于前景-背景分解不准确，从 ID 数据中挖掘出的不相关上下文可能是虚假的，从而限制了 OOD 检测性能。在这项工作中，我们提出了一个新框架，即自校准调整 (SCT)，以缓解此问题，以便仅使用给定的少量 ID 数据进行有效的 OOD 检测。具体而言，SCT 分别在原始学习目标的两个组成部分上引入了调节因子。它在训练期间自适应地指导两个任务之间的优化过程，以具有不同的预测不确定性的数据来​​校准 OOD 正则化的影响，这与许多基于即时调整的 OOD 检测方法兼容。已经进行了大量的实验和分析，以描述和证明所提出的 SCT 的有效性。该代码已公开。]]></description>
      <guid>https://arxiv.org/abs/2411.03359</guid>
      <pubDate>Thu, 07 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过端到端船载原始数据分析增强海上态势感知</title>
      <link>https://arxiv.org/abs/2411.03403</link>
      <description><![CDATA[arXiv:2411.03403v1 公告类型：新
摘要：卫星机载数据处理对于需要及时高效快速响应的时间敏感型应用至关重要。边缘人工智能的进步正在将计算能力从地面中心转移到在轨平台，改变“感知-通信-决策-反馈”周期，并减少从获取到交付的延迟。当前的研究提出了一个框架，解决了小型卫星严格的带宽、能量和延迟限制，重点是海上监控。这项研究贡献了三个主要创新。首先，它研究了深度学习技术在从原始卫星图像直接检测和分类船舶中的应用。通过简化机载处理链，我们的方法有助于直接分析，而无需校准和正射校正等计算密集型步骤。其次，为了解决原始卫星数据稀缺的问题，我们引入了两个新的数据集 VDS2Raw 和 VDV2Raw，它们分别来自 Sentinel-2 和植被与环境监测新微型卫星 (VENuS) 任务的原始数据，并丰富了自动识别系统 (AIS) 记录。第三，我们通过在两个数据集上验证的统计和基于特征的分析来描述任务的最佳单光谱和多光谱波段组合。总之，我们通过在类似 CubeSat 的硬件上进行概念验证来证明所提出方法的可行性，证实了这些模型在基于卫星的海上监测方面的潜力。]]></description>
      <guid>https://arxiv.org/abs/2411.03403</guid>
      <pubDate>Thu, 07 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>3D 视觉接地的细粒度空间和语言损失</title>
      <link>https://arxiv.org/abs/2411.03405</link>
      <description><![CDATA[arXiv:2411.03405v1 公告类型：新
摘要：3D 视觉基础包括识别随附语言描述引用的 3D 场景中的实例。虽然在常用的选择基础框架内已经提出了几种架构，但利用的损失相对较少。特别是，大多数方法依赖于对候选实例的预测分布的基本监督交叉熵损失，这无法对实例之间的空间关系和口头引用的内部细粒度词级结构进行建模。稀疏尝试通过从描述中学习所引用实例的类别或使用言语视觉对比来更好地分离实例嵌入来全局额外监督言语嵌入，但这并不能从根本上消除上述限制。针对这些缺点，我们为 3D 视觉接地引入了两个新的损失：视觉级偏移损失，即从每个实例到参考真实实例的回归向量偏移；语言相关跨度损失，即对描述中参考实例的单词级跨度的预测。此外，我们为新的 3D 视觉接地架构 AsphaltNet 的动词-视觉融合模块配备了自上而下的双向注意力融合块，这使得来自两个损失的监督信号能够传播到网络的各个反向分支，从而帮助后者学习上下文感知实例嵌入和接地感知动词嵌入。AsphaltNet 提出了新的辅助损失来辅助 3D 视觉接地，与 ReferIt3D 基准上的最新成果相比，其结果具有竞争力。]]></description>
      <guid>https://arxiv.org/abs/2411.03405</guid>
      <pubDate>Thu, 07 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于学习人体扫描和运动的潜在空间表征的自监督网络</title>
      <link>https://arxiv.org/abs/2411.03475</link>
      <description><![CDATA[arXiv:2411.03475v1 公告类型：新
摘要：本文介绍了自监督神经网络模型来解决 3D 人体分析和处理领域的几个基本问​​题。首先，我们提出了 VariShaPE（Varifold 形状参数估计器），这是一种用于检索身体形状和姿势的潜在空间表示的新型架构。该网络提供了一种快速而强大的方法来估计任意未注册网格在潜在空间中的嵌入。其次，我们用 MoGeN（运动几何网络）补充了潜在代码的估计，这是一个在潜在空间本身上学习几何的框架。这是通过将身体姿势参数空间提升到更高维的欧几里得空间来实现的，在这个空间中，来自 4D 数据训练集的身体运动小序列可以通过简单的线性插值来近似。使用 SMPL 潜在空间表示，我们说明了这些网络模型的组合在经过训练后如何用于以非常有限的计算成本执行各种任务。这包括运动插值、外推和传输以及随机形状和姿势生成等操作。]]></description>
      <guid>https://arxiv.org/abs/2411.03475</guid>
      <pubDate>Thu, 07 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用多任务生成对抗网络对 C 波段合成孔径雷达进行降雨回归</title>
      <link>https://arxiv.org/abs/2411.03480</link>
      <description><![CDATA[arXiv:2411.03480v1 公告类型：新
摘要：本文介绍了一种数据驱动的方法，用于以每像素 200 米的空间分辨率从合成孔径雷达 (SAR) 估算降水率。它解决了与 SAR 和气象雷达数据搭配相关的先前挑战，特别是搭配中的错位和强风下降雨示例的稀缺性。为了应对这些挑战，本文提出了一种多目标公式，引入了斑块级组件和对抗组件。它利用完整的 NEXRAD 档案来寻找与 Sentinel-1 数据的潜在共存。通过对训练程序的进一步增强和加入额外的输入，生成的模型显示出降雨估计的准确性提高，并且能够将其性能扩展到高达 15 m/s 的场景。]]></description>
      <guid>https://arxiv.org/abs/2411.03480</guid>
      <pubDate>Thu, 07 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用视觉语言模型的应用无关自动目标识别系统</title>
      <link>https://arxiv.org/abs/2411.03491</link>
      <description><![CDATA[arXiv:2411.03491v1 公告类型：新
摘要：我们提出了一种使用开放词汇对象检测和分类模型的新型自动目标识别 (ATR) 系统。这种方法的主要优点是，非技术最终用户可以在运行时之前定义目标类别，使用目标的一些自然语言文本描述或一些图像样本，或两者兼而有之。所需目标的细微差别可以用自然语言表达，这对于几乎没有或没有训练数据的独特目标很有用。我们还实施了几种技术的新组合来提高性能，例如利用重叠帧序列中的附加信息来执行小管识别（即顺序边界框匹配）、边界框重新评分和小管链接。此外，我们开发了一种技术，将许多重叠帧的聚合输出可视化为空中监视或侦察期间扫描的区域的马赛克，以及检测到的目标的核密度估计（或热图）。我们最初将该 ATR 系统应用于检测和清除机场跑道上未爆炸弹药的用例，目前正在将我们的研究扩展到其他实际应用。]]></description>
      <guid>https://arxiv.org/abs/2411.03491</guid>
      <pubDate>Thu, 07 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SynthSet：精准农业语义分割的生成扩散模型</title>
      <link>https://arxiv.org/abs/2411.03505</link>
      <description><![CDATA[arXiv:2411.03505v1 公告类型：新
摘要：本文介绍了一种生成合成注释数据的方法，以解决精准农业领域语义分割任务中的数据稀缺问题。利用去噪扩散概率模型 (DDPM) 和生成对抗网络 (GAN)，我们提出了一种双扩散模型架构，用于合成真实的注释农业数据，无需任何人工干预。我们采用超分辨率来增强合成图像的表型特征及其与相应生成的掩模的一致性。我们展示了所提出的方法在小麦头分割中的实用性。合成数据的高质量凸显了所提出的方法在生成图像掩模对方面的有效性。此外，在我们生成的数据上训练的模型在外部多样化的真实麦田数据集上测试时表现出良好的性能。结果表明，所提出的方法对于解决语义分割任务的数据稀缺性是有效的。此外，所提出的方法可以轻松适应精准农业及其他领域的各种分割任务。]]></description>
      <guid>https://arxiv.org/abs/2411.03505</guid>
      <pubDate>Thu, 07 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>超越完整形状：3D 形状匹配算法的定量评估</title>
      <link>https://arxiv.org/abs/2411.03511</link>
      <description><![CDATA[arXiv:2411.03511v1 公告类型：新
摘要：寻找 3D 形状之间的对应关系是计算机视觉、图形等领域的一个重要且长期存在的问题。虽然基于机器学习的方法主导着现代 3D 形状匹配，但几乎所有现有的（基于学习的）方法都要求所涉及的形状中至少一个是完整的。相比之下，最具挑战性且可以说是最实际相关的匹配部分观察到的形状的设置目前尚未得到充分探索。一个重要因素是现有数据集仅包含少量形状（通常低于 100 个），无法满足数据密集型机器学习方法的需求，尤其是在无监督机制下。此外，现有数据集中存在的偏倚类型通常是人为的，远非现实。为了解决这些限制并鼓励对这些相关设置的研究，我们提供了一个通用且灵活的框架，用于程序化生成具有挑战性的部分形状匹配场景。我们的框架允许从具有完整几何形状的有限集合中几乎无限地生成部分形状匹配实例。此外，我们手动创建了七个现有（完整几何）形状匹配数据集之间的跨数据集对应关系，总共有 2543 种形状。在此基础上，我们提出了几个具有挑战性的部分基准设置，我们评估了各自的最新方法作为基线。]]></description>
      <guid>https://arxiv.org/abs/2411.03511</guid>
      <pubDate>Thu, 07 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过多模式视频理解进行个性化视频摘要</title>
      <link>https://arxiv.org/abs/2411.03531</link>
      <description><![CDATA[arXiv:2411.03531v1 公告类型：新
摘要：视频摘要技术已被证明可以在访问和理解视频内容时改善整体用户体验。如果知道用户的偏好，视频摘要可以从输入视频中识别出重要信息或相关内容，帮助他们获取必要的信息或确定他们观看原始视频的兴趣。将视频摘要调整为各种类型的视频和用户偏好需要大量的训练数据和昂贵的人工标记。为了促进此类研究，我们提出了一个新的视频摘要基准，可以捕捉各种用户偏好。此外，我们提出了一种称为带语言的视频摘要 (VSL) 的管道，用于基于预训练的视觉语言模型 (VLM) 的用户偏好视频摘要，以避免需要在大型训练数据集上训练视频摘要系统。该管道将视频和隐藏式字幕作为输入，并通过将视频帧转换为文本在场景级别执行语义分析。随后，用户的类型偏好被用作选择相关文本场景的基础。实验结果表明，我们提出的管道优于当前最先进的无监督视频摘要模型。我们表明，与基于监督查询的视频摘要模型相比，我们的方法在不同数据集上的适应性更强。最后，运行时分析表明，在扩大用户偏好和视频的数量时，我们的管道更适合实际使用。]]></description>
      <guid>https://arxiv.org/abs/2411.03531</guid>
      <pubDate>Thu, 07 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过虚拟面部身​​份数据集对视觉语言模型进行基准测试</title>
      <link>https://arxiv.org/abs/2411.03554</link>
      <description><![CDATA[arXiv:2411.03554v1 公告类型：新
摘要：机器反学习已成为一种忘记训练数据中特定信息的有效策略。然而，随着视觉数据的日益融合，视觉语言模型 (VLM) 中的隐私问题仍未得到充分探索。为了解决这个问题，我们引入了面部身份反学习基准 (FIUBench)，这是一种新颖的 VLM 反学习基准，旨在稳健地评估被遗忘权设置下反学习算法的有效性。具体来说，我们通过构建虚构面部身份 VQA 数据集来制定 VLM 反学习任务，并应用两阶段评估流程，旨在精确控制信息源及其暴露水平。在评估方面，由于 VLM 支持以多种形式提出具有相同语义含义的问题，我们还提供了稳健的评估指标，包括成员推理攻击和精心设计的对抗性隐私攻击，以评估算法的性能。通过对 FIUBench 中的四种基准 VLM 反学习算法进行评估，我们发现所有方法的反学习性能仍然有限，模型效用和遗忘质量之间存在显著的权衡。此外，我们的研究结果还强调了隐私攻击对于稳健评估的重要性。我们希望 FIUBench 能够推动开发更有效的 VLM 反学习算法。]]></description>
      <guid>https://arxiv.org/abs/2411.03554</guid>
      <pubDate>Thu, 07 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 3D 高斯溅射进行对象和接触点跟踪的演示</title>
      <link>https://arxiv.org/abs/2411.03555</link>
      <description><![CDATA[arXiv:2411.03555v1 公告类型：新
摘要：本文介绍了一种通过从视频演示中提取触摸交互点和跟踪物体运动来增强交互式模仿学习 (IIL) 的方法。该方法通过为机器人提供有关在何处以及如何与物体（尤其是门和抽屉等复杂的铰接式物体）交互的详细知识来扩展当前的 IIL 系统。通过利用 3D Gaussian Splatting 和 FoundationPose 等尖端技术进行跟踪，该方法使机器人能够更好地理解和操纵动态环境中的物体。该研究为自主机器人系统中更有效的任务学习和执行奠定了基础。]]></description>
      <guid>https://arxiv.org/abs/2411.03555</guid>
      <pubDate>Thu, 07 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从双重稀疏自我中心视频数据估计自我身体姿势</title>
      <link>https://arxiv.org/abs/2411.03561</link>
      <description><![CDATA[arXiv:2411.03561v1 公告类型：新
摘要：我们研究从以自我为中心的视频中估计相机佩戴者的身体运动的问题。当前的自我身体姿势估计方法依赖于时间密集的传感器数据，例如来自空间稀疏的身体部位（如头部和手部）的 IMU 测量值。然而，我们认为即使是时间稀疏的观察，例如在自然或周期性手部运动期间从以自我为中心的视频中间歇地捕获的手部姿势，也可以有效地限制整体身体运动。单纯地应用扩散模型从头部姿势和稀疏手部姿势生成全身姿势会导致次优结果。为了克服这个问题，我们开发了一种两阶段方法，将问题分解为时间完成和空间完成。首先，我们的方法采用掩蔽自动编码器，通过利用头部姿势序列和间歇性手部姿势之间的时空相关性来推断手部轨迹，从而提供不确定性估计。随后，我们采用条件扩散模型，根据头部和手部的时间密集轨迹生成合理的全身运动，并以插补的不确定性估计为指导。我们的方法的有效性经过了严格的测试和验证，我们通过使用 AMASS 和 Ego-Exo4D 数据集在各种 HMD 设置上进行的全面实验。]]></description>
      <guid>https://arxiv.org/abs/2411.03561</guid>
      <pubDate>Thu, 07 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>混合注意力机制在现实条件下实现稳健的 RGB-T 行人检测</title>
      <link>https://arxiv.org/abs/2411.03576</link>
      <description><![CDATA[arXiv:2411.03576v1 公告类型：新
摘要：近年来，多光谱行人检测引起了广泛关注，尤其是在自动驾驶应用中。为了应对对抗性照明条件带来的挑战，热图像和可见图像的组合已显示出其优势。然而，现有的融合方法依赖于 RGB-热 (RGB-T) 图像对完全重叠的关键假设。这些假设在实际应用中通常不成立，在实际应用中，由于传感器配置，图像之间可能仅发生部分重叠。此外，传感器故障会导致一种模态的信息丢失。在本文中，我们提出了一种称为混合注意 (HA) 机制的新模块，作为我们的主要贡献，以减轻由部分重叠和传感器故障（即当场景的至少一部分仅由一个传感器获取时）引起的性能下降。我们提出了一种改进的 RGB-T 融合算法，该算法可以抵抗实际应用中推理过程中遇到的部分重叠和传感器故障。我们还利用移动友好的主干来应对嵌入式系统中的资源限制。我们通过模拟各种部分重叠和传感器故障场景进行了实验，以评估我们提出的方法的性能。结果表明，我们的方法优于最先进的方法，展示了其在处理现实世界挑战方面的优势。]]></description>
      <guid>https://arxiv.org/abs/2411.03576</guid>
      <pubDate>Thu, 07 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>StreamingBench：评估 MLLM 实现流视频理解的差距</title>
      <link>https://arxiv.org/abs/2411.03628</link>
      <description><![CDATA[arXiv:2411.03628v1 公告类型：新
摘要：多模态大型语言模型 (MLLM) 的快速发展已将其功能从图像理解扩展到视频理解。然而，这些 MLLM 中的大多数主要关注离线视频理解，在进行任何查询之前需要对所有视频帧进行大量处理。与人类实时观看、聆听、思考和响应流式输入的能力相比，这存在很大差距，凸显了当前 MLLM 的局限性。在本文中，我们介绍了 StreamingBench，这是第一个旨在评估 MLLM 的流式视频理解能力的综合基准。StreamingBench 评估了流式视频理解的三个核心方面：(1) 实时视觉理解、(2) 全源理解和 (3) 上下文理解。基准测试包括 18 个任务，包括 900 个视频和 4,500 个人工策划的 QA 对。每个视频都有五个在不同时间点呈现的问题，以模拟连续流式传输场景。我们在 StreamingBench 上使用 13 个开源和专有 MLLM 进行了实验，发现即使是最先进的专有 MLLM（如 Gemini 1.5 Pro 和 GPT-4o）的表现也远远低于人类水平的流媒体视频理解能力。我们希望我们的工作能够促进 MLLM 的进一步发展，使它们能够在更现实的场景中接近人类水平的视频理解和交互。]]></description>
      <guid>https://arxiv.org/abs/2411.03628</guid>
      <pubDate>Thu, 07 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>