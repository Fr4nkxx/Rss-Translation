<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>CS.CV更新arxiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.cv更新arxiv.org e-print存档。</description>
    <lastBuildDate>Fri, 28 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>通过ISP-CNN Fusion在不均匀照明下通过ISP-CNN融合来增强夹夹的多模式图像</title>
      <link>https://arxiv.org/abs/2502.19450</link>
      <description><![CDATA[ARXIV：2502.19450V1公告类型：新 
摘要：清晰的监视图像对于煤矿视频事物（IOVT）系统的安全操作至关重要。但是，在地下环境中，低照明和不均匀的亮度显着降低了图像质量，对通常依赖于难以实现的配对参考图像的增强方法提出了挑战。此外，在IOVT系统内的边缘设备上的增强性能与计算效率之间存在权衡。为了解决这些问题，我们提出了一种针对煤矿IOVT量身定制的多模式图像增强方法，该方法利用ISP-CNN融合体系结构优化了用于不均匀照明的ISP-CNN融合体系结构。这种两阶段的策略结合了全球增强和细节优化，有效地提高了图像质量，尤其是在光线不足的领域。基于夹的多模式迭代优化可以无监督的增强算法训练。 By integrating traditional image signal processing (ISP) with convolutional neural networks (CNN), our approach reduces computational complexity while maintaining high performance, making it suitable for real-time deployment on edge devices.Experimental results demonstrate that our method effectively mitigates uneven brightness and enhances key image quality metrics, with PSNR improvements of 2.9%-4.9%, SSIM by 4.3%-11.4%, and VIF by 4.9％-17.8％，而七种最先进的算法。模拟煤矿监控方案验证了我们方法平衡性能和计算需求的能力，促进实时增强和支持更安全的采矿操作。]]></description>
      <guid>https://arxiv.org/abs/2502.19450</guid>
      <pubDate>Fri, 28 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>评估不同口腔内扫描分辨率对深度学习牙齿分割的适用性</title>
      <link>https://arxiv.org/abs/2502.19515</link>
      <description><![CDATA[ARXIV：2502.19515V1公告类型：新 
摘要：口腔内扫描被广泛用于数字牙科，用于牙科修复，治疗计划和正畸程序。这些扫描包含详细的拓扑信息，但是这些扫描的手动注释仍然是一项耗时的任务。已经开发了基于深度学习的方法来自动化诸如牙齿分割之类的任务。典型的口腔内扫描包含超过200,000个网细胞，使直接处理的计算昂贵。通常对模型训练，通常有10,000或16,000个单元格。先前的研究表明，下采样可能会降低分割精度，但是这种降解的程度尚不清楚。了解降解的程度对于在边缘设备上部署ML模型至关重要。这项研究通过减少分辨率评估了性能降解的程度。我们在逐渐扫描中训练一个深度学习模型（PointMLP），该模型被减少到16K，10K，8K，6K，4K和2K网状细胞。在高分辨率扫描中测试了在较低分辨率上训练的模型以评估性能。我们的目标是确定平衡计算效率和细分精度的决议。]]></description>
      <guid>https://arxiv.org/abs/2502.19515</guid>
      <pubDate>Fri, 28 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于词典的框架，用于可解释和一致的对象解析</title>
      <link>https://arxiv.org/abs/2502.19540</link>
      <description><![CDATA[ARXIV：2502.19540V1公告类型：新 
摘要：在这项工作中，我们提出了Cocal，这是一个基于基于字典的掩码变压器的可解释且一致的对象解析框架。 Cocal围绕对比组件和逻辑约束而设计，可重新考虑用于分割的现有基于群集的掩模变压器体系结构；具体而言，Cocal使用一组字典组件，每个组件都明确链接到特定的语义类。为了促进这一概念，Cocal引入了与语义层次结构一致的字典组件的层次结构表述。这是通过将级内对比组件和跨逻辑约束的整合来实现的。具体而言，Cocal在每个语义级别采用组件的对比算法采用组件，从而使同一类中的字典成分与来自不同类别的词典的对比。此外，Cocal通过确保代表特定部分的字典组件比通过跨层次对比度学习目标更接近其相应的对象组件来解决逻辑问题。为了进一步增强我们的逻辑关系建模，我们实现了一个后处理功能，该功能灵感来自以下原则：分配给零件的像素也应分配给其相应的对象。通过这些创新，Cocal在Partimagenet和Pascal-Part-108上建立了新的最先进的性能，在MIOU的一部分中的表现分别超过了先前的方法，分别超过了2.08％和0.70％。此外，Cocal在这些基准测试的对象级指标中表现出显着的增强，这突出了其不仅可以在更精细的水平上完善解析的能力，还可以提高对象细分的整体质量。]]></description>
      <guid>https://arxiv.org/abs/2502.19540</guid>
      <pubDate>Fri, 28 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>告诉我为什么：视觉基础模型作为可自我解释的分类器</title>
      <link>https://arxiv.org/abs/2502.19577</link>
      <description><![CDATA[ARXIV：2502.19577V1公告类型：新 
摘要：视觉基础模型（VFM）由于其最新性能而变得越来越流行。但是，可解释性对于关键应用仍然至关重要。从这个意义上讲，自我解释的模型（SEM）旨在提供可解释的分类器，将预测分解为可解释的概念的加权总和。尽管他们承诺，但最近的研究表明，这些解释常常缺乏忠诚。在这项工作中，我们将VFM与新颖的原型结构和专门的培训目标相结合。通过在冷冻VFM的顶部训练轻巧的头（约100万参数），我们的方法（ProtoFM）提供了有效且可解释的解决方案。评估表明，我们的方法实现了竞争性的分类性能，同时胜过从文献中获得的一系列可解释性指标的现有模型。代码可在https://github.com/hturbe/proto-fm上找到。]]></description>
      <guid>https://arxiv.org/abs/2502.19577</guid>
      <pubDate>Fri, 28 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>与扩散模型和SWIN变压器的CT泌尿照图中的3D肾脏图像合成</title>
      <link>https://arxiv.org/abs/2502.19623</link>
      <description><![CDATA[ARXIV：2502.19623V1公告类型：新 
摘要：目的：本研究旨在使用与基于SWIN Transformer的深度学习方法集成的扩散模型在CT泌尿照学（CTU）检查中合成3D肾学相图像的方法。材料和方法：这项回顾性研究得到了当地机构审查委员会的批准。一个包含327例患者的数据集（平均$ \ pm $ $ sd年龄，63美元$ \ pm $ 15岁； 174名男性，153名女性）进行了策划，以进行深度学习模型开发。每个患者的三个阶段与仿射登记算法对齐。开发并实施了一个自定义的深度学习模型创建的DSSNICT（CT中的SWIN变压器的扩散模型用于合成肾上腺学相图像），以合成肾脏图像。使用峰值信噪比（PSNR），结构相似性指数（SSIM），平均绝对误差（MAE）和FR \&#39;{E} CHET视频距离（FVD）评估性能。进行了两名受过奖学金训练的腹部放射科医生的定性评估。结果：我们提出的方法生成的合成肾图像可实现高PSNR（26.3 $ \ pm $ 4.4 dB），SSIM（0.84 $ \ pm $ 0.069），MAE（12.74 $ \ pm $ 5.22 HU）和FVD（1323）。两名放射科医生在李克特量表为1-5的综合图像（p值= 0.5）的真实图像的平均得分为3.5，3.4的平均得分为3.4，这表明我们的合成图像与真实图像非常相似。结论：所提出的方法有效地合成了高质量的3D肾学相图像。该模型可用于在不损害图像质量的情况下将CTU中的辐射剂量降低33.3 \％，从而增强了CT泌尿外图的安全性和诊断效用。]]></description>
      <guid>https://arxiv.org/abs/2502.19623</guid>
      <pubDate>Fri, 28 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>EV-3DOD：使用事件摄像机推动3D对象检测的时间边界</title>
      <link>https://arxiv.org/abs/2502.19630</link>
      <description><![CDATA[ARXIV：2502.19630V1公告类型：新 
摘要：检测点云中的3D对象在自主驾驶系统中起着至关重要的作用。最近，结合相机信息的高级多模式方法已取得了显着的性能。对于安全有效的自主驾驶系统，不仅在准确性，而且速度和潜伏期低的算法都是必不可少的。但是，由于固定帧速率传感器的延迟和带宽的限制，例如LIDAR和相机，现有算法无法满足这些要求。为了解决此限制，我们首次将异步事件摄像机引入3D对象检测中。我们利用它们的高时间分辨率和低带宽来实现高速3D对象检测。我们的方法通过通过事件摄像机检索以前的3D信息，即使在同步数据不可用时也可以在框架间间隔内进行检测。此外，我们介绍了第一个基于事件的3D对象检测数据集DSEC-3DOD，其中包括100 fps的接地图3D边界框，为基于事件的3D检测器建立了第一个基准。代码和数据集可在https://github.com/mickeykang16/ev3dod上获得。]]></description>
      <guid>https://arxiv.org/abs/2502.19630</guid>
      <pubDate>Fri, 28 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MEDVLM-R1：通过增强学习激励视觉模型（VLM）的医学推理能力</title>
      <link>https://arxiv.org/abs/2502.19634</link>
      <description><![CDATA[ARXIV：2502.19634V1公告类型：新 
摘要：推理是进行医学图像分析的关键领域，在临床医生信任和监管机构批准中，透明度和可信赖性在中心作用。尽管医学视觉语言模型（VLM）对放射学任务显示出希望，但大多数现有的VLM仅产生最终答案而没有揭示潜在的推理。为了解决这一差距，我们引入了MedVLM-R1，这是一种医学VLM，明确产生自然语言推理以提高透明度和可信度。 MedVLM-R1不依靠监督的微调（SFT）（SFT）遭受过度拟合的培训分布，而无法促进真正的推理，而是采用了一个强化学习框架，该框架激励该模型发现无需使用任何推理参考文献而发现人类破解的推理路径。尽管培训数据有限（600个视觉问题回答样品）和模型参数（2B），但MRI，CT和X射线基准测试的MEDVLM-R1仍将准确性从55.11％提高到78.22％，超过对一百万多个样品的较大型号的表现。它还在分布任务下证明了强大的域概括。通过用明确的推理统一医学图像分析，MEDVLM-R1标志着在临床实践中朝着值得信赖和可解释的AI迈出的关键步骤。]]></description>
      <guid>https://arxiv.org/abs/2502.19634</guid>
      <pubDate>Fri, 28 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>虚拟现实中360度视频的持续感知质量评估的自适应得分对齐学习</title>
      <link>https://arxiv.org/abs/2502.19644</link>
      <description><![CDATA[ARXIV：2502.19644V1公告类型：新 
摘要：虚拟现实视频质量评估（VR-VQA）旨在评估360度视频的感知质量，这对于确保无失真的用户体验至关重要。在静态数据集中训练有限的失真多样性努力以平衡相关性和精度，传统的VR-VQA方法在静态数据集上进行了培训。当概括到不同的VR含量并不断适应动态和不断发展的视频分布变化时，这变得尤其重要。为了应对这些挑战，我们提出了一种新颖的方法来评估VR视频的感知质量，自适应得分对准学习（ASAL）。 ASAL将相关性损失与误差损失相结合，以增强与人类主观评分的一致性，并在预测感知质量方面的精确度。特别是，ASAL自然可以通过特征空间平滑过程来不断地更改分布，从而增强概括性对看不见的内容。为了进一步改善对动态VR环境的持续适应，我们随着新颖的持续学习（CL）框架而随着自适应记忆重播而扩展。与传统的CL模型不同，ASAL利用关键框架提取和功能适应来解决非稳态变化的独特挑战，并使用VR设备的计算和存储限制。我们为VR-VQA及其CL对应物建立了全面的基准，引入了新的数据分割和评估指标。我们的实验表明，在静态关节训练环境中，ASAL的表现优于最近的强基线模型，在静态关节训练环境中达到了高达4.78 \％的总体相关性增长，在各种数据集中的动态CL设置中达到了12.19 \％。这验证了ASAL在解决VR-VQA.our代码固有挑战方面的有效性，请访问https://github.com/zhoukanglei/asal_cvqa。]]></description>
      <guid>https://arxiv.org/abs/2502.19644</guid>
      <pubDate>Fri, 28 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>能节能3D点云降解的噪声尖峰图卷积</title>
      <link>https://arxiv.org/abs/2502.19660</link>
      <description><![CDATA[Arxiv：2502.19660V1公告类型：新 
摘要：受生物神经系统的尖峰计算范式启发的尖峰神经网络（SNN）在2D分类任务中表现出优于传统人工神经网络（ANN）的能源效率较高。但是，SNN的回归潜力尚未得到很好的探索，尤其是在3D点云处理中。在本文中，我们提出了注射噪声的尖峰图卷积网络，以利用SNN在3D点云降解中的完整回归潜力。具体而言，我们首先模拟注入噪声的神经元动力学，以构建注入噪声的尖峰神经元。在此基础上，我们设计了注射噪声的尖峰图卷积，用于促进3D点上的尖峰表示。从尖峰图卷积开始，我们构建了两个基于SNN的Denoising网络。一个是一个纯粹的尖峰图卷积网络，与某些基于ANN的替代方案相比，该网络的精度损失较低，同时导致两个基准数据集（PU-NET和PC-NET）的能耗大大降低。另一个是一种混合体系结构，将基于ANN的学习与仅几个时间步骤的高性能效率折衷相结合。我们的工作揭示了SNN的3D点云降级潜力，并注入了探索神经形态芯片部署的新观点，同时为开发节能3D数据采集设备铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2502.19660</guid>
      <pubDate>Fri, 28 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>医学图像分割的测试时间模态概括</title>
      <link>https://arxiv.org/abs/2502.19671</link>
      <description><![CDATA[ARXIV：2502.19671V1公告类型：新 
摘要：可推广的医学图像分割对于确保在各种看不见的临床环境中保持一致性至关重要。但是，现有方法通常忽略了在任意看不见的方式上有效概括的能力。在本文中，我们介绍了一种新型的测试时间模态概括（TTMG）框架，该框架包括两个核心组件：模态感知样的样式投影（MASP）和模态敏感的实例美白（MSIW），旨在增强在任意看不见的模态数据集中增强概括。 MASP估计了属于每种见过的模式的测试实例的可能性，并使用特定于模态样式碱基将其映射到分布中，从而有效地指导其投影。此外，由于高特征协方差阻碍了对看不见的方式的概括，因此在训练期间应用了MSIW，以选择性地抑制对模态敏感的信息，同时保留模态不变特征。通过整合MASP和MSIW，TTMG框架在看不见的方式中证明了医学图像分割的强大概括能力，这是当前方法在很大程度上被忽略的挑战。我们在十一个数据集中评估了TTMG以及跨越四种模式（结肠镜检查，超声，皮肤镜检查和放射学）的其他域概括技术，始终在各种模态组合中实现卓越的分割性能。]]></description>
      <guid>https://arxiv.org/abs/2502.19671</guid>
      <pubDate>Fri, 28 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过动态视觉语言对准攻击提高MLLM中的对抗性可传递性</title>
      <link>https://arxiv.org/abs/2502.19672</link>
      <description><![CDATA[ARXIV：2502.19672V1公告类型：新 
摘要：基于LLM的多模式大语言模型（MLLM）最近因其在图像识别和理解方面的能力而引起了人们的关注。但是，尽管MLLM容易受到对抗性攻击的影响，但这些攻击在不同模型中的转移性仍然有限，尤其是在目标攻击设置下。现有方法主要集中于特定视觉的扰动，但与视觉方式一致性的复杂性质斗争。在这项工作中，我们介绍了动态视觉语言对准（DynVLA）攻击，这是一种新颖的方法，将动态扰动注入视觉语言连接器中，以增强不同模型的各种视觉平行的概括。我们的实验结果表明，Dynvla显着提高了对抗性示例在各种MLLM中的转移性，包括Blip2，TenderchBlip，Minigpt4，Llava，Llava和Gemini等封闭式模型。]]></description>
      <guid>https://arxiv.org/abs/2502.19672</guid>
      <pubDate>Fri, 28 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Subzero：通过零击个性化组成主题，样式和行动</title>
      <link>https://arxiv.org/abs/2502.19673</link>
      <description><![CDATA[ARXIV：2502.19673V1公告类型：新 
摘要：扩散模型在生成任务中越来越流行，包括对象和样式的个性化组成。尽管扩散模型可以生成用户指定的主题，以自定义样式执行文本指导的操作，但它们需要微调，并且对于在移动设备上的个性化不可行。因此，无调的个性化方法（例如IP-适配器）逐渐获得了吸引力。但是，对于受试者和样式的组成，由于依赖控制网，或显示内容和样式泄漏伪像，这些作品的灵活性较小。为了解决这些问题，我们提出了Subzero，这是一个新颖的框架，可以以任何样式生成任何主题，执行任何动作而无需进行微调。我们提出了一组新颖的约束，以增强主题和样式相似性，同时减少泄漏。此外，我们在Denoising模型的交叉注意区块中提出了一个正交的时间聚合方案，有效地调理了文本提示，以及单个主题和样式图像。我们还提出了一种新颖的方法来训练定制的内容和样式投影仪，以减少内容和样式泄漏。通过广泛的实验，我们表明，我们提出的方法虽然适合在边缘运行，但对执行主题，样式和动作组成的最先进作品的改善表现出显着改善。]]></description>
      <guid>https://arxiv.org/abs/2502.19673</guid>
      <pubDate>Fri, 28 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MICINET：多层间令人困惑的信息删除，以进行可靠的多模式分类</title>
      <link>https://arxiv.org/abs/2502.19674</link>
      <description><![CDATA[ARXIV：2502.19674V1公告类型：新 
摘要：在存在嘈杂数据的情况下，可靠的多模式学习是一个广泛关注的问题，尤其是在关键安全应用中。许多可靠的多模式方法研究了解决模式特异性或跨模式噪声。但是，他们无法有效处理两种类型的噪声的共存。此外，缺乏对全球和个人级别噪声的全面考虑限制了其可靠性。为了解决这些问题，提出了一种可靠的多模式分类方法，称为多层阶层间令人困惑的信息删除网络（MICINET）。 Micinet通过将两种类型的噪声统一到阶层间令人困惑的信息（\ textit {ici}）的概念中来实现可靠的去除，并在全球和个体级别上都消除了噪声。具体而言，MiCinet首先可靠地通过所提出的\ textbf {\ textit {global \ textbf {ici}学习模块}}来可靠地学习全局\ textit {ici}分布。然后，它将\ textbf {\ textIt {全局引导的样本ICI学习模块}}从示例功能中有效地删除全局级别\ textit {ici}，利用示例功能，利用学习的global \ textit {ici {ici}分布。 Subsequently, the \textbf{\textit{Sample-adaptive Cross-modality Information Compensation module}} is designed to remove individual-level \textit{ICI} from each sample reliably.这是通过基于歧视特征与\ textit {ici}之间的互补关系以及对相对歧视能力引入的模态相对质量的感知来实现的。在四个数据集上的实验表明，在各种噪声条件下，MICINET优于其他最先进的可靠多模式分类方法。]]></description>
      <guid>https://arxiv.org/abs/2502.19674</guid>
      <pubDate>Fri, 28 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>朝着各种模糊区域的差异处理，以精确的图像去除</title>
      <link>https://arxiv.org/abs/2502.19677</link>
      <description><![CDATA[ARXIV：2502.19677V1公告类型：新 
摘要：图像Deblurring旨在通过消除不希望的退化来恢复高质量的图像。尽管现有方法产生了令人鼓舞的结果，但它们要么忽略了模糊图像的不同区域的不同程度的降解程度，要么通过堆叠众多非线性激活函数来近似非线性函数属性。在本文中，我们建议一个差分处理网络（DHNET）为不同的模糊区域执行差异处理。具体而言，我们设计了一个Volterra块（VBLOCK），以将非线性特性集成到Deblurring网络中，从而避免了以前的运行，将非线性激活功能的数量堆叠以映射复杂的输入输出关系。为了使该模型能够适应模糊区域的不同降解度，我们设计了降级学位识别专家模块（DDRE）。该模块最初将来自训练有素的模型的先验知识纳入估计空间可变的模糊信息。因此，路由器可以根据降解程度和区域的大小来绘制学习的降解表示形式，并将权重分配给专家。全面的实验结果表明，DHNET有效地超过了合成数据集和现实数据集的最新方法（SOTA）方法。]]></description>
      <guid>https://arxiv.org/abs/2502.19677</guid>
      <pubDate>Fri, 28 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于M-LLM的视频框架选择，以进行有效的视频理解</title>
      <link>https://arxiv.org/abs/2502.19680</link>
      <description><![CDATA[ARXIV：2502.19680V1公告类型：新 
摘要：多模式大语言模型（M-LLM）的最新进展在视频推理中显示出令人鼓舞的结果。流行的多模式大型语言模型（M-LLM）框架通常应用幼稚的统一抽样，以减少被送入M-LLM的视频帧数量，尤其是对于长上下文视频。但是，在视频的某些时期，它可能会失去关键环境，因此下游M-LLM可能没有足够的视觉信息来回答问题。为了攻击这个疼痛点，我们提出了一种基于M-LLM的轻质框架选择方法，该方法适应性地选择了与用户查询更相关的帧。为了训练提出的框架选择器，我们引入了两个监督信号（i）空间信号，其中单帧的重要性得分通过提示m-llm； （ii）时间信号，其中通过所有框架候选者的标题提示大型语言模型（LLM）来选择多个帧。然后，通过冷冻的下游视频M-LLM来消化所选的帧，以进行视觉推理和问答。经验结果表明，所提出的M-LLM视频框架选择器改善了跨介质（ActivityNet，Next-QA）和Long（Egoschema，longVideObench）上下文视频的各种下游视频大型语言模型（视频-LLM）的表演。]]></description>
      <guid>https://arxiv.org/abs/2502.19680</guid>
      <pubDate>Fri, 28 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>