<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Thu, 30 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>分离的模态间/模态内融合促进组合零样本学习</title>
      <link>https://arxiv.org/abs/2501.17171</link>
      <description><![CDATA[arXiv:2501.17171v1 公告类型：新
摘要：组合零样本学习 (CZSL) 旨在通过在训练期间使用已知和未知概念来识别含义的细微差异或状态和对象的组合。现有方法要么侧重于提示配置，要么侧重于使用提示来调整预训练的视觉语言模型。然而，这些方法在准确识别含义的细微差异或将状态与对象结合起来方面面临挑战。为了共同消除上述问题并构建一种高效且有效的 CZSL 技术，我们提出了一种通过在涉及细微语义差异和多个对象的场景理解中利用具有跨/内模态融合合成器的多样化提示学习来提高属性识别性能的方法。]]></description>
      <guid>https://arxiv.org/abs/2501.17171</guid>
      <pubDate>Thu, 30 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ViT-2SPN：基于视觉变换器的双流自监督预训练网络，用于视网膜 OCT 分类</title>
      <link>https://arxiv.org/abs/2501.17260</link>
      <description><![CDATA[arXiv:2501.17260v1 公告类型：新
摘要：光学相干断层扫描 (OCT) 是一种非侵入性成像方式，对于诊断各种眼部疾病至关重要。尽管具有临床意义，但开发基于 OCT 的诊断工具仍面临挑战，例如有限的公共数据集、稀疏注释和隐私问题。尽管深度学习在自动化 OCT 分析方面取得了进展，但这些挑战仍未解决。为了解决这些限制，我们引入了基于 Vision Transformer 的双流自监督预训练网络 (ViT-2SPN)，这是一种旨在增强特征提取和提高诊断准确性的新颖框架。ViT-2SPN 采用三阶段工作流程：监督预训练、自监督预训练 (SSP) 和监督微调。预训练阶段利用 OCTMNIST 数据集（四个疾病类别中的 97,477 张未标记图像）进行数据增强以创建双增强视图。 Vision Transformer (ViT-Base) 主干提取特征，而负余弦相似度损失对齐特征表示。预训练进行了 50 个时期，学习率为 0.0001，动量为 0.999。使用 10 倍交叉验证对 OCTMNIST 的分层 5.129% 子集进行微调。ViT-2SPN 的平均 AUC 为 0.93，准确率为 0.77，精确率为 0.81，召回率为 0.75，F1 得分为 0.76，优于现有的基于 SSP 的方法。]]></description>
      <guid>https://arxiv.org/abs/2501.17260</guid>
      <pubDate>Thu, 30 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>风格转变下新颖性检测的对比师生框架</title>
      <link>https://arxiv.org/abs/2501.17289</link>
      <description><![CDATA[arXiv:2501.17289v1 公告类型：新
摘要：已经进行了许多努力来提高新颖性检测 (ND) 的性能。然而，ND 方法在环境变化导致的轻微分布变化（称为风格变化）下通常会遭受显着的性能下降。这一挑战源于 ND 设置，其中训练期间缺乏分布外 (OOD) 样本导致检测器偏向分布内 (ID) 数据中的主导风格特征。结果，模型错误地学习将风格与核心特征相关联，使用这种捷径进行检测。稳健的 ND 对于自动驾驶和医学成像等现实世界的应用至关重要，其中测试样本可能具有与训练数据不同的风格。受此启发，我们提出了一种稳健的 ND 方法，该方法制作一个辅助 OOD 集，其风格特征与 ID 集相似，但具有不同的核心特征。然后，利用基于任务的知识提炼策略来区分核心特征和风格特征，并帮助我们的模型依靠核心特征来区分精心制作的 OOD 和 ID 集。我们通过对多个数据集（包括合成和真实基准）进行大量实验评估，对比九种不同的 ND 方法，验证了我们方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2501.17289</guid>
      <pubDate>Thu, 30 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>WASUP：具有权重输入对齐和类别判别支持向量的可解释分类</title>
      <link>https://arxiv.org/abs/2501.17328</link>
      <description><![CDATA[arXiv:2501.17328v1 公告类型：新
摘要：在关键领域部署深度学习模型需要在高精度和可解释性之间取得平衡。我们引入了 WASUP，这是一种本质上可解释的神经网络，可为其决策过程提供局部和全局解释。我们通过满足既定的解释公理来证明这些解释是可靠的。利用基于案例的推理的概念，WASUP 从训练图像中提取类代表性支持向量，确保它们捕获相关特征同时抑制不相关特征。通过计算和汇总这些支持向量与输入的潜在特征向量之间的相似度得分来做出分类决策。我们采用 B-Cos 变换，将模型权重与输入对齐，以实现将潜在特征忠实地映射回输入空间，从而促进基于案例的推理的局部解释和全局解释。我们对 WASUP 进行了三项评估：对 Stanford Dogs 进行细粒度分类、对 Pascal VOC 进行多标签分类以及对 RSNA 数据集进行病理检测。结果表明，WASUP 不仅与最先进的黑盒模型相比具有竞争力，而且还提供了经过理论分析验证的深刻解释。我们的研究结果强调了 WASUP 在理解模型决策与决策本身同样重要的应用中的潜力。]]></description>
      <guid>https://arxiv.org/abs/2501.17328</guid>
      <pubDate>Thu, 30 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>3D 医学图像分割的训练后量化：真实推理引擎的实践研究</title>
      <link>https://arxiv.org/abs/2501.17343</link>
      <description><![CDATA[arXiv:2501.17343v1 公告类型：新
摘要：量化深度神经网络，降低其计算的精度（位宽），可以显著减少内存使用量并加速处理，使这些模型更适合计算资源有限的大规模医学成像应用。然而，许多现有方法研究了“假量化”，它在推理过程中模拟了较低精度的操作，但实际上并没有减小模型大小或提高实际推理速度。此外，在现代 GPU 上部署真正的 3D 低位量化的潜力仍未得到探索。在本研究中，我们引入了一个真正的训练后量化 (PTQ) 框架，该框架成功地在最先进的 (SOTA) 3D 医学分割模型（即 U-Net、SegResNet、SwinUNETR、nnU-Net、UNesT、TransUNet、ST-UNet 和 VISTA3D）上实现了真正的 8 位量化。我们的方法涉及两个主要步骤。首先，我们使用 TensorRT 对权重和激活执行假量化，并使用未标记的校准数据集。其次，我们通过真实 GPU 上的 TensorRT 引擎将此假量化转换为真实量化，从而实际减少模型大小和推理延迟。大量实验表明，我们的框架可以在 GPU 上有效地执行 8 位量化，而不会牺牲模型性能。这一进步使得在计算资源受限的医学成像应用中部署高效的深度学习模型成为可能。代码和模型已经发布，包括在 BTCV 数据集上预训练的用于腹部（13 个标签）分割的 U-Net、TransUNet、在全脑数据集上预训练的用于全脑（133 个标签）分割的 UNesT，以及在 TotalSegmentator V2 上预训练的用于全身（104 个标签）分割的 nnU-Net、SegResNet、SwinUNETR 和 VISTA3D。https://github.com/hrlblab/PTQ。]]></description>
      <guid>https://arxiv.org/abs/2501.17343</guid>
      <pubDate>Thu, 30 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>水印的共存与集成</title>
      <link>https://arxiv.org/abs/2501.17356</link>
      <description><![CDATA[arXiv:2501.17356v1 公告类型：新
摘要：水印是将不可察觉的信息嵌入图像、视频、音频和文本等媒体的做法，对于知识产权保护、内容出处和归属至关重要。数字生态系统日益复杂，需要在同一媒体中嵌入不同用途的水印。然而，要检测和解码所有水印，它们需要很好地共存。我们首次研究了深度图像水印方法的共存，与直觉相反，我们发现各种开源水印可以共存，对图像质量和解码鲁棒性的影响很小。水印的共存也为集成水印方法开辟了道路。我们展示了如何通过集成来增加整体消息容量，并在容量、准确性、鲁棒性和图像质量之间实现新的权衡，而无需重新训练基础模型。]]></description>
      <guid>https://arxiv.org/abs/2501.17356</guid>
      <pubDate>Thu, 30 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>评估基于 YOLO 和 Transformer 的物体检测器的实时杂草检测能力</title>
      <link>https://arxiv.org/abs/2501.17387</link>
      <description><![CDATA[arXiv:2501.17387v2 公告类型：新
摘要：点喷是一种有效且可持续的方法，可减少农田中使用的农药（特别是除草剂）的数量。为实现这一目标，在现场和实时条件下可靠地区分作物和杂草，甚至区分单个杂草种类至关重要。为了评估实时应用的适用性，对目前最先进的不同物体检测模型进行了比较。YOLOv8、YOLOv9、YOLOv10 和 RT-DETR 的所有可用模型都经过训练，并使用来自真实田间情况的图像进行评估。图像被分成两个不同的数据集：在初始数据集中，每种植物都经过单独训练；在后续数据集中，区分单子叶杂草、双子叶杂草和三种选定的作物。结果表明，虽然所有模型在评估指标中的表现同样出色，但 YOLOv9 模型（尤其是 YOLOv9s 和 YOLOv9e）在强大的召回率（66.58％ 和 72.36％）以及 mAP50（73.52％ 和 79.86％）和数据集 2 中的 mAP50-95（43.82％ 和 47.00％）方面脱颖而出。然而，RT-DETR 模型（尤其是 RT-DETR-l）的精确度表现出色，在数据集 1 上达到 82.44％，在数据集 2 中达到 81.46％，这使得它们特别适合于最小化假阳性至关重要的场景。具体来说，YOLO 模型的最小变体（YOLOv8n、YOLOv9t 和 YOLOv10n）在 NVIDIA GeForce RTX 4090 GPU 上对数据集 2 进行一帧分析的推理时间显著缩短至 7.58 毫秒，同时保持了具有竞争力的准确性，突显了它们在资源受限的嵌入式计算设备中的部署潜力，这些设备通常用于生产设置。]]></description>
      <guid>https://arxiv.org/abs/2501.17387</guid>
      <pubDate>Thu, 30 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>学习多模态法学硕士 (LLM) 的免费标记减少</title>
      <link>https://arxiv.org/abs/2501.17391</link>
      <description><![CDATA[arXiv:2501.17391v1 公告类型：新
摘要：视觉语言模型 (VLM) 在一系列多模态任务中取得了显著的成功；然而，它们的实际部署往往受到高计算成本和延长推理时间的限制。由于视觉模态通常比文本模态携带更多信息，因此压缩视觉提示提供了一种有希望的解决方案来缓解这些挑战。现有方法主要侧重于改进模型架构或直接减少视觉标记的数量。然而，由于缺乏对视觉数据独特的空间和时间特征的考虑，这些方法往往会损害推理性能。在这项工作中，我们提出了一种在空间和时间维度上运行的标记压缩范式。我们的方法包括一个无需学习、即插即用的压缩管道，可以无缝集成到大多数多模态大型语言模型 (MLLM) 框架中。通过利用这种方法，我们增强了模型推理能力，同时降低了其计算成本。 Video-QA 任务上的实验结果证明了所提出方法的有效性，在不牺牲性能的情况下显著提高了效率。]]></description>
      <guid>https://arxiv.org/abs/2501.17391</guid>
      <pubDate>Thu, 30 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>视觉和语言导航的通用场景适配</title>
      <link>https://arxiv.org/abs/2501.17403</link>
      <description><![CDATA[arXiv:2501.17403v1 公告类型：新
摘要：视觉和语言导航 (VLN) 任务主要基于在多个环境中一次性执行单个指令来评估代理，旨在开发能够在任何环境中以零样本方式运行的代理。然而，现实世界的导航机器人通常在具有相对一致的物理布局、视觉观察和来自指导员的语言风格的持久环境中运行。任务设置中的这种差距为通过结合对特定环境的持续适应来改进 VLN 代理提供了机会。为了更好地反映这些现实世界的条件，我们引入了 GSA-VLN，这是一项新颖的任务，要求代理在特定场景中执行导航指令并同时适应它以随着时间的推移提高性能。为了评估提出的任务，必须解决现有 VLN 数据集中的两个挑战：缺乏 OOD 数据，以及每个场景的指令数量和风格多样性有限。因此，我们提出了一个新的数据集 GSA-R2R，它显著扩展了 R2R 数据集的环境和指令的多样性和数量，以评估代理在 ID 和 OOD 环境中的适应性。此外，我们设计了一个三阶段指令编排管道，利用 LLM 来改进说话者生成的指令，并应用角色扮演技术将指令改写成不同的说话风格。这是基于以下观察：每个用户的指令中通常都有一致的签名或偏好。我们对 GSA-R2R 进行了广泛的实验，以彻底评估我们的数据集并对各种方法进行基准测试。根据我们的研究结果，我们提出了一种新方法 GR-DUET，它将基于内存的导航图与特定于环境的训练策略结合起来，在所有 GSA-R2R 分割上都取得了最先进的结果。]]></description>
      <guid>https://arxiv.org/abs/2501.17403</guid>
      <pubDate>Thu, 30 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SIGN：用于预测注视时间的统计注视网络</title>
      <link>https://arxiv.org/abs/2501.17422</link>
      <description><![CDATA[arXiv:2501.17422v1 公告类型：新
摘要：我们提出了 SIGN 的第一个版本，即统计信息凝视网络，用于预测图像上的总体凝视时间。我们开发了一个基础统计模型，并为此推导出涉及 CNN 和 Visual Transformers 的深度学习实现，从而能够预测总体凝视时间。该模型使我们能够从总体凝视时间中得出底层凝视模式，作为图像中所有区域的概率图，其中每个区域的概率表示在所有可能的扫描路径上被凝视的可能性。我们在 AdGaze3500（包含总体凝视时间的广告图像数据集）和 COCO-Search18（包含搜索期间收集的个人级注视模式的数据集）上测试了 SIGN 的性能。我们证明，SIGN (1) 在两个数据集上都显著提高了凝视持续时间预测，远超最先进的深度学习基准，并且 (2) 可以提供与 COCO-Search18 中的经验注视模式相对应的合理凝视模式。这些结果表明，SIGN 的第一个版本在凝视时间预测方面大有可为，值得进一步开发。]]></description>
      <guid>https://arxiv.org/abs/2501.17422</guid>
      <pubDate>Thu, 30 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使流程图图像可机器解释</title>
      <link>https://arxiv.org/abs/2501.17441</link>
      <description><![CDATA[arXiv:2501.17441v1 公告类型：新
摘要：计算机编程教科书和软件文档通常包含流程图来说明算法或程序的流程。现代 OCR 引擎通常将这些流程图标记为图形并在进一步处理中忽略它们。在本文中，我们致力于通过将流程图图像转换为可执行的 Python 代码来使流程图图像可机器解释。为此，受最近自然语言到代码生成文献中成功的启发，我们提出了一种基于变换器的新型框架，即 FloCo-T5。我们的模型非常适合这项任务，因为它可以有效地学习编程语言的语义、结构和模式，并利用这些来生成语法正确的代码。我们还使用特定于任务的预训练目标，使用大量保留逻辑的增强代码样本对 FloCo-T5 进行预训练。此外，为了对这个问题进行严格的研究，我们引入了包含 11,884 张流程图图像及其对应的 Python 代码的 FloCo 数据集。我们的实验结果令人鼓舞，FloCo-T5 在代码生成指标上明显优于相关竞争基线。我们公开了我们的数据集和实现。]]></description>
      <guid>https://arxiv.org/abs/2501.17441</guid>
      <pubDate>Thu, 30 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用快速迭代重噪法求解扩散逆问题</title>
      <link>https://arxiv.org/abs/2501.17468</link>
      <description><![CDATA[arXiv:2501.17468v1 公告类型：新
摘要：可以使用预训练的扩散模型以无监督的方式解决成像逆问题。在大多数情况下，这涉及在逆过程中近似测量条件得分函数的梯度。由于现有方法产生的近似值非常差，特别是在逆过程的早期，我们提出了一种新方法，在每个扩散步骤中对图像进行多次重新估计和重新噪声化。重新噪声化添加了精心塑造的有色噪声，确保预训练的扩散模型看到白高斯误差，这与训练方式一致。我们证明了我们的“DDfire”方法在 20、100 和 1000 次神经功能评估中对线性逆问题和相位检索的有效性。]]></description>
      <guid>https://arxiv.org/abs/2501.17468</guid>
      <pubDate>Thu, 30 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>3DSES：具有来自 3D 模型的真实和伪标签的室内激光雷达点云分割数据集</title>
      <link>https://arxiv.org/abs/2501.17534</link>
      <description><![CDATA[arXiv:2501.17534v1 公告类型：新
摘要：室内点云的语义分割已在机器人、导航和建筑信息模型 (BIM) 的数字孪生创建中得到广泛应用。然而，大多数现有的标记室内点云数据集都是通过摄影测量获得的。相比之下，地面激光扫描 (TLS) 可以获取密集的亚厘米点云，并已成为测量员的标准。我们提出了 3DSES（ESGT 点云的 3D 分割），这是一个新的室内密集 TLS 彩色点云数据集，覆盖了一所工程学校的 427 平方米。3DSES 具有独特的双注释格式：在点级别注释的语义标签与建筑物的完整 3D CAD 模型一起。我们引入了一种模型到云算法，用于使用现有的 3D CAD 模型自动标记室内点云。3DSES 有 3 种不同语义和几何复杂性的变体。我们表明，我们的模型到云对齐可以在点云上生成伪标签，准确率高达 95%，与手动标记相比，这使我们能够以显著节省时间的方式训练深度模型。3DSES 上的第一个基线显示了现有模型在分割与 BIM 相关的对象（例如照明和安全设施）时遇到的困难。我们表明，可以利用伪标签和激光雷达强度​​来提高分割精度，这是当前数据集中很少考虑的信息。代码和数据将开源。]]></description>
      <guid>https://arxiv.org/abs/2501.17534</guid>
      <pubDate>Thu, 30 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 3D 生成模型实现无需训练的开放世界分类</title>
      <link>https://arxiv.org/abs/2501.17547</link>
      <description><![CDATA[arXiv:2501.17547v1 公告类型：新
摘要：3D 开放世界分类是动态和非结构化现实世界场景中一项具有挑战性但必不可少的任务，需要开放类别和开放姿势识别。为了应对这些挑战，最近的智慧通常采用复杂的 2D 预训练模型来提供丰富而稳定的表示。然而，这些方法很大程度上依赖于如何将 3D 对象投影到 2D 空间中，不幸的是，这个问题没有得到很好的解决，因此大大限制了它们的性能。与目前的这些努力不同，在本文中，我们对 3D 开放世界分类的 3D 生成模型进行了开创性的探索。利用来自 3D 生成模型的丰富先验知识，我们还设计了一个旋转不变的特征提取器。这种创新的协同作用使我们的流程具有无需训练、开放类别和姿势不变的优势，因此非常适合 3D 开放世界分类。在基准数据集上进行的大量实验证明了生成模型在 3D 开放世界分类中的潜力，在 ModelNet10 和 McGill 上取得了最先进的性能，整体准确率分别提高了 32.0% 和 8.7%。]]></description>
      <guid>https://arxiv.org/abs/2501.17547</guid>
      <pubDate>Thu, 30 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用时间移位模块和集成学习进行动作识别</title>
      <link>https://arxiv.org/abs/2501.17550</link>
      <description><![CDATA[arXiv:2501.17550v1 公告类型：新
摘要：本文介绍了多模态动作识别挑战赛的一流解决方案，该挑战赛是 \acl{ICPR} 2024 多模态视觉模式识别研讨会的一部分。该竞赛旨在使用从多模态源收集的 20 个动作类别的多样化数据集来识别人类动作。所提出的方法建立在 \acl{TSM} 的基础上，该技术旨在有效捕获视频数据中的时间动态，并结合多种数据输入类型。我们的策略包括迁移学习以利用预先训练的模型，然后对挑战的特定数据集进行细致的微调，以优化 20 个动作类别的性能。我们精心选择了一个主干网络来平衡计算效率和识别准确性，并使用集成不同模态输出的集成技术进一步完善模型。事实证明，这种集成方法对于提高整体性能至关重要。我们的解决方案在测试集上实现了完美的 top-1 准确率，证明了所提出的方法在识别 20 个类别的人类动作方面的有效性。我们的代码可在线获取 https://github.com/ffyyytt/TSM-MMVPR。]]></description>
      <guid>https://arxiv.org/abs/2501.17550</guid>
      <pubDate>Thu, 30 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>