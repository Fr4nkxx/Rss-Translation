<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Fri, 22 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>公共卫生倡导数据集：来自社交媒体的烟草使用视频数据集</title>
      <link>https://arxiv.org/abs/2411.13572</link>
      <description><![CDATA[arXiv:2411.13572v1 公告类型：新
摘要：公共卫生宣传数据集 (PHAD) 是一个全面的集合，包含来自 TikTok 和 YouTube 等社交媒体平台的 5,730 个与烟草产品相关的视频。该数据集包含 430 万帧，并包括详细的元数据，例如用户参与度指标、视频描述和搜索关键字。这是第一个具有这些特征的数据集，为分析与烟草相关的内容及其影响提供了宝贵的资源。我们的研究采用两阶段分类方法，结合视觉语言 (VL) 编码器，在准确分类各种类型的烟草产品和使用场景方面表现出色。分析揭示了显着的用户参与趋势，特别是在吸电子烟和电子烟内容方面，突出了有针对性的公共卫生干预领域。PHAD 解决了公共卫生研究中对多模式数据的需求，提供了可以为监管政策和公共卫生战略提供信息的见解。该数据集是了解和减轻烟草使用影响的关键一步，确保公共卫生工作更加包容和有效。]]></description>
      <guid>https://arxiv.org/abs/2411.13572</guid>
      <pubDate>Fri, 22 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>COOD：基于概念的零样本 OOD 检测</title>
      <link>https://arxiv.org/abs/2411.13578</link>
      <description><![CDATA[arXiv:2411.13578v1 公告类型：新
摘要：模型如何在无需大量再训练的情况下有效检测复杂多标签设置中的分布外 (OOD) 样本？现有的 OOD 检测方法难以捕捉多标签设置中固有的复杂语义关系和标签共现，通常需要大量训练数据并且无法推广到看不见的标签组合。虽然大型语言模型彻底改变了零样本 OOD 检测，但它们主要关注单标签场景，在处理样本可能与多个相互依赖的标签相关联的实际任务时留下了关键空白。为了应对这些挑战，我们引入了 COOD，一种新颖的零样本多标签 OOD 检测框架。COOD 利用预先训练的视觉语言模型，通过基于概念的标签扩展策略和新的评分函数对其进行增强。通过为每个标签用正面和负面概念丰富语义空间，我们的方法可以模拟复杂的标签依赖关系，无需额外训练即可精确区分 OOD 样本。大量实验表明，我们的方法明显优于现有方法，在 VOC 和 COCO 数据集上实现约 95% 的平均 AUROC，同时在不同数量的标签和不同类型的 OOD 样本中保持稳健的性能。]]></description>
      <guid>https://arxiv.org/abs/2411.13578</guid>
      <pubDate>Fri, 22 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>深度特征响应判别校准</title>
      <link>https://arxiv.org/abs/2411.13582</link>
      <description><![CDATA[arXiv:2411.13582v1 公告类型：新
摘要：深度神经网络 (DNN) 在各个领域都有广泛的应用。已经提出了几种优化技术，例如 ResNet 和 SENet，以提高模型准确性。这些技术通过根据统一标准调整或校准特征响应来提高模型性能。然而，它们缺乏对不同特征的判别校准，从而在模型输出中引入了限制。因此，我们提出了一种判别校准特征响应的方法。初步实验结果表明，神经特征响应遵循高斯分布。因此，我们采用高斯概率密度函数计算置信度值，然后将这些值与原始响应值相积分。这种整合的目的是提高神经特征响应的特征可辨别性。基于校准值，我们提出了一个基于插件的校准模块，该模块合并到改进的 ResNet 架构中，称为响应校准网络 (ResCNet)。在 CIFAR-10、CIFAR-100、SVHN 和 ImageNet 等数据集上进行的大量实验证明了所提方法的有效性。开发的代码可在 https://github.com/tcmyxc/ResCNet 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2411.13582</guid>
      <pubDate>Fri, 22 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>揭示扩散变压器 (DiT) 中的冗余：一项系统研究</title>
      <link>https://arxiv.org/abs/2411.13588</link>
      <description><![CDATA[arXiv:2411.13588v1 公告类型：新 
摘要：扩散变压器 (DiT) 的模型容量增加以及对生成更高分辨率图像和视频的需求导致推理延迟显着增加，对实时性能产生不利影响。虽然先前的研究强调了相邻扩散步骤之间激活值的高度相似性（称为冗余）并提出了各种缓存机制来减轻计算开销，但现有文献中对冗余的探索仍然有限，研究结果通常不能推广到不同的 DiT 模型中。本研究旨在通过对广泛的主流 DiT 模型中的冗余进行全面调查来解决这一差距。我们的实验分析揭示了不同 DiT 模型中扩散步骤之间冗余分布的巨大差异。有趣的是，在单个模型中，无论输入提示、步数或调度策略如何变化，冗余分布都保持稳定。由于不同模型之间缺乏一致的模式，为特定模型组设计的缓存策略可能无法轻易转移到其他模型上。为了克服这一挑战，我们引入了一种用于分析单个模型冗余的工具，使后续研究能够为特定模型架构开发定制的缓存策略。该项目已公开发布，网址为 https://github.com/xdit-project/DiTCacheAnalysis。]]></description>
      <guid>https://arxiv.org/abs/2411.13588</guid>
      <pubDate>Fri, 22 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>深度学习水道助力农村基础设施建设</title>
      <link>https://arxiv.org/abs/2411.13590</link>
      <description><![CDATA[arXiv:2411.13590v1 公告类型：新
摘要：令人惊讶的是，地球上许多水道仍未绘制，其中相当一部分位于中低收入国家。我们在此构建了一个计算机视觉模型（WaterNet），基于高分辨率卫星图像和数字高程模型来了解美国水道的位置，然后将其部署到非洲大陆的新环境中。我们的输出提供了迄今为止未绘制的水道结构的详细信息。当根据社区对农村桥梁建设的需求要求进行评估时，我们发现这些新生成的水道平均满足了 93%（国家范围：88-96%）的这些要求，而 Open Street Map 和 TDX-Hydro 的最新数据分别只满足了 36%（5-72%）和 62%（37%-85%）。由于这些新的机器学习地图是基于公共和运营数据采集而构建的，因此这种方法有望捕捉人道主义需求，并规划迄今为止制图工作未能实现的社会发展。在识别现有数据遗漏的社区需求方面，其性能的提高表明，这对于农村基础设施建设和更好地针对发展干预措施具有重要价值。]]></description>
      <guid>https://arxiv.org/abs/2411.13590</guid>
      <pubDate>Fri, 22 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过迭代缩小来改进 GUI 基础</title>
      <link>https://arxiv.org/abs/2411.13591</link>
      <description><![CDATA[arXiv:2411.13591v1 公告类型：新
摘要：GUI 基础，即从自然语言查询中识别界面图像上的精确位置的任务，在增强视觉语言模型 (VLM) 代理的功能方面起着至关重要的作用。虽然通用 VLM（例如 GPT-4V）在各种任务中都表现出色，但它们在 GUI 基础方面的熟练程度仍然不够理想。最近的研究重点是专门针对一次性 GUI 基础对这些模型进行微调，从而显著提高基线性能。我们引入了一个称为迭代缩小 (IN) 的视觉提示框架，以进一步增强通用和微调模型在 GUI 基础方面的性能。为了进行评估，我们在包含不同 UI 平台的综合基准上测试了我们的方法。]]></description>
      <guid>https://arxiv.org/abs/2411.13591</guid>
      <pubDate>Fri, 22 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>迈向无障碍学习：基于深度学习的潜在书写障碍检测和潜在书写障碍手写体的 OCR</title>
      <link>https://arxiv.org/abs/2411.13595</link>
      <description><![CDATA[arXiv:2411.13595v1 公告类型：新
摘要：书写障碍是一种影响书写能力的学习障碍，使儿童难以清晰一致地书写。早期发现和监测对于提供及时的支持和干预至关重要。本研究应用深度学习技术来解决对可能患有书写障碍症状的儿童手写样本进行书写障碍检测和光学字符识别 (OCR) 的双重任务。使用来自马来西亚学童的手写样本数据集，我们开发了一个自定义卷积神经网络 (CNN) 模型，以及 VGG16 和 ResNet50，将笔迹分类为书写障碍或非书写障碍。自定义 CNN 模型优于预训练模型，测试准确率为 91.8%，具有高精度、召回率和 AUC，证明了其在识别书写障碍手写特征方面的稳健性。此外，还创建了一个 OCR 管道来分割和识别书写障碍手写体中的单个字符，字符识别准确率约为 43.5%。这项研究凸显了深度学习在支持书写障碍评估方面的潜力，为帮助教育工作者和临床医生识别书写障碍并跟踪书写进度的工具奠定了基础。这些发现有助于学习障碍辅助技术的进步，为在教育和临床环境中提供更方便、更准确的诊断工具带来了希望。]]></description>
      <guid>https://arxiv.org/abs/2411.13595</guid>
      <pubDate>Fri, 22 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>增强双向手语交流：集成 YOLOv8 与 NLP 实现实时手势识别和翻译</title>
      <link>https://arxiv.org/abs/2411.13597</link>
      <description><![CDATA[arXiv:2411.13597v1 公告类型：新
摘要：本研究的主要关注点是通过实时摄像机镜头获取美国手语 (ASL) 数据，并能够将数据和信息转换为文本。除此之外，我们还专注于创建一个框架，该框架也可以实时将文本转换为手语，这可以帮助我们为有需要的人打破语言障碍。在这项工作中，为了识别美国手语 (ASL)，我们使用了 You Only Look Once (YOLO) 模型和卷积神经网络 (CNN) 模型。YOLO 模型实时运行，无需任何先验知识即可自动从原始视频流中提取判别性时空特征，从而消除设计缺陷。这里的 CNN 模型也实时运行以进行手语检测。我们引入了一种将基于文本的输入转换为手语的新方法，即创建一个框架，该框架将以一个句子作为输入，从该句子中识别出关键词，然后显示一段视频，其中实时根据输入的句子执行手语。据我们所知，这是一项罕见的研究，展示了美国手语 (ASL) 中实时的双向手语交流。]]></description>
      <guid>https://arxiv.org/abs/2411.13597</guid>
      <pubDate>Fri, 22 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>RadPhi-3：放射学小型语言模型</title>
      <link>https://arxiv.org/abs/2411.13604</link>
      <description><![CDATA[arXiv:2411.13604v1 公告类型：新
摘要：基于 LLM 的副驾驶助手在日常任务中很有用。在以可靠的方式支持放射学工作流程的 AI 助手用例的探索中，出现了激增。在这项工作中，我们提出了 RadPhi-3，这是一种从 Phi-3-mini-4k-instruct 调整而来的小型语言模型指令，具有 3.8B 个参数，可协助完成放射学工作流程中的各种任务。虽然印象摘要生成是先前关于胸部 X 光放射学报告的工作中探索的主要任务，但我们还探索了其他有用的任务，例如比较当前放射学报告和其先前报告的变化摘要生成、从放射学报告中提取部分、用各种病理和管、线或设备标记报告等。此外，指令调整 RadPhi-3 涉及从放射科医生使用的可靠知识来源 Radiopaedia.org 学习。 RadPhi-3 既可用于为放射学相关查询提供可靠答案，也可用于执行与放射学报告相关的有用任务。RadPhi-3 在 RaLEs 放射学报告生成基准上取得了 SOTA 结果。]]></description>
      <guid>https://arxiv.org/abs/2411.13604</guid>
      <pubDate>Fri, 22 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>VioPose：通过分层视听推理进行小提琴表演 4D 姿势估计</title>
      <link>https://arxiv.org/abs/2411.13607</link>
      <description><![CDATA[arXiv:2411.13607v1 公告类型：新
摘要：音乐家巧妙地控制自己的身体来创作音乐。有时，他们的动作太细微，人眼无法捕捉到。为了分析他们如何移动来创作音乐，我们需要估计精确的 4D 人体姿势（随时间变化的 3D 姿势）。然而，由于遮挡、部分视图和人与物体的相互作用，当前最先进的 (SoTA) 视觉姿势估计算法难以产生准确的单眼 4D 姿势。它们受到摄像机的视角、像素密度和采样率的限制，无法估计快速和细微的动作，例如颤音的音乐效果。我们利用所创作的音乐与创作它们的人体动作之间的直接因果关系来解决这些挑战。我们提出了 VioPose：一种分层估计动态的新型多模态网络。高级特征级联到低级特征并集成到贝叶斯更新中。我们的架构被证明可以生成准确的姿势序列，促进精确的运动分析，并且优于 SoTA。作为这项工作的一部分，我们收集了最大、最多样化的校准小提琴演奏数据集，包括视频、声音和 3D 运动捕捉姿势。项目页面：可在 https://sj-yoo.info/viopose/ 上找到。]]></description>
      <guid>https://arxiv.org/abs/2411.13607</guid>
      <pubDate>Fri, 22 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>眼见为实：一种基于视觉和物理的新型视频生成质量评估指标</title>
      <link>https://arxiv.org/abs/2411.13609</link>
      <description><![CDATA[arXiv:2411.13609v1 公告类型：新
摘要：随着视频生成模型的快速发展，评估生成视频的质量变得越来越重要。现有的指标，例如 Fr\&#39;echet 视频距离 (FVD)、初始分数 (IS) 和 ClipSim，主要在潜在空间而不是从人类视觉角度来衡量质量，往往忽略了外观和运动与物理定律的一致性等关键方面。在本文中，我们提出了一种新颖的指标 VAMP（视觉外观和运动合理性），用于评估生成视频的视觉外观和物理合理性。VAMP 由两个主要部分组成：外观分数，用于评估跨帧的颜色、形状和纹理一致性，以及运动分数，用于评估物体运动的真实性。我们通过两个实验来验证 VAMP：损坏的视频评估和生成的视频评估。在损坏的视频评估中，我们将各种类型的损坏引入真实视频中，并测量损坏严重程度与 VAMP 分数之间的相关性。在生成的视频评估中，我们使用最先进的模型根据精心设计的提示生成视频，并将 VAMP 的表现与人类评估者的排名进行比较。我们的结果表明，VAMP 有效地捕捉了视觉保真度和时间一致性，提供了比传统方法更全面的视频质量评估。]]></description>
      <guid>https://arxiv.org/abs/2411.13609</guid>
      <pubDate>Fri, 22 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Video2BEV：将无人机视频转换为 BEV，实现基于视频的地理定位</title>
      <link>https://arxiv.org/abs/2411.13610</link>
      <description><![CDATA[arXiv:2411.13610v1 公告类型：新
摘要：现有的无人机视觉地理定位方法主要采用基于图像的设置，其中单个无人机视图快照与来自其他平台的图像进行匹配。然而，这种任务制定未充分利用无人机固有的视频输出，并且对遮挡和环境限制敏感。为了解决这些限制，我们制定了一个新的基于视频的无人机地理定位任务并提出了 Video2BEV 范式。该范式将视频转换为鸟瞰图 (BEV)，简化了后续的匹配过程。特别是，我们采用高斯 Splatting 来重建 3D 场景并获得 BEV 投影。与现有的变换方法（例如极坐标变换）不同，我们的 BEV 保留了更细粒度的细节，而没有明显的失真。为了进一步提高模型对各种 BEV 和卫星数据的可扩展性，我们的 Video2BEV 范例还结合了基于扩散的模块来生成硬负样本，这有助于进行判别性特征学习。为了验证我们的方法，我们引入了 UniV，这是一个新的基于视频的地理定位数据集，它扩展了基于图像的 University-1652 数据集。UniV 以 $30^\circ$ 和 $45^\circ$ 仰角提供飞行路径，帧速率提高到每秒 10 帧 (FPS)。在 UniV 数据集上进行的大量实验表明，我们的 Video2BEV 范例实现了具有竞争力的召回率，并且优于传统的基于视频的方法。与其他方法相比，我们提出的方法在较低海拔和更多遮挡下表现出稳健性。]]></description>
      <guid>https://arxiv.org/abs/2411.13610</guid>
      <pubDate>Fri, 22 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于分布外检测的非线性异常值合成</title>
      <link>https://arxiv.org/abs/2411.13619</link>
      <description><![CDATA[arXiv:2411.13619v1 公告类型：新
摘要：监督分类器的可靠性因其在处理意外输入方面的局限性而受到严重阻碍，从而引起了人们对分布外 (OOD) 检测的极大兴趣。最近，在合成异常值（尤其是由大型扩散模型生成的异常值）上训练的 OOD 检测器在定义稳健的 OOD 决策边界方面表现出了良好的效果。在此基础上，我们提出了 NCIS，它通过直接在扩散的模型嵌入空间中操作而不是像以前的工作那样组合不相交的模型，并通过用条件体积保持网络对类条件流形进行建模来提高合成异常值的质量，从而更具表现力地表征训练分布。我们证明这些改进在标准 ImageNet100 和 CIFAR100 基准上产生了新的最先进的 OOD 检测结果，并深入了解了数据预处理和其他关键设计选择的重要性。我们的代码可在 \url{https://github.com/LarsDoorenbos/NCIS} 上获取。]]></description>
      <guid>https://arxiv.org/abs/2411.13619</guid>
      <pubDate>Fri, 22 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>稳健的 SG-NeRF：稳健的场景图辅助神经表面重建</title>
      <link>https://arxiv.org/abs/2411.13620</link>
      <description><![CDATA[arXiv:2411.13620v1 公告类型：新
摘要：神经表面重建严重依赖于准确的相机姿势作为输入。尽管使用了 COLMAP 或 ARKit 等高级姿势估计器，相机姿势仍然可能很嘈杂。现有的姿势-NeRF 联合优化方法可以有效处理噪声较小的姿势（内点），但难以处理大噪声（异常值），例如镜像姿势。在这项工作中，我们专注于减轻异常姿势的影响。我们的方法集成了内点-异常值置信度估计方案，利用在数据准备阶段收集的场景图信息。与以前直接使用渲染指标作为参考的工作不同，我们采用了一个分离的颜色网络，该网络省略了观看方向作为输入，以最大限度地减少形状辐射模糊性造成的影响。这种增强的置信度更新策略有效地区分了内点和异常姿势，使我们能够从内点姿势中采样更多射线以构建更可靠的辐射场。此外，我们引入了基于当前有符号距离函数 (SDF) 和姿势估计的重新投影损失，加强了匹配图像对之间的约束。对于异常姿势，我们采用蒙特卡洛重新定位方法来寻找更好的解决方案。我们还设计了一种场景图更新策略，以在整个训练过程中提供更准确的信息。我们在 SG-NeRF 和 DTU 数据集上验证了我们的方法。在各种数据集上的实验结果表明，我们的方法可以持续提高重建质量和姿势精度。]]></description>
      <guid>https://arxiv.org/abs/2411.13620</guid>
      <pubDate>Fri, 22 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>无监督基础模型无关的幻灯片级表示学习</title>
      <link>https://arxiv.org/abs/2411.13623</link>
      <description><![CDATA[arXiv:2411.13623v1 公告类型：新
摘要：病理全幻灯片图像 (WSI) 的表示学习主要依赖于多实例学习 (MIL) 的弱监督。这种方法可使幻灯片表示高度适应特定的临床任务。自监督学习 (SSL) 已成功应用于训练组织病理学基础模型 (FM) 以生成补丁嵌入。然而，生成患者或幻灯片级别的嵌入仍然具有挑战性。现有的幻灯片表示学习方法通​​过对齐幻灯片的不同增强或利用多模态数据将 SSL 的原理从补丁级别学习扩展到整个幻灯片。通过集成来自多个 FM 的图块嵌入，我们提出了一种新的单模态 SSL 方法，该方法在特征空间中生成有用的幻灯片表示。我们的对比预训练策略称为 COBRA，采用多个 FM 和基于 Mamba-2 的架构。尽管 COBRA 仅在 TCGA 的 3048 个 WSI 上进行了预训练，但在四个不同的公共 CPTAC 队列中，其性能平均比最先进的滑动编码器高出至少 +3.8% AUC。此外，COBRA 在推理时可与以前从未见过的特征提取器轻松兼容。]]></description>
      <guid>https://arxiv.org/abs/2411.13623</guid>
      <pubDate>Fri, 22 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>