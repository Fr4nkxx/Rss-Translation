<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Mon, 05 Aug 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>EIUP：一种无需培训的方法，用于消除基于隐性不安全提示的不合规概念</title>
      <link>https://arxiv.org/abs/2408.01014</link>
      <description><![CDATA[arXiv:2408.01014v1 公告类型：新
摘要：文本到图像的扩散模型已经显示出学习各种概念的能力。然而，值得注意的是，它们也可能产生不良输出，从而引起重大的安全问题。具体来说，可能会遇到诸如不适合工作 (NSFW) 的内容和潜在的风格版权侵犯等问题。由于图像生成以文本为条件，提示净化是内容安全的直接解决方案。与 LLM 采取的方法类似，已经做出了一些努力通过净化提示来控制安全输出的生成。然而，同样重要的是要注意，即使有了这些努力，无毒文本仍然有生成不合规图像的风险，这被称为隐式不安全提示。此外，一些现有的工作对模型进行了微调，以从模型权重中删除不需要的概念。每当概念更新时，这种方法都需要多次训练迭代，这可能非常耗时，并且可能会导致灾难性的遗忘。为了应对这些挑战，我们提出了一种简单而有效的方法，将不合规的概念纳入擦除提示中。此擦除提示主动参与图像空间特征和文本嵌入的融合。通过注意机制，我们的方法能够识别图像空间中不合规概念的特征表示。我们重新加权这些特征，以有效抑制以原始隐式不安全提示为条件的不安全图像的生成。与最先进的基线相比，我们的方法表现出卓越的擦除效果，同时在图像保真度方面取得了高分。警告：本文包含可能令人反感的模型输出。]]></description>
      <guid>https://arxiv.org/abs/2408.01014</guid>
      <pubDate>Mon, 05 Aug 2024 06:18:24 GMT</pubDate>
    </item>
    <item>
      <title>可见光-热多目标跟踪：大规模视频数据集和渐进融合方法</title>
      <link>https://arxiv.org/abs/2408.00969</link>
      <description><![CDATA[arXiv:2408.00969v1 公告类型：新
摘要：可见光和热红外数据的互补优势被广泛应用于各种计算机视觉任务，例如视觉跟踪、语义分割和物体检测，但在多物体跟踪 (MOT) 中很少被探索。在这项工作中，我们为 MOT 贡献了一个大规模可见光-热视频基准，称为 VT-MOT。VT-MOT 具有以下主要优势。1) 数据规模大、多样性高。VT-MOT 包括来自监控、无人机和手持平台的 582 个视频序列对、401k 帧对。2) 跨模态对齐非常准确。我们邀请了几位专业人员逐帧执行空间和时间对齐。3) 注释密集且高质量。VT-MOT 有 399 万个注释框，由专业人员注释和仔细检查，包括严重遮挡和物体重新获取（物体消失和重新出现）挑战。为了提供强大的基础，我们设计了一个简单而有效的跟踪框架，该框架有效地以渐进的方式融合了两种模态的时间信息和互补信息，以实现稳健的可见热 MOT。对 VT-MOT 进行了全面的实验，结果证明了所提出的方法与最新方法相比的优越性和有效性。根据评估结果和分析，我们为可见热 MOT 指定了几个潜在的未来方向。该项目发布在 https://github.com/wqw123wqw/PFTrack。]]></description>
      <guid>https://arxiv.org/abs/2408.00969</guid>
      <pubDate>Mon, 05 Aug 2024 06:18:23 GMT</pubDate>
    </item>
    <item>
      <title>FBSDiff：即插即用的扩散特征频带替换，实现高度可控的文本驱动图像翻译</title>
      <link>https://arxiv.org/abs/2408.00998</link>
      <description><![CDATA[arXiv:2408.00998v1 公告类型：新
摘要：大规模文本到图像扩散模型是生成式人工智能和多模态技术演进的革命性里程碑，能够基于自然语言文本提示生成非凡的图像。然而，此类模型缺乏可控性的问题限制了它们在现实内容创作中的实际应用，为此人们将注意力集中在利用参考图像来控制文本到图像的合成上。由于参考图像和生成的图像之间密切相关，这个问题也可以看作是根据文本操纵（或编辑）参考图像的任务，即文本驱动的图像到图像转换。本文提出了一种新颖、简洁、高效的方法，将预先训练的大规模文本到图像 (T2I) 扩散模型即插即用地应用于图像到图像 (I2I) 范式，无需任何模型训练、模型微调或在线优化过程即可实现高质量、多功能的文本驱动的 I2I 转换。为了使用参考图引导 T2I 生成，我们建议在 DCT 频谱空间中对具有不同扩散特征频带的各种引导因子进行建模，并据此设计一种新颖的频带替换层，该层在反向采样过程中动态地将扩散特征的某个 DCT 频带替换为参考图的相应频带。我们证明，我们的方法可以灵活地实现高度可控的文本驱动的 I2I 转换，无论是在引导因子方面还是在参考图的引导强度方面，只需分别调整替换频带的类型和带宽即可。大量定性和定量实验验证了我们的方法在 I2I 翻译视觉质量、多功能性和可控性方面优于相关方法。]]></description>
      <guid>https://arxiv.org/abs/2408.00998</guid>
      <pubDate>Mon, 05 Aug 2024 06:18:23 GMT</pubDate>
    </item>
    <item>
      <title>MIS-ME：土壤水分估算的多模态框架</title>
      <link>https://arxiv.org/abs/2408.00963</link>
      <description><![CDATA[arXiv:2408.00963v1 公告类型：新
摘要：土壤水分估算是一项重要任务，它有助于精准农业制定灌溉、施肥和收获的最佳计划。通常利用统计和机器学习模型从传统数据源（例如天气预报、土壤特性和作物特性）估算土壤水分。然而，人们越来越有兴趣利用航空和地理空间图像来估算土壤水分。虽然这些图像可以捕捉高分辨率的作物细节，但它们的整理成本很高，而且难以解释。想象一下，一个人工智能增强的软件工具，它使用智能手机捕捉到的视觉线索和天气预报给出的统计数据来预测土壤水分。这项工作是朝着开发多模态土壤水分估算方法的目标迈出的第一步。特别是，我们整理了一个数据集，该数据集由从地面站拍摄的真实图像及其相应的天气数据组成。我们还提出了 MIS-ME——基于气象和图像的土壤水分估算器，这是一种用于土壤水分估算的多模态框架。我们的广泛分析表明，MIS-ME 实现了 10.79% 的 MAPE，优于传统的单峰方法，气象数据的 MAPE 降低了 2.6%，图像数据的 MAPE 降低了 1.5%，凸显了定制多峰方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2408.00963</guid>
      <pubDate>Mon, 05 Aug 2024 06:18:22 GMT</pubDate>
    </item>
    <item>
      <title>从 LiDAR 和航空图像中提取物体高度</title>
      <link>https://arxiv.org/abs/2408.00967</link>
      <description><![CDATA[arXiv:2408.00967v1 公告类型：新
摘要：这项工作展示了一种从 LiDAR 和航空图像中提取物体高度的程序方法。我们讨论了如何获取高度以及 LiDAR 和图像处理的未来。SOTA 对象分割使我们能够在没有深度学习背景的情况下获取物体高度。工程师将跟踪跨代的世界数据并对其进行重新处理。他们将使用本文等较旧的程序方法和此处讨论的较新方法。SOTA 方法超越了分析并进入了生成 AI。我们涵盖了程序方法和使用语言模型执行的较新方法。这些包括点云、图像和文本编码，允许空间感知 AI。]]></description>
      <guid>https://arxiv.org/abs/2408.00967</guid>
      <pubDate>Mon, 05 Aug 2024 06:18:22 GMT</pubDate>
    </item>
    <item>
      <title>数据驱动的大都市交叉路口交通模拟</title>
      <link>https://arxiv.org/abs/2408.00943</link>
      <description><![CDATA[arXiv:2408.00943v1 公告类型：新
摘要：我们提出了一种新颖的数据驱动模拟环境，用于对大都市街道交叉口的交通进行建模。使用在较长时间内收集的真实世界跟踪数据，我们训练轨迹预测模型以学习难以通过传统方式捕获的代理交互和环境约束。新代理的轨迹首先通过从空间和时间生成分布中采样粗略生成，然后使用最先进的轨迹预测模型进行细化。模拟可以自动运行，也可以在以生成分布为条件的明确人为控制下运行。我们针对各种模型配置进行了实验。在迭代预测方案下，航点监督的 TrajNet++ 模型在 NVIDIA A100 GPU 上以 20 FPS 获得了 0.36 的最终位移误差 (FDE)。]]></description>
      <guid>https://arxiv.org/abs/2408.00943</guid>
      <pubDate>Mon, 05 Aug 2024 06:18:21 GMT</pubDate>
    </item>
    <item>
      <title>PrivateGaze：在黑盒移动注视跟踪服务中保护用户隐私</title>
      <link>https://arxiv.org/abs/2408.00950</link>
      <description><![CDATA[arXiv:2408.00950v1 公告类型：新
摘要：目光注视包含有关人类注意力和认知过程的丰富信息。此功能使被称为目光追踪的底层技术成为许多无处不在的应用程序的关键推动因素，并引发了易于使用的目光估计服务的开发。事实上，通过利用平板电脑和智能手机上无处不在的摄像头，用户可以轻松访问许多目光估计服务。在使用这些服务时，用户必须向目光估计器提供他们的全脸图像，而目光估计器通常是一个黑匣子。这对用户构成了重大的隐私威胁，尤其是当恶意服务提供商收集大量面部图像来对敏感的用户属性进行分类时。在这项工作中，我们提出了 PrivateGaze，这是第一种可以在黑匣子目光追踪服务中有效保护用户隐私而不影响目光估计性能的方法。具体来说，我们提出了一个新颖的框架来训练隐私保护器，将全脸图像转换为模糊的对应物，这些对应物对于目光估计有效，但不包含隐私信息。在四个数据集上的评估表明，混淆后的图像可以保护用户的隐私信息（例如身份和性别）免遭未经授权的属性分类。同时，当黑盒凝视估计器直接使用混淆后的图像作为输入时，其跟踪性能可与传统的、未受保护的全脸图像相媲美。]]></description>
      <guid>https://arxiv.org/abs/2408.00950</guid>
      <pubDate>Mon, 05 Aug 2024 06:18:21 GMT</pubDate>
    </item>
    <item>
      <title>回收残留知识：低位量化的新范式</title>
      <link>https://arxiv.org/abs/2408.00923</link>
      <description><![CDATA[arXiv:2408.00923v1 公告类型：新
摘要：本文探讨了一种低位（即 4 位或更低）量化的新范式，与现有的最先进方法不同，它将最佳量化定义为卷积神经网络 (ConvNets) 中的架构搜索问题。我们的框架被称为 \textbf{CoRa}（最佳量化残差 \textbf{Co}nvolutional Operator Low-\textbf{Ra}nk Adaptation），由两个关键方面推动。首先，量化残差知识，即浮点权重和量化权重之间丢失的信息，长期以来一直被研究界忽视。以极小的额外参数成本重新获得关键的残差知识可以在无需训练的情况下逆转性能下降。其次，最先进的量化框架会搜索最佳量化权重来解决性能下降问题。然而，权重优化中巨大的搜索空间对大型模型的有效优化提出了挑战。例如，最先进的 BRECQ 需要 $2 \times 10^4$ 次迭代来量化模型。与现有方法的根本不同，\textbf{CoRa} 在比权重空间小几个数量级的搜索空间内搜索低秩适配器的最佳架构，回收关键量化残差知识。低秩适配器近似于以前方法中丢弃的量化残差权重。我们在 ImageNet 上对多个预训练的 ConvNets 评估了我们的方法。\textbf{CoRa} 在 4 位和 3 位量化中，通过使用少于 250 次迭代，在包含 1600 张图像的小型校准集上实现了与最先进的量化感知训练和训练后量化基线相当的性能。因此，\textbf{CoRa} 在低位量化的优化效率方面确立了新的领先地位。]]></description>
      <guid>https://arxiv.org/abs/2408.00923</guid>
      <pubDate>Mon, 05 Aug 2024 06:18:20 GMT</pubDate>
    </item>
    <item>
      <title>利用视觉语言模型实现建筑环境的零样本注释（愿景论文）</title>
      <link>https://arxiv.org/abs/2408.00932</link>
      <description><![CDATA[arXiv:2408.00932v1 公告类型：新
摘要：公平的城市交通应用需要高保真的建筑环境数字表示：不仅是街道和人行道，还包括自行车道、有标记和无标记的十字路口、路缘坡道和切口、障碍物、交通信号灯、标牌、街道标记、坑洼等等。直接检查和手动注释在规模上成本过高。传统的机器学习方法需要大量带注释的训练数据才能获得足够的性能。在本文中，我们将视觉语言模型视为一种从卫星图像中注释各种城市特征的机制，减少了对人工注释的依赖，从而产生了大量的训练集。虽然这些模型在描述从人类角度捕获的图像中的常见物体方面取得了令人印象深刻的结果，但它们的训练集不太可能包含建筑环境中深奥特征的强信号，因此它们在这些设置中的表现尚不清楚。我们展示了概念验证，结合了最先进的视觉语言模型和提示策略的变体，该策略要求模型独立于原始图像考虑分割元素。对两个城市特征（停车线和凸起桌子）进行的实验表明，虽然直接零样本提示几乎无法正确注释任何图像，但预分割策略可以注释图像，交集准确率接近 40%。我们描述了这些结果如何为自动注释建筑环境的新研究议程提供信息，以提高广泛和多样化环境中的公平性、可访问性和安全性。]]></description>
      <guid>https://arxiv.org/abs/2408.00932</guid>
      <pubDate>Mon, 05 Aug 2024 06:18:20 GMT</pubDate>
    </item>
    <item>
      <title>用于监控视频异常检测的可扩展通用深度学习框架</title>
      <link>https://arxiv.org/abs/2408.00792</link>
      <description><![CDATA[arXiv:2408.00792v1 公告类型：新
摘要：由于暴力、入店行窃和破坏等活动的复杂性、噪音和多样性，视频中的异常检测具有挑战性。虽然深度学习 (DL) 在这一领域表现出色，但现有方法难以将 DL 模型应用于不同的异常任务，而无需进行大量的再训练。这种重复的再训练既耗时又耗算，而且不公平。为了解决这一限制，本研究引入了一个新的 DL 框架，它由三个关键组件组成：迁移学习以增强特征泛化、模型融合以改善特征表示，以及多任务分类以在引入新任务时将分类器推广到多个任务，而无需从头开始训练。该框架的主要优势在于它能够进行泛化，而无需为每个新任务从头开始重新训练。实证评估证明了该框架的有效性，在 RLVS 数据集（暴力检测）上实现了 97.99% 的准确率，在 UCF 数据集（商店盗窃检测）上实现了 83.59% 的准确率，在两个数据集上使用单个分类器无需重新训练即可实现 88.37% 的准确率。此外，在对未见过的数据集进行测试时，该框架实现了 87.25% 的准确率。该研究还利用两种可解释性工具来识别潜在偏差，确保稳健性和公平性。这项研究代表了异常检测中泛化问题的首次成功解决，标志着该领域的重大进步。]]></description>
      <guid>https://arxiv.org/abs/2408.00792</guid>
      <pubDate>Mon, 05 Aug 2024 06:18:19 GMT</pubDate>
    </item>
    <item>
      <title>医疗 SAM 2：通过 Segment Anything Model 2 将医疗图像分割为视频</title>
      <link>https://arxiv.org/abs/2408.00874</link>
      <description><![CDATA[arXiv:2408.00874v1 公告类型：新
摘要：在本文中，我们介绍了 Medical SAM 2 (MedSAM-2)，这是一种利用 SAM 2 框架解决 2D 和 3D 医学图像分割任务的高级分割模型。通过采用将医学图像作为视频的理念，MedSAM-2 不仅适用于 3D 医学图像，而且还解锁了新的一键分割功能。这允许用户仅为一个或针对某个对象的特定图像提供提示，之后模型可以在所有后续图像中自主分割相同类型的对象，而不管图像之间的时间关系如何。我们在各种医学成像模式下评估了 MedSAM-2，包括腹部器官、视神经盘、脑肿瘤、甲状腺结节和皮肤病变，并将其与传统和交互式分割设置中的最新模型进行了比较。我们的研究结果表明，MedSAM-2 不仅在性能上超越了现有模型，而且在一系列医学图像分割任务中表现出卓越的泛化能力。我们的代码将在以下网址发布：https://github.com/MedicineToken/Medical-SAM2]]></description>
      <guid>https://arxiv.org/abs/2408.00874</guid>
      <pubDate>Mon, 05 Aug 2024 06:18:19 GMT</pubDate>
    </item>
    <item>
      <title>CATD：用于 EEG 到 fMRI 跨模态生成的统一表征学习</title>
      <link>https://arxiv.org/abs/2408.00777</link>
      <description><![CDATA[arXiv:2408.00777v1 公告类型：新 
摘要：多模态神经影像分析对于全面了解大脑功能和病理至关重要，因为它允许整合不同的成像技术，从而克服单个模态的局限性。然而，某些模态的高成本和有限的可用性带来了重大挑战。为了解决这些问题，本文提出了条件对齐时间扩散 (CATD) 框架，用于端到端跨模态神经影像合成，从而能够从更易于获取的脑电图 (EEG) 信号生成功能性磁共振成像 (fMRI) 检测的血氧水平依赖 (BOLD) 信号。通过构建条件对齐块 (CAB)，异构神经图像被对齐到潜在空间，实现统一表示，为神经影像中的跨模态转换奠定基础。与构建的动态时频分割 (DTFS) 模块相结合，还可以利用 EEG 信号提高 BOLD 信号的时间分辨率，从而增强对大脑动态细节的捕捉。实验验证证明了该框架在提高神经活动预测准确性、识别异常大脑区域和增强 BOLD 信号时间分辨率方面的有效性。提出的框架通过将异构神经成像数据统一到潜在的表示空间中，为神经成像的跨模态合成建立了新范式，在改善帕金森病预测和识别异常大脑区域等医学应用中显示出良好的前景。]]></description>
      <guid>https://arxiv.org/abs/2408.00777</guid>
      <pubDate>Mon, 05 Aug 2024 06:18:18 GMT</pubDate>
    </item>
    <item>
      <title>数据驱动的 DNN 对象识别验证</title>
      <link>https://arxiv.org/abs/2408.00783</link>
      <description><![CDATA[arXiv:2408.00783v1 公告类型：新
摘要：本文提出了一种新的深度神经网络 (DNN) 测试方法，使用无梯度优化来查找成功伪造测试 DNN 的扰动链，超越了现有的基于网格或组合的测试。将其应用于检测图像中的铁路轨道的图像分割任务，我们证明该方法可以成功识别测试 DNN 在特定测试图像簇上针对常见扰动（例如雨、雾、模糊、噪音）的特定组合的弱点。]]></description>
      <guid>https://arxiv.org/abs/2408.00783</guid>
      <pubDate>Mon, 05 Aug 2024 06:18:18 GMT</pubDate>
    </item>
    <item>
      <title>比较光流和深度学习，利用空间填充曲线实现计算高效的交通事件检测</title>
      <link>https://arxiv.org/abs/2408.00768</link>
      <description><![CDATA[arXiv:2408.00768v1 公告类型：新
摘要：在各种交通情况下收集数据和识别事件仍然是系统评估感知系统性能的重要挑战。分析从视频、雷达和激光雷达获得的大规模、通常是非结构化的、多模态的时间序列数据在计算上要求很高，特别是在缺少元信息或注释时。我们比较了光流 (OF) 和深度学习 (DL)，通过对来自前置车载摄像头的视频数据进行空间填充曲线来提供计算效率高的事件检测。我们的第一种方法利用车辆周围环境中 OF 场的意外干扰；第二种方法是使用人类视觉注意力训练的 DL 模型来预测驾驶员的注视以发现潜在的事件位置。我们将这些结果提供给空间填充曲线以降低维度并实现计算效率高的事件检索。我们通过从大型虚拟数据集 (SMIRK) 获取两种方法的特征模式，系统地评估了我们的概念，并将我们的发现应用于 Zenseact Open Dataset (ZOD)，这是一个大型多模式真实世界数据集，两年来在 14 个不同的欧洲国家收集。我们的结果表明，OF 方法在特异性方面表现出色并减少了假阳性，而 DL 方法表现出卓越的灵敏度。这两种方法都提供了相当的处理速度，使它们适合实时应用。]]></description>
      <guid>https://arxiv.org/abs/2408.00768</guid>
      <pubDate>Mon, 05 Aug 2024 06:18:17 GMT</pubDate>
    </item>
    <item>
      <title>具有学习不连续性的二维神经场</title>
      <link>https://arxiv.org/abs/2408.00771</link>
      <description><![CDATA[arXiv:2408.00771v1 公告类型：新
摘要：有效表示二维图像是数字图像处理的基础，其中传统方法（如光栅和矢量图形）分别在清晰度和纹理复杂性方面存在困难。当前的神经场提供高保真度和分辨率独立性，但需要具有已知不连续性的预定义网格，从而限制了它们的实用性。我们观察到，通过将所有网格边缘视为潜在的不连续性，我们可以用连续变量表示不连续性的幅度并进行优化。基于这一观察，我们引入了一种新颖的不连续神经场模型，该模型联合近似目标图像并恢复不连续性。通过系统评估，与 InstantNGP 相比，我们的神经场在去噪和超分辨率任务中表现出色，分别实现了超过 5dB 和 10dB 的改进。我们的模型在准确捕捉不连续性方面也优于基于 Mumford-Shah 的方法，Chamfer 距离比地面实况近 3.5 倍。此外，我们的方法在处理复杂的艺术绘画和自然图像方面表现出卓越的能力。]]></description>
      <guid>https://arxiv.org/abs/2408.00771</guid>
      <pubDate>Mon, 05 Aug 2024 06:18:17 GMT</pubDate>
    </item>
    </channel>
</rss>