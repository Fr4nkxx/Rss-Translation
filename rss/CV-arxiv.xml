<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Thu, 12 Sep 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>HSR-KAN：通过 Kolmogorov-Arnold 网络实现高效的高光谱图像超分辨率</title>
      <link>https://arxiv.org/abs/2409.06705</link>
      <description><![CDATA[arXiv:2409.06705v1 公告类型：新
摘要：高光谱图像 (HSI) 因其丰富的光谱信息而在各种视觉任务中具有巨大潜力。然而，由于物理成像的限制，获取高分辨率高光谱图像仍然具有挑战性。受 Kolmogorov-Arnold 网络 (KAN) 的启发，我们提出了一种高效的 HSI 超分辨率 (HSI-SR) 模型来融合低分辨率 HSI (LR-HSI) 和高分辨率多光谱图像 (HR-MSI)，从而产生高分辨率 HSI (HR-HSI)。为了实现来自 HR-MSI 的空间信息的有效整合，我们设计了一个基于 KAN 的融合模块，称为 KAN-Fusion。进一步受到通道注意机制的启发，我们设计了一个称为 KAN 通道注意块 (KAN-CAB) 的光谱通道注意模块，用于融合后特征提取。 KAN-CAB 作为与 KAN 集成的通道注意模块，不仅增强了深度网络的细粒度调整能力，使网络能够准确模拟光谱序列和空间纹理的细节，而且还能有效避免维数灾难 (COD)。大量实验表明，与目前最先进的 (SOTA) HSI-SR 方法相比，提出的 HSR-KAN 在定性和定量评估方面均取得了最佳性能。我们的代码可在以下网址获取：https://github.com/Baisonm-Li/HSR-KAN。]]></description>
      <guid>https://arxiv.org/abs/2409.06705</guid>
      <pubDate>Thu, 12 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>安全驾驶中行人过街预测的门控合成真实知识</title>
      <link>https://arxiv.org/abs/2409.06707</link>
      <description><![CDATA[arXiv:2409.06707v1 公告类型：新
摘要：行人过街预测（PCP）在驾驶场景中对智能车辆安全运行起着至关重要的作用。由于对典型情况下行人过街行为的观察有限，最近的研究开始利用具有灵活变化的合成数据来提高预测性能，采用领域自适应框架。然而，不同领域知识具有不同的跨领域分布差距，这需要适合 PCP 任务的领域知识自适应方法。在本文中，我们提出了一种针对 PCP 的门控合成到现实知识迁移方法（Gated-S2R-PCP），该方法有两个目的：1）为不同类型的跨领域知识设计合适的领域自适应方法，2）通过门控知识融合为特定情况迁移合适的知识。具体来说，我们设计了一个框架，其中包含三种域自适应方法，包括风格迁移、分布近似和知识提炼，用于各种信息，例如视觉、语义、深度、位置等。采用可学习门控单元 (LGU) 融合合适的跨域知识来增强行人过街预测。我们构建了一个新的合成基准 S2R-PCP-3181，其中包含 3181 个序列（489,740 帧），其中包含行人位置、RGB 帧、语义图像和深度图像。利用合成的 S2R-PCP-3181，我们将知识迁移到两个真实的具有挑战性的数据集 PIE 和 JAAD，并且获得了优于最先进方法的 PCP 性能。]]></description>
      <guid>https://arxiv.org/abs/2409.06707</guid>
      <pubDate>Thu, 12 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>McGrids：用于等值面提取的蒙特卡罗驱动自适应网格</title>
      <link>https://arxiv.org/abs/2409.06710</link>
      <description><![CDATA[arXiv:2409.06710v1 公告类型：新
摘要：从隐式场中提取等值面是计算机视觉和图形各种应用中的基本过程。在处理具有复杂几何细节的几何形状时，许多现有算法都存在计算成本高和内存占用高的问题。本文提出了一种提高等值面提取效率的新方法 McGrids。关键思想是构建用于等值面提取的自适应网格，而不是像现有技术那样使用简单的均匀网格。具体而言，我们将构建自适应网格的问题公式化为概率抽样问题，然后通过蒙特卡洛过程进行求解。我们通过大量实验证明了 McGrids 的能力，这些实验包括从表面网格计算出的分析 SDF 和从真实多视图图像中学习到的隐式场。实验结果表明，我们的 McGrids 可以显著减少隐式场查询的数量，从而显著减少内存，同时生成具有丰富几何细节的高质量网格。]]></description>
      <guid>https://arxiv.org/abs/2409.06710</guid>
      <pubDate>Thu, 12 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于复杂全息图生成的量化神经网络</title>
      <link>https://arxiv.org/abs/2409.06711</link>
      <description><![CDATA[arXiv:2409.06711v1 公告类型：新
摘要：计算机生成全息术 (CGH) 是一种很有前途的增强现实显示器技术，例如头戴式或平视显示器。然而，其高计算需求使其不切实际。最近将神经网络集成到 CGH 中的努力成功地加快了计算速度，展示了克服计算成本和图像质量之间权衡的潜力。然而，在计算受限的嵌入式系统上部署基于神经网络的 CGH 算法需要更高效的模型，具有更低的计算成本、内存占用和功耗。在本研究中，我们通过引入神经网络量化开发了一个用于复杂全息图生成的轻量级模型。具体来说，我们建立了一个基于张量全息术的模型，并将其从 32 位浮点精度 (FP32) 量化为 8 位整数精度 (INT8)。我们的性能评估表明，所提出的 INT8 模型实现了与 FP32 模型相当的全息图质量，同时将模型尺寸缩小了约 70%，速度提高了四倍。此外，我们在系统级模块上实现了 INT8 模型，以证明其在嵌入式平台上的可部署性和高功率效率。]]></description>
      <guid>https://arxiv.org/abs/2409.06711</guid>
      <pubDate>Thu, 12 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于反馈的模态相互搜索攻击视觉语言预训练模型</title>
      <link>https://arxiv.org/abs/2409.06726</link>
      <description><![CDATA[arXiv:2409.06726v1 公告类型：新
摘要：尽管视觉语言预训练 (VLP) 模型在跨模态任务上取得了显著进展，但它们仍然容易受到对抗性攻击。使用数据增强和跨模态交互在代理模型上生成可迁移的对抗性示例，基于迁移的黑盒攻击已成为攻击 VLP 模型的主流方法，因为它们在现实场景中更为实用。然而，由于不同模型之间的特征表示差异，它们的可迁移性可能受到限制。为此，我们提出了一种称为基于反馈的模态相互搜索 (FMMS) 的新攻击范式。FMMS 引入了一种新颖的模态相互损失 (MML)，旨在推开匹配的图像-文本对，同时在特征空间中随机拉近不匹配的对，从而指导对抗性示例的更新方向。此外，FMMS 利用目标模型反馈迭代地细化对抗性示例，将它们驱入对抗区域。据我们所知，这是第一项利用目标模型反馈探索多模态对抗边界的研究。对 Flickr30K 和 MSCOCO 数据集进行的图像文本匹配任务的大量实证评估表明，FMMS 的表现明显优于最先进的基线。]]></description>
      <guid>https://arxiv.org/abs/2409.06726</guid>
      <pubDate>Thu, 12 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用幂函数对图像色调二分法进行建模</title>
      <link>https://arxiv.org/abs/2409.06764</link>
      <description><![CDATA[arXiv:2409.06764v1 公告类型：新
摘要：本文的主要目的是介绍基于幂函数的图像照明建模中的二分法概念。特别是，我们回顾了幂函数的几个数学性质，以确定其局限性，并提出了一种能够抽象照明二分法的新数学模型。该方程的简单性为经典和现代图像分析和处理开辟了新的途径。本文提供了实用且说明性的图像示例来解释新模型如何管理图像感知中的二分法。本文表明，尽管与色调、亮度和色彩感知相关的对比度较差，但二分图像空间仍是从图像中提取丰富信息的可行方法。此外，与图像增强中最先进的方法的比较证明了该方法的价值。]]></description>
      <guid>https://arxiv.org/abs/2409.06764</guid>
      <pubDate>Thu, 12 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>gsplat：高斯溅射的开源库</title>
      <link>https://arxiv.org/abs/2409.06765</link>
      <description><![CDATA[arXiv:2409.06765v1 公告类型：新
摘要：gsplat 是一个开源库，旨在训练和开发高斯 Splatting 方法。它具有与 PyTorch 库兼容的 Python 绑定的前端和具有高度优化的 CUDA 内核的后端。gsplat 提供了许多增强高斯 Splatting 模型优化的功能，包括速度、内存和收敛时间的优化改进。实验结果表明，与原始实现相比，gsplat 的训练时间减少了 10%，内存减少了 4 倍。gsplat 已在多个研究项目中使用，并在 GitHub 上得到积极维护。源代码可在 Apache 许可证 2.0 下的 https://github.com/nerfstudio-project/gsplat 上获得。我们欢迎开源社区的贡献。]]></description>
      <guid>https://arxiv.org/abs/2409.06765</guid>
      <pubDate>Thu, 12 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>人体运动合成_运动拼接和中间融合的扩散方法</title>
      <link>https://arxiv.org/abs/2409.06791</link>
      <description><![CDATA[arXiv:2409.06791v1 公告类型：新
摘要：人体运动生成是许多领域的重要研究领域。在这项工作中，我们解决了运动拼接和中间化的问题。当前的方法要么需要手动操作，要么无法处理较长的序列。为了应对这些挑战，我们提出了一种基于变压器的降噪器的扩散模型来生成逼真的人体运动。我们的方法在生成中间序列方面表现出色，将可变数量的输入姿势转换为由 75 帧以 15 fps 组成的平滑逼真的运动序列，总持续时间为 5 秒。我们使用定量指标（例如 Frechet 初始距离 (FID)、多样性和多模态性）以及对生成的输出的视觉评估来展示我们方法的性能评估。]]></description>
      <guid>https://arxiv.org/abs/2409.06791</guid>
      <pubDate>Thu, 12 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DetailCLIP：面向细节的细粒度任务 CLIP</title>
      <link>https://arxiv.org/abs/2409.06809</link>
      <description><![CDATA[arXiv:2409.06809v1 公告类型：新
摘要：在本文中，我们介绍了 DetailCLIP：一种面向细节的 CLIP，以解决基于对比学习的视觉语言模型（尤其是 CLIP）在处理面向细节和细粒度任务（如分割）方面的局限性。虽然 CLIP 及其变体在图像和文本表示的全局对齐方面表现出色，但它们往往难以捕捉精确分割所需的细粒度细节。为了克服这些挑战，我们提出了一个新颖的框架，该框架采用自蒸馏和像素级重建损失的补丁级比较，并通过基于注意力的标记删除机制进行增强。这种方法有选择地保留了语义相关的标记，使模型能够专注于与我们模型的特定功能（包括文本信息处理、补丁比较和图像重建）相一致的图像关键区域，从而确保模型学习高级语义和详细的视觉特征。我们的实验表明，DetailCLIP 在分割准确度方面超越了现有的基于 CLIP 和传统的自监督学习 (SSL) 模型，并在不同的数据集中表现出卓越的泛化能力。DetailCLIP 代表了视觉语言建模的重大进步，为需要高级语义理解和详细特征提取的任务提​​供了强大的解决方案。https://github.com/KishoreP1/DetailCLIP。]]></description>
      <guid>https://arxiv.org/abs/2409.06809</guid>
      <pubDate>Thu, 12 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用海面多路径的水下前向扫描声纳图像进行对象建模</title>
      <link>https://arxiv.org/abs/2409.06815</link>
      <description><![CDATA[arXiv:2409.06815v1 公告类型：新
摘要：我们提出了一种优化技术，用于从已知姿势的二维前向扫描声纳图像进行三维水下物体建模。对于在海面附近成像的物体，一个关键贡献是解决由于空气-水界面引起的多路径伪影。在这里，由直接目标后向散射形成的物体图像几乎总是被鬼影破坏，有时也被镜像组件（由多径传播产生）破坏。假设平面空气-水界面，我们在每个视图中建模、定位和丢弃损坏的物体区域，从而避免恢复的三维形状的扭曲。此外，在合适的声纳姿势下，镜像组件边界的互补视觉提示被用来提高三维建模的准确性。
优化是通过在 3-D 表面网格模型中移动三角形面片的顶点来实现的，目的是尽量减少 3-D 物体模型的数据和合成视图之间的差异。为此，我们首先确定对齐数据和合成视图中的物体区域的 2-D 运动场，然后计算三角形面片中心的 3-D 运动，最后计算模型顶点的 3-D 运动。使用应用于相同数据的早期空间雕刻方法的解决方案初始化 3-D 模型。在各种实验中应用相同的参数，使用 2 个真实数据集、混合真实合成数据集和由真实实验的一般发现指导的计算机生成的数据，探索非平坦空气-水界面的影响。结果证实，在大约六次迭代中生成了一个精细的 3-D 模型。]]></description>
      <guid>https://arxiv.org/abs/2409.06815</guid>
      <pubDate>Thu, 12 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Sam2Rad：具有可学习提示的医学图像分割模型</title>
      <link>https://arxiv.org/abs/2409.06821</link>
      <description><![CDATA[arXiv:2409.06821v1 公告类型：新
摘要：像分割任何东西模型这样的基础模型需要高质量的手动提示来进行医学图像分割，这既耗时又需要专业知识。由于域偏移，SAM 及其变体通常无法分割超声 (US) 图像中的结构。
我们提出了 Sam2Rad，这是一种即时学习方法，用于调整 SAM 及其变体以进行无需人工提示的 US 骨骼分割。它引入了一个带有交叉注意模块的即时预测网络 (PPN)，以预测来自图像编码器特征的即时嵌入。PPN 输出边界框和掩码提示，以及感兴趣区域的 256 维嵌入。该框架允许可选的手动提示，并且可以使用参数高效微调 (PEFT) 进行端到端训练。
Sam2Rad 在 3 个肌肉骨骼 US 数据集上进行了测试：手腕（3822 张图像）、肩袖（1605 张图像）和臀部（4849 张图像）。它无需手动提示即可提高所有数据集的性能，将臀部/手腕的 Dice 分数提高 2-7%，将肩部数据的 Dice 分数提高 33%。Sam2Rad 只需 10 张带标签的图像即可进行训练，并且与任何 SAM 架构兼容，可实现自动分割。]]></description>
      <guid>https://arxiv.org/abs/2409.06821</guid>
      <pubDate>Thu, 12 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>针对 LiDAR 点云的跨模态自监督学习与有效对比单元</title>
      <link>https://arxiv.org/abs/2409.06827</link>
      <description><![CDATA[arXiv:2409.06827v1 公告类型：新
摘要：LiDAR 点云中的 3D 感知对于自动驾驶汽车在 3D 环境中正确行驶至关重要。然而，手动标记点云既困难又昂贵。人们对 3D 感知模型的自监督预训练的兴趣日益浓厚。随着图像对比学习的成功，当前的方法大多仅对点云进行对比预训练。然而，自动驾驶汽车通常配备多个传感器，包括摄像头和 LiDAR。在此背景下，我们系统地研究了点云对比学习的单模态、跨模态和多模态，并表明跨模态优于其他替代方案。此外，考虑到 2D 图像和 3D 点云中的训练源之间的巨大差异，如何为 LiDAR 设计更有效的对比单元仍不清楚。因此，我们提出了针对自动驾驶点云量身定制的实例感知和相似性平衡对比单元。大量实验表明，在基于 LiDAR 的 3D 物体检测和 3D 语义分割的下游感知任务中，我们的方法在 Waymo Open Dataset、nuScenes、SemanticKITTI 和 ONCE 等四个流行基准上实现了比各种点云模型显著的性能提升。]]></description>
      <guid>https://arxiv.org/abs/2409.06827</guid>
      <pubDate>Thu, 12 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>少量学习：将身份证出示攻击检测扩展到身份不明的国家</title>
      <link>https://arxiv.org/abs/2409.06842</link>
      <description><![CDATA[]]></description>
      <guid>https://arxiv.org/abs/2409.06842</guid>
      <pubDate>Thu, 12 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://arxiv.org/abs/2409.06845</link>
      <description><![CDATA[arXiv:2409.06845v1 公告类型：新
摘要：在 COVID-19 大流行期间，口罩在我们的生活中无处不在。口罩可能会导致某些人脸识别模型失败，因为它们会遮住脸部的很大一部分。此外，从捕获的图像或视频中删除口罩可能是可取的，例如，为了更好的社交互动以及为了图像/视频编辑和增强目的。因此，我们提出了一种生成式人脸修复方法来有效地恢复/重建人脸的遮罩部分。与传统修复相比，人脸修复更具挑战性，因为它需要高保真度，同时保持身份。我们提出的方法包括一个多尺度通道空间注意模块 (M-CSAM)，以减轻空间信息丢失并学习通道间和通道内相关性。此外，我们引入了一种方法，强制监督信号聚焦于遮罩区域而不是整个图像。我们还从 CelebA 数据集中合成了我们自己的 Masked-Faces 数据集，其中融合了五种不同类型的口罩，包括外科口罩、普通口罩和围巾，这些口罩也覆盖了颈部区域。实验结果表明，我们提出的方法在结构相似性指数测量、峰值信噪比和 l1 损失方面优于不同的基线，同时在质量上也提供了更好的输出。代码将公开提供。代码可在 GitHub 上找到。]]></description>
      <guid>https://arxiv.org/abs/2409.06845</guid>
      <pubDate>Thu, 12 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过材质一致的阴影边缘进行阴影去除细化</title>
      <link>https://arxiv.org/abs/2409.06848</link>
      <description><![CDATA[arXiv:2409.06848v1 公告类型：新
摘要：阴影边界可能与材料边界混淆，因为两者都表现出场景内亮度或对比度的急剧变化。然而，阴影不会改变表面的固有颜色或纹理。因此，在穿过具有相同材料的区域的阴影边缘的两侧，如果阴影被正确去除，原始颜色和纹理应该是相同的。这些阴影/无阴影对是非常有用但难以收集的监督信号。本文的关键贡献是学习如何识别那些穿过材料一致区域的阴影边缘，以及如何在测试期间将它们用作自我监督以改进阴影去除。为了实现这一点，我们对图像分割基础模型 SAM 进行了微调，以产生阴影不变的分割，然后通过将 SAM 分割与阴影掩模进行比较来提取材料一致的阴影边缘。利用这些阴影边缘，我们引入颜色和纹理一致性损失来增强阴影去除过程。我们证明了我们的方法在改善更具挑战性的自然图像上的阴影去除结果方面的有效性，其效果优于最先进的阴影去除方法。此外，我们提出了一种新的指标和带注释的数据集，用于评估阴影去除方法的性能，而无需配对阴影/无阴影数据。]]></description>
      <guid>https://arxiv.org/abs/2409.06848</guid>
      <pubDate>Thu, 12 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>