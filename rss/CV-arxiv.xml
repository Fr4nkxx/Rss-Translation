<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Fri, 25 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>TextureMeDefect：基于 LLM 的移动设备铁路部件缺陷纹理生成</title>
      <link>https://arxiv.org/abs/2410.18085</link>
      <description><![CDATA[arXiv:2410.18085v1 公告类型：新
摘要：纹理图像生成已在各种应用中得到研究，包括游戏和娱乐。然而，针对工业应用的特定情境逼真纹理生成（例如在铁路部件上生成缺陷纹理）仍未得到探索。一种基于 LLM 的移动友好型工具可生成细粒度的缺陷特征，为理解缺陷对实际发生的影响这一挑战提供了解决方案。我们推出了 TextureMeDefect，这是一种利用基于 LLM 的 AI 推理引擎的创新工具。该工具允许用户在使用智能手机或平板电脑拍摄的铁路部件图像上以交互方式创建逼真的缺陷纹理。我们进行了多方面的评估，以评估在 iOS 和 Android 平台上使用此工具时生成的纹理、时间和成本的相关性。我们还分析了三种情况下的软件可用性评分 (SUS)。TextureMeDefect 通过更快地生成有意义的纹理，优于传统的图像生成工具，展示了消费级设备上 AI 驱动的移动应用程序的潜力。]]></description>
      <guid>https://arxiv.org/abs/2410.18085</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Gesture2Text：通过轨迹粗离散化和预训练实现 XR 中文字手势键盘的通用解码器</title>
      <link>https://arxiv.org/abs/2410.18099</link>
      <description><![CDATA[arXiv:2410.18099v1 公告类型：新
摘要：使用单词手势键盘 (WGK) 进行文本输入正在成为一种流行的方法，并成为扩展现实 (XR) 的关键交互。然而，这些环境中交互模式、键盘大小和视觉反馈的多样性引入了不同的单词手势轨迹数据模式，从而导致将轨迹解码为文本的复杂性。模板匹配解码方法（例如 SHARK^2）通常用于这些 WGK 系统，因为它们易于实现和配置。然而，这些方法容易受到噪声轨迹解码不准确的影响。虽然已经提出了基于单词手势轨迹数据训练的传统基于神经网络的解码器（神经解码器）来提高准确性，但它们也有自己的局限性：它们需要大量数据进行训练和深度学习专业知识才能实现。为了应对这些挑战，我们提出了一种新颖的解决方案，它结合了易于实施和高解码精度：一种可通用的神经解码器，通过在大规模粗离散化单词手势轨迹上进行预训练来实现。这种方法产生了一个随时可用的 WGK 解码器，该解码器可在增强现实 (AR) 和虚拟现实 (VR) 中的空中和地面 WGK 系统中通用，这可以从四个不同数据集上 90.4% 的稳健平均 Top-4 准确率中看出。它的表现明显优于 SHARK^2，提高了 37.2%，并比传统的神经解码器高出 7.4%。此外，预训练的神经解码器在量化后的大小仅为 4 MB，不会牺牲准确性，并且可以实时运行，在 Quest 3 上仅需 97 毫秒即可执行。]]></description>
      <guid>https://arxiv.org/abs/2410.18099</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>RingGesture：基于深度学习词语预测框架的基于环的空中手势打字系统</title>
      <link>https://arxiv.org/abs/2410.18100</link>
      <description><![CDATA[arXiv:2410.18100v1 公告类型：新
摘要：文本输入是任何现代计算体验的关键功能，轻量级增强现实 (AR) 眼镜也不例外。轻量级 AR 眼镜专为全天候佩戴而设计，其局限性在于限制了多个摄像头在手部跟踪中的广泛视野。这一限制凸显了对额外输入设备的需求。我们提出了一个系统来解决这一差距：一种基于环的空中手势输入技术 RingGesture，利用电极标记手势轨迹的开始和结束，并利用惯性测量单元 (IMU) 传感器进行手部跟踪。这种方法提供了类似于 VR 耳机中基于光线投射的空中手势输入的直观体验，允许将手部动作无缝转换为光标导航。为了提高准确率和输入速度，我们提出了一种新颖的深度学习单词预测框架 Score Fusion，它由三个关键组件组成：a) 单词手势解码模型、b) 空间拼写校正模型和 c) 轻量级上下文语言模型。相比之下，该框架融合了这三个模型的分数，以更高的准确率预测最可能的单词。我们进行了比较和纵向研究，以证明两个关键发现：首先，RingGesture 的整体有效性，其平均文本输入速度为每分钟 27.3 个单词 (WPM)，峰值性能为 47.9 WPM。其次，我们强调了 Score Fusion 框架的卓越性能，与传统的单词预测框架 Naive Correction 相比，它的未校正字符错误率提高了 28.2%，从而使 RingGesture 的文本输入速度提高了 55.2%。此外，RingGesture 获得了 83 分的系统可用性分数，表明其可用性极佳。]]></description>
      <guid>https://arxiv.org/abs/2410.18100</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过整合北方森林季节性光学、SAR 和有限的 GEDI LiDAR 数据，采用深度学习方法估算冠层高度和不确定性</title>
      <link>https://arxiv.org/abs/2410.18108</link>
      <description><![CDATA[arXiv:2410.18108v1 公告类型：新
摘要：准确的森林冠层高度估计对于评估地上生物量和碳储量动态、支持生态系统监测服务（如木材供应、气候变化缓解和生物多样性保护）至关重要。然而，尽管机载激光雷达技术取得了进步，但由于轨道和采样限制，北半球高纬度地区的数据仍然有限。本研究介绍了一种使用深度学习回归模型生成空间连续、高分辨率冠层高度和不确定性估计的方法。我们整合了来自 Sentinel-1、Landsat 和 ALOS-PALSAR-2 的多源、多季节卫星数据，并以机载 GEDI 激光雷达作为参考数据。我们的方法在加拿大安大略省进行了测试，并通过机载激光雷达进行了验证，表现出色。通过将季节性 Sentinel-1 和 Landsat 特征与 PALSAR 数据结合起来，取得了最佳效果，得出的 R 平方为 0.72、RMSE 为 3.43 米、偏差为 2.44 米。使用季节性数据代替仅限夏季的数据可以将变异性提高 10%、误差减少 0.45 米、偏差减少 1 米。与最近的全球模型相比，深度学习模型的加权策略显着减少了高大树冠高度估计的误差，尽管它高估了较低的树冠高度。不确定性地图突显了森林边缘附近的更大不确定性，GEDI 测量在那里容易出错，SAR 数据可能会遇到后向散射问题，如缩短、重叠和阴影。这项研究增强了缺乏机载 LiDAR 覆盖的地区树冠高度估计技术，为林业、环境监测和碳储量估算提供了重要工具。]]></description>
      <guid>https://arxiv.org/abs/2410.18108</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>NaVIP：面向视障人士的以图像为中心的室内导航解决方案</title>
      <link>https://arxiv.org/abs/2410.18109</link>
      <description><![CDATA[arXiv:2410.18109v1 公告类型：新
摘要：由于缺乏卫星定位，室内导航具有挑战性。对于缺乏从寻路标牌获取信息能力的视障人士 (VIP) 来说，这一挑战更加严峻。其他传感器信号（例如蓝牙和激光雷达）可用于为用户创建带有位置更新的逐向导航解决方案。不幸的是，这些解决方案需要在整个环境中安装标签或使用相当昂贵的硬件。此外，这些解决方案需要高度的手动参与，这会增加成本，从而妨碍可扩展性。我们提出了一个图像数据集和相关的以图像为中心的解决方案 NaVIP，旨在实现无需基础设施且可扩展的视觉智能，并可以帮助 VIP 了解周围环境。具体来说，我们首先在四层楼的研究大楼中整理大规模手机摄像头数据，其中包含 300K 张图像，为创建以图像为中心的包容性室内导航和探索解决方案奠定基础。每幅图像都标有精确的 6DoF 相机姿势、室内 PoI 的详细信息和描述性标题，以协助 VIP。我们在两个主要方面进行基准测试：1) 定位系统和 2) 探索支持，优先考虑训练可扩展性和实时推理，以验证基于图像的解决方案在室内导航方面的前景。数据集、代码和模型检查点在 https://github.com/junfish/VIP_Navi 上公开提供。]]></description>
      <guid>https://arxiv.org/abs/2410.18109</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 Bits-back 编码进行点云压缩</title>
      <link>https://arxiv.org/abs/2410.18115</link>
      <description><![CDATA[arXiv:2410.18115v1 公告类型：新
摘要：本文介绍了一种使用位回编码压缩点云数据几何属性的新型无损压缩方法。我们的方法专门使用基于深度学习的概率模型来估计点云信息的香农熵，即 3D 浮点的几何属性。一旦使用卷积变分自动编码器 (CVAE) 估计出点云数据集的熵，我们就使用学习到的 CVAE 模型通过位回编码技术压缩点云的几何属性。我们的位回编码方法的新颖之处在于利用 CVAE 的学习到的隐变量模型来压缩点云数据。通过使用位回编码，我们可以将数据点之间的潜在相关性（例如形状和散射区域等相似的空间特征）捕获到低维潜在空间中，以进一步降低压缩率。我们的方法的主要见解是，我们可以实现与传统基于深度学习的方法一样具有竞争力的压缩率，同时显著降低存储和/或通信压缩编解码器的开销成本，使我们的方法更适用于实际场景。通过综合评估，我们发现与压缩大型点云数据集时压缩率的降低相比，开销成本要小得多。实验结果表明，我们提出的方法平均可以实现 1.56 位/点的压缩率，这明显低于 Google 的 Draco 等基线方法（压缩率为 1.83 位/点）。]]></description>
      <guid>https://arxiv.org/abs/2410.18115</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过变分扩散策略提高神经辐射场的超分辨率</title>
      <link>https://arxiv.org/abs/2410.18137</link>
      <description><![CDATA[arXiv:2410.18137v1 公告类型：新
摘要：我们提出了一种用于神经渲染中视图一致超分辨率 (SR) 的扩散引导框架的新方法。我们的方法利用现有的 2D SR 模型与变分分数蒸馏 (VSD) 和 LoRA 微调助手等先进技术相结合，通过空间训练显着提高放大的 2D 图像的质量和一致性，与文献中以前的方法相比，例如 DiSR-NeRF (1) 中提出的 Renoised Score Distillation (RSD) 或 DreamFusion 中提出的 SDS。VSD 分数有助于对 SR 模型进行精确微调，从而产生高质量、视图一致的图像。为了解决独立 SR 2D 图像之间不一致的常见挑战，我们集成了 DiSR-NeRF 框架中的迭代 3D 同步 (I3DS)。我们在 LLFF 数据集上的定量基准和定性结果表明，与 DiSR-NeRF 等现有方法相比，我们的系统具有更优异的性能。]]></description>
      <guid>https://arxiv.org/abs/2410.18137</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>现实环境中面向用户特定对象的个性化基于实例的导航</title>
      <link>https://arxiv.org/abs/2410.18195</link>
      <description><![CDATA[arXiv:2410.18195v1 公告类型：新
摘要：近年来，对室内环境中物体的视觉导航的研究兴趣显著增长。这种增长可以归因于最近在照片般逼真的模拟环境中可用的大型导航数据集，如 Gibson 和 Matterport3D。然而，这些数据集支持的导航任务通常仅限于获取时环境中存在的对象。此外，它们无法解释现实场景，其中目标对象是用户特定的实例，很容易与类似对象混淆，并且可能在环境中的多个位置找到。为了解决这些限制，我们提出了一项名为个性化实例导航 (PIN) 的新任务，其中具体代理的任务是通过在同一类别的多个实例中区分它来定位和到达特定的个人对象。该任务伴随着 PInNED，这是一个专用的新数据集，由照片般逼真的场景和额外的 3D 对象组成。在每一集中，目标对象使用两种方式呈现给代理：一组中性背景上的视觉参考图像和手动注释的文本描述。通过全面的评估和分析，我们展示了 PIN 任务的挑战以及当前可用的对象驱动导航方法的性能和缺点，同时考虑了模块化和端到端代理。]]></description>
      <guid>https://arxiv.org/abs/2410.18195</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>重新思考对比学习中的正对</title>
      <link>https://arxiv.org/abs/2410.18200</link>
      <description><![CDATA[arXiv:2410.18200v1 公告类型：新
摘要：对比学习是一种重要的表征学习方法，传统上假设正对是密切相关的样本（同一图像或类别），而负对是不同的样本。我们通过提出从任意对中学习来挑战这一假设，允许任何样本对在我们的框架内为正。所提出方法的主要挑战在于将对比学习应用于语义上相距甚远的不同对。受 SimCLR 可以在子空间中分离给定的任意对（例如，袜带蛇和台灯）这一发现的启发，我们提出了一种类对条件下的特征过滤器，它通过门向量选择性地激活或停用维度来创建必要的子空间。该过滤器可以通过传统对比学习机制中的梯度下降进行优化。
我们提出了 Hydra，这是一种用于视觉表征的通用对比学习框架，它扩展了传统的对比学习以适应任意对。我们的方法已使用 IN1K 进行验证，其中 1K 个不同的类别组成 500,500 对，其中大多数是不同的。令人惊讶的是，Hydra 在这种具有挑战性的环境中取得了卓越的表现。其他好处包括防止维度崩溃和发现类别关系。我们的工作突出了学习任意对的共同特征的价值，并可能扩大对比学习技术对具有弱关系的样本对的适用性。]]></description>
      <guid>https://arxiv.org/abs/2410.18200</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用深度学习自动检测和分级 Piarom 枣的缺陷</title>
      <link>https://arxiv.org/abs/2410.18208</link>
      <description><![CDATA[arXiv:2410.18208v1 公告类型：新
摘要：Piarom 枣是一种主要在伊朗种植的优质高价值品种，由于缺陷的复杂性和多变性，以及缺乏专门针对这种水果的自动化系统，其分级和质量控制面临着重大挑战。传统的人工检查方法劳动密集、耗时且容易出现人为错误，而现有的基于人工智能的分类解决方案不足以解决 Piarom 枣的细微特征。在本研究中，我们提出了一个创新的深度学习框架，专门用于实时检测、分类和分级 Piarom 枣。我们的框架利用一个自定义数据集，该数据集包含 11 个不同缺陷类别中注释的 9,900 多张高分辨率图像，集成了最先进的物体检测算法和卷积神经网络 (CNN)，以实现高精度的缺陷识别。此外，我们采用先进的分割技术来估计每个枣的面积和重量，从而根据行业标准优化分级过程。实验结果表明，我们的系统在准确性和计算效率方面明显优于现有方法，非常适合需要实时处理的工业应用。这项工作不仅为 Piarom 枣行业的自动化质量控制提供了强大且可扩展的解决方案，而且还为更广泛的 AI 驱动食品检测技术领域做出了贡献，可能应用于各种农产品。]]></description>
      <guid>https://arxiv.org/abs/2410.18208</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MsMorph：用于脑图像配准的无监督金字塔学习网络</title>
      <link>https://arxiv.org/abs/2410.18228</link>
      <description><![CDATA[arXiv:2410.18228v1 公告类型：新
摘要：在医学图像分析领域，图像配准是一项关键技术。尽管已经提出了许多配准模型，但现有方法在准确性和可解释性方面仍然存在不足。在本文中，我们提出了一个基于深度学习的图像配准框架 MsMorph，旨在模仿手动配准图像对的过程以实现更相似的变形，其中配准的图像对表现出特征的一致性或相似性。通过使用梯度提取图像对之间在各个方面的特征差异，该框架解码不同尺度的语义信息并不断补偿预测的变形场，从而驱动参数的优化以显着提高配准精度。所提出的方法模拟了手动配准方法，重点关注图像对的不同区域及其邻域来预测两幅图像之间的变形场，这提供了很强的可解释性。我们在两个公共脑 MRI 数据集上比较了几种现有的配准方法，包括 LPBA 和 Mindboggle。实验结果表明，我们的方法在 Dice 得分、豪斯多夫距离、平均对称表面距离和非雅可比等指标方面始终优于最先进的方法。源代码已公开发布在 https://github.com/GaodengFan/MsMorph]]></description>
      <guid>https://arxiv.org/abs/2410.18228</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CARLA2Real：一种用于减少 CARLA 模拟器中 sim2real 差距的工具</title>
      <link>https://arxiv.org/abs/2410.18238</link>
      <description><![CDATA[arXiv:2410.18238v1 公告类型：新
摘要：模拟器对于自动驾驶汽车、自动机器人和无人机等自主系统的研究是必不可少的。尽管在图形真实感等各种模拟方面取得了重大进展，但虚拟环境和现实世界环境之间仍然存在明显的差距。由于最终目标是在现实世界中部署自主系统，因此缩小 sim2real 差距至关重要。在本文中，我们采用了一种最先进的方法来增强模拟数据的照片真实感，使其与现实世界数据集的视觉特征保持一致。基于此，我们开发了 CARLA2Real，这是一种易于使用、可公开获得的工具（插件），适用于广泛使用的开源 CARLA 模拟器。该工具可以近乎实时地增强 CARLA 的输出，实现 13 FPS 的帧速率，将其转换为现实世界数据集（如 Cityscapes、KITTI 和 Mapillary Vistas）的视觉风格和真实感。通过使用所提出的工具，我们从模拟器和增强模型输出中生成了合成数据集，包括与自动驾驶相关任务的相应地面实况注释。然后，我们进行了大量实验，以评估所提出的方法在增强的合成数据上进行训练时对特征提取和语义分割方法的影响。结果表明，sim2real 差距很大，并且确实可以通过引入的方法缩小差距。]]></description>
      <guid>https://arxiv.org/abs/2410.18238</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>KhmerST：低资源高棉场景文本检测和识别基准</title>
      <link>https://arxiv.org/abs/2410.18277</link>
      <description><![CDATA[arXiv:2410.18277v1 公告类型：新
摘要：开发有效的场景文本检测和识别模型取决于大量的训练数据，而获取这些数据既费力又费钱，尤其是对于资源匮乏的语言而言。针对拉丁字符量身定制的传统方法在处理非拉丁文字时往往会失败，因为字符堆叠、变音符号和可变字符宽度没有明确的单词边界等挑战。在本文中，我们介绍了第一个高棉场景文本数据集，其中包含 1,544 张专家注释图像，包括 997 个室内场景和 547 个室外场景。这个多样化的数据集包括平面文本、凸起文本、光线不足的文本、远处和部分模糊的文本。注释为每个场景提供行级文本和多边形边界框坐标。该基准包括场景文本检测和识别任务的基线模型，为未来的研究工作提供了一个强大的起点。 KhmerST 数据集可在 https://gitlab.com/vannkinhnom123/khmerst 公开访问。]]></description>
      <guid>https://arxiv.org/abs/2410.18277</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AVHBench：用于视听大型语言模型的跨模态幻觉基准</title>
      <link>https://arxiv.org/abs/2410.18325</link>
      <description><![CDATA[arXiv:2410.18325v1 公告类型：新
摘要：继大型语言模型 (LLM) 取得成功之后，将其边界扩展到新模态代表了多模态理解的重大范式转变。人类感知本质上是多模态的，不仅依赖文本，还依赖听觉和视觉线索来全面理解世界。认识到这一事实，视听 LLM 最近应运而生。尽管取得了有希望的发展，但缺乏专门的基准对理解和评估模型构成了挑战。在这项工作中，我们表明视听 LLM 难以辨别音频和视觉信号之间的微妙关系，从而导致幻觉，这凸显了对可靠基准的需求。为了解决这个问题，我们推出了 AVHBench，这是第一个专门用于评估视听 LLM 的感知和理解能力的综合基准。我们的基准包括评估幻觉的测试，以及这些模型的跨模态匹配和推理能力。我们的结果表明，由于感知复杂多模态信号及其关系的能力有限，大多数现有的视听 LLM 都难以应对由模态间交叉相互作用引起的幻觉。此外，我们还证明，使用我们的 AVHBench 进行简单训练可以提高视听 LLM 对抗幻觉的稳健性。]]></description>
      <guid>https://arxiv.org/abs/2410.18325</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>实时 3D 感知人像视频补光</title>
      <link>https://arxiv.org/abs/2410.18355</link>
      <description><![CDATA[arXiv:2410.18355v1 公告类型：新
摘要：在自定义光照条件和视角下合成逼真的说话脸部视频有益于视频会议等各种下游应用。然而，大多数现有的重新照明方法要么耗时，要么无法调整视点。在本文中，我们提出了第一种基于神经辐射场 (NeRF) 的实时 3D 感知方法，用于重新照明野外说话脸部视频。给定输入肖像视频，我们的方法可以在新颖的视图和新颖的光照条件下合成说话的面孔，并具有照片般逼真和解开的 3D 表示。具体而言，我们使用快速双编码器推断出反照率三平面以及基于每个视频帧的所需光照条件的阴影三平面。我们还利用时间一致性网络来确保平滑过渡并减少闪烁伪影。我们的方法在消费级硬件上以 32.98 fps 的速度运行，并在重建质量、照明误差、照明不稳定性、时间一致性和推理速度方面取得了最佳效果。我们在具有不同照明和观看条件的各种肖像视频上展示了我们的方法的有效性和交互性。]]></description>
      <guid>https://arxiv.org/abs/2410.18355</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>