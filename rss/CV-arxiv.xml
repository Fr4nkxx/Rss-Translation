<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>CS.CV更新arxiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.cv更新arxiv.org e-print存档。</description>
    <lastBuildDate>Tue, 04 Mar 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>Voila：评估MLLM的感知理解和类比推理</title>
      <link>https://arxiv.org/abs/2503.00043</link>
      <description><![CDATA[ARXIV：2503.00043V2公告类型：新 
摘要：多模式大语言模型（MLLM）已成为整合视觉和文本信息的强大工具。尽管在视觉理解基准测试方面具有出色的表现，但测量其在多个图像中抽象推理的能力仍然是一个重大挑战。为了解决这个问题，我们介绍了Voila，这是一种大型，开放式，动态的基准测试，旨在评估MLLM的感知理解和抽象的关系推理。 Voila在视觉域中采用类似的映射方法，要求模型生成一个图像，该图像在不依赖预定义的选择的情况下完成了两个给定的图像对，参考和应用之间的类比。我们的实验表明，Voila中的类似推理任务对MLLM提出了挑战。通过多步分析，我们揭示了当前的MLLM难以理解图像间的关系并在高级关系推理中表现出有限的能力。值得注意的是，我们观察到，遵循最小至最小提示的多步策略时的性能会有所提高。对开源模型和GPT-4O的全面评估表明，在基于文本的答案中，挑战性场景的最佳准确性是13％（Llama 3.2），即使对于更简单的任务也是29％（GPT-4O），而人类绩效在两个难度水平上的70％处于70％的范围内显着更高。]]></description>
      <guid>https://arxiv.org/abs/2503.00043</guid>
      <pubDate>Tue, 04 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于YOLO的高级实时电力线检测植被管理</title>
      <link>https://arxiv.org/abs/2503.00044</link>
      <description><![CDATA[ARXIV：2503.00044V1公告类型：新 
摘要：电力线基础设施是电力系统的关键组成部分，并且正在迅速扩展以满足不断增长的能源需求。植被侵占是对电力线安全运行的重大威胁，需要可靠和及时的管理以增强电力网络的韧性和可靠性。整合智能电网技术，尤其是无人驾驶汽车（UAV），为用高级成像技术彻底改变了广泛的电力线网络的管理提供了巨大的潜力。但是，处理无人机巡逻捕获的大量图像仍然是一个重大挑战。本文引入了一个智能的实时监控框架，用于检测电源线和相邻的植被。它是根据深度学习卷积神经网络（CNN）开发的，您只能看一次（Yolo），以其高速对象检测功能而闻名。与现有的基于深度学习的方法不同，该框架通过将Yolov8与定向过滤器集成来提高准确性。他们可以提取电源线及其附近的定向特征和纹理，从而生成面向的边界框（OBB）以进行更精确的本地化。此外，开发了一种后处理算法，以创建电力线的植被侵占指标，从而可以对周围植被分布进行定量评估。使用广泛使用的电源线数据集证明了所提出的框架的有效性。]]></description>
      <guid>https://arxiv.org/abs/2503.00044</guid>
      <pubDate>Tue, 04 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用大型模型评估新内容：广告创造力的案例研究</title>
      <link>https://arxiv.org/abs/2503.00046</link>
      <description><![CDATA[ARXIV：2503.00046V1公告类型：新 
摘要：评估创造力甚至对人类来说都是挑战，这不仅是因为其主观性，还因为它涉及复杂的认知过程。受市场营销工作的启发，我们试图将视觉广告创造力分解为非典型性和独创性。通过对这些维度进行细粒度的人类注释，我们提出了针对此类主观问题的一系列任务。我们还评估了最先进的（SOTA）视觉语言模型（VLM）与人类之间的一致性，这表明了使用VLM进行自动创造力评估的承诺和挑战。]]></description>
      <guid>https://arxiv.org/abs/2503.00046</guid>
      <pubDate>Tue, 04 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Omni-Sila：朝着视频中识别，定位和归因于Omni-Scene驱动的视觉情感</title>
      <link>https://arxiv.org/abs/2503.00049</link>
      <description><![CDATA[ARXIV：2503.00049V1公告类型：新 
摘要：关于视觉情感理解的先前研究（VSU）主要依赖于明确的场景信息（例如面部表达）来判断视觉情感，这些信息在很大程度上忽略了隐式场景信息（例如，人类的行动，异议，反对关系和视觉背景），而这些信息对于精确发现的视觉情感至关重要。本文提出了一种新的Omni-Scene驱动的视觉情感，以识别，定位和归因于视频（Omni-Sila）任务，旨在通过露骨和隐式场景信息进行互动，精确地识别，定位，定位和属性的视觉情感。此外，本文认为，这项Omni-Sila任务面临两个关键挑战：建模场景并突出明确的隐式场景。为此，本文提出了一种隐性增强的因果MOE（ICM）来解决Omni-Sila任务。具体而言，场景均衡的MOE（SBM）和隐式增强因果（IEC）块是为模拟场景信息而定制的，并分别突出了隐式场景信息，分别超出了显式。对我们构建的明确和隐式Omni-Sila数据集的广泛实验结果证明了所提出的ICM方法比高级视频插件的优势。]]></description>
      <guid>https://arxiv.org/abs/2503.00049</guid>
      <pubDate>Tue, 04 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用模式的无对应姿势估计：多维视觉的统一方法</title>
      <link>https://arxiv.org/abs/2503.00051</link>
      <description><![CDATA[ARXIV：2503.00051V1公告类型：新 
摘要：6D姿势估计是机器人视觉中的核心问题。与基于点对应关系或其稳健版本的姿势估计相比，无对应方法通常更灵活。但是，现有的无对应方法通常依赖于特征表示对准或端到端回归。为此，提出了一种新的无对应姿势估计方法及其实际算法，其关键思想是通过添加的过程消除未知数，以使姿势估计与通讯分开。通过将考虑的点集作为模式，引入了用于描述这些模式的功能，以建立足够数量的优化方程。所提出的方法适用于非线性转换，例如透视投影，可以涵盖从3D到3D点，3D到2D点和2D到2D点的各种姿势估计。提供了模拟和实际数据的实验结果，以证明该方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2503.00051</guid>
      <pubDate>Tue, 04 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>rura-net：一种基于零射门学习的一般疾病诊断方法</title>
      <link>https://arxiv.org/abs/2503.00052</link>
      <description><![CDATA[ARXIV：2503.00052V1公告类型：新 
摘要：深度学习模型的培训取决于大量标记的数据。但是，医疗标签的高成本严重阻碍了医学领域的深度学习的发展。我们的研究提出了一种基于零射学习的一般疾病诊断方法。暹罗神经网络用于寻找针对靶疾病的类似疾病，U-NET分割模型用于准确细分该疾病的关键病变。最后，基于重新整体聚类算法，对聚类模型进行了大量类似疾病的样本数据的训练，以获得对靶疾病的近似诊断。然后成功实现了目标疾病的零射。为了评估模型的有效性，我们在CFP模式中的眼科疾病数据集上验证了我们的方法。外部数据集用于测试其性能，精度= 0.8395，精度= 0.8094，召回= 0.8463，F1得分= 0.8274，AUC = 0.9226，它超过了大多数少数几次学习和一次性学习模型的索引。它证明我们的方法在医学领域具有巨大的潜力和参考价值，在那里注释数据通常很少且获得昂贵。]]></description>
      <guid>https://arxiv.org/abs/2503.00052</guid>
      <pubDate>Tue, 04 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>解释投诉方面：迈向带有视频投诉数据集的基于方面的投诉标识模型</title>
      <link>https://arxiv.org/abs/2503.00054</link>
      <description><![CDATA[ARXIV：2503.00054V1公告类型：新 
摘要：在当今的竞争营销格局中，有效的投诉管理对于客户服务和业务成功至关重要。视频投诉，整合文本和图像内容，通过解决客户的不满并划定产品福利和缺点来提供宝贵的见解。但是，理解大量每日多模式财务数据中细微的投诉方面仍然是一个巨大的挑战。在解决这一差距方面，我们策划了一个专有的多模式视频投诉数据集，其中包括433个公开访问的实例。每个实例在话语层面上都经过精心注释，其中包括五个不同类别的财务方面及其相关的投诉标签。为了支持这项努力，我们推出了Solution 3.0，这是一种用于基于多模式的投诉标识任务的模型。解决方案3.0是针对执行三个关键任务而定制的：1）处理多模式功能（音频和视频），2）便利多标记方面分类，以及3）进行多任务针对方面分类和投诉标识的相似之处。解决方案3.0利用基于夹子的双冷冻编码器，具有用于全局特征融合的集成图像段编码器，通过上下文注意（ISEC）增强了（ISEC），以提高准确性和效率。我们提出的框架超过了当前的多模式基线，几乎通过开放新方法来加强适当的客户服务计划并有效地帮助个人解决问题，从而在几乎所有指标中都表现出了卓越的性能。]]></description>
      <guid>https://arxiv.org/abs/2503.00054</guid>
      <pubDate>Tue, 04 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用LLM生成的合成数据改进了Yolov12</title>
      <link>https://arxiv.org/abs/2503.00057</link>
      <description><![CDATA[ARXIV：2503.00057V1公告类型：新 
摘要：本研究评估了Yolov12对象检测模型的性能，并使用大语模型（LLMS）生成的合成图像与Yolov11和Yolov11进行了与Yolov11和Yolov10进行比较。 Yolov12n配置表现出色，达到0.916的最高精度，最高召回率为0.969，平均平均精度最高（MAP@50）为0.978。相比之下，Yolov11系列由Yolo11X领导，Yolo11X的最高精度为0.857，召回0.85，MAP@50的MAP@50在0.91处。对于Yolov10系列，Yolov10b和Yolov10l以0.85的最高精度并列，而Yolov10n的召回率最高为0.8，MAP@50在0.89处获得。该研究还强调了处理速度的效率，与Yolov12n的5.6 ms和Yolov10n的5.9 ms相比，Yolov11n报告的推理时间最低。尽管Yolov12的新功能比Yolov11更准确，而Yolov10，但Yolo11n仍然是Yolov10，Yolov11和Yolov12中最快的Yolo算法。这些发现表明，在接受高质量LLM生成的数据集接受培训时，Yolov12不仅超过了其主要绩效指标的前身，而且还通过减少对现场广泛的手动数据收集的需求，提供了一种具有成本效益的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2503.00057</guid>
      <pubDate>Tue, 04 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过深度学习使用服装识别的非洲性别分类</title>
      <link>https://arxiv.org/abs/2503.00058</link>
      <description><![CDATA[ARXIV：2503.00058V1公告类型：新 
摘要：人类属性识别和分类对于计算机视觉至关重要，推动了创新识别系统的发展。传统的性别分类方法主要依赖于面部识别，尽管有效，但在非理想条件下的斗争，例如模糊，侧视观点或部分遮挡。这项研究通过利用服装识别，特别关注非洲传统服装来探讨另一种方法，该服装具有文化意义和特定性别的特征。
  我们使用Afrifashion1600数据集，该数据集是1,600张非洲传统服装图像的策划收藏集，该图像标记为两个性别类别：男性和女性。基于修改后的VGG16体系结构并使用转移学习培训的深度学习模型是为分类而开发的。应用数据增强来解决相对较小的数据集带来的挑战并减轻过度拟合。该模型在测试集中的精度达到了87％，尽管数据集失衡有利于女性样本，但表现出强大的预测能力。
  这些发现凸显了基于服装的识别作为在非洲背景下对性别分类面部识别的补充技术的潜力。未来的研究应着重于扩展和平衡数据集，以增强分类鲁棒性并提高基于服装的性别识别系统的适用性。]]></description>
      <guid>https://arxiv.org/abs/2503.00058</guid>
      <pubDate>Tue, 04 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>综合大型语言模型中调查和增强视觉审计能力</title>
      <link>https://arxiv.org/abs/2503.00059</link>
      <description><![CDATA[ARXIV：2503.00059V1公告类型：新 
摘要：综合大型语言模型（OLLMS）在整合视觉和文本方面已显示出重大进展，但仍在整合视觉和音频方面努力，与文本查询相比，处理音频查询时通常表现出次优性能。这种差异主要是由于训练过程中视力和音频方式之间的一致性不足，因此在使用音频查询时会引起对视觉信息的关注。为了减轻此问题，我们提出了一种自我知识蒸馏（自我KD）培训方法，其中OLLM的愿景文本组成部分是老师，并且作为学生的视觉 - 审计组成部分。这使模型能够以类似于文本处理的方式处理音频。我们的实验结果表明，自我KD是一种通过从视觉文本组件中学习来增强OLLM的视觉原理能力的有效方法，该组件随后改善了音频与图像之间的相互作用，并在多模式任务上提高了性能。]]></description>
      <guid>https://arxiv.org/abs/2503.00059</guid>
      <pubDate>Tue, 04 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SAC-VIT：带有早期出口的语义意识聚类视觉变压器</title>
      <link>https://arxiv.org/abs/2503.00060</link>
      <description><![CDATA[ARXIV：2503.00060V1公告类型：新 
摘要：Vision Transformer（VIT）在全球建模中表现出色，但由于其注意力机制的二次计算复杂性，面临资源受限设备的部署挑战。为了解决这个问题，我们提出了语义吸引的聚类视觉变压器（SAC-VIT），这是一种提高VIT计算效率的非著作方法。 SAC-VIT分为两个阶段：早期出口（EE）和语义意识聚类（SAC）。在EE阶段，处理下采样的输入图像以提取全局语义信息并生成初始推理结果。如果这些结果不符合EE终止标准，则将这些信息聚集到目标和非目标令牌中。在SAC阶段，目标令牌映射回原始图像，裁剪和嵌入。然后将这些目标令牌与从EE阶段重复使用的非目标令牌结合使用，并在每个群集中应用注意力机制。这种两阶段的设计具有端到端优化，可降低空间冗余并提高计算效率，从而显着提高整体VIT性能。广泛的实验证明了SAC-VIT的功效，减少了DEIT的62％，并实现了1.98次吞吐量而不会损害性能。]]></description>
      <guid>https://arxiv.org/abs/2503.00060</guid>
      <pubDate>Tue, 04 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Nopain：通过最佳传输单数边界通过最佳传输量攻击无框点云攻击</title>
      <link>https://arxiv.org/abs/2503.00063</link>
      <description><![CDATA[ARXIV：2503.00063V2公告类型：新 
摘要：对抗性攻击利用了深层模型对对抗样本的脆弱性。现有的点云攻击者是针对特定型号量身定制的，基于白色框或黑色盒子设置中的渐变对扰动进行迭代优化。尽管他们有希望的攻击性能，但由于过度拟合替代模型的特定参数，他们通常很难生产可转移的对抗样本。为了克服这个问题，我们将重点转移到数据分布本身，并引入了一种名为Nopain的新方法，该方法采用了最佳运输（OT）来确定跨网络点云攻击的数据歧管的固有奇异界限。具体而言，我们首先计算从噪声到目标特征空间的OT映射，然后通过定位非差异位置来识别单数边界。最后，我们沿着单数边界进行采样，以生成对抗点云。一旦确定了奇异边界，Nopain就可以有效地产生对抗样本，而无需迭代更新或替代分类器的指导。广泛的实验表明，拟议的端到端方法在可转让性和效率方面都优于基线方法，同时甚至在针对国防策略方面也保持了显着的优势。源代码将公开可用。]]></description>
      <guid>https://arxiv.org/abs/2503.00063</guid>
      <pubDate>Tue, 04 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PI-HMR：通过接触压力传感，朝着强大的内部颞型人体形状重建</title>
      <link>https://arxiv.org/abs/2503.00068</link>
      <description><![CDATA[ARXIV：2503.00068V1公告类型：新 
摘要：医疗保健内的长期内床监测益处自动​​和实时健康管理，人类形态重建技术的进步进一步增强了用户活动模式的表示和可视化。但是，现有技术主要基于视觉提示，面临着非视线和隐私敏感的床单的严重挑战。压力感应床单为实时运动重建提供了有希望的解决方案。然而，模型设计和数据的探索有限阻碍了其进一步的发展。为了解决这些问题，我们提出了一个一般框架，该框架弥合了数据注释和模型设计中的差距。首先，我们介绍了Smplify-IB，这是一种优化方法，该方法通过重力约束克服了顶级视图中的深度歧义问题，从而为内部的数据集生成了高质量的3D人类形状注释。然后，我们提出PI-HMR，这是一种基于时间的人形估计器，可从压力序列回归网格。通过将多尺度特征融合与高压分布和空间位置先验进行集成，PI-HMR的表现优于SOTA方法，SOTA方法具有17.01mm的均值下降 -  eRROR降低。这项工作提供了整体]]></description>
      <guid>https://arxiv.org/abs/2503.00068</guid>
      <pubDate>Tue, 04 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>我明白您的意思：多模式对话中参考分辨率的共同语音手势</title>
      <link>https://arxiv.org/abs/2503.00071</link>
      <description><![CDATA[ARXIV：2503.00071V1公告类型：新 
摘要：在面对面的互动中，我们使用多种方式，包括语音和手势，来传达信息并解决对象的引用。但是，从计算的角度来看，代表性的共同语音手势是指对象仍然被忽略了。在这项工作中，我们通过引入以代表性手势为中心的多模式参考分辨率任务来解决这一差距，同时解决了学习强大的手势嵌入的挑战。我们提出了一种自我监督的预训练方法，用于以口语为基础的身体运动。我们的实验表明，学到的嵌入与专家注释保持一致，并具有重要的预测能力。此外，当（1）使用多模式的手势表示时，即使在推理时间不可用的语音以及（2）利用对话历史记录时，参考分辨率精度也会进一步提高。总体而言，我们的发现突出了参考分辨率中手势和语音的互补作用，为更自然的人类机器相互作用提供了一步。]]></description>
      <guid>https://arxiv.org/abs/2503.00071</guid>
      <pubDate>Tue, 04 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>预测体积视频的全脑神经元活动</title>
      <link>https://arxiv.org/abs/2503.00073</link>
      <description><![CDATA[ARXIV：2503.00073V1公告类型：新 
摘要：具有荧光钙指标的大规模神经元活动记录越来越普遍，产生高分辨率的2D或3D视频。传统的分析管道通过细分感兴趣的区域将这些数据减少到1D跟踪，从而导致不可避免的信息丢失。受到深度学习在其他领域中最低处理数据的成功的启发，我们直接研究了预测神经元活动的潜力。为了捕获高分辨率体积全脑记录中的远距离依赖性，我们设计了一个具有大型接收场的模型，使其能够整合大脑内部遥远地区的信息。我们探讨了预训练并执行广泛的模型选择的影响，分析时空权衡以产生准确的预测。我们的模型在Zapbench上的表现优于基于痕量的预测方法，这是斑马鱼全脑活动预测的最近提出的基准，证明了保留神经元活性空间结构的优势。]]></description>
      <guid>https://arxiv.org/abs/2503.00073</guid>
      <pubDate>Tue, 04 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>