<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Tue, 24 Sep 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>内在单图像 HDR 重建</title>
      <link>https://arxiv.org/abs/2409.13803</link>
      <description><![CDATA[arXiv:2409.13803v1 公告类型：新
摘要：普通相机的低动态范围 (LDR) 无法捕捉自然场景中的丰富对比度，导致饱和像素中的颜色和细节丢失。从单个 LDR 照片重建场景中存在的高动态范围 (HDR) 亮度是一项重要任务，在计算摄影和图像的真实显示中有许多应用。HDR 重建任务旨在使用场景中存在的上下文推断丢失的细节，需要神经网络理解高级几何和照明线索。这使得数据驱动算法难以生成准确和高分辨率的结果。在这项工作中，我们引入了内在域中 HDR 重建问题的物理启发式重构。内在模型允许我们训练单独的网络来扩展阴影域中的动态范围并恢复反照率域中丢失的色彩细节。我们表明，将问题分成两个更简单的子任务可以提高各种照片的性能。]]></description>
      <guid>https://arxiv.org/abs/2409.13803</guid>
      <pubDate>Tue, 24 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ViTGuard：针对 Vision Transformer 的对抗性示例的注意力感知检测</title>
      <link>https://arxiv.org/abs/2409.13828</link>
      <description><![CDATA[arXiv:2409.13828v1 公告类型：新
摘要：Transformer 在视觉任务中的应用挑战了卷积神经网络 (CNN) 在计算机视觉 (CV) 中的传统主导地位。对于图像分类任务，Vision Transformer (ViT) 有效地建立了图像内补丁之间的空间关系，将注意力引向重要区域以进行准确预测。然而，与 CNN 类似，ViT 容易受到对抗性攻击，这种攻击会误导图像分类器对具有精心设计的扰动的图像做出错误的决策。此外，对抗性补丁攻击会在小范围内引入任意扰动，对 ViT 构成更严重的威胁。更糟糕的是，最初为 CNN 模型设计的传统检测方法在应用于 ViT 时不切实际或性能显著下降，而且它们通常会忽略补丁攻击。
在本文中，我们提出了 ViTGuard 作为一种通用检测方法，用于保护 ViT 模型免受对抗性攻击，包括扰动蔓延到整个输入的典型攻击和补丁攻击。 ViTGuard 使用蒙版自动编码器 (MAE) 模型从未蒙版区域恢复随机蒙版补丁，从而提供灵活的图像重建策略。然后，基于阈值的检测器利用独特的 ViT 特征（包括注意力图和分类 (CLS) 标记表示）来区分正常样本和对抗样本。MAE 模型在训练期间不涉及任何对抗样本，从而确保了我们的检测器对看不见的攻击的有效性。在三个数据集上的九种攻击下，将 ViTGuard 与七种现有检测方法进行了比较。评估结果表明 ViTGuard 优于现有检测器。最后，考虑到潜在的检测逃避，我们进一步证明了 ViTGuard 对逃避自适应攻击的鲁棒性。]]></description>
      <guid>https://arxiv.org/abs/2409.13828</guid>
      <pubDate>Tue, 24 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于脑扩散 MRI 视野扩展的多模态条件变分 U-Net</title>
      <link>https://arxiv.org/abs/2409.13846</link>
      <description><![CDATA[arXiv:2409.13846v1 公告类型：新
摘要：扩散磁共振成像 (dMRI) 中的不完整视场 (FOV) 会严重阻碍全脑白质连接的体积和束分析。尽管现有研究已经研究了使用深度生成模型来填补缺失区域，但如何具体利用来自成对多模态数据的附加信息以及这是否可以提高填补质量并对下游纤维束成像有用仍不清楚。为了填补这一空白，我们提出了一种新颖的框架，通过将 FOV 获得部分中学习到的扩散特征整合到完整的大脑解剖结构中，在 FOV 的不完整部分中填补 dMRI 扫描。我们假设通过这种设计，所提出的框架可以增强 dMRI 扫描的填补性能，因此可用于修复 FOV 不完整损坏的 dMRI 扫描中的全脑纤维束成像。我们在来自不同地点的两个队列中测试了我们的框架，总共 96 名受试者，并将其与平等处理 T1w 和 dMRI 扫描信息的基线插补方法进行了比较。所提出的框架在插补性能方面取得了显著的改善，如角度相关系数 (p &lt; 1E-5) 所示，并在下游纤维束成像准确性方面取得了显著的改善，如 Dice 评分 (p &lt; 0.01) 所示。结果表明，与基线方法相比，所提出的框架通过专门利用来自成对多模态数据的附加信息，提高了 dMRI 扫描的插补性能。所提出的框架实现的插补增强了全脑纤维束成像，因此减少了分析与神经退行性疾病相关的束时的不确定性。]]></description>
      <guid>https://arxiv.org/abs/2409.13846</guid>
      <pubDate>Tue, 24 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SSE：面向工业规模数据同化的多模态语义数据选择与丰富</title>
      <link>https://arxiv.org/abs/2409.13860</link>
      <description><![CDATA[arXiv:2409.13860v1 公告类型：新
摘要：近年来，为人工智能收集的数据已增长到难以管理的数量。特别是在自动驾驶汽车等工业应用中，模型训练计算预算超出预期，而模型性能却已达到饱和状态——而且还有更多数据不断涌入。为了驾驭海量数据，我们提出了一个框架来选择语义上最多样化和最重要的数据集部分。然后，我们通过从大量未标记的数据池中发现有意义的新数据来进一步丰富它的语义。重要的是，我们可以利用基础模型为每个数据点生成语义，从而提供可解释性。我们定量地表明，我们的语义选择和丰富框架 (SSE) 可以 a) 使用较小的训练数据集成功保持模型性能，b) 通过丰富较小的数据集而不超出原始数据集大小来提高模型性能。因此，我们证明语义多样性对于最佳数据选择和模型性能至关重要。]]></description>
      <guid>https://arxiv.org/abs/2409.13860</guid>
      <pubDate>Tue, 24 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过对比学习的 Graph-GCCA 进行脑认知指纹识别</title>
      <link>https://arxiv.org/abs/2409.13887</link>
      <description><![CDATA[arXiv:2409.13887v1 公告类型：新
摘要：许多纵向神经影像学研究旨在通过研究大脑功能和认知之间的动态相互作用来提高对大脑衰老和疾病的理解。这样做需要准确编码它们的多维关系，同时考虑到个体随时间的变化。为此，我们提出了一种无监督学习模型（称为基于 \underline{\textbf{对比学习的 \underline{\textbf{Gra}}ph 广义 \underline{\textbf{典型相关分析 (CoGraCa)），该模型通过图形注意网络和广义典型相关分析对它们的关系进行编码。为了创建反映每个人独特神经和认知表型的大脑认知指纹，该模型还依赖于个性化和多模态对比学习。我们将 CoGraCa 应用于健康个体的纵向数据集，该数据集由每个参与者多次访问时获得的静息状态功能 MRI 和认知测量组成。生成的指纹可有效捕捉显著的个体差异，在识别性别和年龄方面优于当前的单模态和基于 CCA 的多模态模型。更重要的是，我们的编码提供了这两种模态之间可解释的交互。]]></description>
      <guid>https://arxiv.org/abs/2409.13887</guid>
      <pubDate>Tue, 24 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>OneBEV：使用一张全景图进行鸟瞰语义映射</title>
      <link>https://arxiv.org/abs/2409.13912</link>
      <description><![CDATA[arXiv:2409.13912v1 公告类型：新
摘要：在自动驾驶领域，鸟瞰图 (BEV) 感知与针孔前视图像和全景图相比，它提供了更全面的信息，因此引起了社区越来越多的关注。传统的 BEV 方法依赖于多个窄视场摄像机和复杂的姿势估计，经常面临校准和同步问题。为了突破上述挑战，在这项工作中，我们引入了 OneBEV，这是一种新颖的 BEV 语义映射方法，仅使用单个全景图像作为输入，简化了映射过程并降低了计算复杂度。一种称为 Mamba View Transformation (MVT) 的失真感知模块专门用于处理全景图中的空间失真，将前视特征转换为 BEV 特征，而无需利用传统的注意机制。除了高效的框架外，我们还贡献了两个数据集，即 nuScenes-360 和 DeepAccident-360，专为 OneBEV 任务量身定制。实验结果表明，OneBEV 在 nuScenes-360 和 DeepAccident-360 上分别以 51.1% 和 36.1% 的 mIoU 实现了最佳性能。这项工作推动了自动驾驶中的 BEV 语义映射，为更先进、更可靠的自动驾驶系统铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2409.13912</guid>
      <pubDate>Tue, 24 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过可分离性、完整性和模型不确定性感知重要性抽样进行数据修剪</title>
      <link>https://arxiv.org/abs/2409.13915</link>
      <description><![CDATA[arXiv:2409.13915v1 公告类型：新
摘要：本文通过引入一种基于重要性抽样的新型剪枝度量和剪枝程序，改进了现有的图像分类数据剪枝方法。所提出的剪枝度量明确考虑了数据可分离性、数据完整性和模型不确定性，而采样程序则适应剪枝率，并考虑了类内和类间分离，以进一步提高剪枝的有效性。此外，该采样方法可以很容易地应用于其他剪枝度量以提高其性能。总体而言，所提出的方法可以很好地扩展到高剪枝率，并且在不同的分类模型中具有更好的泛化能力，这已在四个基准数据集（包括细粒度分类场景）上的实验中得到证明。]]></description>
      <guid>https://arxiv.org/abs/2409.13915</guid>
      <pubDate>Tue, 24 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>TalkMosaic：具有多模式 LLM 问答交互的交互式 PhotoMosaic</title>
      <link>https://arxiv.org/abs/2409.13941</link>
      <description><![CDATA[arXiv:2409.13941v1 公告类型：新
摘要：我们使用各种各样的汽车图像来合成鸟类或狮子等动物的图像，以保护环境为主题，以最大限度地在单个合成图像中包含有关汽车的信息，并提高人们对环境挑战的认识。我们提出了一种与艺术合成的照片马赛克图像进行图像交互的新颖方式，其中使用简单的“单击并显示”操作来演示照片马赛克图像中的平铺图像与相应的原始汽车图像之间的交互切换，该图像将自动保存在桌面上。我们通过将汽车图像信息和相关知识整合到 ChatGPT 中，构建了一个名为 TalkMosaic 的多模态自定义 GPT。通过将原始汽车图像上传到 TalkMosaic，我们可以针对给定的汽车图像提出问题并高效有效地获得相应的答案，例如在哪里可以买到汽车图像中满足高环保标准的轮胎。我们深入分析了如何使用稀疏注意力和量化技术加速多模态 LLM 的推理，并提出了概率 FlashAttention (PrFlashAttention) 和阶梯自适应量化 (SAQ) 方法。实施的原型证明了所提方法的可行性和有效性。]]></description>
      <guid>https://arxiv.org/abs/2409.13941</guid>
      <pubDate>Tue, 24 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>深度学习可实现快速分割和关键尺寸计量与表征，从而实现 AR/VR 设计和制造</title>
      <link>https://arxiv.org/abs/2409.13951</link>
      <description><![CDATA[arXiv:2409.13951v1 公告类型：新
摘要：显微镜图像的定量分析对于增强现实/虚拟现实 (AR/VR) 模块中使用的组件的设计和制造至关重要。然而，从这些复杂的图像中分割感兴趣的区域 (ROI) 并提取关键尺寸 (CD) 需要新技术，例如深度学习模型，这些模型对于流程、材料和设备优化的可行决策至关重要。在本研究中，我们报告了使用多样化的电子显微镜图像数据集对预训练的 Segment Anything 模型 (SAM) 进行微调。我们采用了低秩自适应 (LoRA) 等方法来减少训练时间并提高 ROI 提取的准确性。该模型能够推广到看不见的图像，这有助于零样本学习，并支持从分割的 ROI 中精确提取 CD 的 CD 提取模型。我们展示了如何在单类和多类模式下从表面浮雕光栅 (SRG) 和菲涅尔透镜的横截面图像中准确提取二进制图像。此外，这些二进制图像还用于识别过渡点，帮助提取相关的 CD。微调分割模型和 CD 提取模型的结合使用通过增强分析能力、数据和洞察时间以及优化制造流程，为各种工业应用提供了巨大的优势。]]></description>
      <guid>https://arxiv.org/abs/2409.13951</guid>
      <pubDate>Tue, 24 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有自适应衰减时间表面和极性感知跟踪的单目事件惯性里程计</title>
      <link>https://arxiv.org/abs/2409.13971</link>
      <description><![CDATA[arXiv:2409.13971v1 公告类型：新
摘要：事件相机因其在低功耗、高动态范围和无运动模糊方面优于传统相机而备受关注。本文提出了一种单目事件惯性里程计，结合了基于自适应衰减核的时间表面和极性感知跟踪。我们利用基于自适应衰减的时间表面从异步事件中提取纹理信息，以适应事件流的动态特性并增强环境纹理的表示。然而，极性加权时间表面在运动方向改变时会受到事件极性变化的影响。为了减轻其对特征跟踪的不利影响，我们通过合并额外的极性反转时间表面来优化特征跟踪以增强鲁棒性。与视觉惯性和事件惯性里程计方法的比较分析表明，我们的方法优于最先进的技术，在各种数据集上均具有竞争力的结果。]]></description>
      <guid>https://arxiv.org/abs/2409.13971</guid>
      <pubDate>Tue, 24 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用频域洞察检测修复视频</title>
      <link>https://arxiv.org/abs/2409.13976</link>
      <description><![CDATA[arXiv:2409.13976v1 公告类型：新 
摘要：视频修复可在帧内无缝删除和替换内容，如果误用，则会带来道德和法律风险。为了减轻这些风险，检测修复视频中的被操纵区域至关重要。以前的检测方法通常仅关注从空间和时间维度得出的特征，这限制了它们的有效性，因为它们忽略了不同修复算法的独特频率特性。在本文中，我们提出了频域洞察网络 (FDIN)，它通过结合频域洞察显着提高了检测准确性。我们的网络具有自适应频带选择响应模块，可辨别特定于各种修复技术的频率特性，以及基于快速傅立叶卷积的注意模块，用于识别修复区域中的周期性伪影。利用 3D ResBlocks 进行时空分析，FDIN 逐步将检测精度从广泛评估提高到详细定位。在公共数据集上的实验评估表明，FDIN 达到了最先进的性能，为视频修复检测树立了新的标杆。]]></description>
      <guid>https://arxiv.org/abs/2409.13976</guid>
      <pubDate>Tue, 24 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>有效利用所有未标记数据来改进 3D 半监督学习</title>
      <link>https://arxiv.org/abs/2409.13977</link>
      <description><![CDATA[arXiv:2409.13977v1 公告类型：新
摘要：半监督学习 (SSL) 已证明其在利用大量未标记数据的同时从少量标记数据中学习有效 3D 表示方面的有效性。传统的半监督方法依赖于预测未标记数据的伪标签并将其纳入学习过程的基本概念。然而，我们发现现有方法没有充分利用所有未标记样本，因此限制了它们的潜在性能。为了解决这个问题，我们提出了 AllMatch，一种基于 SSL 的新型 3D 分类框架，可以有效地利用所有未标记样本。 AllMatch 包含三个模块：（1）自适应硬增强模块，对具有较低损失值的高置信度未标记样本应用相对硬的增强，从而增强此类样本的贡献；（2）逆向学习模块，通过学习不学习的内容进一步提高未标记数据的利用率；（3）对比学习模块，确保在监督和无监督设置中从所有样本中学习。在两个流行的 3D 数据集上进行的综合实验表明，使用 1% 的标记数据，性能提升高达 11.2%，远远超过了 SOTA。此外，AllMatch 还展示了其在有效利用所有未标记数据方面的效率，仅 10% 的标记数据就能达到与使用所有标记数据的全监督学习几乎相同的性能。我们的工作代码可在以下位置找到：https://github.com/snehaputul/AllMatch。]]></description>
      <guid>https://arxiv.org/abs/2409.13977</guid>
      <pubDate>Tue, 24 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FracGM：Geman-McClure 稳健估计的快速分数规划技术</title>
      <link>https://arxiv.org/abs/2409.13978</link>
      <description><![CDATA[arXiv:2409.13978v1 公告类型：新
摘要：稳健估计在计算机视觉、机器人和导航中至关重要，旨在最大限度地减少异常测量的影响以提高准确性。我们提出了一种利用分数规划技术的 Geman-McClure 稳健估计的快速算法 FracGM。该求解器将原始的非凸分数问题重新表述为凸对偶问题和线性方程组，以交替优化模式迭代求解它们。与分级非凸性方法相比，该策略表现出更快的收敛速度和更好的异常值拒绝能力。此外，在给定条件下可以保证所提出的求解器的全局最优性。我们用 Wahba 的旋转问题和 3-D 点云配准以及松弛预处理和投影后处理来演示所提出的 FracGM 求解器。与最先进的算法相比，当异常率从 20\% 增加到 80\% 时，FracGM 的旋转和平移增量分别降低了 53\% 和 88\%。在实际场景中，FracGM 在 18 个结果中的 13 个中取得了更好的结果，同时计算时间提高了 19.43\%。]]></description>
      <guid>https://arxiv.org/abs/2409.13978</guid>
      <pubDate>Tue, 24 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>增强大型语言模型的高级视觉推理能力</title>
      <link>https://arxiv.org/abs/2409.13980</link>
      <description><![CDATA[arXiv:2409.13980v1 公告类型：新
摘要：视觉语言 (VL) 研究的最新进展为复杂视觉推理带来了新的基准，对模型的高级推理能力提出了挑战。传统的视觉语言模型 (VLM) 在视觉感知任务中表现良好，但在复杂的推理场景中却举步维艰。相反，大型语言模型 (LLM) 表现出强大的文本推理能力；然而，它们缺乏视觉敏锐度。为了弥补这一差距，我们提出了复杂视觉推理大型语言模型 (CVR-LLM)，利用 VLM 的视觉感知能力和 LLM 的广泛推理能力。与最近需要投影层的多模态大型语言模型 (MLLM) 不同，我们的方法使用迭代自细化循环将图像转换为详细的、上下文感知的描述，并利用 LLM 的文本知识进行准确预测，而无需额外的训练。我们还引入了一种新颖的多模态情境学习 (ICL) 方法，以增强 LLM 的情境理解和推理能力。此外，我们还引入了比较链 (CoC)，这是一种逐步比较技术，可以对比预测的各个方面。我们的 CVR-LLM 首次对各种复杂的视觉推理任务进行了全面的研究，并在所有任务中取得了 SOTA 性能。]]></description>
      <guid>https://arxiv.org/abs/2409.13980</guid>
      <pubDate>Tue, 24 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CUS3D：基于 CLIP 的对象级去噪无监督 3D 分割</title>
      <link>https://arxiv.org/abs/2409.13982</link>
      <description><![CDATA[arXiv:2409.13982v1 公告类型：新
摘要：为了缓解在 3D 数据中获取注释标签的难度，一种常用的方法是使用无监督和开放词汇语义分割，利用 2D CLIP 语义知识。在本文中，与以前的研究忽略从 2D 到 3D 的特征投影过程中产生的“噪音”不同，我们提出了一种名为 CUS3D 的新型蒸馏学习框架。在我们的方法中，设计了一个对象级去噪投影模块来筛选出“噪音”并确保更准确的 3D 特征。基于获得的特征，设计了一个多模态蒸馏学习模块，将 3D 特征与具有以对象为中心的约束的 CLIP 语义特征空间对齐，以实现高级无监督语义分割。我们对无监督和开放词汇分割进行了全面的实验，结果一致展示了我们的模型在实现高级无监督分割结果方面的优越性以及其在开放词汇分割中的有效性。]]></description>
      <guid>https://arxiv.org/abs/2409.13982</guid>
      <pubDate>Tue, 24 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>