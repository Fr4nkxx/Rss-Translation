<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Fri, 26 Jul 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>PEEKABOO：隐藏图像的部分内容，以实现无监督的物体定位</title>
      <link>https://arxiv.org/abs/2407.17628</link>
      <description><![CDATA[arXiv:2407.17628v1 公告类型：新
摘要：由于缺乏关键的视觉信息（例如物体的外观、类型和数量），以及缺乏通常在监督设置中可用的标记物体类别，因此以无监督方式定位物体带来了重大挑战。虽然最近的无监督物体定位方法通过利用自监督的视觉表征取得了重大进展，但它们通常需要计算密集型的训练过程，导致在计算、可学习参数和数据方面的资源需求很高。它们还缺乏视觉上下文的明确建模，可能会限制它们在物体定位方面的准确性。为了应对这些挑战，我们提出了一个单阶段学习框架，称为 PEEKABOO，用于无监督物体定位，通过图像掩蔽在定位物体的像素和形状级别学习基于上下文的表示。关键思想是选择性地隐藏图像的某些部分，并利用剩余的图像信息在没有明确监督的情况下推断物体的位置。在各种基准数据集上进行的定量和定性实验结果都表明，与单目标发现和无监督显著目标检测任务中最先进的方法相比，我们的方法简单、有效且性能极具竞争力。代码和预训练模型可在以下位置获取：https://github.com/hasibzunair/peekaboo]]></description>
      <guid>https://arxiv.org/abs/2407.17628</guid>
      <pubDate>Fri, 26 Jul 2024 06:17:47 GMT</pubDate>
    </item>
    <item>
      <title>S-E 管道：基于视觉转换器 (ViT) 的弹性分类管道，用于抵御对抗性攻击的医学成像</title>
      <link>https://arxiv.org/abs/2407.17587</link>
      <description><![CDATA[arXiv:2407.17587v1 公告类型：新
摘要：视觉变换器 (ViT) 凭借其强大的自注意力机制，在医学成像中自动准确诊断疾病方面越来越受欢迎。然而，ViT 仍然容易受到对抗性攻击，这些攻击可能会导致诊断过程故意错误分类重大疾病，从而阻碍诊断过程。在本文中，我们提出了一种新颖的图像分类管道，即 S-E 管道，它执行多个预处理步骤，允许 ViT 接受关键特征的训练，以减少对手输入扰动的影响。我们的方法结合使用分割和图像增强技术，例如对比度限制自适应直方图均衡化 (CLAHE)、反锐化掩模 (UM) 和高频强调滤波 (HFE) 作为预处理步骤，以识别即使在对抗性扰动后仍保持完整的关键特征。实验研究表明，我们新颖的流程有助于将对抗性攻击的影响降低 72.22%（ViT-b32 模型）和 86.58%（ViT-l32 模型）。此外，我们还展示了在 NVIDIA Jetson Orin Nano 板上端到端部署我们提出的方法，以展示其在通常资源受限的现代手持设备中的实际用例。]]></description>
      <guid>https://arxiv.org/abs/2407.17587</guid>
      <pubDate>Fri, 26 Jul 2024 06:17:46 GMT</pubDate>
    </item>
    <item>
      <title>质量保证：重新思考影像 AI 中的注释策略</title>
      <link>https://arxiv.org/abs/2407.17596</link>
      <description><![CDATA[arXiv:2407.17596v1 公告类型：新
摘要：本文并未描述一种新方法。相反，它研究了可靠的基准测试和最终的基于 AI 的图像分析的实际应用的重要基础：生成高质量的参考注释。先前的研究主要关注众包作为​​外包注释的一种方式。然而，到目前为止，很少有人关注注释公司，特别是其内部质量保证 (QA) 流程。因此，我们的目标是评估注释公司采用的 QA 对注释质量的影响，并设计最大化数据注释效率的方法。基于从四家注释公司和 Amazon Mechanical Turk (MTurk) 的总共 924 名注释者和 34 名 QA 工作者获得的总共 57,648 张实例分割图像，我们得出以下见解：(1) 与广泛使用的平台 MTurk 相比，注释公司在数量和质量方面的表现都更好。(2) 注释公司的内部 QA 仅提供微小的改进（如果有的话）。然而，改进标注说明而不是投资 QA 可以大幅提高标注性能。（3）内部 QA 的好处取决于特定的图像特征。我们的工作可以让研究人员从固定的标注预算中获得更多价值，并改变标注公司进行内部 QA 的方式。]]></description>
      <guid>https://arxiv.org/abs/2407.17596</guid>
      <pubDate>Fri, 26 Jul 2024 06:17:46 GMT</pubDate>
    </item>
    <item>
      <title>CoMoTo：非配对跨模态病变提取可改善断层合成中的乳腺病变检测</title>
      <link>https://arxiv.org/abs/2407.17620</link>
      <description><![CDATA[arXiv:2407.17620v1 公告类型：新
摘要：数字乳腺断层合成 (DBT) 是一种先进的乳腺成像方式，与传统乳房 X 线摄影相比，它具有更高的病变检测精度，尽管读取时间更长。有限的数据可用性和巨大的注释成本阻碍了使用深度学习加速 DBT 病变检测。解决此问题的一个可能方法是利用乳房 X 线摄影等更广泛可用的方式提供的信息来增强 DBT 病变检测。在本文中，我们提出了一个用于改善 DBT 中病变检测的新型框架 CoMoTo。我们的框架利用未配对的乳房 X 线摄影数据来增强 DBT 模型的训练，通过消除推理过程中对乳房 X 线摄影的需求来提高实用性。具体而言，我们提出了两个新颖的组件，即病变特定知识提炼 (LsKD) 和模式内点对齐 (ImPA)。 LsKD 选择性地将病变特征从乳房 X 线摄影教师模型提炼到 DBT 学生模型，忽略背景特征。ImPA 通过在将知识提炼给学生之前确保教师模型中的病变特征对齐，进一步丰富了 LsKD。我们的综合评估表明，CoMoTo 优于传统的预训练和图像级 KD，在低数据设置下将性能提高了 7% 的平均敏感度。我们的代码可在 https://github.com/Muhammad-Al-Barbary/CoMoTo 获得。]]></description>
      <guid>https://arxiv.org/abs/2407.17620</guid>
      <pubDate>Fri, 26 Jul 2024 06:17:46 GMT</pubDate>
    </item>
    <item>
      <title>用于多任务生成模型的扩散模型</title>
      <link>https://arxiv.org/abs/2407.17571</link>
      <description><![CDATA[arXiv:2407.17571v1 公告类型：新
摘要：基于扩散的生成模型已经在各种生成任务上取得了最先进的成果。然而，大多数扩散模型仅限于单代建模。我们能否将具有多模态生成训练能力的扩散模型推广到更具推广性的建模？在本文中，我们提出了一种定义扩散模型的原则性方法，即在公共扩散空间中构建统一的多模态扩散模型。我们将正向扩散过程定义为由来自多种类型任务数据的信息聚合驱动，例如，生成任务的图像和分类任务的标签。在反向过程中，我们通过使用附加的模态特定解码器头参数化共享主干去噪网络来强制信息共享。这样的结构可以同时学习生成具有多任务损失的不同类型的多模态数据，该损失源自推广标准扩散模型的新多模态变分下界。我们提出了几种多模态生成设置来验证我们的框架，包括图像转换、蒙版图像训练、联合图像标签和联合图像表示生成建模。ImageNet 上的大量实验结果表明我们的框架对于各种多模态生成建模的有效性，我们认为这是一个值得未来进一步探索的重要研究方向。]]></description>
      <guid>https://arxiv.org/abs/2407.17571</guid>
      <pubDate>Fri, 26 Jul 2024 06:17:45 GMT</pubDate>
    </item>
    <item>
      <title>CityX：可控的无边界 3D 城市程序内容生成</title>
      <link>https://arxiv.org/abs/2407.17572</link>
      <description><![CDATA[arXiv:2407.17572v1 公告类型：新
摘要：由于涉及大量 3D 资产、各种城市风格和严格的布局约束，生成逼真的大型 3D 虚拟城市仍然是一项复杂的挑战。现有方法为使用 Blender 代理创建大规模场景的程序内容生成提供了有希望的尝试。然而，它们面临着关键问题，例如难以扩大生成能力和在语义布局级别实现细粒度控制。为了解决这些问题，我们提出了一种新颖的多模态可控程序内容生成方法 CityX，该方法在多种布局条件（包括 OSM、语义地图和卫星图像）的指导下增强了逼真的无界 3D 城市生成。具体而言，所提出的方法包含用于集成各种 PCG 插件的通用协议和用于将指令转换为可执行 Blender 操作的多代理框架。通过这个有效的框架，CityX 展示了通过弥合生成资产质量与工业要求之间的差距来构建创新的 3D 场景生成生态系统的潜力。大量实验证明了我们的方法在多模态条件引导下创建高质量、多样化、无边界城市的有效性。我们的项目页面：https://cityx-lab.github.io。]]></description>
      <guid>https://arxiv.org/abs/2407.17572</guid>
      <pubDate>Fri, 26 Jul 2024 06:17:45 GMT</pubDate>
    </item>
    <item>
      <title>StreamTinyNet：使用时空 TinyML 进行视频流分析</title>
      <link>https://arxiv.org/abs/2407.17524</link>
      <description><![CDATA[arXiv:2407.17524v1 公告类型：新
摘要：微型机器学习 (TinyML) 是机器学习 (ML) 的一个分支，它构成了 ML 世界与嵌入式系统生态系统（即物联网设备、嵌入式设备和边缘计算单元）之间的桥梁，使 ML 算法能够在内存、计算能力和功耗受限的设备上执行。视频流分析 (VSA) 是 TinyML 最有趣的任务之一，它以流式方式扫描一系列帧，目的是识别有趣的模式。鉴于这些微型设备的严格限制，所有当前解决方案都依赖于逐帧分析，因此不会利用数据流中的时间分量。在本文中，我们介绍了 StreamTinyNet，这是第一个执行多帧 VSA 的 TinyML 架构，它支持各种需要时空分析的用例，而这些用例以前不可能在 TinyML 级别进行。在公开数据集上的实验结果证明了所提解决方案的有效性和效率。最后，StreamTinyNet 已在 Arduino Nicla Vision 上进行移植和测试，证明了所提方案的可行性。]]></description>
      <guid>https://arxiv.org/abs/2407.17524</guid>
      <pubDate>Fri, 26 Jul 2024 06:17:44 GMT</pubDate>
    </item>
    <item>
      <title>使用可区分的替代方法学习黑盒模型的特定实例参数</title>
      <link>https://arxiv.org/abs/2407.17530</link>
      <description><![CDATA[arXiv:2407.17530v1 公告类型：新
摘要：调整不可微分或黑盒计算的参数具有挑战性。现有方法主要依赖于参数空间中的随机采样或网格采样。此外，使用所有当前方法，都不可能向黑盒提供任何输入特定参数。据我们所知，在这项工作中，我们首次能够学习黑盒的输入特定参数。作为测试应用，我们选择了一种流行的图像去噪方法 BM3D 作为我们的黑盒计算。然后，我们使用可微分代理模型（神经网络）来近似黑盒行为。接下来，以端到端的方式使用另一个神经网络来学习黑盒的输入实例特定参数。从 Tseng 等人的工作中汲取灵感。[1]，我们将我们的方法应用于智能手机图像去噪数据集 (SIDD) 进行图像去噪。结果令人信服，表明 PSNR 显著增加，SSIM 显著改善，接近 0.93。实验结果强调了我们的方法在模型性能和优化效率方面取得显着改进的有效性。有关代码和实现详细信息，请参阅我们的 GitHub 存储库。
[1] Ethan Tseng、Felix Yu、Yuting Yang、Fahim Mannan、Karl St. Arnaud、Derek Nowrouzezahrai、Jean-Francois Lalonde 和 Felix Heide。使用可微分代理进行黑盒图像处理中的超参数优化。ACM Transactions on Graphics (TOG)，38(4)，7 2019。]]></description>
      <guid>https://arxiv.org/abs/2407.17530</guid>
      <pubDate>Fri, 26 Jul 2024 06:17:44 GMT</pubDate>
    </item>
    <item>
      <title>PatchEX：通过基于补丁的并行外推实现高质量实时时间过采样</title>
      <link>https://arxiv.org/abs/2407.17501</link>
      <description><![CDATA[arXiv:2407.17501v1 公告类型：新
摘要：近年来，由于游戏、专业显示器和医学成像等专业应用对卓越视觉质量的需求，高刷新率显示器变得非常流行。然而，仅靠高刷新率显示器并不能保证卓越的视觉体验；GPU 需要以匹配的速率渲染帧。否则，我们会观察到令人不安的视觉伪影，例如屏幕撕裂和卡顿。时间超采样是一种有效的技术，它通过从其他渲染帧预测新帧来提高帧速率。这个领域有两种方法：插值和外推。基于插值的方法以更高的延迟为代价提供良好的图像质量，因为它们还需要下一个渲染帧。另一方面，外推方法的速度要快得多，但质量却要低得多。本文介绍了一种新颖的帧外推方法 PatchEX，旨在以外推的速度提供插值的质量。它巧妙地将外推任务划分为子任务并并行执行，以提高质量和延迟。然后，它使用基于块的修复方法和自定义阴影预测方法来融合生成的子帧。这种方法显著降低了总体延迟，同时保持了输出质量。我们的结果表明，与最新的外推方法 ExtraNet 和 ExtraSS 相比，PatchEX 的 PSNR 分别提高了 65.29% 和 48.46%，同时速度分别提高了 6 倍和 2 倍。]]></description>
      <guid>https://arxiv.org/abs/2407.17501</guid>
      <pubDate>Fri, 26 Jul 2024 06:17:43 GMT</pubDate>
    </item>
    <item>
      <title>CORT：面向类的嵌入式系统实时跟踪</title>
      <link>https://arxiv.org/abs/2407.17521</link>
      <description><![CDATA[arXiv:2407.17521v1 公告类型：新 
摘要：人工智能在自主系统中的日益广泛使用极大地促进了多目标跟踪研究的发展，该技术已在许多实时应用中（例如自动驾驶、无人机监视、机器人）采用，用于定位和跟踪在摄像机前移动的多个物体的轨迹。当前的跟踪算法可以分为两大类：一些方法引入了复杂的启发式和重新识别模型来提高跟踪精度并减少识别切换次数，而没有特别关注时间性能；而其他方法则旨在通过消除重新识别阶段来减少响应时间，从而降低跟踪精度。这项工作提出了一种新的多类对象跟踪方法，可以实现更小、更可预测的执行时间，而不会降低跟踪性能。这个想法是通过将匈牙利矩阵按类别拆分，并仅在对较少数量的元素严格必要时调用第二个重新识别阶段，将预测与检测匹配的问题简化为较小的子问题。提出的解决方案在复杂的城市场景中进行了评估，其中包含多个不同类型的物体（如汽车、卡车、自行车和行人），表明多类别方法相对于最先进的跟踪器是有效的。]]></description>
      <guid>https://arxiv.org/abs/2407.17521</guid>
      <pubDate>Fri, 26 Jul 2024 06:17:43 GMT</pubDate>
    </item>
    <item>
      <title>通过黑盒视觉提示实现基础模型的稳健适应</title>
      <link>https://arxiv.org/abs/2407.17491</link>
      <description><![CDATA[arXiv:2407.17491v1 公告类型：新 
摘要：随着大规模预训练模型 (PTM) 的激增，将这些模型适应众多下游任务成为一个关键问题。因此，大型模型的参数高效迁移学习 (PETL) 引起了极大的关注。虽然 PETL 方法表现出令人印象深刻的性能，但它们通常依赖于两个乐观的假设：1) PTM 的所有参数都可用，2) 配备足够大的内存容量来缓存所有中间激活以计算梯度。然而，在大多数实际应用中，PTM 用作黑盒 API 或专有软件，没有明确的参数可访问性。此外，很难满足现代 PTM 的大内存要求。这项工作提出了黑盒视觉提示 (BlackVIP)，它可以有效地调整 PTM，而无需了解模型架构和参数。BlackVIP 有两个组件； 1) 协调器和 2) 同时扰动随机近似梯度校正 (SPSA-GC)。协调器设计输入相关的视觉提示，使目标 PTM 能够在野外适应。SPSA-GC 有效地估计 PTM 的梯度以更新协调器。此外，我们提出了一个变体 BlackVIP-SE，它显著减少了 BlackVIP 的运行时间和计算成本。在 19 个数据集上进行的大量实验表明，BlackVIP 能够以最小的内存要求对不同的领域和任务进行稳健的适应。我们进一步通过展示它们与经过认证的随机平滑稳健性的联系，对视觉提示方法的泛化进行了理论分析。]]></description>
      <guid>https://arxiv.org/abs/2407.17491</guid>
      <pubDate>Fri, 26 Jul 2024 06:17:42 GMT</pubDate>
    </item>
    <item>
      <title>ReDiFine：可重复使用的扩散微调，用于减轻扩散链中的退化</title>
      <link>https://arxiv.org/abs/2407.17493</link>
      <description><![CDATA[arXiv:2407.17493v1 公告类型：新
摘要：扩散模型在图像生成建模方面取得了巨大的进步，实现了人类无法区分的高质量生成。图像的质量已经达到了一个阈值，在这个阈值上，我们可以重新使用合成图像来训练机器学习模型。这吸引了该领域，因为它可以减轻数据收集的高成本，并从根本上解决数据有限领域的许多问题。在本文中，我们重点关注一个实际场景，其中使用一组合成图像迭代微调预训练的文本到图像扩散模型，我们称之为扩散链。微调模型生成的图像用于下一次微调迭代。我们首先演示这些迭代过程如何导致图像质量严重下降。彻底的调查揭示了导致质量下降的最重要因素，我们提出了可以有效解决质量下降的微调和生成策略。我们的方法可重复使用扩散微调 (ReDiFine) 结合了条件下降微调和 CFG 调度，以在整个迭代过程中保持生成图像的质量。ReDiFine 可有效地用于多个数据集和模型，而无需进一步进行超参数搜索，使合成图像可重复使用以微调未来的生成模型。]]></description>
      <guid>https://arxiv.org/abs/2407.17493</guid>
      <pubDate>Fri, 26 Jul 2024 06:17:42 GMT</pubDate>
    </item>
    <item>
      <title>从记忆中学习：非参数记忆增强的视觉特征自监督学习</title>
      <link>https://arxiv.org/abs/2407.17486</link>
      <description><![CDATA[arXiv:2407.17486v1 公告类型：新
摘要：本文介绍了一种通过利用已见概念的非参数记忆来提高自监督学习 (SSL) 方法的训练稳定性的新方法。所提出的方法涉及用记忆组件增强神经网络，以随机比较当前图像视图与以前遇到的概念。此外，我们引入了随机记忆块来规范训练并强制图像视图之间的一致性。我们在许多视觉任务上对我们的方法进行了广泛的基准测试，例如线性探测、迁移学习、低样本分类和许多数据集上的图像检索。实验结果巩固了所提出方法的有效性，无需额外的正则化器即可实现稳定的 SSL 训练，同时学习高度可迁移的表示并需要更少的计算时间和资源。]]></description>
      <guid>https://arxiv.org/abs/2407.17486</guid>
      <pubDate>Fri, 26 Jul 2024 06:17:41 GMT</pubDate>
    </item>
    <item>
      <title>基于 Yolov4-tiny 的 PPE 实时自动穿戴和脱下检测</title>
      <link>https://arxiv.org/abs/2407.17471</link>
      <description><![CDATA[arXiv:2407.17471v1 公告类型：新
摘要：维护医院和诊所的患者安全和医护人员 (HCW) 的安全在很大程度上取决于遵循正确的穿戴和脱下个人防护设备 (PPE) 的规程。HCW 可以在穿戴和脱下过程中受益于反馈系统，因为这个过程需要认知能力，而且错误很常见。疾病控制和预防中心 (CDC) 提供了正确使用 PPE 的指南，应予以遵循。实时物体检测以及独特的排序算法用于实时识别和确定穿戴和脱下过程。这项技术研究的目的有两个方面：如果用户在穿戴或脱下过程中没有遵循正确的程序，他们会实时收到他们在序列中错过的步骤的警报。其次，在嵌入式系统架构中使用微型机器学习 (yolov4-tiny) 使其在不同的医疗保健环境中部署变得可行且具有成本效益。]]></description>
      <guid>https://arxiv.org/abs/2407.17471</guid>
      <pubDate>Fri, 26 Jul 2024 06:17:40 GMT</pubDate>
    </item>
    <item>
      <title>通用近似理论：基于深度学习的计算机视觉模型的基本理论</title>
      <link>https://arxiv.org/abs/2407.17480</link>
      <description><![CDATA[arXiv:2407.17480v1 Announce Type: new 
摘要：计算机视觉（CV）是人工智能领域中最重要的领域之一。近年来，各种基于卷积神经网络（CNN）和Transformer的深度学习模型被设计出来以解决CV中的各种问题。这些算法已经在机器人、人脸识别等领域得到了实际应用。尽管目前的CV模型的威力越来越大，但仍有几个基本问​​题没有解决：为什么CNN需要深层？什么保证了CNN的泛化能力？为什么基于残差的网络比VGG等全卷积网络表现更好？基于残差的CNN和基于Transformer的网络之间的根本区别是什么？为什么CNN可以利用LoRA和剪枝技术？这些问题的根本原因在于CV中的深度学习模型缺乏坚实的理论基础。为了解决这些关键问题和技术，我们利用通用近似定理 (UAT) 为 CV 中的卷积和基于 Transformer 的模型提供理论基础。通过这样做，我们旨在从理论角度阐明这些问题。]]></description>
      <guid>https://arxiv.org/abs/2407.17480</guid>
      <pubDate>Fri, 26 Jul 2024 06:17:40 GMT</pubDate>
    </item>
    </channel>
</rss>