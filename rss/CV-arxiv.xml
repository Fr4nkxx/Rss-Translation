<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>CS.CV更新arxiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.cv更新arxiv.org e-print存档。</description>
    <lastBuildDate>Thu, 27 Mar 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>使用2D骨架检测和3D姿势估计的舞蹈运动匹配率的研究：为什么十七个表现如此之所以如此（完美同步）？</title>
      <link>https://arxiv.org/abs/2503.19917</link>
      <description><![CDATA[ARXIV：2503.19917V1公告类型：新 
摘要：十七个是一个K-POP组，总共有大量成员13，并且在K-POP组中最高和最短成员之间的物理差异很大。但是，尽管它们的数量很大和身体差异，但他们的舞蹈表演在K-Pop行业中表现出无与伦比的统一性。根据一种理论，他们的舞蹈同步率据说为90％甚至97％。但是，几乎没有具体数据来证实这种同步率。在这项研究中，我们使用YouTube上的视频分析了十七场舞蹈表演。我们应用了2D骨架检测和3D姿势估计来评估关节角度，身体部位运动以及跳跃和蹲下运动，以研究导致其性能统一性的因素。分析显示，在跳跃运动过程中，在身体部位的运动方向以及脚踝和头部位置以及蹲下运动过程中的头部位置异常高。这些发现表明，十七个高同步速率可以归因于运动方向的一致性以及在跳跃和蹲下运动过程中踝关节和头部高度的同步。]]></description>
      <guid>https://arxiv.org/abs/2503.19917</guid>
      <pubDate>Thu, 27 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于域概括的水下机器人的强大对象检测</title>
      <link>https://arxiv.org/abs/2503.19929</link>
      <description><![CDATA[ARXIV：2503.19929V1公告类型：新 
摘要：对象检测旨在获得给定图像中特定对象的位置和类别，其中包括两个任务：分类和位置。近年来，研究人员倾向于将对象检测应用于配备有视觉系统的水下机器人，以完成包括海鲜捕鱼，养鱼，生物多样性监测等任务。但是，水下环境的多样性和复杂性为对象检测带来了新的挑战。首先，水生生物倾向于生活在一起，从而导致严重的阻塞。其次，戏剧生物擅长隐藏自己，它们的颜色与背景具有相似的颜色。第三，各种水质以及可变和极端的照明条件会导致水下摄像头获得的扭曲，低对比度，蓝色或绿色图像，从而导致域移动。而且，深层模型通常容易面临域转移。第四，水下机器人的运动导致捕获的图像的模糊，并使水泥变得泥泞，从而导致水的可见度较低。本文调查了上面提到的水下环境带来的问题，并旨在设计高性能和健壮的水下对象探测器。]]></description>
      <guid>https://arxiv.org/abs/2503.19929</guid>
      <pubDate>Thu, 27 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>VisuerQuest：用于评估LLMS视觉识别的多样性图像数据集</title>
      <link>https://arxiv.org/abs/2503.19936</link>
      <description><![CDATA[ARXIV：2503.19936V1公告类型：新 
摘要：本文介绍了VisualQuest，这是一个新型图像数据集，旨在评估大型语言模型（LLMS）解释非传统，风格化图像的能力。与传统的摄影基准不同，VisualQuest挑战模型，其中包含抽象，符号和隐喻元素，需要集成域特异性知识和高级推理。通过多个过滤，注释和标准化的多个阶段对数据集进行了精心策划，以确保高质量和多样性。我们使用几种最先进的多模式LLM的评估揭示了显着的性能变化，这些变化强调了事实背景知识和推论能力在视觉识别任务中的重要性。因此，VisualQuest为推进多模式推理和模型架构设计的研究提供了强大而全面的基准。]]></description>
      <guid>https://arxiv.org/abs/2503.19936</guid>
      <pubDate>Thu, 27 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>反向提示：在文本到图像中破解食谱</title>
      <link>https://arxiv.org/abs/2503.19937</link>
      <description><![CDATA[ARXIV：2503.19937V1公告类型：新 
摘要：文本到图像的生成越来越流行，但是实现所需图像通常需要广泛的及时工程。在本文中，我们探讨了如何从参考图像中解码文本提示，这是我们称为图像反向提示工程的过程。这项技术使我们能够从参考图像中获得见解，了解伟大艺术家的创作过程，并产生令人印象深刻的新图像。为了应对这一挑战，我们提出了一种称为自动反向提示优化（ARPO）的方法。具体而言，我们的方法通过迭代模拟梯度提示优化过程将初始提示完善到高质量的提示中：1）从当前提示中生成重新创建的图像以实例化其指导能力； 2）产生文本梯度，这些梯度是候选的提示，旨在减少已重新创建的图像和参考图像之间的差异； 3）使用贪婪的搜索方法更新当前提示符，以最大程度地提高提示图和参考图像之间的剪辑相似性。我们将ARPO与几种基线方法进行比较，包括手工制作的技术，基于梯度的提示方法，图像字幕和数据驱动的选择方法。定量和定性结果都表明，我们的ARPO迅速收敛以产生高质量的反向提示。更重要的是，我们可以通过直接编辑这些反向提示来轻松创建具有不同样式和内容的新颖图像。代码将公开可用。]]></description>
      <guid>https://arxiv.org/abs/2503.19937</guid>
      <pubDate>Thu, 27 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>消失深度：一个深度适配器，具有代码编码的位置深度编码的位置深度</title>
      <link>https://arxiv.org/abs/2503.19947</link>
      <description><![CDATA[ARXIV：2503.19947V1公告类型：新 
摘要：通用度量深度理解对于精确的视觉指导机器人技术至关重要，当前最新的（SOTA）视力编码器不支持这一点。为了解决这个问题，我们提出了消失的深度，这是一种自制的训练方法，该方法扩展了预验证的RGB编码器，以将指标深度纳入其特征嵌入。基于我们新颖的位置深度编码，我们启用稳定的深度密度和深度分布不变特征提取。我们在相关的下游任务中实现了绩效的改进和SOTA结果 - 无需对编码器进行填充。最值得注意的是，我们在Sun-RGBD细分时达到了56.05 MIOU，在Void的深度完成下达到88.3 RMSE，在NYUV2场景分类上达到了83.8顶级1的精度。在6D对象的姿势估计中，我们在几个相关的RGBD下游任务中，我们的前身DINOV2，EVA-02和OMNIVORE的前身均优于Dinov2，EVA-02和Omnivore的前身，并获得了非传输编码器的SOTA结果。]]></description>
      <guid>https://arxiv.org/abs/2503.19947</guid>
      <pubDate>Thu, 27 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过VLM和软奖励通过视觉人类偏好进行测试时间推理</title>
      <link>https://arxiv.org/abs/2503.19948</link>
      <description><![CDATA[ARXIV：2503.19948V1公告类型：新 
摘要：视觉语言模型（VLM）可以有效地捕获人类的视觉偏好吗？这项工作通过培训VLM来解决这个问题，以在测试时考虑偏好，采用受DeepSeek R1和OpenAI O1启发的强化学习方法。使用ImageRward和人类偏好得分V2（HPSV2）之类的数据集，我们的模型在Imagerward测试集（通过ImageRand官方分配中训练）的精度为64.9％，在HPSV2上获得了65.4％的精度（对大约25％的数据进行了培训）。这些结果与传统的基于编码器的模型相匹配，同时提供透明的推理和增强的概括。这种方法不仅可以使用丰富的VLM世界知识，而且可以使用其思考的潜力，从而产生可解释的结果，从而有助于决策过程。通过证明当前VLM合理的人类视觉偏好，我们为图像排名提供了有效的软奖励策略，超越了简单的选择或评分方法。这种推理能力使VLM可以对纵横比或复杂性的无任意图像进行排名 - 可能会扩大视觉偏好优化的有效性。通过减少对广泛的标记的需求，同时提高奖励的概括和解释性，我们的发现可能是一个强大的英里石，可以进一步增强文本到视觉模型。]]></description>
      <guid>https://arxiv.org/abs/2503.19948</guid>
      <pubDate>Thu, 27 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Acvubench：以音频为中心的视频理解基准测试</title>
      <link>https://arxiv.org/abs/2503.19951</link>
      <description><![CDATA[ARXIV：2503.19951V1公告类型：新 
摘要：音频通常是视频理解音频视频模型（LLMS）的任务的辅助方式，只是有助于理解视觉信息。但是，对视频的透彻理解在很大程度上取决于听觉信息，因为音频提供了关键的上下文，情感提示和语义，这意味着仅视觉数据通常就缺乏。本文提出了一个以音频为中心的视频理解基准（ACVubench），以评估多模式LLM的视频理解能力，特别关注听觉信息。具体而言，Acvubench结合了2,662个视频，涵盖了18个不同的域以及丰富的听觉信息，以及超过13k的高质量人类注释或经过验证的问题解答对。此外，Acvubench推出了一套精心设计的以音频为中心的任务，从整体上测试了视频中对音频内容和视听互动的理解。对各种开源和专有的多模式LLM进行了彻底的评估，然后进行了视听LLMS缺陷的分析。演示可从https://github.com/lark-png/acvubench获得。]]></description>
      <guid>https://arxiv.org/abs/2503.19951</guid>
      <pubDate>Thu, 27 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过优化反事实来对运动概念的自我监督学习</title>
      <link>https://arxiv.org/abs/2503.19953</link>
      <description><![CDATA[ARXIV：2503.19953V1公告类型：新 
摘要：视频中的运动估算是许多下游应用程序（包括可控的视频生成和机器人）的必不可少的计算机视觉问题。当前的解决方案主要是使用合成数据训练的，或者需要对特定情况的启发式方法进行调整，这本质上限制了这些模型在实际环境中的功能。尽管从视频中进行了大规模的自我监督学习方面的发展，但利用此类表示的运动估计仍然相对不受影响。在这项工作中，我们开发了OPT-CWM，这是一种从预先训练的下一框架预测模型中进行流量和遮挡估算的自我监督技术。 OPT-CWM通过学习优化反事实探针的工作作品，这些探针从基本视频模型中提取运动信息，从而避免了对无限制视频输入培训的固定启发式方法的需求。我们在不需要标记的数据的同时，在现实世界视频上实现了最新的运动估算。]]></description>
      <guid>https://arxiv.org/abs/2503.19953</guid>
      <pubDate>Thu, 27 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>滑道：欺骗意识的单级脸反欺骗与语言图像预处理</title>
      <link>https://arxiv.org/abs/2503.19982</link>
      <description><![CDATA[ARXIV：2503.19982V1公告类型：新 
摘要：面部反欺骗（FAS）在确保面部识别系统的安全性和可靠性方面起着关键作用。随着视觉预读（VLP）模型的进步，最近的两级FAS技术利用了使用VLP指导的优势，而这种潜力仍未探索一级FAS方法。一级FA的重点是学习固有的LIVISE特征，仅从现场训练图像到与现场和欺骗面之间的区分。但是，缺乏欺骗训练数据会导致一级FAS模型无意间合并与现场/欺骗区别无关的域信息（例如面部含量），在使用新的应用程序域进行测试时会导致性能降解。为了解决这个问题，我们提出了一个新颖的框架，称为“欺骗”一流的脸部反欺骗，并预处理语言图像（Slip）。鉴于理想情况下，现场面孔不应被任何欺骗与攻击相关的物体（例如纸张或面具）遮盖，并被认为会产生零欺骗提示图，我们首先提出了有效的语言引导提示图估计，以通过模拟攻击性的对象和生成对象的范围，以增强单层FAS模型，并生成不合时宜的对象。接下来，我们引入了一种新颖的及时驱动的LIVICES DEMENTANGERPENT，以通过删除与现场/欺骗相关的域和域依赖性信息来减轻实时/欺骗性的领域变化。最后，我们通过融合实时图像的潜在特征和欺骗提示产生欺骗图像特征，从而使潜在的欺骗特征融合在一起，从而设计有效的增强策略，从而促进学习一级FAS。我们广泛的实验和消融研究支持，持续优于以前的一级FAS方法。]]></description>
      <guid>https://arxiv.org/abs/2503.19982</guid>
      <pubDate>Thu, 27 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CoralsCapes数据集：珊瑚礁中的语义场景理解</title>
      <link>https://arxiv.org/abs/2503.20000</link>
      <description><![CDATA[ARXIV：2503.20000V1公告类型：新 
摘要：由于气候变化和当地压力源，全球珊瑚礁正在下降。为了告知有效的保护或恢复，需要以最高的空间和时间分辨率进行监控。传统的珊瑚礁测量方法由于依靠专家劳动时间而限制可扩展性，激发了使用计算机视觉工具来自动化图像中实时珊瑚的识别和丰度估计。但是，由于缺乏大型高质量数据集而阻碍了对此类工具的设计和评估。我们发布了CoralsCapes数据集，这是针对珊瑚礁的第一个通用语义分割数据集，涵盖了2075张图像，39个底栖类别和174K分段掩模，由专家注释。 CoralsCapes具有与广泛使用的城市景观数据集相似的范围和相同的结构，可用于城市场景细分，从而在一个新的具有挑战性的领域中对语义细分模型进行基准测试，这需要专家知识才能注释。我们对广泛的语义分割模型进行了基准，并发现从珊瑚尺寸转移到现有较小数据集的将学习始终导致最先进的性能。 CoralsCapes将根据计算机视觉促进对高效，可扩展和标准化的珊瑚礁测量方法的研究，并具有简化水下生态机器人技术发展的潜力。]]></description>
      <guid>https://arxiv.org/abs/2503.20000</guid>
      <pubDate>Thu, 27 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>自动驾驶汽车多模式不确定性融合的高维不确定性定量</title>
      <link>https://arxiv.org/abs/2503.20011</link>
      <description><![CDATA[ARXIV：2503.20011V1公告类型：新 
摘要：不确定性量化（UQ）对于确保现实世界自治系统中部署的机器学习模型的可靠性至关重要。但是，现有方法通常会量化任务级的输出预测不确定性，而无需在多模式特征融合水平上考虑认知不确定性，从而导致次优最佳结果。此外，由于培训和推断中的高计算成本，流行的不确定性量化方法，例如贝叶斯近似值，在实践中部署仍然具有挑战性。在本文中，我们提出了HyperDum，这是一种新型的确定性不确定性方法（DUM），该方法通过利用高维计算来有效地量化特征水平的认知不确定性。我们的方法分别通过通道和斑块的投影和捆绑技术捕获了通道和空间不确定性。然后，将多模式传感器特征自适应加权以减轻不确定性传播并改善特征融合。我们的评估表明，在3D对象检测中，HyperDum平均比最先进的算法（SOTA）算法高达2.01％/1.27％，在各种类型的不确定性下，在语义细分任务中的基线比基线相比提高了1.29％。值得注意的是，HyperDum需要比SOTA方法少2.36倍的浮点操作，最多需要38.30倍的参数，这为实际自治系统提供了有效的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2503.20011</guid>
      <pubDate>Thu, 27 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>INATAG：由大规模基准数据集启用的多级分类模型，其2,959种作物和杂草物种图像</title>
      <link>https://arxiv.org/abs/2503.20068</link>
      <description><![CDATA[ARXIV：2503.20068V1公告类型：新 
摘要：对农作物和杂草物种的准确鉴定对于精确农业和可持续农业至关重要。但是，由于各种因素，这仍然是一项具有挑战性的任务 - 物种之间的高度视觉相似性，环境变异性以及持续缺乏大型农业特定图像数据。我们介绍了Inatag，这是一个大型图像数据集，其中包含超过470万张2,959种独特的作物和杂草物种的图像，并在分类学层次结构上进行了精确的注释，从二元农作物/杂草标签到特定物种标签。 Inatag从更广泛的inaturalist数据库中策划，包含来自每个大陆的数据，并准确地反映了自然图像捕获和环境的可变性。通过这些数据启用，我们训练基于Swin Transformer体系结构的基准模型，并评估各种修改的影响，例如地理空间数据的合并和Lora Finetuning。我们的最佳模型在所有分类分类任务中实现最先进的表现，在作物和杂草分类方面达到92.38％。此外，数据集的规模使我们能够探索错误的错误分类并解锁植物物种的新分析可能性。通过结合大规模的物种覆盖范围，多任务标签和地理多样性，Inatag为建立强大的，地理位置感知的农业分类系统提供了新的基础。我们通过AGML（https://github.com/project-agml/agml）公开发布INATAG数据集，从而直接访问并集成到农业机器学习工作流程中。]]></description>
      <guid>https://arxiv.org/abs/2503.20068</guid>
      <pubDate>Thu, 27 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多模式（推理）LLM可以用作DeepFake探测器吗？</title>
      <link>https://arxiv.org/abs/2503.20084</link>
      <description><![CDATA[ARXIV：2503.20084V1公告类型：新 
摘要：在先进的生成模型时代，深层检测仍然是一个关键的挑战，尤其是随着合成介质变得更加复杂。在这项研究中，我们探讨了艺术状态多模式（推理）大语言模型（LLMS）用于深击图像检测的潜力，例如（OpenAi O1/4O，Gemini Thinking Flash 2，DeepSeek Janus，Grok 3，Llama 3.2，Llama 3.2，Qwen 2/2.5 VL，Mistral Pixtral，Claude 3.5 Sonnet）。我们基准了12个最新的多模式LLM，可针对多个数据集的传统深层检测方法，包括最近发布的现实世界中的深层图像。为了提高性能，我们采用迅速调整并对模型推理途径进行深入分析，以确定其决策过程中的关键因素。我们的发现表明，最佳的多模式LLM可以通过有希望的概括能力（零射击）实现竞争性能，甚至超过传统的深层捕获探测管道，而LLM家族的其余部分则表现出比随机猜测更糟糕的令人失望的。此外，我们发现较新的模型版本和推理功能在此类DeepFake检测的这种利基任务中并没有贡献，而模型尺寸在某些情况下确实有帮助。这项研究突出了将多模式推理整合到未来的深击检测框架中的潜力，并为在现实世界情景中的鲁棒性提供了模型可解释性的见解。]]></description>
      <guid>https://arxiv.org/abs/2503.20084</guid>
      <pubDate>Thu, 27 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>EBS-EKF：准确和高频基于事件的星星跟踪</title>
      <link>https://arxiv.org/abs/2503.20101</link>
      <description><![CDATA[ARXIV：2503.20101V1公告类型：新 
摘要：基于事件的传感器（EBS）是一项有希望的新技术，因为它们的延迟低和功率效率低，但迄今为止，在使用简化的信号模型的模拟中，已经对先前的工作进行了预先评估。我们为基于事件的星形跟踪提出了一种新型算法，该算法基于对EBS电路的分析和扩展的Kalman滤波器（EKF）。我们使用真实的夜空数据定量评估了我们的方法，将其结果与适用于空间活性像素传感器（APS）星形跟踪器的结果进行了比较。我们证明，由于改进的信号建模和状态估计，我们的方法比现有方法更准确，同时提供了比常规APS跟踪器提供更频繁的更新和更大的运动公差。我们提供所有代码和与APS解决方案同步的事件的第一个数据集。]]></description>
      <guid>https://arxiv.org/abs/2503.20101</guid>
      <pubDate>Thu, 27 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>偷窥者和像素：低分辨率面孔的人类识别精度</title>
      <link>https://arxiv.org/abs/2503.20108</link>
      <description><![CDATA[ARXIV：2503.20108V1公告类型：新 
摘要：自动化一对多（1：N）面部识别是执法机构常用的强大调查工具。在这种情况下，自动化1：n识别的潜在匹配是由人类考官在可能使用作为调查潜在客户之前审查的。虽然自动化1：n识别可以在理想的成像条件下达到接近完美的精度，但操作场景可能需要使用监视成像，而监视成像通常会以各种质量的维度下降。一个重要的质量维度是图像分辨率，通常通过面部像素数量来量化。这样的常见度量是瞳孔间距离（IPD），它测量了学生之间像素的数量。众所周知，低IPD会降低自动面部识别的准确性。但是，人脸识别可靠性的阈值IPD仍然不确定。这项研究旨在通过系统测试一系列IPD值的精度来探索人类识别准确性的边界。我们发现，在低IPD（10px，5px）下，人类的准确性处于或低于机会水平（50.7％，35.9％），即使对决策的信心保持相对较高（77％，70.7％）。我们的发现表明，对于低IPD图像，人类识别能力可能是整体系统准确性的限制因素。]]></description>
      <guid>https://arxiv.org/abs/2503.20108</guid>
      <pubDate>Thu, 27 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>