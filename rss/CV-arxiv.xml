<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Thu, 26 Sep 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>使用对齐重要性热图解释人类比较</title>
      <link>https://arxiv.org/abs/2409.16292</link>
      <description><![CDATA[arXiv:2409.16292v1 公告类型：新
摘要：我们提出了一种用于人类比较任务的计算可解释性方法，使用从深度视觉模型中得出的对齐重要性分数 (AIS) 热图。AIS 反映了特征图对深度神经网络 (DNN) 表示几何与人类几何之间的对齐的独特贡献。我们首先通过展示当仅使用从训练集中识别出的得分较高的 AIS 特征图构建表示时，样本外人类相似性判断的预测会得到改善来验证 AIS。然后，我们计算特定于图像的热图，以直观的方式指示与具有较高 AIS 分数的特征图相对应的区域。这些图直观地解释了与队列中的其他图像相比，哪些图像区域更重要。我们观察到这些热图与凝视预测模型生成的显着性图之间存在对应关系。然而，在某些情况下，会出现有意义的差异，因为与比较相关的维度不一定是视觉上最显着的。总而言之，对齐重要性可以提高从 DNN 嵌入对人类相似性判断的预测能力，并为图像空间中的相关信息提供可解释的见解。]]></description>
      <guid>https://arxiv.org/abs/2409.16292</guid>
      <pubDate>Thu, 26 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GenCAD：基于变换器的对比表示和扩散先验的图像条件计算机辅助设计生成</title>
      <link>https://arxiv.org/abs/2409.16294</link>
      <description><![CDATA[arXiv:2409.16294v1 公告类型：新 
摘要：通过计算机辅助设计 (CAD) 创建可制造和可编辑的 3D 形状仍然是一项高度手动且耗时的任务，受到 3D 实体边界表示的复杂拓扑和不直观的设计工具的阻碍。本文介绍了 GenCAD，这是一种生成模型，它采用自回归变换器和潜在扩散模型将图像输入转换为参数化 CAD 命令序列，从而产生可编辑的 3D 形状表示。GenCAD 将基于自回归变换器的架构与对比学习框架相结合，增强了从输入图像生成 CAD 程序的能力，并为与工程设计相关的多种数据模式提供了表示学习框架。广泛的评估表明，GenCAD 在生成的 3D 形状的精度和可修改性方面明显优于现有的最先进方法。值得注意的是，GenCAD 在长序列 3D 形状生成准确性方面显示出显著的提高，支持其在复杂设计任务中的应用。此外，GenCAD 的对比嵌入功能有助于使用数据库中的图像查询检索 CAD 模型，这是 CAD 社区面临的一个关键挑战。虽然 3D 形状生成文献中的大多数工作都侧重于网格、体素或点云等表示，但实际工程应用需要可修改性和多模式条件生成的能力。我们的结果在这方面迈出了重要的一步，凸显了生成模型加快整个设计到生产流程并无缝集成不同设计模式的潜力。]]></description>
      <guid>https://arxiv.org/abs/2409.16294</guid>
      <pubDate>Thu, 26 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LiDAR-3DGS：LiDAR 增强 3D 高斯散射，用于多模态辐射场渲染</title>
      <link>https://arxiv.org/abs/2409.16296</link>
      <description><![CDATA[arXiv:2409.16296v1 公告类型：新
摘要：在本文中，我们探索了基于 3D 高斯溅射 (3DGS) 的辐射场渲染的多模态输入功能。我们提出了 LiDAR-3DGS，这是一种使用 LiDAR 生成的点云增强 3DGS 输入的新方法，可显著提高 3D 模型的准确性和细节。我们展示了一种 LiDAR 增强 3DGS 的系统方法，以便能够捕获重要特征，例如螺栓、孔径和其他细节，而这些特征通常仅通过基于图像的特征就会被忽略。这些细节对于远程监控和维护等工程应用至关重要。在不修改底层 3DGS 算法的情况下，我们证明即使适度添加 LiDAR 生成的点云也会显著提高模型的感知质量。在 30k 次迭代中，我们的方法生成的模型的 PSNR 分别增加了 7.064%，SSIM 增加了 0.565%。由于本研究中使用的 LiDAR 是一种常用的商业级设备，因此观察到的改进很小，并且可以通过更高级的 LiDAR 系统进一步增强。此外，这些改进可以补充 Radiance Field Rendering 的其他衍生作品，并为未来的 LiDAR 和计算机视觉集成建模提供新的见解。]]></description>
      <guid>https://arxiv.org/abs/2409.16296</guid>
      <pubDate>Thu, 26 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Sentinel-2 卫星图像数据集的开发和应用，用于深度学习驱动的森林火灾检测</title>
      <link>https://arxiv.org/abs/2409.16380</link>
      <description><![CDATA[arXiv:2409.16380v1 公告类型：新
摘要：由于自然事件（例如野火）造成的森林损失代表着日益严峻的全球挑战，需要先进的分析方法来有效检测和缓解。为此，卫星图像与深度学习 (DL) 方法的集成已变得至关重要。然而，这种方法需要大量标记数据才能产生准确的结果。在本研究中，我们使用来自 Google Earth Engine (GEE) 的双时相 Sentinel-2 卫星图像来构建加州野火地理成像数据集 (CWGID)，这是一个高分辨率标记卫星图像数据集，包含超过 100,000 个标记前后森林野火图像对，可通过 DL 检测野火。我们的方法包括从权威来源获取数据、数据处理以及使用三个预训练的卷积神经网络 (CNN) 架构进行初始数据集分析。我们的结果表明，EF EfficientNet-B0 模型在检测森林火灾方面实现了超过 92% 的最高准确率。事实证明，CWGID 及其构建方法是训练和测试用于森林火灾检测的 DL 架构的宝贵资源。]]></description>
      <guid>https://arxiv.org/abs/2409.16380</guid>
      <pubDate>Thu, 26 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用合成点云对钢筋混凝土桥梁进行实例分割</title>
      <link>https://arxiv.org/abs/2409.16381</link>
      <description><![CDATA[arXiv:2409.16381v1 公告类型：新
摘要：国家桥梁检查标准要求进行详细的元素级桥梁检查。传统上，检查员根据损坏程度对结构部件进行评级，手动分配状况等级，但此过程劳动密集且耗时。自动化元素级桥梁检查过程可以促进更全面的状况记录，从而改善整体桥梁管理。虽然已经研究了桥梁点云的语义分割，但对桥梁元素的实例分割的研究有限，部分原因是缺乏带注释的数据集，以及难以概括训练好的模型。为了解决这个问题，我们提出了一种使用三种不同方法生成合成数据的新方法。我们的框架利用 Mask3D Transformer 模型，通过超参数调整和新颖的遮挡技术进行了优化。该模型分别在真实的 LiDAR 和摄影测量桥梁点云上实现了最先进的性能，展示了该框架在自动化元素级桥梁检查方面的潜力。]]></description>
      <guid>https://arxiv.org/abs/2409.16381</guid>
      <pubDate>Thu, 26 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在患者约束下生成合成数据以提高视频中疼痛识别率</title>
      <link>https://arxiv.org/abs/2409.16382</link>
      <description><![CDATA[arXiv:2409.16382v1 公告类型：新
摘要：识别视频中的疼痛对于改善患者-计算机交互系统至关重要，但该领域的传统数据收集带来了重大的道德和后勤挑战。这项研究介绍了一种利用合成数据增强基于视频的疼痛识别模型的新方法，提供了一种合乎道德且可扩展的替代方案。我们提出了一种流程，通过从一小部分参与者池中捕捉细微的面部动作并将其映射到不同的合成头像上，合成逼真的 3D 面部模型。这个过程生成了 8,600 个合成面孔，准确反映了不同角度和视角的真实疼痛表情。
利用先进的面部捕捉技术，并利用 CelebV-HQ 和 FFHQ-UV 等公共数据集实现人口多样性，我们的新合成数据集显著增强了模型训练，同时通过面部替换匿名身份确保了隐私。
实验结果表明，使用合成数据与少量真实参与者的组合训练的模型在疼痛识别方面取得了优异的表现，有效地弥合了合成模拟与现实世界应用之间的差距。我们的方法解决了数据稀缺和道德问题，为疼痛检测提供了一种新的解决方案，并为隐私保护数据集生成的研究开辟了新的途径。所有资源均公开可用，以鼓励该领域的进一步创新。]]></description>
      <guid>https://arxiv.org/abs/2409.16382</guid>
      <pubDate>Thu, 26 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过球面镜的单幅图像进行相机校准和立体成像</title>
      <link>https://arxiv.org/abs/2409.16386</link>
      <description><![CDATA[arXiv:2409.16386v1 公告类型：新
摘要：本文介绍了一种使用包含球面镜的单视图进行相机校准的新技术。利用图像中可见的球体轮廓及其反射的独特特征，我们展示了我们的方法在实现精确校准方面的有效性。此外，镜面反射提供了图像帧之外的周围场景的更多信息。
我们的方法为开发简单的折反射立体系统铺平了道路。我们探讨了使用单个镜面球体所带来的挑战和机遇，强调了这种设置在实际场景中的潜在应用。本文深入探讨了利用球面镜的折反射立体所涉及的几何和校准程序的复杂性。
实验结果包括合成数据和真实世界数据，以说明我们方法的可行性和准确性。]]></description>
      <guid>https://arxiv.org/abs/2409.16386</guid>
      <pubDate>Thu, 26 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过增量学习方法提高基于前臂超声的手势分类的阶段间可重复性</title>
      <link>https://arxiv.org/abs/2409.16415</link>
      <description><![CDATA[arXiv:2409.16415v1 公告类型：新
摘要：前臂超声图像可用于对手势进行分类，以开发人机界面。在我们之前的工作中，我们已经展示了使用超声波对单个受试者进行手势分类，而无需在评估前移除探头。这在使用上有局限性，因为一旦移除和更换探头，准确度就会下降，因为分类器性能对手臂上的探头位置很敏感。在本文中，我们建议在多个数据收集会话上训练模型以创建一个通用模型，通过微调利用增量学习。在一个会话内（不移除和重新放上探头）和跨会话获取 5 个手势的超声数据。本研究使用具有 5 个级联卷积层的卷积神经网络 (CNN)。预先训练的 CNN 经过微调，卷积块充当特征提取器，其余层的参数以增量方式更新。微调是在一个会话内和多个会话之间使用不同的会话分割进行的。我们发现，增量式微调可以通过更多微调会话来帮助提高分类准确率。在每次实验进行 2 次微调会话后，我们发现分类准确率提高了约 10%。这项研究表明，通过对基于超声波的手势分类进行微调，增量式学习可以提高准确率，同时节省存储、处理能力和时间。它可以扩展到多个主题之间，并用于开发个性化的可穿戴设备。]]></description>
      <guid>https://arxiv.org/abs/2409.16415</guid>
      <pubDate>Thu, 26 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用局部结构改进模型解释：一种信息传播方法</title>
      <link>https://arxiv.org/abs/2409.16429</link>
      <description><![CDATA[arXiv:2409.16429v1 公告类型：新
摘要：最近开发了许多解释方法来解释深度神经网络 (DNN) 模型做出的决策。对于图像分类器，这些方法通常为图像中的每个像素提供归因分数，以量化其对预测的贡献。然而，大多数这些解释方法将归因分数独立地分配给像素，即使人类和 DNN 都是通过同时分析一组密切相关的像素来做出决策。因此，像素的归因分数应该通过考虑其自身及其结构相似的像素来联合评估。我们提出了一种称为 IProp 的方法，它将每个像素的单独归因分数建模为解释信息的来源，并通过在所有像素上动态传播信息来解释图像预测。为了制定信息传播，IProp 采用了马尔可夫奖励过程，这保证了收敛，最终状态指示所需像素的归因分数。此外，IProp 与任何现有的基于归因的解释方法兼容。对各种解释方法和 DNN 模型进行的大量实验验证了 IProp 在各种可解释性指标上显著提高了它们的性能。]]></description>
      <guid>https://arxiv.org/abs/2409.16429</guid>
      <pubDate>Thu, 26 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 3D 卷积神经网络对前臂超声视频片段进行手势分类</title>
      <link>https://arxiv.org/abs/2409.16431</link>
      <description><![CDATA[arXiv:2409.16431v1 公告类型：新
摘要：基于超声波的手部运动估计是人机交互应用的一个重要研究领域。前臂超声提供了手部运动过程中肌肉形态变化的详细信息，可用于估计手势。以前的工作重点是使用卷积神经网络 (CNN) 等技术分析二维 (2D) 超声图像帧。然而，这种 2D 技术不会从对应于连续手部运动的超声数据片段中捕获时间特征。本研究使用基于 3D CNN 的技术来捕获超声视频片段中的时空模式以进行手势识别。我们将基于 2D 卷积的网络与基于 (2+1)D 卷积、基于 3D 卷积和我们提出的网络的性能进行了比较。与使用 2D 卷积层训练的网络相比，我们的方法将手势分类准确率从 96.5 +/- 2.3% 提高到 98.8 +/- 0.9%。这些结果证明了使用超声视频片段对于提高手势分类性能的优势。]]></description>
      <guid>https://arxiv.org/abs/2409.16431</guid>
      <pubDate>Thu, 26 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于探地雷达的地下测绘与定位</title>
      <link>https://arxiv.org/abs/2409.16446</link>
      <description><![CDATA[arXiv:2409.16446v1 公告类型：新
摘要：近年来，基于深度神经网络的 3D 物体重建受到越来越多的关注。然而，对地下物体进行 3D 重建以生成点云图仍然是一个挑战。探地雷达 (GPR) 是用于检测和定位地下物体（如植物根系和管道）的最强大和最广泛使用的工具之一，具有成本效益和不断发展的技术。本文介绍了一种基于深度卷积神经网络的抛物线信号检测网络，利用来自 GPR 传感器的 B 扫描图像。检测到的关键点可以帮助准确拟合用于将原始 GPR B 扫描图像解释为物体模型横截面的抛物线。此外，设计了一个多任务点云网络来同时执行点云分割和完成，填充稀疏点云图。对于未知位置，可以利用GPR A扫描数据与构建的地图中对应的A扫描数据进行匹配，从而精确定位，以验证模型构建地图的准确性。实验结果证明了该方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2409.16446</guid>
      <pubDate>Thu, 26 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>高斯溅射重建中基于频率的视图选择</title>
      <link>https://arxiv.org/abs/2409.16470</link>
      <description><![CDATA[arXiv:2409.16470v1 公告类型：新
摘要：三维重建是机器人感知中的一个基本问题。我们研究了主动视图选择问题，以便使用尽可能少的输入图像执行 3D 高斯 Splatting 重建。尽管 3D 高斯 Splatting 在图像渲染和 3D 重建方面取得了重大进展，但重建的质量受到 2D 图像的选择和通过运动结构 (SfM) 算法估计相机姿势的强烈影响。当前直接依赖于遮挡、深度模糊或神经网络预测的不确定性来选择视图的方法不足以处理该问题，并且难以推广到新场景。通过在频域中对潜在视图进行排序，我们能够在没有地面真实数据的情况下有效地估计新视点的潜在信息增益。通过克服当前模型架构和功效的限制，我们的方法在视图选择方面取得了最先进的结果，展示了其高效的基于图像的 3D 重建的潜力。]]></description>
      <guid>https://arxiv.org/abs/2409.16470</guid>
      <pubDate>Thu, 26 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>主动方案：针对社会公益的对抗性攻击调查</title>
      <link>https://arxiv.org/abs/2409.16491</link>
      <description><![CDATA[arXiv:2409.16491v1 公告类型：新
摘要：计算机视觉中的对抗性攻击利用机器学习模型的漏洞，对输入数据引入细微的扰动，通常导致错误的预测或分类。随着深度学习的出现，这些攻击变得越来越复杂，给关键应用带来了重大挑战，可能对社会造成危害。然而，从变革性的角度来看，也有丰富的研究利用对抗技术造福社会。具体来说，我们研究了主动方案的兴起——使用称为模板的附加信号加密输入数据的方法，以增强深度学习模型的性能。通过将这些难以察觉的模板嵌入到数字媒体中，主动方案可应用于各种应用程序，从简单的图像增强到复杂的深度学习框架以提高性能，而被动方案则不会改变其框架的输入数据分布。本调查深入探讨了这些主动方案背后的方法、加密和学习过程，以及它们在现代计算机视觉和自然语言处理应用中的应用。此外，它还讨论了主动方案面临的挑战、潜在漏洞和未来发展方向，最终强调了它们在促进深度学习技术负责任和安全进步方面的潜力。]]></description>
      <guid>https://arxiv.org/abs/2409.16491</guid>
      <pubDate>Thu, 26 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型视觉语言模型的统一幻觉缓解框架</title>
      <link>https://arxiv.org/abs/2409.16494</link>
      <description><![CDATA[arXiv:2409.16494v1 Announce Type: new 
摘要：幻觉是大型视觉语言模型（LVLM）中一个普遍存在且难以根除的问题，产生幻觉的生成部分与图像内容不一致。为了缓解幻觉，当前的研究要么关注模型推理的过程，要么关注模型生成的结果，但它们设计的解决方案有时不能适当地处理各种类型的查询以及这些查询的生成过程中产生的幻觉。为了准确处理各种幻觉，我们提出了一个统一的幻觉缓解框架——Dentist。核心步骤是首先对查询进行分类，然后根据分类结果执行不同的幻觉缓解过程，就像牙医先观察牙齿然后制定计划一样。在简单的部署中，Dentist 可以将查询分类为感知或推理，并轻松缓解答案中的潜在幻觉，这已在我们的实验中得到证明。在 MMbench 上，我们在粗感知视觉问答 (VQA) 任务“图像质量”的准确率上，与基线 InstructBLIP/LLaVA/VisualGLM 相比，提高了 13.44%/10.2%/15.8%。]]></description>
      <guid>https://arxiv.org/abs/2409.16494</guid>
      <pubDate>Thu, 26 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>废弃印刷电路板中电子元件的实时检测：基于变压器的方法</title>
      <link>https://arxiv.org/abs/2409.16496</link>
      <description><![CDATA[arXiv:2409.16496v1 公告类型：新
摘要：铜、锰、镓和各种稀土等关键原材料 (CRM) 对电子工业具有重要意义。为了提高单个 CRM 的浓度，从而方便从废弃印刷电路板 (WPCB) 中提取它们，我们提出了一种实用方法，该方法涉及使用由人工视觉技术引导的机电一体化系统选择性地拆卸 WPCB 中的不同类型的电子元件。在本文中，我们评估了实时检测变压器模型架构的电子元件检测和定位的实时准确性。变压器最近因在自然语言处理和机器翻译中获得的非凡结果而变得非常流行。同样在这种情况下，变压器模型实现了非常好的性能，通常优于最新的最先进的物体检测和定位模型 YOLOv8 和 YOLOv9。]]></description>
      <guid>https://arxiv.org/abs/2409.16496</guid>
      <pubDate>Thu, 26 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>