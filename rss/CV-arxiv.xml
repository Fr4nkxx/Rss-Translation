<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Fri, 10 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>利用视觉节律和累积线分析实现视频中高效的车牌识别</title>
      <link>https://arxiv.org/abs/2501.04750</link>
      <description><![CDATA[arXiv:2501.04750v1 公告类型：新
摘要：基于视频的自动车牌识别 (ALPR) 涉及从视频捕获中提取车牌文本信息。传统系统通常严重依赖高端计算资源并利用多帧来识别车牌，从而增加计算开销。在本文中，我们提出了两种方法，能够有效地从每辆车中提取一帧并从该单幅图像中识别其车牌字符，从而显着降低计算需求。第一种方法使用视觉节奏 (VR) 从视频生成时空图像，而第二种方法采用累积线分析 (ALA)，这是一种基于单线视频处理的新算法，可实时操作。这两种方法都利用 YOLO 在帧内进行车牌检测，并利用卷积神经网络 (CNN) 进行光学字符识别 (OCR) 以提取文本信息。对真实视频的实验表明，所提出的方法可实现与传统逐帧方法相当的结果，处理速度提高了三倍。]]></description>
      <guid>https://arxiv.org/abs/2501.04750</guid>
      <pubDate>Fri, 10 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用生成式人工智能对包含事件和背景信息的视频进行摘要</title>
      <link>https://arxiv.org/abs/2501.04764</link>
      <description><![CDATA[arXiv:2501.04764v1 公告类型：新
摘要：视频内容制作的激增导致了大量数据，对分析效率和资源利用率提出了巨大挑战。解决这个问题需要开发强大的视频分析工具。本文提出了一种利用生成人工智能 (GenAI) 来促进简化视频分析的新方法。我们的工具旨在提供用户定义查询的定制文本摘要，在大量视频数据集中提供有针对性的见解。与提供通用摘要或有限动作识别的传统框架不同，我们的方法利用 GenAI 的功能来提取相关信息，从而提高分析精度和效率。我们的解决方案采用 YOLO-V8 进行对象检测，采用 Gemini 进行全面的视频和文本分析，实现了更高的上下文准确性。通过将 YOLO 与 Gemini 相结合，我们的方法提供了从大量 CCTV 录像中提取的文本摘要，使用户能够快速浏览和验证相关事件，而无需进行详尽的手动审查。定量评估显示相似度为 72.8%，定性评估准确度为 85%，证明了所提出方法的能力。]]></description>
      <guid>https://arxiv.org/abs/2501.04764</guid>
      <pubDate>Fri, 10 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>TREAD：用于高效架构无关扩散训练的代币路由</title>
      <link>https://arxiv.org/abs/2501.04765</link>
      <description><![CDATA[arXiv:2501.04765v1 公告类型：新
摘要：扩散模型已成为视觉生成的主流方法。然而，这些模型通常存在样本效率低下和训练成本高的问题。这个问题在标准扩散变压器架构中尤其明显，因为它相对于输入长度具有二次复杂度。最近的研究通过减少模型中处理的标记数量（通常是通过掩码）解决了这个问题。相反，这项工作旨在通过使用预定义路线来提高扩散主干的训练效率，这些路线存储这些信息，直到将其重新引入模型的更深层，而不是完全丢弃这些标记。此外，我们结合了多种路线，并引入了一种适应性的辅助损失，该损失考虑了所有应用的路线。我们的方法不仅限于常见的基于变压器的模型——它还可以应用于状态空间模型。与大多数当前方法不同，TREAD 无需修改架构即可实现这一点。最后，我们展示了我们的方法降低了计算成本，同时在类条件合成的标准基准 ImageNet-1K 256 x 256 上提高了模型性能。与 DiT 相比，这两个优势在 400K 次训练迭代中实现了 9.55 倍的收敛速度，与 DiT 在 7M 次训练迭代中的最佳基准性能相比实现了 25.39 倍的收敛速度。]]></description>
      <guid>https://arxiv.org/abs/2501.04765</guid>
      <pubDate>Fri, 10 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GaussianVideo：通过分层高斯分层实现高效视频表示</title>
      <link>https://arxiv.org/abs/2501.04782</link>
      <description><![CDATA[arXiv:2501.04782v1 公告类型：新
摘要：动态视频场景的高效神经表示对于从视频压缩到交互式模拟等应用至关重要。然而，现有方法通常面临与高内存使用率、长时间训练和时间一致性相关的挑战。为了解决这些问题，我们引入了一种新颖的神经视频表示，将 3D 高斯溅射与连续相机运动建模相结合。通过利用神经 ODE，我们的方法可以学习平滑的相机轨迹，同时通过高斯保持明确的 3D 场景表示。此外，我们引入了时空分层学习策略，逐步细化空间和时间特征以提高重建质量并加速收敛。这种内存高效的方法以惊人的速度实现了高质量的渲染。实验结果表明，我们的分层学习与强大的相机运动建模相结合，可以捕捉具有强大时间一致性的复杂动态场景，在高运动和低运动场景中的各种视频数据集中实现最先进的性能。]]></description>
      <guid>https://arxiv.org/abs/2501.04782</guid>
      <pubDate>Fri, 10 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用 Vision Transformers 中的寄存器实现稳健的自适应</title>
      <link>https://arxiv.org/abs/2501.04784</link>
      <description><![CDATA[arXiv:2501.04784v1 公告类型：新
摘要：视觉变换器 (ViT) 因其能够捕获全局图像表示而已在各种任务中取得成功。最近的研究发现，ViT 中存在高范数标记，这可能会干扰无监督对象发现。为了解决这个问题，提出了使用“寄存器”的建议，这些标记是隔离高范数补丁标记同时捕获全局图像级信息的附加标记。虽然寄存器已被广泛研究用于对象发现，但它们的泛化特性，特别是在分布外 (OOD) 场景中的泛化特性，仍未得到充分探索。在本文中，我们研究了寄存器标记嵌入在提供附加功能以改进泛化和异常拒绝方面的实用性。为此，我们提出了一种简单的方法，将 ViT 中常用的特殊 CLS 标记嵌入与平均池化寄存器嵌入相结合，以创建特征表示，随后用于训练下游分类器。我们发现这可以增强 OOD 泛化和异常拒绝能力，同时保持分布内 (ID) 性能。对使用和不使用寄存器进行训练的多个 ViT 主干进行的大量实验表明，top-1 OOD 准确率持续提高 2-4\%，异常检测的误报率降低 2-3\%。重要的是，这些增益是在没有额外计算开销的情况下实现的。]]></description>
      <guid>https://arxiv.org/abs/2501.04784</guid>
      <pubDate>Fri, 10 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用双层表示学习和自适应提示实现可推广的轨迹预测</title>
      <link>https://arxiv.org/abs/2501.04815</link>
      <description><![CDATA[arXiv:2501.04815v1 公告类型：新
摘要：现有的车辆轨迹预测模型在通用性、预测不确定性和处理复杂交互方面存在困难。这通常是由于诸如为特定数据集定制的复杂架构和低效的多模态处理等限制。我们提出了一种新颖的轨迹预测框架——带有寄存器查询的感知器 (PerReg+)，它引入了：(1) 通过自蒸馏 (SD) 和掩蔽重建 (MR) 进行双层表示学习，捕获全局上下文和细粒度细节。此外，我们通过查询删除从掩蔽输入中重建段级轨迹和车道段的方法，可以有效利用上下文信息并提高泛化能力；(2) 使用基于寄存器的查询和预训练增强多模态性，无需聚类和抑制；(3) 在微调期间进行自适应提示调整，冻结主架构并优化少量提示以实现有效适应。 PerReg+ 在 nuScenes [1]、Argoverse 2 [2] 和 Waymo Open Motion Dataset (WOMD) [3] 上创下了新的最佳性能。值得注意的是，我们的预训练模型在较小的数据集上将错误率降低了 6.8%，而多数据集训练则增强了泛化能力。在跨域测试中，与未经预训练的变体相比，PerReg+ 将 B-FDE 降低了 11.8%。]]></description>
      <guid>https://arxiv.org/abs/2501.04815</guid>
      <pubDate>Fri, 10 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>EDMB：使用 Mamba 的边缘检测器</title>
      <link>https://arxiv.org/abs/2501.04846</link>
      <description><![CDATA[arXiv:2501.04846v1 公告类型：新
摘要：基于 Transformer 的模型在边缘检测方面取得了重大进展，但其高计算成本令人望而却步。最近，视觉 Mamba 表现出了高效捕获长距离依赖关系的出色能力。从中汲取灵感，我们提出了一种基于 Mamba 的新型边缘检测器，称为 EDMB，以高效生成高质量的多粒度边缘。在 EDMB 中，Mamba 与全局-局部架构相结合，因此它可以同时关注全局信息和细粒度线索。细粒度线索在边缘检测中起着至关重要的作用，但通常被普通的 Mamba 忽略。我们设计了一种新颖的解码器，通过融合全局特征和细粒度特征来构建可学习的高斯分布。并通过从分布中采样生成多粒度边。为了使多粒度边适用于单标签数据，我们引入了证据下界损失来监督分布的学习。在多标签数据集 BSDS500 上，我们提出的 EDMB 无需多尺度测试或额外的 PASCAL-VOC 数据即可实现具有竞争力的单粒度 ODS 0.837 和多粒度 ODS 0.851。值得注意的是，EDMB 可以扩展到单标签数据集，例如 NYUDv2 和 BIPED。源代码可在 https://github.com/Li-yachuan/EDMB 上找到。]]></description>
      <guid>https://arxiv.org/abs/2501.04846</guid>
      <pubDate>Fri, 10 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LayerMix：通过分形积分增强数据，实现稳健的深度学习</title>
      <link>https://arxiv.org/abs/2501.04861</link>
      <description><![CDATA[arXiv:2501.04861v1 公告类型：新
摘要：深度学习模型在各种计算机视觉任务中都表现出色，但它们对分布变化的脆弱性仍然是一个关键挑战。尽管神经网络架构复杂，但现有模型在面对分布外 (OOD) 样本（包括自然损坏、对抗性扰动和异常模式）时往往难以保持一致的性能。我们引入了 LayerMix，这是一种创新的数据增强方法，它通过基于结构化分形的图像合成系统地增强模型的鲁棒性。通过将结构复杂性精心集成到训练数据集中，我们的方法生成了语义一致的合成样本，从而显着提高了神经网络的泛化能力。与依赖随机变换的传统增强技术不同，LayerMix 采用结构化混合管道，在引入受控可变性的同时保留原始图像语义。在多个基准数据集（包括 CIFAR-10、CIFAR-100、ImageNet-200 和 ImageNet-1K）上进行的大量实验表明，LayerMix 在分类准确度方面表现出色，并大幅提高了关键的机器学习 (ML) 安全指标，包括对自然图像损坏的恢复能力、对对抗性攻击的鲁棒性、改进的模型校准和增强的预测一致性。LayerMix 通过解决深度学习泛化的基本挑战，代表了在开发更可靠、适应性更强的人工智能系统方面取得的重大进步。代码可在 https://github.com/ahmadmughees/layermix 上找到。]]></description>
      <guid>https://arxiv.org/abs/2501.04861</guid>
      <pubDate>Fri, 10 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>回到家乡：贝壳分类和生态系统修复的机器学习方法</title>
      <link>https://arxiv.org/abs/2501.04873</link>
      <description><![CDATA[arXiv:2501.04873v1 公告类型：新
摘要：在哥斯达黎加，每年平均有 5 吨贝壳从生态系统中被提取。由于缺乏来源识别，被没收的贝壳无法返回其生态系统。为了解决这个问题，我们开发了一个专门用于贝壳识别的卷积神经网络 (CNN)。我们从头开始构建了一个数据集，其中包含来自太平洋和加勒比海岸的大约 19000 张图像。使用该数据集，该模型实现了超过 85% 的分类准确率。该模型已集成到一个用户友好的应用程序中，该应用程序迄今为止已对超过 36,000 个贝壳进行了分类，每张图像在 3 秒内提供实时结果。为了进一步提高系统的准确性，加入了异常检测机制来过滤掉不相关或异常的输入，确保只处理有效的贝壳图像。]]></description>
      <guid>https://arxiv.org/abs/2501.04873</guid>
      <pubDate>Fri, 10 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用拓扑数对 $2$D 离散二值图像中的点进行拓扑分类</title>
      <link>https://arxiv.org/abs/2501.04878</link>
      <description><![CDATA[arXiv:2501.04878v1 公告类型：新
摘要：本文提出了二维离散二值图像点的拓扑分类。该分类基于拓扑数微积分的值。提出了六类点：孤立点、内部点、简单点、曲线点、3 条曲线的交点、4 条曲线的交点。还给出了每类的配置数量。]]></description>
      <guid>https://arxiv.org/abs/2501.04878</guid>
      <pubDate>Fri, 10 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于朝觐视频帧中人群密度分类的机器学习模型</title>
      <link>https://arxiv.org/abs/2501.04911</link>
      <description><![CDATA[arXiv:2501.04911v1 公告类型：新
摘要：管理每年大规模的朝觐和副朝集会面临重大挑战，尤其是沙特政府旨在增加朝觐者人数的情况下。目前，约有 200 万朝觐者参加朝觐，2600 万朝觐者参加副朝，因此，尤其是在朝觐期间的大清真寺等关键地区的人群控制是一个主要问题。在阿拉法特等关键地点管理密集人群时还存在额外风险，因为那里可能发生踩踏、火灾和流行病，对公共安全构成严重威胁。这项研究提出了一个机器学习模型，将朝觐期间记录的视频帧中的人群密度分为三个级别：中等人群、过度拥挤和非常密集的人群，当检测到非常密集的人群时，会闪烁红灯以实时提醒组织者。虽然目前处理朝觐监控视频的研究工作仅侧重于使用 CNN 检测异常行为，但这项研究更多地关注可能导致灾难的高风险人群。危险的人群状况需要一种稳健的方法，因为错误的分类可能会触发不必要的警报和政府干预，而分类失败则可能导致灾难。所提出的模型集成了局部二值模式 (LBP) 纹理分析，该分析增强了特征提取以区分人群密度水平，以及边缘密度和基于区域的特征。该模型在 KAU-Smart Crowd &#39;HAJJv2&#39; 数据集上进行了测试，该数据集包含朝觐期间来自各个关键地点的 18 个视频，包括“Massaa”、“Jamarat”、“Arafat”和“Tawaf”。该模型的准确率为 87%，错误率（错误分类率为 2.14%），证明了其能够有效检测和分类各种人群状况。这有助于在朝觐等大型活动期间加强人群管理和安全性。]]></description>
      <guid>https://arxiv.org/abs/2501.04911</guid>
      <pubDate>Fri, 10 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从网格完成到 AI 设计牙冠</title>
      <link>https://arxiv.org/abs/2501.04914</link>
      <description><![CDATA[arXiv:2501.04914v1 公告类型：新
摘要：设计牙冠是一个耗时且劳动密集的过程。我们的目标是简化牙冠设计并尽量减少手动调整的繁琐，同时仍确保最高的准确性和一致性。为此，我们提出了一种新的端到端深度学习方法，称为牙科网格完成 (DMC)，以生成基于点云环境的牙冠网格。牙科环境包括准备接受牙冠的牙齿及其周围环境，即相邻的两颗牙齿和对颌中最接近的三颗牙齿。我们根据完成此点云环境来制定牙冠生成。特征提取器首先将输入点云转换为一组表示点云中局部区域的特征向量。然后将该组特征向量输入到转换器中，以预测缺失区域（牙冠）的一组新特征向量。随后，使用点重建头和多层感知器来预测具有法线的密集点集。最后，可微分的点到网格层用于重建冠表面网格。我们将 DMC 方法与基于图的卷积神经网络进行比较，后者学习将冠网格从通用冠形状变形为目标几何形状。在我们的数据集上进行的大量实验证明了我们方法的有效性，平均倒角距离为 0.062。代码可在以下位置获得：https://github.com/Golriz-code/DMC.gi]]></description>
      <guid>https://arxiv.org/abs/2501.04914</guid>
      <pubDate>Fri, 10 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Image2CADSeq：计算机辅助设计序列和产品图像的知识推理</title>
      <link>https://arxiv.org/abs/2501.04928</link>
      <description><![CDATA[arXiv:2501.04928v1 公告类型：新
摘要：计算机辅助设计 (CAD) 工具使设计人员能够通过一系列 CAD 操作（通常称为 CAD 序列）来设计和修改 3D 模型。在无法访问数字 CAD 文件的情况下，逆向工程 (RE) 已用于重建 3D CAD 模型。最近的进展见证了 RE 数据驱动方法的兴起，主要侧重于将 3D 数据（例如点云）转换为边界表示 (B-rep) 格式的 3D 模型。然而，获取 3D 数据带来了重大挑战，并且 B-rep 模型不会揭示有关设计的 3D 建模过程的知识。为此，我们的研究引入了一种使用 Image2CADSeq 神经网络模型的新型数据驱动方法。该模型旨在通过处理图像作为输入并生成 CAD 序列来逆向工程 CAD 模型。然后可以使用实体建模内核将这些序列转换为 B-rep 模型。与 B-rep 模型不同，CAD 序列提供了更大的灵活性，可以修改模型创建的各个步骤，从而更深入地了解 CAD 模型的构建过程。为了定量和严格地评估 Image2CADSeq 模型的预测性能，我们开发了一个用于模型评估的多级评估框架。该模型在专门合成的数据集上进行训练，并探索了各种网络架构以优化性能。实验和验证结果表明，该模型在从二维图像数据生成 CAD 序列方面具有巨大潜力。]]></description>
      <guid>https://arxiv.org/abs/2501.04928</guid>
      <pubDate>Fri, 10 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>即插即用 DISep：分离高分辨率遥感图像中场景到像素弱监督变化检测的密集实例</title>
      <link>https://arxiv.org/abs/2501.04934</link>
      <description><![CDATA[arXiv:2501.04934v1 公告类型：新
摘要：现有的弱监督变化检测（WSCD）方法在场景级监督下经常遇到“实例聚集”的问题，特别是在变化实例（即变化对象）分布密集的场景中。在这些场景中，变化实例之间不变的像素也会被错误地识别为变化，导致多个变化被错误地视为一个。在实际应用中，这个问题阻碍了对变化数量的准确量化。为了解决这个问题，我们提出了一种密集实例分离（DISep）方法作为即插即用的解决方案，在场景级监督下从统一实例角度细化像素特征。具体来说，我们的 DISep 包含一个三步迭代训练过程：1）实例定位：我们使用高通类激活图定位变化像素的实例候选区域。2）实例检索：我们通过连通性搜索识别这些变化的像素并将其分组到不同的实例 ID 中。然后，根据分配的实例 ID，我们在每个实例的基础上提取相应的像素级特征。3) 实例分离：我们引入分离损失来强制嵌入空间中的实例内像素一致性，从而确保可分离的实例特征表示。所提出的 DISep 仅增加最小的训练成本，而不增加推理成本。它可以无缝集成以增强现有的 WSCD 方法。我们通过增强 LEVIR-CD、WHU-CD、DSIFN-CD、SYSU-CD 和 CDD 数据集上的 {三种基于 Transformer 的方法和四种基于 ConvNet 的方法} 实现了最先进的性能。此外，我们的 DISep 可用于改进全监督变化检测方法。代码可在 https://github.com/zhenghuizhao/Plug-and-Play-DISep-for-Change-Detection 上获得。]]></description>
      <guid>https://arxiv.org/abs/2501.04934</guid>
      <pubDate>Fri, 10 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于引用视频对象分割的多上下文时间一致性建模</title>
      <link>https://arxiv.org/abs/2501.04939</link>
      <description><![CDATA[arXiv:2501.04939v1 公告类型：新
摘要：引用视频对象分割旨在根据给定的文本描述对视频中的对象进行分割。现有的基于变换器的时间建模方法面临着与查询不一致和对上下文考虑有限相关的挑战。查询不一致会在视频中间产生不同对象的不稳定掩码。对上下文的考虑有限导致分割不正确的对象，因为无法充分考虑给定文本和实例之间的关系。为了解决这些问题，我们提出了多上下文时间一致性模块 (MTCM)，它由对齐器和多上下文增强器 (MCE) 组成。对齐器从查询中去除噪声并对齐它们以实现查询一致性。MCE 通过考虑多上下文来预测与文本相关的查询。我们将 MTCM 应用于四种不同的模型，提高了所有模型的性能，特别是在 MeViS 上实现了 47.6 J&amp;F。代码可在https://github.com/Choi58/MTCM获取。]]></description>
      <guid>https://arxiv.org/abs/2501.04939</guid>
      <pubDate>Fri, 10 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>