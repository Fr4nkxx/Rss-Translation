<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Wed, 12 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>自动驾驶辅助视觉集成法学硕士：人类表现比较与信任评估</title>
      <link>https://arxiv.org/abs/2502.06843</link>
      <description><![CDATA[arXiv:2502.06843v1 公告类型：新 
摘要：传统的自动驾驶系统由于对空间关系的理解有限，在复杂、意外的场景中推理时经常遇到困难。为此，本研究引入了一种基于大型语言模型 (LLM) 的自动驾驶 (AD) 辅助系统，该系统集成了视觉适配器和 LLM 推理模块，以增强视觉理解和决策。视觉适配器结合 YOLOv4 和 Vision Transformer (ViT)，提取全面的视觉特征，而 GPT-4 则能够进行类似人类的空间推理和响应生成。对 45 名经验丰富的驾驶员进行的实验评估表明，该系统在描述情况方面与人类的表现非常相似，并且在生成适当的响应方面与人类的决策适度一致。]]></description>
      <guid>https://arxiv.org/abs/2502.06843</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AutoSketch：VLM 辅助的样式感知矢量草图完成</title>
      <link>https://arxiv.org/abs/2502.06860</link>
      <description><![CDATA[arXiv:2502.06860v1 公告类型：新
摘要：能够自动完成描绘复杂场景的部分草图（例如“公园里的女人和男人聊天”）非常有用。然而，现有的草图生成方法都是从头开始创建草图；它们无法以原始风格完成部分草图。为了应对这一挑战，我们引入了 AutoSketch，这是一种可适应多种草图风格的风格感知矢量草图完成方法。我们的主要观察是，自然语言中草图的风格描述在自动草图完成过程中保留了风格。因此，我们使用预训练的视觉语言模型 (VLM) 以自然语言描述部分草图的风格，并使用新生成的笔画复制这些风格。我们最初优化笔画以匹配输入提示，并通过从 VLM 中提取的风格描述进行增强。这样的描述允许该方法建立与部分草图紧密一致的扩散先验。接下来，我们利用 VLM 生成可执行的样式调整代码，调整笔画以符合所需的样式。我们将我们的方法与现有方法在各种草图样式和提示中进行比较，进行了广泛的消融研究和定性和定量评估，并证明 AutoSketch 可以支持各种草图场景。]]></description>
      <guid>https://arxiv.org/abs/2502.06860</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>BF-GAN：利用生成对抗网络开发 AI 驱动的气泡流图像生成模型</title>
      <link>https://arxiv.org/abs/2502.06863</link>
      <description><![CDATA[arXiv:2502.06863v1 公告类型：新
摘要：开发了一种称为气泡流生成对抗网络（BF-GAN）的生成式AI架构，旨在通过物理条件输入jg和jf生成逼真的高质量气泡流图像。首先，在不同条件下进行了52组气泡流实验，以收集140,000张带有物理标签jg和jf的气泡流图像作为训练数据。然后开发了一个多尺度损失函数，结合失配损失和像素损失，以进一步增强BF-GAN的生成性能。关于生成式AI的评估指标，BF-GAN已经超越了传统的GAN。在物理上，提取了BF-GAN生成的气泡流的关键参数，并与测量值和经验相关性进行了比较，验证了BF-GAN的生成性能。对比分析表明，BF-GAN 可以在研究范围内以任意给定的 jg 和 jf 生成逼真的高质量气泡流图像。
BF-GAN 为两相流研究提供了生成式 AI 解决方案，大大降低了获取高质量数据所需的时间和成本。此外，它可以作为气泡流检测和分割算法的基准数据集生成器，从而提高该研究领域的整体生产力。BF-GAN 模型可在线获取（https://github.com/zhouzhouwen/BF-GAN）。]]></description>
      <guid>https://arxiv.org/abs/2502.06863</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>超越视觉：大型语言模型如何根据效价-唤醒值解读面部表情</title>
      <link>https://arxiv.org/abs/2502.06875</link>
      <description><![CDATA[arXiv:2502.06875v1 公告类型：新
摘要：大型语言模型主要通过基于文本的输入和输出进行操作，但人类的情感是通过口头和非口头线索传达的，包括面部表情。虽然视觉语言模型可以分析图像中的面部表情，但它们是资源密集型的，可能更多地依赖于语言先验而不是视觉理解。为了解决这个问题，本研究调查了 LLM 是否可以从面部表情的维度（效价和唤醒值，结构化的数字表示）推断情感意义，而不是使用原始的视觉输入。使用 Facechannel 从面部表情图像中提取 VA 值，并在两个任务中提供给 LLM：（1）将面部表情分为基本情绪（在 IIMI 数据集上）和复杂情绪（在表情数据集上）和（2）生成面部表情的语义描述（在表情数据集上）。分类任务的结果表明，LLM 很难将 VA 值划分为离散的情绪类别，尤其是对于基本极性以外的情绪（例如快乐、悲伤）。然而，在语义描述任务中，LLM 生成的文本描述与人类生成的解释非常吻合，表明其对面部表情进行自由文本情感推断的能力更强。]]></description>
      <guid>https://arxiv.org/abs/2502.06875</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过联邦学习进行安全的视觉数据处理</title>
      <link>https://arxiv.org/abs/2502.06889</link>
      <description><![CDATA[arXiv:2502.06889v1 公告类型：新
摘要：随着视觉数据管理中对隐私的需求不断增长，保护敏感信息已成为一项关键挑战。本文通过利用联邦学习来解决大规模视觉数据处理中对隐私保护解决方案的需求。尽管该领域已经取得了进展，但先前的研究主要集中在将对象检测与匿名化或联邦学习相结合。然而，这些组合往往无法解决复杂的隐私问题。一方面，单独使用匿名化的对象检测可能容易受到逆向技术的攻击。另一方面，联邦学习可能无法提供足够的隐私保障。因此，我们提出了一种结合对象检测、联邦学习和匿名化的新方法。结合这三个组件旨在通过解决视觉数据中的不同漏洞来提供强大的隐私保护策略。我们的解决方案与传统的集中式模型进行了评估，结果表明，虽然准确性略有下降，但隐私优势巨大，非常适合隐私敏感型应用程序。]]></description>
      <guid>https://arxiv.org/abs/2502.06889</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一种用于 TikTok 上疑似虚假信息的多模态检测的新型混合智能方法</title>
      <link>https://arxiv.org/abs/2502.06893</link>
      <description><![CDATA[arXiv:2502.06893v1 公告类型：新
摘要：在多媒体内容快速传播的背景下，识别 TikTok 等社交媒体平台上的虚假信息是一项重大挑战。本研究引入了一个混合框架，将深度学习的计算能力与模糊逻辑的可解释性相结合，以检测 TikTok 视频中的可疑虚假信息。该方法由两个核心组件组成：一个多模态特征分析器，用于从文本、音频和视频中提取和评估数据；以及基于模糊逻辑的多模态虚假信息检测器。这些系统协同运行以评估传播虚假信息的嫌疑，利用人类的行为线索，例如肢体语言、语音模式和文本连贯性。进行了两个实验：一个侧重于特定情境的虚假信息，另一个侧重于模型在更广泛主题中的可扩展性。对于每个评估的视频，都会生成高质量、全面、结构良好的报告，提供虚假信息行为的详细视图。]]></description>
      <guid>https://arxiv.org/abs/2502.06893</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>人工智能驱动的 HSI：多模态、融合、挑战和深度学习革命</title>
      <link>https://arxiv.org/abs/2502.06894</link>
      <description><![CDATA[arXiv:2502.06894v1 公告类型：新
摘要：高光谱成像 (HSI) 可捕获空间和光谱数据，从而能够分析传统系统无法看到的特征。该技术在天气监测、食品质量控制、假冒检测、医疗诊断等领域至关重要，同时还扩展到国防、农业和工业自动化领域。HSI 在光谱分辨率、小型化和计算方法方面取得了进步。本研究概述了 HSI、其应用、数据融合中的挑战以及深度学习模型在处理 HSI 数据中的作用。我们讨论了多模态 HSI 与人工智能（尤其是深度学习）的集成如何提高分类准确性和操作效率。深度学习增强了 HSI 在特征提取、变化检测、去噪解混、降维、土地覆盖制图、数据增强、光谱构造和超分辨率等领域的分析能力。一个新兴的焦点是高光谱相机与大型语言模型 (LLM) 的融合，称为高脑 LLM，从而能够开发诸如低能见度碰撞检测和人脸反欺骗等高级应用。我们还重点介绍了 HSI 行业的关键参与者、其复合年增长率和日益增长的工业意义。目的是为技术和非技术受众提供见解，涵盖 HSI 的图像、趋势和未来方向，同时提供有关 HSI 数据集和软件库的宝贵信息。]]></description>
      <guid>https://arxiv.org/abs/2502.06894</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GAS：基于单幅图​​像的生成头像合成</title>
      <link>https://arxiv.org/abs/2502.06957</link>
      <description><![CDATA[arXiv:2502.06957v1 公告类型：新
摘要：我们引入了一个可通用的统一框架，用于从单个图像合成视图一致且时间连贯的头像，解决了单图像头像生成的难题。虽然最近的方法采用以深度或法线图等人体模板为条件的扩散模型，但由于稀疏驱动信号与实际人体主体之间的差异，它们往往难以保留外观信息，从而导致多视图和时间不一致。我们的方法通过将基于回归的 3D 人体重建的重建能力与扩散模型的生成能力相结合来弥补这一差距。来自初始重建人体的密集驱动信号提供了全面的调节，确保了忠实于参考外观和结构的高质量合成。此外，我们提出了一个统一的框架，使从野外视频的新姿势合成中学习到的泛化能够自然地转移到新视图合成。我们的基于视频的扩散模型通过高质量视图一致性渲染增强了解缠结合成，从而实现了新颖视图和新颖姿势动画中逼真的非刚性变形。结果证明了我们的方法在域内和域外野外数据集中的卓越泛化能力。项目页面：https://humansensinglab.github.io/GAS/]]></description>
      <guid>https://arxiv.org/abs/2502.06957</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从单一全景图估算室内光照和热量</title>
      <link>https://arxiv.org/abs/2502.06973</link>
      <description><![CDATA[arXiv:2502.06973v1 公告类型：新
摘要：本文介绍了一种新颖的应用程序，可直接从捕获的室内外高动态范围 (HDR) 全景图中估计室内光和热图。在我们的基于图像的渲染方法中，室内全景图用于估计 3D 房间布局，而相应的室外全景图则用作环境图来推断空间变化的光和材料特性。我们建立了室内光传输和热传输之间的联系，并实施瞬态热模拟以生成室内热全景图。对各种热参数进行灵敏度分析，并将得到的热图与热像仪在现实场景中捕获的图像进行比较。此数字应用程序可自动估计室内光和热，无需手动输入和繁琐的现场测量。]]></description>
      <guid>https://arxiv.org/abs/2502.06973</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从图像到视频：扩散表征的实证研究</title>
      <link>https://arxiv.org/abs/2502.07001</link>
      <description><![CDATA[arXiv:2502.07001v1 公告类型：新
摘要：扩散模型彻底改变了生成建模，使图像和视频合成具有前所未有的真实感。这一成功激发了人们对利用其表示进行视觉理解任务的兴趣。虽然最近的研究已经探索了这种图像生成的潜力，但视频扩散模型的视觉理解能力仍然很大程度上未知。为了解决这一差距，我们系统地比较了为视频和图像生成训练的相同模型架构，分析了它们在各种下游任务中的潜在表示的性能，包括图像分类、动作识别、深度估计和跟踪。结果表明，视频扩散模型的表现始终优于图像扩散模型，尽管我们发现这种优越性的程度范围惊人。我们进一步分析了从不同层和不同噪声水平提取的特征，以及模型大小和训练预算对表示和生成质量的影响。这项工作标志着首次直接比较视频和图像扩散目标以实现视觉理解，深入了解了时间信息在表示学习中的作用。]]></description>
      <guid>https://arxiv.org/abs/2502.07001</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AstroLoc：强大的空间到地面图像定位器</title>
      <link>https://arxiv.org/abs/2502.07003</link>
      <description><![CDATA[arXiv:2502.07003v1 公告类型：新
摘要：宇航员每天从国际空间站拍摄数千张地球照片，这些照片一旦定位在地球表面，便可用于多种任务，从气候变化研究到灾害管理。定位过程几十年来一直由人工执行，最近通过图像检索解决方案来实现：给定一张宇航员照片，在大量带有地理标记的卫星图像数据库中找到与其最相似的匹配，这项任务称为宇航员摄影定位 (APL)。然而，现有的 APL 方法仅使用卫星图像进行训练，而没有利用数百万张开源宇航员照片。在这项工作中，我们提出了第一个能够利用宇航员照片进行训练的 APL 管道。我们首先通过自动化管道为 300,000 张手动弱标记的宇航员照片生成完整的定位信息，然后使用这些图像来训练一个名为 AstroLoc 的模型。 AstroLoc 通过两种损失来学习地球表面特征的稳健表示：将宇航员照片与其对应的卫星照片配对，形成成对损失；通过无监督挖掘，对卫星图像集群进行第二次损失，这些卫星图像集群按与宇航员照片的相关性加权。我们发现，与之前的 SOTA 相比，AstroLoc 在召回率@1 方面实现了惊人的 35% 平均提升，突破了现有数据集的极限，召回率@100 始终超过 99%。最后，我们注意到，AstroLoc 无需任何微调，就能为相关任务提供出色的结果，例如太空中丢失的卫星问题和历史太空图像定位。]]></description>
      <guid>https://arxiv.org/abs/2502.07003</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>物理学中创造力的基础：AIGC 中物理先验的简要概述</title>
      <link>https://arxiv.org/abs/2502.07007</link>
      <description><![CDATA[arXiv:2502.07007v1 公告类型：新
摘要：人工智能生成内容的最新进展显著提高了 3D 和 4D 生成的真实感。然而，大多数现有方法优先考虑外观一致性，而忽略了底层物理原理，导致不切实际的变形、不稳定的动态和不合理的物体相互作用等伪影。将物理先验融入生成模型已成为增强结构完整性和运动真实感的重要研究方向。本综述回顾了物理感知生成方法，系统地分析了物理约束如何融入 3D 和 4D 生成。首先，我们研究了最近将物理先验融入静态和动态 3D 生成中的研究，根据表示类型对方法进行分类，包括基于视觉、基于 NeRF 和基于高斯 Splatting 的方法。其次，我们探索 4D 生成的新兴技术，重点关注使用物理模拟对时间动态进行建模的方法。最后，我们对主要方法进行了比较分析，重点介绍了它们的优势、局限性以及对不同材料和运动动力学的适用性。通过对基于物理的 AIGC 进行深入分析，本调查旨在弥合生成模型与物理现实之间的差距，提供启发未来物理一致内容生成研究的见解。]]></description>
      <guid>https://arxiv.org/abs/2502.07007</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过以快照为中心的视频分析进行腹腔镜胆囊切除术早期手术难度评估</title>
      <link>https://arxiv.org/abs/2502.07008</link>
      <description><![CDATA[arXiv:2502.07008v1 公告类型：新
摘要：目的：腹腔镜胆囊切除术 (LC) 手术难度 (LCOD) 变化很大，会影响结果。尽管在手术工作流程分析方面进行了广泛的 LC 研究，但使用术中视频数据探索 LCOD 的努力有限。早期识别 LCOD 可以让专家外科医生及时审查，增强手术室 (OR) 规划，并改善手术结果。
方法：我们提出了使用有限的视频观察进行早期 LCOD 评估的临床任务。我们设计了 SurgPrOD，这是一种深度学习模型，通过分析观察到的 LC 视频的全局和局部时间分辨率 (快照) 的特征来评估 LCOD。此外，我们提出了一种新颖的快照中心注意力 (SCA) 模块，跨快照起作用，以增强 LCOD 预测。我们引入了 CholeScore 数据集，该数据集具有视频级 LCOD 标签来验证我们的方法。
结果：我们在 CholeScore 数据集中根据 3 个 LCOD 评估量表评估了 SurgPrOD。在我们评估早期和稳定正确预测的新指标上，SurgPrOD 至少比基线高出 0.22 分。SurgPrOD 在 F1 得分和 top1 准确率方面分别比基线提高了至少 9 个百分点和 5 个百分点，证明了其在正确预测方面的有效性。
结论：我们提出了一项用于早期 LCOD 评估的新任务和一种新模型 SurgPrOD，该模型从全局和局部角度分析手术视频。我们在 CholeScore 数据集上的结果为使用术中视频数据研究 LCOD 建立了新的基准。]]></description>
      <guid>https://arxiv.org/abs/2502.07008</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PrismAvatar：边缘设备上的实时动画 3D 神经头部化身</title>
      <link>https://arxiv.org/abs/2502.07030</link>
      <description><![CDATA[arXiv:2502.07030v1 公告类型：新
摘要：我们提出了 PrismAvatar：一种 3D 头部头像模型，专门设计用于在资源受限的边缘设备上实现实时动画和渲染，同时在训练时仍可享受神经体积渲染的好处。通过将装配棱镜晶格与 3D 可变形头部模型相结合，我们使用混合渲染模型同时重建基于网格的头部和可变形 NeRF 模型，用于 3DMM 未表示的区域。然后，我们将可变形 NeRF 提炼为装配网格和神经纹理，可以在传统三角形渲染管道的限制内高效地进行动画和渲染。除了在移动设备上以 60 fps 的速度运行且内存使用率低之外，我们发现我们训练的模型具有与桌面设备上最先进的 3D 头像模型相当的质量。]]></description>
      <guid>https://arxiv.org/abs/2502.07030</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>长距离序列建模对于结直肠肿瘤分割是否必要？</title>
      <link>https://arxiv.org/abs/2502.07120</link>
      <description><![CDATA[arXiv:2502.07120v1 公告类型：新
摘要：3D 医学成像中结直肠癌 (CRC) 肿瘤的分割既复杂又具有临床意义，为有效的放射治疗计划和生存结果评估提供重要支持。最近，结合远程序列建模机制的 3D 体积分割架构（例如 Transformers 和 Mamba）因其在 3D 医学图像分割中实现高精度的能力而受到关注。在这项工作中，我们通过在我们新推出的结直肠肿瘤分割数据集 (CTS-204) 的背景下将这些全局标记建模技术与我们提出的 MambaOutUNet 进行比较来评估这些全局标记建模技术的有效性。我们的研究结果表明，在感兴趣的区域较小且解剖学复杂的情况下，强大的局部标记交互可以胜过远程建模技术，这表明 3D 肿瘤分割研究可能会发生转变。]]></description>
      <guid>https://arxiv.org/abs/2502.07120</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>