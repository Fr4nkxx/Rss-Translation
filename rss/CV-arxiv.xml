<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Thu, 05 Dec 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>emg2pose：用于表面肌电手部姿势估计的大型多样化基准</title>
      <link>https://arxiv.org/abs/2412.02725</link>
      <description><![CDATA[arXiv:2412.02725v1 公告类型：新
摘要：手是人类与世界互动的主要方式。可靠且始终可用的手势推理可以为人机交互产生新的直观控制方案，特别是在虚拟现实和增强现实中。计算机视觉是有效的，但需要一个或多个摄像头，并且可能会遇到遮挡、视野有限和光线不足的问题。可穿戴腕式表面肌电图 (sEMG) 是一种有前途的替代方案，可作为一种始终可用的模式来感知驱动手部运动的肌肉活动。然而，sEMG 信号在很大程度上依赖于用户解剖结构和传感器位置，现有的 sEMG 模型需要数百名用户和设备位置才能有效概括。为了促进 sEMG 姿势推理的进展，我们引入了 emg2pose 基准，这是最大的公开可用的高质量手势标签和腕部 sEMG 记录数据集。 emg2pose 包含 2kHz、16 通道 sEMG 和姿势标签，这些标签来自 26 个摄像头动作捕捉装置，适用于 193 位用户、370 小时和 29 个阶段，手势各异 - 这一规模可与基于视觉的手势数据集相媲美。我们提供具有竞争力的基线和具有挑战性的任务，以评估现实世界的泛化场景：留出用户、传感器位置和阶段。emg2pose 为机器学习社区提供了一个探索复杂泛化问题的平台，具有显著增强基于 sEMG 的人机交互发展的潜力。]]></description>
      <guid>https://arxiv.org/abs/2412.02725</guid>
      <pubDate>Thu, 05 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Prithvi-EO-2.0：用于地球观测应用的多功能多时间基础模型</title>
      <link>https://arxiv.org/abs/2412.02732</link>
      <description><![CDATA[arXiv:2412.02732v1 公告类型：新
摘要：本技术报告介绍了 Prithvi-EO-2.0，这是一种新的地理空间基础模型，与其前身 Prithvi-EO-1.0 相比有显著改进。新的 300M 和 600M 参数模型以 30 米分辨率对 NASA 的 Harmonized Landsat 和 Sentinel-2 数据档案中的 4.2M 全球时间序列样本进行训练，结合了时间和位置嵌入，以增强各种地理空间任务的性能。通过对 GEO-Bench 进行广泛的基准测试，600M 版本在一系列任务中的表现比之前的 Prithvi-EO 模型高出 8\%。当对来自不同领域和分辨率（即从 0.1m 到 15m）的遥感任务进行基准测试时，它的表现也优于其他六种地理空间基础模型。结果证明了该模型在传统地球观测和高分辨率应用中的多功能性。最终用户和主题专家 (SME) 的早期参与是促成该项目成功的关键因素之一。特别是，SME 的参与使得模型和数据集设计能够不断获得反馈，并能够成功定制各种 SME 主导的应用，包括灾害响应、土地使用和农作物制图以及生态系统动态监测。Prithvi-EO-2.0 可在 Hugging Face 和 IBM terratorch 上获得，其他资源可在 GitHub 上获得。该项目体现了所有参与组织所采用的可信开放科学方法。]]></description>
      <guid>https://arxiv.org/abs/2412.02732</guid>
      <pubDate>Thu, 05 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MVCTrack：通过多模式引导虚拟提示增强 3D 点云跟踪</title>
      <link>https://arxiv.org/abs/2412.02734</link>
      <description><![CDATA[arXiv:2412.02734v1 公告类型：新
摘要：3D 单物体跟踪在自动驾驶和机器人技术中至关重要。现有方法通常难以应对稀疏和不完整的点云场景。为了解决这些限制，我们提出了一种多模态引导虚拟线索投影 (MVCP) 方案，该方案可生成虚拟线索以丰富稀疏点云。此外，我们引入了基于生成的虚拟线索的增强型跟踪器 MVCTrack。具体而言，MVCP 方案将 RGB 传感器无缝集成到基于 LiDAR 的系统中，利用一组 2D 检测来创建密集的 3D 虚拟线索，从而显着改善点云的稀疏性。这些虚拟线索可以自然地与现有的基于 LiDAR 的 3D 跟踪器集成，从而显着提高性能。大量实验表明，我们的方法在 NuScenes 数据集上实现了具有竞争力的性能。]]></description>
      <guid>https://arxiv.org/abs/2412.02734</guid>
      <pubDate>Thu, 05 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>混合物理先验适配器，实现参数高效微调</title>
      <link>https://arxiv.org/abs/2412.02759</link>
      <description><![CDATA[arXiv:2412.02759v1 公告类型：新
摘要：大多数参数高效微调 (PEFT) 方法依赖于低秩表示来调整模型。然而，这些方法往往过于简化表示，特别是当底层数据具有高秩或高频分量时。这种限制阻碍了模型有效捕获复杂数据交互的能力。在本文中，我们提出了一种新方法，通过利用物理先验的组合来建模网络权重，从而实现更准确的近似。我们使用三个基础方程——热扩散、波传播和泊松稳态方程——每个方程都具有独特的建模特性：热扩散增强局部平滑度，波传播促进长距离相互作用，泊松方程捕捉全局平衡。为了有效地结合这些先验，我们引入了混合物理先验适配器 (MoPPA)，使用高效的离散余弦变换 (DCT) 实现。为了动态平衡这些先验，设计了一种路由正则化机制来自适应地调整它们的贡献。MoPPA 是一个轻量级的即插即用模块，可以无缝集成到 Transformer 架构中，并根据本地环境调整复杂度。具体来说，使用 MAE 预训练的 ViT-B，MoPPA 在 VTAB-1K 图像分类中将 PEFT 准确率提高了 2.1%，同时可训练参数数量相当，并且通过跨各种视觉主干的实验进一步验证了优势，展示了 MoPPA 的有效性和适应性。代码将公开。]]></description>
      <guid>https://arxiv.org/abs/2412.02759</guid>
      <pubDate>Thu, 05 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用对抗性环境攻击劫持视觉和语言导航代理</title>
      <link>https://arxiv.org/abs/2412.02795</link>
      <description><![CDATA[arXiv:2412.02795v1 公告类型：新
摘要：可以用自然语言指示在开放世界环境中执行任务的辅助具体代理有可能对制造业或家庭护理等劳动任务产生重大影响——使那些依赖它们的人的生活中受益。在这项工作中，我们考虑了这种好处如何被代理操作环境外观的本地修改所劫持。具体来说，我们以流行的视觉和语言导航 (VLN) 任务为代表设置，并开发一种白盒对抗攻击，该攻击优化了 3D 攻击对象的外观，以在环境中观察它的预训练 VLN 代理中诱导所需的行为。我们证明，所提出的攻击可能导致 VLN 代理在遇到攻击对象后忽略其指令并执行替代操作——即使对于优化攻击时未考虑的指令和代理路径也是如此。对于这些新设置，我们发现我们的攻击可以引发提前终止行为或使代理沿着攻击者定义的多步轨迹移动。在这两种情况下，环境攻击都会显著降低代理成功遵循用户指令的能力。]]></description>
      <guid>https://arxiv.org/abs/2412.02795</guid>
      <pubDate>Thu, 05 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用纯相位镜头以任意分辨率将灰度转换为高光谱</title>
      <link>https://arxiv.org/abs/2412.02798</link>
      <description><![CDATA[arXiv:2412.02798v1 公告类型：新
摘要：我们考虑从使用单个衍射光学元件和无滤光全色光电传感器捕获的 $H\times W$ 灰度快照测量中重建 $H\times W\times 31$ 高光谱图像的问题。这个问题严重不适定，我们提出了第一个能够产生高质量结果的模型。我们训练一个条件去噪扩散模型，将一个小的灰度测量补丁映射到高光谱补丁。然后，我们将模型并行部署到许多补丁，使用基于全局物理的指导来同步补丁预测。我们的模型可以使用小型高光谱数据集进行训练，然后部署以重建任意大小的高光谱图像。此外，通过使用不同的种子绘制多个样本，我们的模型可以生成有用的不确定性图。我们表明，我们的模型在以前的快照高光谱基准测试中实现了最先进的性能，其中重建条件更好。我们的工作为新型、紧凑、高效的高分辨率高光谱成像仪奠定了基础。]]></description>
      <guid>https://arxiv.org/abs/2412.02798</guid>
      <pubDate>Thu, 05 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>高斯溅射受到攻击：调查 3D 对象中的对抗性噪声</title>
      <link>https://arxiv.org/abs/2412.02803</link>
      <description><![CDATA[arXiv:2412.02803v1 公告类型：新
摘要：3D Gaussian Splatting 具有先进的辐射场重建功能，可在 3D 建模中实现高质量的视图合成和快速渲染。虽然针对 2D 图像的对抗性攻击对物体检测模型的研究已经很深入，但它们对 3D 模型的影响仍未得到充分探索。这项工作引入了掩蔽迭代快速梯度符号方法 (M-IFGSM)，旨在生成针对 CLIP 视觉语言模型的对抗性噪声。M-IFGSM 通过将扰动集中在掩蔽区域来专门改变感兴趣的对象，从而降低 CLIP 的零样本物体检测能力在应用于 3D 模型时的性能。使用来自 Common Objects 3D (CO3D) 数据集的八个对象，我们证明我们的方法有效地降低了模型的准确性和置信度，对抗性噪声对人类观察者来说几乎不可察觉。原始模型的 top-1 准确率从训练图像的 95.4% 下降到 12.5%，从测试图像的 91.2% 下降到 35.4%，置信度水平反映了从真实分类到错误分类的转变，凸显了自动驾驶、机器人和监控等应用中 3D 模型遭受对抗性攻击的风险。这项研究的意义在于它有可能揭示现代 3D 视觉模型（包括辐射场）中的漏洞，从而促使在关键的现实应用中开发更强大的防御和安全措施。]]></description>
      <guid>https://arxiv.org/abs/2412.02803</guid>
      <pubDate>Thu, 05 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>STORM：罕见事件分类模式的战略协调</title>
      <link>https://arxiv.org/abs/2412.02805</link>
      <description><![CDATA[arXiv:2412.02805v1 公告类型：新
摘要：在生物医学等领域，专家见解对于选择人工智能 (AI) 方法中最具信息量的模式至关重要。然而，使用所有可用的模式会带来挑战，特别是在确定每种模式对性能的影响以及优化它们的组合以实现准确分类方面。传统方法采用手动试错法，缺乏辨别最相关模式的系统框架。此外，尽管多模态学习能够整合来自不同来源的信息，但利用所有可用的模式通常是不切实际和不必要的。为了解决这个问题，我们引入了一种基于熵的算法 STORM 来解决罕见事件的模态选择问题。该算法系统地评估各个模态及其组合的信息内容，确定罕见类别分类任务所必需的最具辨别力的特征。通过癫痫发作区检测案例研究，我们证明了我们的算法在提高分类性能方面的有效性。通过选择有用的子集，我们的方法为更有效的人工智能驱动的生物医学分析铺平了道路，从而促进临床环境中的疾病诊断。]]></description>
      <guid>https://arxiv.org/abs/2412.02805</guid>
      <pubDate>Thu, 05 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>时间一致的动态场景图：一种用于动作轨迹生成的端到端方法</title>
      <link>https://arxiv.org/abs/2412.02808</link>
      <description><![CDATA[arXiv:2412.02808v1 公告类型：新
摘要：理解视频内容对于推进活动识别、自主系统和人机交互等现实世界应用至关重要。虽然场景图擅长捕捉各个帧中对象之间的空间关系，但扩展这些表示以捕捉视频序列之间的动态交互仍然是一项重大挑战。为了解决这个问题，我们提出了 TCDSG，即时间一致的动态场景图，这是一个创新的端到端框架，可以检测、跟踪和链接跨时间的主客体关系，生成动作轨迹、时间一致的实体序列及其交互。我们的方法利用了一种新颖的二分匹配机制，并通过自适应解码器查询和反馈循环进行增强，确保了扩展序列的时间连贯性和稳健的跟踪。该方法不仅通过在 Action Genome、OpenPVSG 和 MEVA 数据集上实现超过 60% 的时间召回率提高建立了新的基准，而且还率先使用持久对象 ID 注释增强 MEVA 以实现全面的轨迹生成。通过无缝集成空间和时间动态，我们的工作为多帧视频分析设立了新标准，为监​​控、自动导航等领域的高影响力应用开辟了新途径。]]></description>
      <guid>https://arxiv.org/abs/2412.02808</guid>
      <pubDate>Thu, 05 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Many-MobileNet：用于稳健视网膜疾病分类的多模型增强</title>
      <link>https://arxiv.org/abs/2412.02825</link>
      <description><![CDATA[arXiv:2412.02825v1 公告类型：新
摘要：在这项工作中，我们提出了 Many-MobileNet，这是一种使用轻量级 CNN 架构进行视网膜疾病分类的有效模型融合策略。我们的方法通过训练具有不同数据增强策略和不同模型复杂度的多个模型来解决过度拟合和数据集变异性有限等关键挑战。通过这种融合技术，我们在数据稀缺领域实现了稳健的泛化，同时平衡了计算效率和特征提取能力。]]></description>
      <guid>https://arxiv.org/abs/2412.02825</guid>
      <pubDate>Thu, 05 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FLAME 3 数据集：释放辐射热无人机影像的强大功能，助力野火管理</title>
      <link>https://arxiv.org/abs/2412.02831</link>
      <description><![CDATA[arXiv:2412.02831v1 公告类型：新
摘要：无人机 (UAV) 的辐射热成像传感器的普及程度不断提高，为推进人工智能驱动的空中野火管理提供了巨大潜力。辐射成像提供每像素温度估计，这比非辐射数据有重大改进，非辐射数据需要使用 RGB 调色板将辐照度测量值转换为可见图像。尽管这项技术有诸多好处，但由于研究人员缺乏可用数据，因此未得到充分利用。本研究通过介绍在规定火灾中使用无人机收集和处理同步可见光谱和辐射热图像的方法来解决这一差距。所包含的图像处理管道大大简化并部分自动化了从数据收集到神经网络输入的每个步骤。此外，我们展示了 FLAME 3 数据集，这是第一个全面的并排可见光谱和野火辐射热图像集合。 FLAME 3 以我们之前的 FLAME 1 和 FLAME 2 数据集为基础，包括辐射热标记图像文件格式 (TIFF) 和天底热图，提供了一种新的数据类型和收集方法。该数据集旨在推动新一代利用辐射热图像的机器学习模型，从而有可能简化空中野火检测、分割和评估等任务。Kaggle 上提供了用于计算机视觉应用的 FLAME 3 单次燃烧子集，读者可以根据要求获得完整的 6 次燃烧集。]]></description>
      <guid>https://arxiv.org/abs/2412.02831</guid>
      <pubDate>Thu, 05 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过双峰测试时间自适应增强 CLIP 对常见损坏的鲁棒性</title>
      <link>https://arxiv.org/abs/2412.02837</link>
      <description><![CDATA[arXiv:2412.02837v1 公告类型：新
摘要：尽管开放词汇分类模型（如对比语言图像预训练 (CLIP)）已经展示了强大的零样本学习能力，但它们对常见图像损坏的鲁棒性仍然不太清楚。通过大量实验，我们表明零样本 CLIP 在测试时对严重程度不断增加的常见图像损坏缺乏鲁棒性，因此需要使用测试时自适应 (TTA) 将 CLIP 适应未标记的损坏图像。然而，我们发现现有的 TTA 方法由于其单峰性质而在适应 CLIP 方面存在严重限制。为了解决这些限制，我们提出了 \framework，这是一种双峰 TTA 方法，专门用于提高 CLIP 对常见图像损坏的鲁棒性。我们方法的关键见解不仅是调整视觉编码器以更好地提取图像特征，而且还通过促进使用伪标签计算的图像类原型与相应文本特征之间的更强关联来加强图像和文本特征之间的对齐。我们在基准图像损坏数据集上评估了我们的方法，并在 CLIP 的 TTA 中取得了最先进的结果，特别是在涉及图像损坏的领域。具体来说，借助 ViT-B/16 视觉主干，我们分别在 CIFAR-10C、CIFAR-100C 和 ImageNet-C 上获得了 9.7%、5.94% 和 5.12% 的平均准确率提升。]]></description>
      <guid>https://arxiv.org/abs/2412.02837</guid>
      <pubDate>Thu, 05 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>轻松高效：低成本修剪扩散模型</title>
      <link>https://arxiv.org/abs/2412.02852</link>
      <description><![CDATA[arXiv:2412.02852v1 公告类型：新
摘要：扩散模型在各种视觉任务中取得了令人瞩目的进步。然而，这些收益通常依赖于增加模型大小，这会增加计算复杂性和内存需求，使部署复杂化，增加推理成本并造成环境影响。虽然一些研究已经探索了修剪技术来提高扩散模型的内存效率，但大多数现有方法都需要大量的再训练才能保持模型性能。重新训练现代大型扩散模型成本极高且资源密集，这限制了这些方法的实用性。在这项工作中，我们通过为扩散模型提出一个与模型无关的结构修剪框架来实现低成本的扩散修剪而无需重新训练，该框架学习可微分掩码来稀疏模型。为了确保有效的修剪以保持最终去噪潜伏的质量，我们设计了一个涵盖整个扩散过程的新型端到端修剪目标。由于端到端修剪占用大量内存，我们进一步提出了时间步长梯度检查点，这是一种显著减少优化期间内存使用量的技术，可在有限的内存预算内实现端到端修剪。对最先进的 U-Net 扩散模型 SDXL 和扩散变换器 (FLUX) 的结果表明，我们的方法可以有效修剪多达 20% 的参数，同时将性能下降降至最低，而且值得注意的是，无需重新训练模型。我们还展示了我们的方法仍然可以在时间步长蒸馏扩散模型的基础上进行修剪。]]></description>
      <guid>https://arxiv.org/abs/2412.02852</guid>
      <pubDate>Thu, 05 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>优化 CNN 以实现快速 3D 点云对象识别</title>
      <link>https://arxiv.org/abs/2412.02855</link>
      <description><![CDATA[arXiv:2412.02855v1 公告类型：新
摘要：本研究介绍了一种使用卷积神经网络 (CNN) 有效检测 3D 点云中物体的方法。我们的方法采用独特的以特征为中心的投票机制来构建卷积层，利用输入数据中观察到的典型稀疏性。我们探索了不同网络架构之间准确性和速度之间的权衡，并主张在过滤器激活上集成 $\mathcal{L}_1$ 惩罚以增强中间层内的稀疏性。这项研究开创了稀疏卷积层与 $\mathcal{L}_1$ 正则化相结合的提议，以有效处理大规模 3D 数据处理。我们的方法的有效性在 MVTec 3D-AD 物体检测基准上得到了证明。只有三层的 Vote3Deep 模型在仅使用激光的方法和组合激光视觉方法方面都优于之前的最先进技术。此外，它们保持了具有竞争力的处理速度。这强调了我们的方法能够大幅提高检测性能，同时确保适合实时应用的计算效率。]]></description>
      <guid>https://arxiv.org/abs/2412.02855</guid>
      <pubDate>Thu, 05 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大规模预训练是实现良好领域泛化的秘诀吗？</title>
      <link>https://arxiv.org/abs/2412.02856</link>
      <description><![CDATA[arXiv:2412.02856v1 公告类型：新
摘要：多源域泛化 (DG) 是在多个源域上进行训练并在看不见的目标域上实现高分类性能的任务。最近的方法将来自网络规模预训练主干的稳健特征与从源数据中学习到的新特征相结合，这极大地改善了基准测试结果。然而，目前尚不清楚 DG 微调方法是否会随着时间的推移而变得更好，或者基准测试性能的提高是否仅仅是更强的预训练的结果。先前的研究表明，与预训练数据的感知相似性与零样本性能相关，但我们发现这种影响在 DG 设置中是有限的。相反，我们假设在预训练中拥有感知相似的数据是不够的；这些数据的学习效果决定了性能。这导致我们引入了对齐假设，该假设指出，当且仅当图像和类标签文本嵌入的对齐程度很高时，最终的 DG 性能才会很高。我们的实验证实了对齐假设是正确的，我们将其用作在 DomainBed 数据集上评估的现有 DG 方法的分析工具，方法是将评估数据分为预训练内 (IP) 和预训练外 (OOP)。我们表明，所有评估的 DG 方法在 DomainBed-OOP 上都表现不佳，而最近的方法在 DomainBed-IP 上表现出色。总之，我们的研究结果强调了对能够超越预训练对齐的 DG 方法的需求。]]></description>
      <guid>https://arxiv.org/abs/2412.02856</guid>
      <pubDate>Thu, 05 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>