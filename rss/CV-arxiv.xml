<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>CS.CV更新arxiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.cv更新arxiv.org e-print存档。</description>
    <lastBuildDate>Tue, 08 Apr 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>船上卫星上海洋异常检测的半监督学习</title>
      <link>https://arxiv.org/abs/2504.03705</link>
      <description><![CDATA[ARXIV：2504.03705V1公告类型：新 
摘要：水生尸体面临着几个海洋异常引起的许多环境威胁。海洋碎片可以通过纠缠来破坏栖息地，危害海洋生物，而有害的藻华会产生对海洋生态系统产生负面影响的毒素。此外，船只可能会排放石油或进行非法和过度捕捞活动，从而进一步造成伤害。可以通过在多光谱卫星图像上应用训练有素的深度学习（DL）模型来识别这些海洋异常。此外，检测其他异常（例如云）可能有益于滤除无关的图像。但是，DL模型通常需要大量的标记数据进行培训，这可能是昂贵且耗时的，特别是对于需要专家注释的海洋异常检测。潜在的解决方案是使用半监督的学习方法，该方法也可以使用未标记的数据。在这个项目中，我们实施和研究FixMatch语义分割的性能，语义分割，这是一种半监督语义分割的算法。首先，我们发现，当标记数据量有限时，半监督模型的表现最佳，高置信度阈值为0.9。其次，我们将半监督模型的性能与完全监督的模型进行了比较，该模型在不同量的标记数据下进行了比较。我们的发现表明，半监督模型优于标记数据有限的完全监督模型，而完全监督的模型的性能稍好一些，具有大量的标记数据。我们提出了两个假设，以解释为什么当使用大量标记数据时，全面监督模型超过半监督者。我们所有的实验均使用具有数量有限的参数的U-NET模型体系结构进行，以确保与空格评级硬件的兼容性。]]></description>
      <guid>https://arxiv.org/abs/2504.03705</guid>
      <pubDate>Tue, 08 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>可扩展的HelioStat表面预测焦点：反向深度学习射线座的SIM到真实转移</title>
      <link>https://arxiv.org/abs/2504.03712</link>
      <description><![CDATA[ARXIV：2504.03712V1公告类型：新 
摘要：集中太阳能（CSP）工厂是向可持续能源过渡的关键技术。其安全有效操作的关键因素是在接收器上的浓缩太阳通量的分布。但是，来自单个螺旋体的通量分布对表面缺陷敏感。在现实世界部署中，测量许多Heliostats的这些表面仍然不切实际。结果，控制系统经常假设理想化的HelioStat表面，从而导致次优的性能和潜在的安全风险。为了解决这个问题，已经引入了逆深度学习射线疗程（IDLR），作为一种新方法，用于从标准校准程序中记录的目标图像推断出HelioStat表面曲线。在这项工作中，我们介绍了IDLR的第一个成功的SIM到真实传输，从现实世界目标图像中直接实现了准确的表面预测。我们在实际操作条件下评估了63个Heliostats的方法。 IDLR表面预测的中值平均绝对误差（MAE）为0.17 mm，并在84％的病例中与偏差测量地面真相显示了良好的一致性。当用于射线疗法模拟中时，与我们的数据集的挠度测量法相比，它可以实现平均准确度的通量密度预测，并且比通常使用的理想的HelioStat表面假设高26％。我们在具有挑战性的双重驱除场景的挑战性的涉及到的太阳位置和接收器投影中测试了这种方法，并发现IDLR保持了高预测精度，突出了其概括能力。我们的结果表明，IDLR是一种可扩展，自动化和具有成本效益的解决方案，用于将现实的HelioStat表面模型整合到数字双胞胎中。这为改进的通量控制，更精确的性能建模以及最终提高了未来CSP工厂的效率和安全性打开了大门。]]></description>
      <guid>https://arxiv.org/abs/2504.03712</guid>
      <pubDate>Tue, 08 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CrowdVLM-R1：扩大R1的能力，以使用模糊组相对策略奖励来对人群进行视觉语言模型</title>
      <link>https://arxiv.org/abs/2504.03724</link>
      <description><![CDATA[ARXIV：2504.03724V1公告类型：新 
摘要：我们提出了模糊小组相对政策奖励（FGRPR），这是一个新颖的框架，将小组相对策略优化（GRPO）与模糊奖励功能集成在一起，以提高学习效率。与传统的二进制0/1精度奖励不同，我们的模糊奖励模型提供了细微的激励措施，鼓励了更精确的产出。实验结果表明，与监督的微调（SFT）相比，具有标准0/1准确性奖励的GRPO表现不佳。相比之下，适用于QWEN2.5-VL（3b和7b）的FGRPR超过了五个域内数据集的所有基线模型，包括GPT4O，Llama2（90b）和SFT。在室外数据集中，FGRPR的性能与SFT相当，但在目标值更大时会出色，因为其模糊奖励功能将更高的奖励分配给更紧密的近似值。这种方法通常适用于答案的精度至关重要的任务。代码和数据：https：//github.com/yeyimilk/crowdvlm-r1]]></description>
      <guid>https://arxiv.org/abs/2504.03724</guid>
      <pubDate>Tue, 08 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从关键点到现实主义：来自2D图像的现实且准确的虚拟尝试网络</title>
      <link>https://arxiv.org/abs/2504.03807</link>
      <description><![CDATA[ARXIV：2504.03807V1公告类型：新 
摘要：基于图像的虚拟试验的目的是生成穿着目标服装的个人的逼真的图像，以确保准确保留目标服装的姿势，身体形状和特征。现有方法通常无法有效地重现目标服装的细节，并且缺乏对新场景的普遍性。在提议的方法中，完全删除了该人的初始服装。随后，使用预测的关键点进行精确的翘曲，以将目标服装与身体结构和姿势完全对齐。根据扭曲的服装，可以更准确地预测身体分割图。然后，使用对齐的段标准化，将删除扭曲的服装和分割图中预测的服装区域之间的未对准区域。最后，发电机以高视觉质量生产最终图像，重建目标服装的精确特征，包括其整体形状和纹理。这种方法强调保留服装特征并提高对各种姿势的适应性，从而为各种应用提供更好的概括。]]></description>
      <guid>https://arxiv.org/abs/2504.03807</guid>
      <pubDate>Tue, 08 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于下一代有条件扩散模型的混合小波 - 风格方法</title>
      <link>https://arxiv.org/abs/2504.03821</link>
      <description><![CDATA[ARXIV：2504.03821V1公告类型：新 
摘要：我们提出了一个新颖的生成建模框架，小波般的扩散，它适应了扩散范式到混合频率表示，以便合成具有改进空间定位的高质量，高效率图像。与仅依赖于像素空间中添加噪声的常规扩散模型相反，我们的方法利用了将小波的副频分解与部分傅立叶步骤相结合的多转换。该策略逐渐降解，然后在正向和反向扩散过程中重建在混合光谱域中的图像。通过通过小波的空间定位功能来补充传统的基于傅立叶的分析，我们的模型可以更有效地捕获全球结构和细粒度的特征。我们通过通过交叉注意来整合嵌入或条件特征，进一步扩展了有条件图像生成的方法。对CIFAR-10，Celeba-HQ和条件成像子集的实验评估表明，我们的方法相对于基线扩散模型和最先进的GAN实现了竞争性或卓越的性能，如FR \&#39;Echet Inception Intection距离（FID）和INCEPTION评分（IS）。我们还展示了基于混合频率的表示如何改善对全局相干性和精细纹理合成的控制，从而为多尺度生成建模的新方向铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2504.03821</guid>
      <pubDate>Tue, 08 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于整流流的文本对图像生成模型中树环水印的检测极限和统计​​可分离性</title>
      <link>https://arxiv.org/abs/2504.03850</link>
      <description><![CDATA[ARXIV：2504.03850V1公告类型：新 
摘要：树环水印是对AI生成的图像进行身份验证的重要技术。但是，它在基于整流的基于流动的模型中的有效性仍未开发，尤其是考虑到这些模型具有噪声潜在反演的固有挑战。通过广泛的实验，我们评估并比较了SD 2.1和Flux.1-DEV模型之间水印的检测和分离性。通过分析各种文本指导配置和增强攻击，我们演示了反演限制如何影响水印恢复和水印和不标记图像之间的统计分离。我们的发现提供了对当前SOTA模型中树环水印的当前局限性的宝贵见解，并突出了对改进反转方法的关键需求，以实现可靠的水印检测和可分离性。官方实施，数据集发布和所有实验结果均可在此\ href {https://github.com/dsgiitr/flux-watermarking} {\ textbf {link}}}中获得。]]></description>
      <guid>https://arxiv.org/abs/2504.03850</guid>
      <pubDate>Tue, 08 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Chatgpt可以从第一人称视频中学习我的生活吗？</title>
      <link>https://arxiv.org/abs/2504.03857</link>
      <description><![CDATA[ARXIV：2504.03857V1公告类型：新 
摘要：我的动机是由生成AI和可穿戴式摄像头设备（例如智能眼镜和AI-a-ables Pins）的改进而进行的，我研究了基础模型通过第一人称相机数据了解佩戴者的个人生活的能力。为了测试这一点，我在一周的时间内戴了相机耳机54小时，生成了各种长度的摘要（例如，长达一分钟，长达一个小时和一整天的摘要），并在由此产生的摘要层次上进行了微调的GPT-4O和GPT-4O和GPT-4O-Mini。通过查询微调模型，我们可以了解模型对我的了解。结果混合在一起：两种模型都学到了有关我的基本信息（例如大约年龄，性别）。此外，GPT-4O正确地推断出我住在匹兹堡，是CMU的博士生，Am Am Am右撇子，并有一只宠物猫。但是，这两种模型也都遭受了幻觉的困扰，并将为我一生的视频录像中的个人构成名字。]]></description>
      <guid>https://arxiv.org/abs/2504.03857</guid>
      <pubDate>Tue, 08 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用地图查询银行在线地图生成的控制地图分布</title>
      <link>https://arxiv.org/abs/2504.03868</link>
      <description><![CDATA[ARXIV：2504.03868V1公告类型：新 
摘要：可靠的自主驾驶系统需要高清（HD）图，其中包含用于计划和导航的详细地图信息。但是，前构建高清图需要大量成本。基于视觉的在线地图生成（OMG）已成为构建本地高清图的替代低成本解决方案。基于查询的BEV变压器一直是该任务的基础模型。该模型从初始地图查询分布中学习高清地图预测，该预测是通过在训练集上的离线优化获得的。除了BEV功能的质量外，该模型的性能还高度依赖于初始地图​​查询分布的能力。但是，此分布受到限制，因为有限的查询编号。为了在每个测试样本上进行最佳的地图预测，对于每个特定方案生成合适的初始分布至关重要。本文建议将整个高清图分布分解为一组点表示，即MAP查询库（MQBANK）。为了构建不同方案的特定地图查询初始分布，将低成本标准定义图（SD地图）数据作为一种先验知识引入。此外，地图解码器网络的每一层都会学习实例级地图查询功能，这将失去每个点的详细信息。但是，BEV功能映射是一个点级密集的功能。与BEV功能映射交互时，将点级信息保留在地图查询中很重要。这也可以通过地图查询银行方法来解决。最终实验显示了有关SD地图先验的新见解，以及OpenLaneV2基准的新记录，在车道和行人区域的地图为40.5％，45.7％的地图。]]></description>
      <guid>https://arxiv.org/abs/2504.03868</guid>
      <pubDate>Tue, 08 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>3D场景通过本地随机访问序列建模理解</title>
      <link>https://arxiv.org/abs/2504.03875</link>
      <description><![CDATA[ARXIV：2504.03875V1公告类型：新 
摘要：来自单个图像的3D场景理解是计算机视觉中的关键问题，具有图形，增强现实和机器人技术中的许多下游应用程序。尽管基于扩散的建模方法已经表现出了希望，但它们通常很难维持对象和场景一致性，尤其是在复杂的现实情况下。为了解决这些局限性，我们提出了一种称为局部随机访问序列（LRAS）建模的自回归生成方法，该方法使用局部补丁量化和随机有序的序列生成。通过利用光流作为3D场景编辑的中间表示，我们的实验表明，LRAS可以实现最新的新型视图合成和3D对象操纵能力。此外，我们表明我们的框架自然地通过简单地修改序列设计来扩展到自我监督的深度估计。通过在多个3D场景理解任务上实现强大的性能，LRAS为建立下一代3D视觉模型提供了一个统一有效的框架。]]></description>
      <guid>https://arxiv.org/abs/2504.03875</guid>
      <pubDate>Tue, 08 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Wildgs-slam：在动态环境中单眼高斯脱落大满贯</title>
      <link>https://arxiv.org/abs/2504.03886</link>
      <description><![CDATA[ARXIV：2504.03886V1公告类型：新 
摘要：我们提出了Wildgs-Slam，这是一种强大而有效的单眼RGB SLAM系统，旨在通过利用不确定性感知的几何映射来处理动态环境。与采用静态场景的传统大满贯系统不同，我们的方法集成了深度和不确定性信息，以在有移动对象的情况下增强跟踪，映射和渲染性能。我们介绍了一个不确定性图，该图由浅层多层感知器和Dinov2功能预测，以指导跟踪和映射过程中的动态对象去除。此不确定性图可增强密集的束调整和高斯图优化，从而提高重建精度。我们的系统在多个数据集上进行了评估，并演示了无伪影的视图合成。与最先进的方法相比，结果展示了Wildgs-Slam在动态环境中的出色表现。]]></description>
      <guid>https://arxiv.org/abs/2504.03886</guid>
      <pubDate>Tue, 08 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用步态模式为生物标志物：引导的深层实例学习网络用于脊柱侧弯分类</title>
      <link>https://arxiv.org/abs/2504.03894</link>
      <description><![CDATA[ARXIV：2504.03894V1公告类型：新 
摘要：脊柱侧弯是一种脊柱曲率疾病，难以早期检测并可以压缩胸腔，从而影响呼吸功能和心脏健康。特别是对于青少年，延迟检测和治疗导致压缩恶化。传统的脊柱侧弯检测方法在很大程度上依赖于临床专业知识，X射线成像构成了辐射风险，从而限制了大规模的早期筛查。我们提出了一种注意力引导的深层多企业学习方法（步态-MIL），以有效地捕获步态模式的歧视性特征，该特征的灵感来自Sconet-MT的开创性使用步态模式用于脊柱侧弯的检测。我们根据脊柱侧弯分类的步态模式在第一个大规模数据集上评估我们的方法。结果表明，我们的研究提高了使用步态作为脊柱侧弯检测的生物标志物的性能，这显着提高了特别具有挑战性的中性病例的检测准确性，在这种情况下，经常忽略了微妙的指标。我们的步态米尔在不平衡的情况下也表现出色，使其成为大规模脊柱侧弯筛查的有前途的工具。]]></description>
      <guid>https://arxiv.org/abs/2504.03894</guid>
      <pubDate>Tue, 08 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过高级脑功能表示和kolmogorov-arnold网络改善脑部疾病诊断</title>
      <link>https://arxiv.org/abs/2504.03923</link>
      <description><![CDATA[ARXIV：2504.03923V1公告类型：新 
摘要：量化功能连通性（FC），这是一种诊断各种脑部疾病的重要度量，传统上依赖于使用预定的脑图集。但是，使用这种图谱可能会导致有关选择偏见和缺乏特异性的问题。解决此问题时，我们提出了一个具有有效的大脑功能表示的新型基于变压器的分类网络（AFBR-KAN），以帮助诊断自闭症谱系障碍（ASD）。 AFBR-KAN利用Kolmogorov-Arnold网络（KAN）阻止了取代传统的多层感知器（MLP）组件。彻底的实验揭示了AFBR-KAN在模型架构的各种配置下改善ASD诊断的有效性。我们的代码可从https://github.com/tbwa233/abfr-kan获得]]></description>
      <guid>https://arxiv.org/abs/2504.03923</guid>
      <pubDate>Tue, 08 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>概率：开放世界中心活动识别的概率跳跃扩散</title>
      <link>https://arxiv.org/abs/2504.03948</link>
      <description><![CDATA[ARXIV：2504.03948V1公告类型：新 
摘要：开放世界的自我中心活动识别构成了基本挑战，这是由于其不受约束的性质，要求模型从广阔的，部分观察到的搜索空间中推断出看不见的活动。我们介绍了基于跳跃扩散的概率残留搜索框架ProBRES，它通过平衡先前引入的探索与可能性驱动的剥削来有效地导航该空间。我们的方法集成了结构常识性先验，以构建语义相干的搜索空间，使用视觉模型（VLMS）自适应地完善预测，并采用随机搜索机制来定位高样子活动标签，同时有效地最大程度地减少详尽的枚举。我们系统地评估了跨多个开放度级别（L0 -L3）的概率，证明了其对提高搜索空间复杂性的适应性。除了在基准数据集（GTEA凝视，GTEA凝视+，Epic-kitchens和Charades-ego）上实现最先进的性能外，我们还建立了一个明确的分类法，以阐明开放世界的识别，从而描述了挑战和方法论的进步，以实现自我中心活动的理解。我们的结果突出了结构化搜索策略的重要性，为可扩展有效的开放世界活动识别铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2504.03948</guid>
      <pubDate>Tue, 08 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Tgraphx：用于多维特征学习的张量感知图神经网络</title>
      <link>https://arxiv.org/abs/2504.03953</link>
      <description><![CDATA[ARXIV：2504.03953V1公告类型：新 
摘要：Tgraphx通过将卷积神经网络（CNN）与图形神经网络（GNN）统一，以增强视觉推理任务，从而提出了一种新颖的深度学习范式。传统的CNN擅长从图像中提取丰富的空间特征，但缺乏模拟对象间关系的固有能力。相反，传统的GNN通常依赖于扁平的节点特征，从而丢弃了重要的空间细节。 Tgraphx通过使用CNN来生成保留局部空间语义的多维节点特征（例如（3*128*128）张量）来克服这些局限性。这些具有空间意识到的节点参与图表，其中使用1*1个卷积执行消息传递，该响应在维护其结构的同时融合了相邻的功能。此外，具有残差连接的深CNN聚合器可用于稳健地完善融合消息，以确保稳定的梯度流量和端到端的训练性。我们的方法不仅弥合了空间特征提取和关系推理之间的差距，而且还显示出对象检测细化和集合推理的显着改善。]]></description>
      <guid>https://arxiv.org/abs/2504.03953</guid>
      <pubDate>Tue, 08 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>录像带：在视频文本模型中推进细粒度的构图和时间对齐方式</title>
      <link>https://arxiv.org/abs/2504.03970</link>
      <description><![CDATA[ARXIV：2504.03970V1公告类型：新 
摘要：我们介绍了VideoComp，这是一种用于推进视频文本组成性理解的基准和学习框架，旨在改善视力语言模型（VLMS），以细粒度的时间对齐方式进行。与关注静态图像文本构图或隔离单事件视频的现有基准分析不同，我们的基准测试目标在连续的多事件视频中对齐。利用具有时间局部的事件字幕（例如ActivityNet-Captions，YouCook2）的视频文本数据集，我们构建了两个构图基准，ActivityNet-Comp和YouCook2-comp。我们创建了具有微妙的时间中断的挑战性负面样本，例如重新排序，动作词更换，部分字幕和综合中断。这些基准测试了跨扩展的凝聚力视频序列的测试模型的组成灵敏度。为了提高模型性能，我们提出了分层的成对偏好损失，以时间精确的对加强对齐方式，并逐渐惩罚越来越多的破坏性，从而鼓励细粒度的构图学习。为了减轻密集注释的视频数据的有限可用性，我们介绍了一种预处理的策略，该策略将简短的视频捕获对加入以模拟多项式序列。我们在基准上评估视频文本基础模型和大型多模式模型（LMM），从而确定了提高组成性的优势和领域。总体而言，我们的工作提供了一个综合框架，用于评估和增强模型功能，以实现精细的，暂时的视频文本对齐。]]></description>
      <guid>https://arxiv.org/abs/2504.03970</guid>
      <pubDate>Tue, 08 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>