<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Wed, 07 Aug 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>DaCapo：用于可扩展 3D 图像分割的模块化深度学习框架</title>
      <link>https://arxiv.org/abs/2408.02834</link>
      <description><![CDATA[arXiv:2408.02834v1 公告类型：新
摘要：DaCapo 是一个专门的深度学习库，旨在加快现有机器学习方法在大型、近各向同性图像数据上的训练和应用。在本文中，我们介绍了 DaCapo 针对这一特定领域优化的独特功能，重点介绍了其模块化结构、高效的实验管理工具和可扩展的部署能力。我们讨论了其改善大规模、各向同性图像分割访问的潜力，并邀请社区探索并为这一开源计划做出贡献。]]></description>
      <guid>https://arxiv.org/abs/2408.02834</guid>
      <pubDate>Thu, 08 Aug 2024 03:13:55 GMT</pubDate>
    </item>
    <item>
      <title>GAReT：使用适配器和自回归变换器的跨视图视频地理定位</title>
      <link>https://arxiv.org/abs/2408.02840</link>
      <description><![CDATA[arXiv:2408.02840v1 公告类型：新
摘要：跨视图视频地理定位 (CVGL) 旨在通过将街景视频与鸟瞰图像对齐来从街景视频中获取 GPS 轨迹。尽管当前的 CVGL 方法性能良好，但它们仍面临重大挑战。这些方法使用相机和里程表数据，而这些数据在实际场景中通常不存在。它们利用多个相邻帧和各种编码器进行特征提取，导致计算成本高昂。此外，这些方法独立预测每个街景帧的位置，导致 GPS 轨迹在时间上不一致。为了应对这些挑战，在这项工作中，我们提出了 GAReT，这是一种完全基于变压器的 CVGL 方法，不需要相机和里程表数据。我们引入了 GeoAdapter，这是一个变压器适配器模块，旨在有效地聚合图像级表示并使其适应视频输入。具体来说，我们在视频帧和航拍图像上训练变压器编码器，然后冻结编码器以优化 GeoAdapter 模块以获得视频级表示。为了解决时间上不一致的轨迹问题，我们引入了 TransRetriever，这是一种编码器-解码器转换器模型，它通过对每帧的前 k 个最近邻预测进行编码并根据前一帧的预测自动回归解码最佳邻居来预测街景帧的 GPS 位置。我们的方法的有效性通过大量实验得到验证，在基准数据集上展示了最先进的性能。我们的代码可在 https://github.com/manupillai308/GAReT 上找到。]]></description>
      <guid>https://arxiv.org/abs/2408.02840</guid>
      <pubDate>Thu, 08 Aug 2024 03:13:55 GMT</pubDate>
    </item>
    <item>
      <title>通过皮肤图像预测病变高度可改善诊断</title>
      <link>https://arxiv.org/abs/2408.02792</link>
      <description><![CDATA[arXiv:2408.02792v1 公告类型：新
摘要：虽然基于深度学习的皮肤病变图像分析计算机辅助诊断正在接近皮肤科医生的表现水平，但有几项研究表明，结合形状先验、纹理、颜色恒常性和照明等其他特征可以进一步提高病变诊断性能。在这项工作中，我们研究了另一个临床上有用的特征，即皮肤病变高度，并研究了预测和利用皮肤病变高度标签的可行性。具体来说，我们使用深度学习模型从 2D 皮肤病变图像中预测图像级病变高度标签。我们在 derm7pt 数据集上测试了高度预测精度，并使用高度预测模型估计来自其他五个数据集的图像的高度标签：ISIC 2016、2017 和 2018 挑战数据集、MSK 和 DermoFit。我们通过将这些估计的高程标签用作诊断模型的辅助输入来评估跨域泛化能力，结果表明这些标签可以提高分类性能，皮肤镜和临床图像的 AUROC 改进率分别高达 6.29% 和 2.69%。代码可在 https://github.com/sfu-mial/LesionElevation 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2408.02792</guid>
      <pubDate>Thu, 08 Aug 2024 03:13:54 GMT</pubDate>
    </item>
    <item>
      <title>基于高斯混合的立体匹配证据学习</title>
      <link>https://arxiv.org/abs/2408.02796</link>
      <description><![CDATA[arXiv:2408.02796v1 公告类型：新
摘要：在本文中，我们介绍了一种基于高斯混合的新型证据学习解决方案，用于稳健的立体匹配。与以前依赖单个高斯分布的证据深度学习方法不同，我们的框架假定单个图像数据在立体匹配中遵循混合高斯分布。这一假设产生了更精确的像素级预测，并更准确地反映了真实世界的图像分布。通过进一步采用逆伽马分布作为每个混合成分的中间先验，我们的概率模型与单个高斯模型相比实现了更好的深度估计，并有效地捕捉了模型的不确定性，从而实现了强大的跨域生成能力。我们通过使用 Scene Flow 数据集训练模型并在 KITTI 2015 和 Middlebury 2014 上对其进行测试来评估我们的立体匹配方法。实验结果一致表明，我们的方法以可靠的方式比基线方法带来了改进。值得注意的是，我们的方法在域内验证数据和跨域数据集上都取得了新的最佳结果，证明了其在立体匹配任务中的有效性和稳健性。]]></description>
      <guid>https://arxiv.org/abs/2408.02796</guid>
      <pubDate>Thu, 08 Aug 2024 03:13:54 GMT</pubDate>
    </item>
    <item>
      <title>分割风格发现：应用于皮肤病变图像</title>
      <link>https://arxiv.org/abs/2408.02787</link>
      <description><![CDATA[arXiv:2408.02787v1 公告类型：新
摘要：医学图像分割中的多变性，源于注释者的偏好、专业知识和他们对工具的选择，已得到充分记录。虽然大多数多注释者分割方法都侧重于对注释者特定的偏好进行建模，但它们需要注释者-分割对应性。在这项工作中，我们介绍了分割风格发现的问题，并提出了 StyleSeg，这是一种分割方法，它从一组图像蒙版对中学习合理、多样且语义一致的分割风格，而无需任何注释者对应性知识。StyleSeg 在四个公开的皮肤病变分割 (SLS) 数据集上始终优于竞争方法。我们还整理了 ISIC-MultiAnnot，这是具有注释者对应性的最大多注释者 SLS 数据集，我们的结果显示，使用我们新提出的度量 AS2，预测的风格和注释者偏好之间存在很强的一致性。代码和数据集可在https://github.com/sfu-mial/StyleSeg获得。]]></description>
      <guid>https://arxiv.org/abs/2408.02787</guid>
      <pubDate>Thu, 08 Aug 2024 03:13:53 GMT</pubDate>
    </item>
    <item>
      <title>GazeXplain：学习预测视觉扫描路径的自然语言解释</title>
      <link>https://arxiv.org/abs/2408.02788</link>
      <description><![CDATA[arXiv:2408.02788v1 公告类型：新
摘要：在探索视觉场景时，人类的扫描路径由其潜在的注意力过程驱动。了解视觉扫描路径对于各种应用至关重要。传统的扫描路径模型预测凝视转移的位置和时间，但没有提供解释，这在理解注视背后的原理方面造成了差距。为了弥补这一差距，我们引入了 GazeXplain，这是一项关于视觉扫描路径预测和解释的新研究。这涉及在眼动追踪数据集中注释注视的自然语言解释，并提出一个具有注意力语言解码器的通用模型，该模型可以联合预测扫描路径并生成解释。它集成了一种独特的语义对齐机制，以增强注视和解释之间的一致性，以及一种跨数据集共同训练方法以实现泛化。这些新颖之处为可解释的人类视觉扫描路径预测提供了一种全面且适应性强的解决方案。在各种眼动追踪数据集上进行的大量实验证明了 GazeXplain 在扫描路径预测和解释方面的有效性，为人类的视觉注意力和认知过程提供了宝贵的见解。]]></description>
      <guid>https://arxiv.org/abs/2408.02788</guid>
      <pubDate>Thu, 08 Aug 2024 03:13:53 GMT</pubDate>
    </item>
    <item>
      <title>单点监督的精细化红外小目标检测方案</title>
      <link>https://arxiv.org/abs/2408.02773</link>
      <description><![CDATA[arXiv:2408.02773v1 Announce Type: new 
摘要：近来，单点监督的红外小目标检测受到广泛关注，但现有方法的检测精度难以满足实际需求。因此，我们提出了一种创新的单点监督的精细化红外小目标检测方案，该方案具有优异的分割精度和检测率。具体而言，我们引入了单点监督的标签进化（LESPS）框架，并基于该框架探索各种优秀红外小目标检测网络的性能。同时，为了提高综合性能，我们构建了完整的后处理策略。一方面，为了提高分割精度，我们采用测试时间增强（TTA）和条件随机场（CRF）相结合的方式进行后处理。另一方面，为了提高检测率，我们引入了一种可调灵敏度（AS）策略进行后处理，充分考虑多种检测结果的优势，合理地将一些置信度较低的区域以质心点的形式添加到细分割图像中。此外，为了进一步提升性能并探索该任务的特点，一方面，我们构造并发现多阶段loss有利于细粒度检测；另一方面，我们发现合理的测试样本滑动窗口裁剪策略对于实际多尺寸样本具有更好的性能。大量实验结果表明，所提方案达到了state-of-the-art（SOTA）性能。值得一提的是，所提方案在“ICPR 2024资源受限红外小目标检测挑战赛Track 1：弱监督红外小目标检测”中获得了第三名。]]></description>
      <guid>https://arxiv.org/abs/2408.02773</guid>
      <pubDate>Thu, 08 Aug 2024 03:13:52 GMT</pubDate>
    </item>
    <item>
      <title>LR-Net：一种用于红外小目标检测的轻量级鲁棒网络</title>
      <link>https://arxiv.org/abs/2408.02780</link>
      <description><![CDATA[arXiv:2408.02780v1 Announce Type: new 
摘要：受限于设备限制以及缺乏目标内在特征，现有的红外小目标检测方法难以满足实际的综合性能要求。因此，我们提出了一种创新的轻量级鲁棒网络（LR-Net），摒弃了复杂的结构，在检测精度和资源消耗之间实现了有效的平衡。具体来说，为了保证轻量级和鲁棒性，一方面，我们构建了一个轻量级的特征提取注意（LFEA）模块，可以充分提取目标特征并加强跨通道的信息交互。另一方面，我们构建了一个简单的细化特征传输（RFT）模块。与直接跨层连接相比，RFT模块可以在较少的资源消耗下提高网络的特征细化提取能力。同时，为了解决高级特征图中小目标丢失的问题，一方面，我们提出了一种低级特征分布（LFD）策略，利用低级特征来补充高级特征的信息。另一方面，我们引入了一种高效的简化双线性插值注意模块（SBAM），以促进低级特征对高级特征的引导约束以及两者的融合。此外，我们摒弃了传统的调整大小方法，采用了一种新的训练和推理裁剪策略，该策略对具有多尺度样本的数据集更具鲁棒性。大量实验结果表明，我们的LR-Net达到了最先进的（SOTA）性能。值得注意的是，在提出的LR-Net的基础上，我们在“ICPR 2024资源受限红外小目标检测挑战赛Track 2：轻量级红外小目标检测”中获得了第三名。]]></description>
      <guid>https://arxiv.org/abs/2408.02780</guid>
      <pubDate>Thu, 08 Aug 2024 03:13:52 GMT</pubDate>
    </item>
    <item>
      <title>ConDL：无检测器密集图像匹配</title>
      <link>https://arxiv.org/abs/2408.02766</link>
      <description><![CDATA[arXiv:2408.02766v1 公告类型：新
摘要：在这项工作中，我们引入了一个用于估计密集图像对应的深度学习框架。我们的全卷积模型为图像生成密集特征图，其中每个像素都与一个可以在多幅图像中匹配的描述符相关联。与以前的方法不同，我们的模型是在包含明显失真（例如透视变化、照明变化、阴影和镜面高光）的合成数据上进行训练的。利用对比学习，我们的特征图对这些失真实现了更大的不变性，从而实现了稳健匹配。值得注意的是，我们的方法消除了对关键点检测器的需求，使其有别于许多现有的图像匹配技术。]]></description>
      <guid>https://arxiv.org/abs/2408.02766</guid>
      <pubDate>Thu, 08 Aug 2024 03:13:51 GMT</pubDate>
    </item>
    <item>
      <title>从识别到预测：利用序列推理进行动作预测</title>
      <link>https://arxiv.org/abs/2408.02769</link>
      <description><![CDATA[arXiv:2408.02769v1 公告类型：新
摘要：动作预测任务是指根据观察到的视频预测将要发生的动作，这需要模型具有很强的总结当前并推理未来的能力。经验和常识表明，不同动作之间存在显著的相关性，这为动作预测任务提供了宝贵的先验知识。然而，以前的方法并没有有效地对这种潜在的统计关系进行建模。为了解决这个问题，我们提出了一种利用注意力机制的新型端到端视频建模架构，称为通过识别和推理进行预测（ARR）。ARR将动作预测任务分解为动作识别和序列推理任务，并通过下一个动作预测（NAP）有效地学习动作之间的统计关系。与现有的时间聚合策略相比，ARR能够从可观察的视频中提取更有效的特征，从而做出更合理的预测。此外，为了应对需要大量训练数据的关系建模挑战，我们提出了一种创新的解码器无监督预训练方法，利用视频固有的时间动态来增强网络的推理能力。在 Epic-kitchen-100、EGTEA Gaze+ 和 50salads 数据集上进行的大量实验证明了所提方法的有效性。代码可在 https://github.com/linuxsino/ARR 上找到。]]></description>
      <guid>https://arxiv.org/abs/2408.02769</guid>
      <pubDate>Thu, 08 Aug 2024 03:13:51 GMT</pubDate>
    </item>
    <item>
      <title>扩散模型作为数据挖掘工具</title>
      <link>https://arxiv.org/abs/2408.02752</link>
      <description><![CDATA[arXiv:2408.02752v1 公告类型：新
摘要：本文演示了如何使用经过图像合成训练的生成模型作为视觉数据挖掘的工具。我们的观点是，由于当代生成模型学习了其训练数据的准确表示，我们可以使用它们通过挖掘视觉模式来总结数据。具体来说，我们表明，在微调条件扩散模型以合成来自特定数据集的图像后，我们可以使用这些模型在该数据集上定义典型性度量。该度量评估不同数据标签（例如地理位置、时间戳、语义标签甚至疾病的存在）的典型视觉元素。这种通过综合进行分析的数据挖掘方法有两个主要优势。首先，它比传统的基于对应的方法扩展得更好，因为它不需要明确比较所有视觉元素对。其次，虽然之前大多数关于视觉数据挖掘的研究都集中在单个数据集上，但我们的方法适用于内容和规模各异的数据集，包括历史汽车数据集、历史人脸数据集、大型全球街景数据集以及更大的场景数据集。此外，我们的方法允许跨类别标签转换视觉元素并分析一致的变化。]]></description>
      <guid>https://arxiv.org/abs/2408.02752</guid>
      <pubDate>Thu, 08 Aug 2024 03:13:50 GMT</pubDate>
    </item>
    <item>
      <title>降维和最近邻算法用于改进医学图像分割中的分布不均检测</title>
      <link>https://arxiv.org/abs/2408.02761</link>
      <description><![CDATA[arXiv:2408.02761v1 公告类型：新
摘要：众所周知，临床部署的基于深度学习的分割模型在其训练分布之外的数据上会失败。虽然临床医生会审查分割结果，但这些模型在大多数情况下往往表现良好，这可能会加剧自动化偏差。因此，在推理时检测分布外的图像对于警告临床医生模型可能失败至关重要。这项工作将马哈拉诺比斯距离 (MD) 事后应用于四个 Swin UNETR 和 nnU-net 模型的瓶颈特征，这些模型在 T1 加权磁共振成像和计算机断层扫描中分割肝脏。通过使用主成分分析或均匀流形近似和投影来降低瓶颈特征的维度，可以以高性能和最小计算负荷检测出模型失败的图像。此外，这项工作探索了 MD 的非参数替代方案，即 k 次最近邻距离 (KNN)。当将 KNN 应用于原始和平均池化瓶颈特征时，与 MD 相比，KNN 显著提高了可扩展性和性能。]]></description>
      <guid>https://arxiv.org/abs/2408.02761</guid>
      <pubDate>Thu, 08 Aug 2024 03:13:50 GMT</pubDate>
    </item>
    <item>
      <title>MMIU：用于评估大型视觉语言模型的多模态多图像理解</title>
      <link>https://arxiv.org/abs/2408.02718</link>
      <description><![CDATA[arXiv:2408.02718v1 公告类型：新
摘要：处理多幅图像的能力对于大型视觉语言模型 (LVLM) 来说至关重要，因为它可以更彻底、更细致地理解场景。最近的多图像 LVLM 已经开始满足这一需求。然而，它们的评估并没有跟上它们发展的步伐。为了填补这一空白，我们引入了多模态多图像理解 (MMIU) 基准，这是一个全面的评估套件，旨在评估各种多图像任务中的 LVLM。MMIU 包含 7 种类型的多图像关系、52 个任务、77K 张图像和 11K 个精心策划的多项选择题，使其成为同类中最广泛的基准。我们对 24 种流行的 LVLM（包括开源和专有模型）的评估揭示了多图像理解方面的重大挑战，特别是在涉及空间理解的任务中。即使是最先进的模型，例如 GPT-4o，在 MMIU 上的准确率也只有 55.7%。通过多方面的分析实验，我们确定了关键的性能差距和局限性，为未来的模型和数据改进提供了宝贵的见解。我们的目标是让 MMIU 推动 LVLM 研究和开发的前沿，使我们朝着实现复杂的多模态多图像用户交互的方向前进。]]></description>
      <guid>https://arxiv.org/abs/2408.02718</guid>
      <pubDate>Thu, 08 Aug 2024 03:13:49 GMT</pubDate>
    </item>
    <item>
      <title>隐私安全虹膜呈现攻击检测</title>
      <link>https://arxiv.org/abs/2408.02750</link>
      <description><![CDATA[arXiv:2408.02750v1 公告类型：新
摘要：本文提出了一种隐私安全的虹膜呈现攻击检测 (PAD) 方法的框架，该方法仅使用合成生成的无身份泄漏的虹膜图像进行设计。经过训练后，该方法将使用最先进的虹膜 PAD 基准以经典方式进行评估。我们设计了两个生成模型来合成符合 ISO/IEC 19794-6 的虹膜图像。第一个模型合成看起来真实的样本。为了避免“身份泄漏”，意外匹配模型训练中使用的样本的生成样本被排除在外。第二个模型合成带有纹理隐形眼镜的虹膜图像，并由给定的隐形眼镜品牌进行调节，以便在形成训练集时更好地控制纹理隐形眼镜的外观。我们的实验表明，与使用从人类受试者收集的虹膜图像训练的解决方案相比，仅使用合成数据训练的模型实现的性能较低但仍然合理。这是首次尝试仅使用合成数据来训练功能齐全的虹膜 PAD 解决方案，尽管常规方法和所提出的方法之间存在性能差距，但这项研究表明，随着生成模型保真度的提高，创建这种隐私安全的虹膜 PAD 方法可能是可能的。本文随附了为这项工作训练的源代码和生成模型。]]></description>
      <guid>https://arxiv.org/abs/2408.02750</guid>
      <pubDate>Thu, 08 Aug 2024 03:13:49 GMT</pubDate>
    </item>
    <item>
      <title>根据视频对物体和事件进行组合物理推理</title>
      <link>https://arxiv.org/abs/2408.02687</link>
      <description><![CDATA[arXiv:2408.02687v1 公告类型：新
摘要：理解和推理自然界中物体的物理特性是人工智能的一个基本挑战。虽然一些属性，如颜色和形状，可以直接观察到，但其他属性，如质量和电荷，则隐藏在物体的视觉外观中。本文解决了从物体的运动和相互作用中推断这些隐藏的物理属性，并根据推断出的物理属性预测相应的动态的独特挑战。我们首先介绍组合物理推理 (ComPhy) 数据集。对于给定的一组对象，ComPhy 包含它们在不同初始条件下移动和相互作用的有限视频。该模型的评估基于其解开组合隐藏属性（如质量和电荷）的能力，并利用这些知识回答一系列问题。除了来自模拟器的合成视频外，我们还收集了一个真实世界的数据集，以进一步展示不同模型的测试物理推理能力。我们在 ComPhy 上评估了最先进的视频推理模型，并揭示了它们捕捉这些隐藏属性的能力有限，这导致性能较差。我们还提出了一种新颖的神经符号框架，即物理概念推理器 (PCR)，它可以从问答中学习和推理可见和隐藏的物理属性。经过训练后，PCR 展示了非凡的能力。它可以跨帧检测和关联对象，确定可见和隐藏的物理属性，做出未来和反事实预测，并利用这些提取的表示来回答具有挑战性的问题。]]></description>
      <guid>https://arxiv.org/abs/2408.02687</guid>
      <pubDate>Thu, 08 Aug 2024 03:13:48 GMT</pubDate>
    </item>
    </channel>
</rss>