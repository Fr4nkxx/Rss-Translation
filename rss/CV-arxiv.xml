<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Tue, 05 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>论安全可信工业应用的目标检测模型的黑盒可解释性</title>
      <link>https://arxiv.org/abs/2411.00818</link>
      <description><![CDATA[arXiv:2411.00818v1 公告类型：新
摘要：在人机交互领域，人工智能已成为加速数据建模任务的有力工具。物体检测方法取得了突出的成果，广泛应用于自动驾驶和视频监控等关键领域。然而，它们在高风险应用中的采用仍然有限，因为错误可能会导致严重后果。可解释人工智能 (XAI) 方法旨在解决这一问题，但许多现有技术都是针对模型的，专为分类任务而设计，这使得它们在物体检测方面效率较低，非专业人士难以解释。在这项工作中，我们专注于与模型无关的物体检测模型 XAI 方法，并提出了 D-MFPP，这是形态碎片扰动金字塔 (MFPP) 的扩展，它使用基于分割的掩模生成。此外，我们引入了 D-Deletion，这是一种结合忠实度和定位度的新型度量，专门用于满足物体检测器的独特需求。我们在现实世界的工业和机器人数据集上评估了这些方法，检查了掩码数量、模型大小和图像分辨率等参数对解释质量的影响。我们的实验使用单阶段物体检测模型，应用于两个安全至关重要的机器人环境：i) 共享的人机工作空间，其中安全至关重要，ii) 电池套件的组装区，由于高风险组件之间可能存在损坏，因此安全至关重要。我们的研究结果表明，当同一场景中出现多个同一类别的元素时，D-Deletion 可以有效地衡量解释的性能，而当使用较少的掩码时，D-MFPP 为 D-RISE 提供了一种有希望的替代方案。]]></description>
      <guid>https://arxiv.org/abs/2411.00818</guid>
      <pubDate>Tue, 05 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于脑电图的多模态表征学习用于情绪识别</title>
      <link>https://arxiv.org/abs/2411.00822</link>
      <description><![CDATA[arXiv:2411.00822v1 公告类型：新
摘要：多模态学习一直是一个热门的研究领域，然而，由于脑电图 (EEG) 数据固有的多变性和有限的可用性，整合这些数据带来了独特的挑战。在本文中，我们介绍了一种新颖的多模态框架，它不仅可以适应视频、图像和音频等传统模态，还可以结合 EEG 数据。我们的框架旨在灵活处理不同的输入大小，同时动态调整注意力以考虑跨模态的特征重要性。我们在最近推出的情绪识别数据集上评估了我们的方法，该数据集结合了三种模态的数据，使其成为多模态学习的理想试验台。实验结果为数据集提供了基准，并证明了所提框架的有效性。这项工作突出了将 EEG 集成到多模态系统中的潜力，为情绪识别及其他领域更强大、更全面的应用铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2411.00822</guid>
      <pubDate>Tue, 05 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>留下一些面部特征</title>
      <link>https://arxiv.org/abs/2411.00824</link>
      <description><![CDATA[arXiv:2411.00824v1 公告类型：新
摘要：面部表情对人类交流至关重要，可以洞察情绪状态。本研究使用 Fer2013 数据集上的面部扰动来研究特定面部特征如何影响情绪分类。正如预期的那样，在删除一些重要面部特征的数据上训练的模型与快乐和惊讶等情绪的基线相比，准确率下降了 85%。令人惊讶的是，对于厌恶情绪，应用掩码后分类器的准确率似乎略有提高。基于这一观察，我们应用了一种训练方案，在训练过程中掩盖面部特征，从而激发了我们提出的扰动方案。该方案包括三个阶段 - 基于注意力的分类、像素聚类和以特征为中心的训练，可提高分类准确率。获得的实验结果表明，在情绪识别任务中去除单个面部特征有一些好处。]]></description>
      <guid>https://arxiv.org/abs/2411.00824</guid>
      <pubDate>Tue, 05 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过 H\"older 散度进行多视图表示学习的不确定性量化</title>
      <link>https://arxiv.org/abs/2411.00826</link>
      <description><![CDATA[arXiv:2411.00826v1 公告类型：新
摘要：基于证据的深度学习代表了一种新兴的不确定性估计范式，它以可忽略不计的额外计算开销提供可靠的预测。现有方法通常采用 Kullback-Leibler 散度来估计网络预测的不确定性，而忽略了各种模态之间的领域差距。为了解决这个问题，本文提出了一种基于 H\&quot;older 散度 (HD) 的新算法，通过解决不完整或噪声数据固有的不确定性挑战来提高多视图学习的可靠性。一般来说，我们的方法通过并行网络分支提取多种模态的表示，然后使用 HD 来估计预测的不确定性。通过 Dempster-Shafer 理论，整合来自不同模态的不确定性，从而生成一个考虑所有可用表示的综合结果。从数学上讲，HD 被证明可以更好地测量真实数据分布和模型预测分布之间的“距离”，并提高多类识别任务的性能。
具体来说，我们的方法在所有评估基准上都超越了现有的最先进方法。
我们进一步在不同的主干上进行了广泛的实验，以验证我们卓越的鲁棒性。事实证明，我们的方法成功地突破了相应的性能界限。最后，我们在更具挑战性的场景上进行了实验，\textit{即}使用不完整或嘈杂的数据进行学习，结果表明我们的方法对此类损坏的数据表现出很高的容忍度。]]></description>
      <guid>https://arxiv.org/abs/2411.00826</guid>
      <pubDate>Tue, 05 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>IDEATOR：使用 VLM 越狱 VLM</title>
      <link>https://arxiv.org/abs/2411.00827</link>
      <description><![CDATA[arXiv:2411.00827v1 公告类型：新 
摘要：随着大型视觉语言模型 (VLM) 继续受到重视，确保它们在实际应用中的安全部署已成为一个关键问题。最近，大量研究工作集中在评估 VLM 抵御越狱攻击的鲁棒性上。由于获取多模态数据的挑战，当前的研究通常通过基于有害文本数据集生成对抗性或查询相关图像来评估 VLM 的鲁棒性。然而，以这种方式生成的越狱图像表现出一定的局限性。对抗性图像需要对目标 VLM 进行白盒访问并且相对容易防御，而查询相关图像必须与目标有害内容相关联，这限制了它们的多样性和有效性。在本文中，我们提出了一种名为 IDEATOR 的新型越狱方法，它可以自主生成用于黑盒越狱攻击的恶意图像-文本对。 IDEATOR 是一种基于 VLM 的方法，其灵感来自于我们的猜想：VLM 本身可能是一个用于生成越狱提示的强大红队模型。具体来说，IDEATOR 使用 VLM 生成越狱文本，同时利用最先进的扩散模型创建相应的越狱图像。大量实验证明了 IDEATOR 的高有效性和可移植性。它以 94% 的成功率成功越狱了 MiniGPT-4，并无缝转移到 LLaVA 和 InstructBLIP，分别实现了 82% 和 88% 的高成功率。IDEATOR 发现了 VLM 中以前未被发现的漏洞，需要先进的安全机制。]]></description>
      <guid>https://arxiv.org/abs/2411.00827</guid>
      <pubDate>Tue, 05 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大声梦想：一种利用发展合理数据训练视觉语言模型的自我合成方法</title>
      <link>https://arxiv.org/abs/2411.00828</link>
      <description><![CDATA[arXiv:2411.00828v1 公告类型：新
摘要：虽然当今的大型语言模型在生成类似人类的文本方面表现出令人印象深刻的能力，但它们在训练期间需要大量数据。我们在此从人类认知发展中汲取灵感，在有限的数据条件下训练模型。具体来说，我们提出了一种自我合成方法，该方法分为四个阶段：第 1 阶段建立基本的语言能力，在小型语料库上从头开始训练模型。然后，在第 2 阶段将语言与视觉环境相关联，将模型与视觉编码器集成以从标记图像生成描述性字幕。在“自我合成”第 3 阶段，模型为未标记的图像生成字幕，然后使用合成文本和先前的真实世界文本进一步训练其语言组件。这个阶段旨在扩展模型的语言库，类似于人类自我注释新体验。最后，第 4 阶段通过对模型进行特定任务（例如视觉问答和推理）的训练来发展高级认知技能。我们的方法为使用发展上合理的数据量训练多模态模型提供了概念证明。]]></description>
      <guid>https://arxiv.org/abs/2411.00828</guid>
      <pubDate>Tue, 05 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于显著性的多样性和公平性度量和 FaceKeepOriginalAugment：一种增强公平性和多样性的新方法</title>
      <link>https://arxiv.org/abs/2411.00831</link>
      <description><![CDATA[arXiv:2411.00831v1 公告类型：新
摘要：数据增强已成为增强计算机视觉任务性能的关键工具，其中 KeepOriginalAugment 方法因其将显着区域智能地合并到不太显着的区域中而成为一种出色的技术，从而实现了两个区域的增强。尽管它在图像分类方面取得了成功，但它在解决偏见方面的潜力仍未被探索。在本研究中，我们引入了 KeepOriginalAugment 方法的扩展，称为 FaceKeepOriginalAugment，它探索了计算机视觉模型中的各种去偏见方面 - 地理、性别和刻板偏见。通过在数据多样性和信息保存之间保持微妙的平衡，我们的方法使模型能够利用不同的显着和非显着区域，从而促进多样性和去偏见效果的增加。我们研究了多种策略来确定显着区域的位置并交换视角以决定哪部分进行增强。利用图像相似性得分 (ISS)，我们量化了一系列数据集的多样性，包括 Flickr Faces HQ (FFHQ)、WIKI、IMDB、Labelled Faces in the Wild (LFW)、UTK Faces 和 Diverse Dataset。我们利用卷积神经网络 (CNN) 和视觉转换器 (ViT) 中的图像-图像关联得分 (IIAS)，评估了 FaceKeepOriginalAugment 在缓解 CEO、工程师、护士和学校教师数据集中的性别偏见方面的有效性。我们的研究结果表明，FaceKeepOriginalAugment 在促进计算机视觉模型中的公平性和包容性方面非常有效，表现为性别偏见的减少和整体公平性的提高。此外，我们引入了一种新的指标——基于显着性的多样性和公平性指标，它可以量化多样性和公平性，同时处理不同数据集之间的数据不平衡。]]></description>
      <guid>https://arxiv.org/abs/2411.00831</guid>
      <pubDate>Tue, 05 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用迁移学习进行瑜伽姿势分类</title>
      <link>https://arxiv.org/abs/2411.00833</link>
      <description><![CDATA[arXiv:2411.00833v1 公告类型：新
摘要：瑜伽最近已成为人类维持健康身心的重要方面。随着生活变得越来越忙碌，人们发现很难抽出时间去健身房锻炼。这种人体姿势估计是一个值得注意的问题，因为它必须处理定位身体关键点或关节。Yoga-82 是一个包含 82 个类别的大规模瑜伽姿势识别基准数据集，其姿势具有挑战性，可能无法进行精确注释。我们使用了 VGG-16、ResNet-50、ResNet-101 和 DenseNet-121，并以不同的方式对它们进行了微调，以获得更好的结果。我们还使用神经架构搜索在这个预训练架构之上添加了更多层。实验结果显示，DenseNet-121 的性能最佳，top-1 准确率达到 85%，top-5 准确率达到 96%，超越了目前最先进的结果。]]></description>
      <guid>https://arxiv.org/abs/2411.00833</guid>
      <pubDate>Tue, 05 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DynaMath：用于评估视觉语言模型的数学推理鲁棒性的动态视觉基准</title>
      <link>https://arxiv.org/abs/2411.00836</link>
      <description><![CDATA[arXiv:2411.00836v1 公告类型：新
摘要：视觉语言模型 (VLM) 的快速发展已显示出在解决涉及视觉背景的数学推理任务方面的巨大潜力。与可以可靠地将解决步骤应用于类似问题并进行微小修改的人类不同，我们发现像 GPT-4o 这样的 SOTA VLM 在这些情况下可能会持续失败，从而揭示了其数学推理能力的局限性。在本文中，我们研究了 VLM 中的数学推理稳健性，并评估了这些模型在同一问题的不同变体（例如视觉数值或函数图的变化）下的表现。虽然已经开发了几种基于视觉的数学基准来评估 VLM 的解决问题的能力，但这些基准仅包含静态问题集，无法轻松评估数学推理稳健性。为了填补这一空白，我们引入了 DynaMath，这是一种动态视觉数学基准，旨在对 VLM 进行深入评估。 DynaMath 包含 501 个高质量、多主题的种子问题，每个问题都以 Python 程序的形式表示。这些程序经过精心设计和注释，可以自动生成更大的具体问题集，包括许多不同类型的视觉和文本变体。DynaMath 允许我们评估 VLM 在种子问题的不同输入条件下的性能，从而评估 VLM 的泛化能力。我们评估了 14 个 SOTA VLM，其中生成了 5,010 个具体问题。我们的结果表明，最坏情况模型准确率（定义为所有 10 个变体中正确回答的种子问题的百分比）明显低于平均情况准确率。我们的分析强调了研究 VLM 推理能力的稳健性的必要性，而 DynaMath 提供了宝贵的见解，以指导开发更可靠的数学推理模型。]]></description>
      <guid>https://arxiv.org/abs/2411.00836</guid>
      <pubDate>Tue, 05 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于纵向乳房 X 线检查的乳腺癌诊断模型：易受对抗性攻击</title>
      <link>https://arxiv.org/abs/2411.00837</link>
      <description><![CDATA[arXiv:2411.00837v1 公告类型：新
摘要：在乳腺癌检测和诊断中，乳房 X 光检查图像的纵向分析至关重要。当代模型擅长检测时间成像特征变化，从而增强了连续成像检查的学习过程。然而，这些纵向模型对对抗性攻击的恢复能力仍未得到充分探索。在本研究中，我们提出了一种新颖的攻击方法，该方法利用纵向模型的两个连续乳房 X 光检查之间的特征级关系，以交叉熵损失和距离度量学习为指导，实现显着的攻击效果，如使用黑盒攻击方式的攻击转移实现。我们在病例对照环境中对 590 名乳腺癌患者（每人接受两次连续的乳房 X 光检查）进行了实验。结果表明，我们提出的方法在欺骗诊断模型给出相反输出方面超越了几种最先进的对抗性攻击。即使采用对抗训练这种常见的防御方法对模型进行训练，我们的方法仍然有效。]]></description>
      <guid>https://arxiv.org/abs/2411.00837</guid>
      <pubDate>Tue, 05 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>面向任务的 IoVT 系统实时视觉推理：神经网络与边缘部署的协同设计框架</title>
      <link>https://arxiv.org/abs/2411.00838</link>
      <description><![CDATA[arXiv:2411.00838v1 公告类型：新
摘要：随着图像数据量的增长，视频物联网 (IoVT) 系统中面向数据的云计算遇到了延迟问题。面向任务的边缘计算通过将数据分析转移到边缘来解决此问题。然而，边缘设备有限的计算能力对执行视觉任务构成了挑战。现有方法难以平衡高模型性能和低资源消耗；轻量级神经网络通常表现不佳，而神经架构搜索 (NAS) 设计的设备特定模型无法适应异构设备。针对这些问题，我们提出了一种新颖的协同设计框架，以优化神经网络架构和高吞吐量推理过程中的部署策略。具体而言，它实现了基于重新参数化的动态模型结构，并结合了基于 Roofline 的模型分区策略，以增强边缘设备的计算性能。我们还采用多目标协同优化方法来平衡吞吐量和准确性。此外，我们推导出分区模型的数学一致性和收敛性。实验结果表明，与基线算法相比，吞吐量显著提高（MNIST 上为 12.05%，ImageNet 上为 18.83%），分类准确率也更高。我们的方法在不同设备上始终保持稳定的性能，凸显了其适应性。模拟实验进一步证实了其在 IoVT 系统中高精度、实时检测小物体的有效性。]]></description>
      <guid>https://arxiv.org/abs/2411.00838</guid>
      <pubDate>Tue, 05 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用基于分数的条件密度估计进行视频预测</title>
      <link>https://arxiv.org/abs/2411.00842</link>
      <description><![CDATA[arXiv:2411.00842v1 公告类型：新
摘要：时间预测本质上是不确定的，但表示自然图像序列中的模糊性是一个具有挑战性的高维概率推理问题。对于自然场景，维数灾难使得显式密度估计在统计和计算上都难以处理。在这里，我们描述了一个基于隐式回归的框架，用于在给定先前观察到的帧的情况下学习和采样视频中下一帧的条件密度。我们表明，在简单的抗噪目标函数上训练的序列到图像深度网络提取了用于时间预测的自适应表示。综合实验表明，这种基于分数的框架可以处理遮挡边界：与对分叉时间轨迹进行平均的经典方法不同，它在可能的轨迹中进行选择，以更高的频率选择更可能的选项。此外，对在自然图像序列上训练的网络的分析表明，该表示会自动根据其可靠性对预测证据进行加权，这是统计推断的标志]]></description>
      <guid>https://arxiv.org/abs/2411.00842</guid>
      <pubDate>Tue, 05 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>深度学习在 3D 点云增强中的应用：综述</title>
      <link>https://arxiv.org/abs/2411.00857</link>
      <description><![CDATA[arXiv:2411.00857v1 公告类型：新
摘要：点云数据现在是许多三维 (3D) 视觉研究领域中流行的数据表示。然而，由于传感器性能有限和传感噪声，原始数据通常存在稀疏性、噪声和不完整性。这对下游点云处理任务提出了巨大挑战。近年来，基于深度学习的点云增强方法正受到广泛的研究关注，该方法旨在利用深度神经网络从低质量的原始点云中获得密集、干净和完整的点云。据我们所知，本文首次对基于深度学习的点云增强方法进行了全面的概述。它涵盖了点云增强的三个主要视角，即 (1) 去噪以获得干净的数据；(2) 完成以恢复看不见的数据；(3) 上采样以获得密集的数据。我们的调查为最近的最先进方法和标准基准上的系统实验结果提供了一种新的分类法。此外，我们分享了我们对使用深度学习进行点云增强的深刻观察、想法和鼓舞人心的未来研究方向。]]></description>
      <guid>https://arxiv.org/abs/2411.00857</guid>
      <pubDate>Tue, 05 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>简单有效的篮球转播镜头时间接地管道</title>
      <link>https://arxiv.org/abs/2411.00862</link>
      <description><![CDATA[arXiv:2411.00862v1 公告类型：新
摘要：我们提出了一种可靠的时间基础管道，用于篮球广播镜头的视频到分析对齐。给定一系列帧作为输入，我们的方法可以快速准确地从篮球广播场景中提取剩余时间和四分之一值。我们的工作旨在加快大型多模态视频数据集的开发，以训练体育动作识别领域中数据密集型视频模型。我们的方法将包含密集事件注释的预标记逐场注释语料库与视频帧对齐，从而能够快速检索标记的视频片段。与以前的方法不同，我们通过微调开箱即用的对象检测器来直接查找语义文本区域，从而放弃了定位游戏时钟的需要。我们的端到端方法提高了我们工作的通用性。此外，插值和并行化技术为我们的管道在大型计算集群中的部署做好了准备。所有代码均公开可用。]]></description>
      <guid>https://arxiv.org/abs/2411.00862</guid>
      <pubDate>Tue, 05 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Emory 膝关节 X 线摄影 (MRKR) 数据集</title>
      <link>https://arxiv.org/abs/2411.00866</link>
      <description><![CDATA[arXiv:2411.00866v1 公告类型：新
摘要：埃默里膝关节 X 光片 (MRKR) 数据集是一个庞大的、人口统计学多样化的集合，包含来自 83,011 名患者的 503,261 张膝关节 X 光片，其中 40% 是非裔美国人。该数据集提供 DICOM 格式的成像数据以及详细的临床信息，包括患者报告的疼痛评分、诊断代码和程序代码，这些信息在类似的数据集中并不常见。MRKR 数据集还具有成像元数据，例如图像侧面性、视图类型和硬件存在，从而增强了其在研究和模型开发中的价值。MRKR 通过提供更具代表性的样本来研究骨关节炎及其相关结果（特别是在少数民族中），从而解决了现有数据集中的重大空白，从而为临床医生和研究人员提供了宝贵的资源。]]></description>
      <guid>https://arxiv.org/abs/2411.00866</guid>
      <pubDate>Tue, 05 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>