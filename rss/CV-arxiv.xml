<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Mon, 30 Sep 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>SSP-RACL：使用自监督预训练和稳健自适应 Credal 损失对嘈杂眼底图像进行分类</title>
      <link>https://arxiv.org/abs/2409.18147</link>
      <description><![CDATA[arXiv:2409.18147v1 公告类型：新
摘要：眼底图像分类在计算机辅助诊断任务中至关重要，但标签噪声严重损害了深度神经网络的性能。为了应对这一挑战，我们提出了一个强大的框架，即具有稳健自适应 Credal 损失的自监督预训练 (SSP-RACL)，用于处理眼底图像数据集中的标签噪声。首先，我们使用蒙版自动编码器 (MAE) 进行预训练以提取不受标签噪声影响的特征。随后，RACL 采用超集学习框架，设置置信度阈值和自适应标签松弛参数来构建可能性分布并提供更可靠的真实估计，从而有效抑制记忆效应。此外，我们引入了基于临床知识的非对称噪声生成来模拟现实世界中的嘈杂眼底图像数据集。实验结果表明，我们提出的方法在处理标签噪声方面优于现有方法，表现出卓越的性能。]]></description>
      <guid>https://arxiv.org/abs/2409.18147</guid>
      <pubDate>Mon, 30 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于机器学习的水印安全性评估：复制和删除攻击</title>
      <link>https://arxiv.org/abs/2409.18211</link>
      <description><![CDATA[arXiv:2409.18211v1 公告类型：新 
摘要：从现实世界或人工智能生成的媒体中捕获的大量数字内容需要版权保护、可追溯性或数据来源验证的方法。数字水印是应对这些挑战的重要方法。它的演变跨越了三代：手工制作的方法、基于自动编码器的方法和基于基础模型的方法。%它的演变跨越了三代：手工制作的方法、基于自动编码器的方案和基于基础模型的方法。虽然这些系统的稳健性是有据可查的，但针对对抗性攻击的安全性仍未得到充分探索。本文评估了利用对抗性嵌入技术的基础模型潜在空间数字水印系统的安全性。一系列实验调查了复制和删除攻击下的安全维度，为这些系统的漏洞提供了实证见解。所有实验代码和结果均可在https://github.com/vkinakh/ssl-watermarking-attacks}{repository]]></description>
      <guid>https://arxiv.org/abs/2409.18211</guid>
      <pubDate>Mon, 30 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在训练和测试分布范围内对自监督模型中的空间增强进行分析</title>
      <link>https://arxiv.org/abs/2409.18228</link>
      <description><![CDATA[arXiv:2409.18228v1 公告类型：新
摘要：在本文中，我们对自监督表示学习方法（对比和非对比）中使用的典型空间增强技术进行了实证研究，即随机裁剪和剪切。我们的贡献是：（a）我们将随机裁剪分解为两个独立的增强，重叠和补丁，并详细分析重叠面积和补丁大小对下游任务准确性的影响。（b）我们深入了解了为什么剪切增强不能学习良好的表示，正如早期文献中所报道的那样。最后，基于这些分析，（c）我们提出了一种基于距离的不变性损失边际，用于学习以场景为中心的表示，用于以对象为中心的分布的下游任务，表明只要一个与以场景为中心的图像中两个空间视图之间的像素距离成比例的边际就可以改善学习到的表示。我们的研究进一步加深了对空间增强的理解，以及训练增强和测试分布之间的领域差距的影响。]]></description>
      <guid>https://arxiv.org/abs/2409.18228</guid>
      <pubDate>Mon, 30 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>视觉概念网络：一种基于图的深度神经网络异常数据检测方法</title>
      <link>https://arxiv.org/abs/2409.18235</link>
      <description><![CDATA[arXiv:2409.18235v1 公告类型：新
摘要：深度神经网络 (DNN) 虽然越来越多地应用于许多应用，但在对异常和分布外 (OOD) 数据的鲁棒性方面却存在问题。当前的 OOD 基准测试通常过于简单，侧重于单对象任务，而不能完全表示复杂的现实世界异常。本文介绍了一种新的、直接的方法，该方法采用图形结构和拓扑特征来有效检测远 OOD 和近 OOD 数据。我们将图像转换为相互关联的人类可理解特征或视觉概念的网络。通过对两个新任务进行大量测试，包括具有大量词汇和多样化任务的消融研究，我们证明了该方法的有效性。这种方法增强了 DNN 对 OOD 数据的弹性，并有望在各种应用中提高性能。]]></description>
      <guid>https://arxiv.org/abs/2409.18235</guid>
      <pubDate>Mon, 30 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>空间可见性和时间动态：自适应点云视频流中的视野预测革命</title>
      <link>https://arxiv.org/abs/2409.18236</link>
      <description><![CDATA[arXiv:2409.18236v1 公告类型：新
摘要：视场 (FoV) 自适应流式传输仅传输观看者视场内的可见点，从而显着降低了沉浸式点云视频 (PCV) 的带宽要求。传统方法通常侧重于基于轨迹的 6 自由度 (6DoF) 视场预测。然后使用预测的视场来计算点可见性。这种方法没有明确考虑视频内容对观看者注意力的影响，并且从视场到点可见性的转换通常容易出错且耗时。我们从细胞可见性的角度重新表述了 PCV 视场预测问题，从而允许根据预测的可见性分布在细胞级别对 3D 数据的传输进行精确决策。我们开发了一种新颖的空间可见性和对象感知图模型，该模型利用历史 3D 可见性数据并结合空间感知、相邻细胞相关性和遮挡信息来预测未来的细胞可见性。我们的模型显著改善了长期细胞可见性预测，与最先进的模型相比，预测 MSE 损失降低了高达 50%，同时对于超过 100 万个点的点云视频保持了实时性能（超过 30fps）。]]></description>
      <guid>https://arxiv.org/abs/2409.18236</guid>
      <pubDate>Mon, 30 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用扩散形状先验估计进行非模态实例分割</title>
      <link>https://arxiv.org/abs/2409.18256</link>
      <description><![CDATA[arXiv:2409.18256v1 公告类型：新
摘要：非模态实例分割 (AIS) 提出了一个有趣的挑战，包括对图像中可见和被遮挡部分的分割预测。以前的方法通常依赖从训练数据中收集的形状先验信息来增强非模态分割。然而，这些方法容易过度拟合并忽略对象类别细节。最近的进展凸显了在大量数据集上预先训练的条件扩散模型从潜在空间生成图像的潜力。从中汲取灵感，我们提出了带有扩散形状先验估计 (DiffSP) 模块的 AISDiff。AISDiff 从预测可见分割掩模和对象类别开始，同时通过预测遮挡掩模进行遮挡感知处理。随后，将这些元素输入到我们的 DiffSP 模块中以推断对象的形状先验。 DiffSP 利用在大量数据集上预训练的条件扩散模型来提取丰富的视觉特征，以进行形状先验估计。此外，我们引入了形状先验非模态预测器，它利用形状先验中基于注意的特征图来优化非模态分割。在各种 AIS 基准上进行的实验证明了我们的 AISDiff 的有效性。]]></description>
      <guid>https://arxiv.org/abs/2409.18256</guid>
      <pubDate>Mon, 30 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PCEvE：基于部分贡献评估的人体素描评估模型解释及其他</title>
      <link>https://arxiv.org/abs/2409.18260</link>
      <description><![CDATA[arXiv:2409.18260v1 公告类型：新 
摘要：对于自动人体绘图 (HFD) 评估任务，例如使用 HFD 图像诊断自闭症谱系障碍 (ASD)，模型决策的清晰度和可解释性至关重要。现有的基于像素级归因的可解释 AI (XAI) 方法要求用户付出大量努力来解释图像中某个区域的语义信息，这通常很耗时且不切实际。为了克服这一挑战，我们提出了一个基于部分贡献评估的模型解释 (PCEvE) 框架。在部分检测的基础上，我们测量每个单独部分的 Shapley 值来评估对模型决策的贡献。与现有的基于归因的 XAI 方法不同，PCEvE 提供了对模型决策的直接解释，即部分贡献直方图。此外，PCEvE 将解释范围从传统的样本级别扩展到类级别和任务级别的见解，从而提供对模型行为更丰富、更全面的理解。我们通过对多个 HFD 评估数据集进行大量实验严格验证了 PCEvE。此外，我们还通过一组受控实验对所提出的方法进行了健全性检查。此外，我们还通过将我们的方法应用于照片级逼真的数据集 Stanford Cars，证明了我们的方法在其他领域的多功能性和适用性。]]></description>
      <guid>https://arxiv.org/abs/2409.18260</guid>
      <pubDate>Mon, 30 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Omni6D：用于类别级 6D 对象姿势估计的大词汇量 3D 对象数据集</title>
      <link>https://arxiv.org/abs/2409.18261</link>
      <description><![CDATA[arXiv:2409.18261v1 公告类型：新
摘要：6D 物体姿态估计旨在确定物体的平移、旋转和缩放，通常来自单个 RGBD 图像。最近的进展将这种估计从实例级扩展到类别级，允许模型在同一类别中对未见过的实例进行概括。然而，这种概括受到现有数据集（如 NOCS）所涵盖的类别范围较窄的限制，这些数据集也往往忽略了遮挡等常见的现实挑战。为了应对这些挑战，我们推出了 Omni6D，这是一个全面的 RGBD 数据集，具有广泛的类别和不同的背景，将任务提升到更现实的环境。1) 该数据集包含 166 个类别、4688 个根据规范姿态调整的实例和超过 80 万次捕获，大大拓宽了评估范围。 2) 我们引入了对称感知度量，并在 Omni6D 上对现有算法进行了系统基准测试，从而全面探索了新的挑战和见解。3) 此外，我们提出了一种有效的微调方法，可将以前数据集中的模型调整到我们广泛的词汇设置中。我们相信，这一举措将为工业和学术领域的新见解和实质性进展铺平道路，推动通用 6D 姿势估计的界限向前发展。]]></description>
      <guid>https://arxiv.org/abs/2409.18261</guid>
      <pubDate>Mon, 30 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用多模态大型语言模型 (MLLM) 推进交通运输中的物体检测：全面回顾与实证检验</title>
      <link>https://arxiv.org/abs/2409.18286</link>
      <description><![CDATA[arXiv:2409.18286v1 公告类型：新
摘要：本研究旨在全面回顾和实证评估多模态大型语言模型 (MLLM) 和大型视觉模型 (VLM) 在交通系统物体检测中的应用。首先，我们介绍了 MLLM 在交通应用中的潜在优势，并对先前研究中的当前 MLLM 技术进行了全面回顾。我们重点介绍了它们在各种交通场景中物体检测的有效性和局限性。第二部分涉及概述交通应用中端到端物体检测的分类和未来方向。在此基础上，我们提出了对三个现实世界交通问题测试 MLLM 的实证分析，包括物体检测任务，即道路安全属性提取、安全关键事件检测和热图像的视觉推理。我们的研究结果对 MLLM 性能进行了详细评估，揭示了优势和需要改进的地方。最后，我们讨论了 MLLM 在增强交通运输物体检测方面的实际局限性和挑战，从而为这一关键领域的未来研究和发展提供了路线图。]]></description>
      <guid>https://arxiv.org/abs/2409.18286</guid>
      <pubDate>Mon, 30 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于食品晶体质量控制的有效显微图像实例分割</title>
      <link>https://arxiv.org/abs/2409.18291</link>
      <description><![CDATA[arXiv:2409.18291v1 公告类型：新
摘要：本文针对食品晶体制造质量控制领域，重点是有效预测食品晶体数量和尺寸分布。以前，制造商使用手动计数方法对食品液体产品的微观图像进行计数，这需要大量的人力，并且存在不一致的问题。由于晶体及其周围的硬模仿物形状多样，食品晶体分割是一个具有挑战性的问题。为了应对这一挑战，我们提出了一种基于物体检测的有效实例分割方法。实验结果表明，我们的方法预测的晶体计数精度与现有的分割方法相当，但速度提高了五倍。根据我们的实验，我们还定义了分离硬模仿物和食品晶体的客观标准，这可能有益于类似数据集上的手动注释任务。]]></description>
      <guid>https://arxiv.org/abs/2409.18291</guid>
      <pubDate>Mon, 30 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SOAR：通过高效的对象感知预训练实现自监督优化的无人机动作识别</title>
      <link>https://arxiv.org/abs/2409.18300</link>
      <description><![CDATA[arXiv:2409.18300v1 公告类型：新
摘要：我们介绍了 SOAR，这是一种用于无人机 (UAV) 拍摄的航拍镜头的新型自监督预训练算法。我们在整个预训练过程中融入了人类物体知识，以提高无人机视频预训练效率和下游动作识别性能。这与之前主要在微调阶段融入对象信息的工作形成了对比。具体来说，我们首先提出了一种新颖的对象感知掩蔽策略，旨在在整个预训练阶段保留与对象相关的某些补丁的可见性。其次，我们引入了一个对象感知损失函数，该函数利用对象信息来调整重建损失，防止偏向信息量较少的背景补丁。在实践中，采用原始 ViT 主干的 SOAR 表现优于最佳无人机动作识别模型，在 NEC-Drone 和 UAV-Human 数据集上，top-1 准确率分别提高了 9.7% 和 21.4%，同时每段视频的推理速度为 18.7 毫秒，速度提高了 2 到 5 倍。此外，SOAR 的准确率与之前的自监督学习 (SSL) 方法相当，同时所需的预训练时间减少了 87.5%，内存使用量减少了 25%]]></description>
      <guid>https://arxiv.org/abs/2409.18300</guid>
      <pubDate>Mon, 30 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用小波变换实现通用的 Deepfake 伪造检测</title>
      <link>https://arxiv.org/abs/2409.18301</link>
      <description><![CDATA[arXiv:2409.18301v1 公告类型：新
摘要：数字图像处理的发展，特别是深度生成模型的进步，对现有的深度伪造检测方法提出了重大挑战，尤其是在深度伪造的来源不明的情况下。为了应对这些伪造日益复杂的问题，我们提出了 \textbf{Wavelet-CLIP}，这是一个深度伪造检测框架，它将小波变换与从 ViT-L/14 架构派生的特征相结合，以 CLIP 方式进行预训练。Wavelet-CLIP 利用小波变换深入分析图像的空间和频率特征，从而增强模型检测复杂深度伪造的能力。为了验证我们方法的有效性，我们对现有的最先进方法进行了广泛的评估，以进行跨数据集泛化和检测由标准扩散模型生成的看不见的图像。我们的方法表现出色，跨数据泛化的平均 AUC 达到 0.749，对未见深度伪造的鲁棒性达到 0.893，优于所有比较方法。代码可以从 repo 中复制：\url{https://github.com/lalithbharadwajbaru/Wavelet-CLIP}]]></description>
      <guid>https://arxiv.org/abs/2409.18301</guid>
      <pubDate>Mon, 30 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>激光粉末床熔化熔化轨迹显微镜图像的自动分割与分析</title>
      <link>https://arxiv.org/abs/2409.18326</link>
      <description><![CDATA[arXiv:2409.18326v1 公告类型：新
摘要：随着金属增材制造 (AM) 的日益普及，研究人员和从业人员正在转向数据驱动的方法来优化打印条件。熔体轨迹的横截面图像为调整工艺参数、开发参数缩放数据和识别缺陷提供了有价值的信息。在这里，我们提出了一个图像分割神经网络，它可以自动识别和测量横截面图像中的熔体轨迹尺寸。我们使用 U-Net 架构对从不同实验室、机器和材料获得的 62 张预标记图像的数据集进行训练，并结合图像增强。当神经网络超参数（例如批次大小和学习率）得到适当调整时，学习到的模型显示分类准确率超过 99%，F1 分数超过 90%。当对不同用户捕获、在不同机器上打印并使用不同显微镜获取的图像进行测试时，神经网络表现出稳健性。后处理模块提取熔池的高度和宽度以及润湿角度。我们讨论了改进模型性能的机会和迁移学习的途径，例如扩展到其他 AM 流程（如定向能量沉积）。]]></description>
      <guid>https://arxiv.org/abs/2409.18326</guid>
      <pubDate>Mon, 30 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DeBaRA：基于去噪的 3D 房间布置生成</title>
      <link>https://arxiv.org/abs/2409.18336</link>
      <description><![CDATA[arXiv:2409.18336v1 公告类型：新
摘要：生成逼真且多样化的室内 3D 场景布局可解锁影响广泛行业的多种交互式应用。对象交互的固有复杂性、可用数据的有限量以及满足空间约束的要求都使 3D 场景合成和排列的生成建模具有挑战性。当前的方法通过自回归或使用现成的扩散目标来解决这些挑战，即同时预测所有属性而无需考虑 3D 推理。在本文中，我们介绍了 DeBaRA，这是一种基于分数的模型，专门用于在有界环境中精确、可控和灵活的排列生成。我们认为场景合成系统中最关键的组成部分是在受限区域内准确确定各种物体的大小和位置。基于这一见解，我们提出了一种以 3D 空间感知为核心的轻量级条件分数模型。我们证明，通过关注对象的空间属性，单个经过训练的 DeBaRA 模型可以在测试时用于执行多个下游应用，例如场景合成、完成和重新排列。此外，我们引入了一种新颖的自我评分评估程序，以便它可以与外部 LLM 模型一起最佳地使用。我们通过大量实验评估我们的方法，并在一系列场景中展示了比最先进方法显著的改进。]]></description>
      <guid>https://arxiv.org/abs/2409.18336</guid>
      <pubDate>Mon, 30 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>端到端自动驾驶真的需要感知任务吗？</title>
      <link>https://arxiv.org/abs/2409.18341</link>
      <description><![CDATA[arXiv:2409.18341v1 公告类型：新
摘要：端到端自动驾驶 (E2EAD) 方法通常依赖于监督感知任务来提取明确的场景信息（例如，对象、地图）。这种依赖需要昂贵的注释，并限制实时应用程序中的部署和数据可扩展性。在本文中，我们介绍了 SSR，这是一种新颖的框架，它仅使用 16 个导航引导标记作为稀疏场景表示，有效地提取了 E2EAD 的关键场景信息。我们的方法消除了对监督子任务的需求，使计算资源能够集中在与导航意图直接相关的基本元素上。我们进一步介绍了一个时间增强模块，它采用鸟瞰图 (BEV) 世界模型，通过自我监督将预测的未来场景与实际的未来场景对齐。 SSR 在 nuScenes 数据集上实现了最先进的规划性能，与领先的 E2EAD 方法 UniAD 相比，L2 错误率降低了 27.2%，碰撞率降低了 51.6%。此外，SSR 的推理速度提高了 10.9 倍，训练时间提高了 13 倍。该框架代表了实时自动驾驶系统的重大飞跃，为未来的可扩展部署铺平了道路。代码将在 \url{https://github.com/PeidongLi/SSR} 发布。]]></description>
      <guid>https://arxiv.org/abs/2409.18341</guid>
      <pubDate>Mon, 30 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>