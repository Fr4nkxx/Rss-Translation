<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Fri, 09 Aug 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>基于几何关系的旋转中心识别用于旋转运动去模糊</title>
      <link>https://arxiv.org/abs/2408.04171</link>
      <description><![CDATA[arXiv:2408.04171v1 公告类型：新
摘要：非盲旋转运动去模糊（RMD）旨在从旋转运动模糊（RMB）图像中恢复潜在的清晰图像。旋转中心是非盲 RMD 方法中的关键输入参数。现有方法直接从 RMB 图像估计旋转中心。然而，它们总是遭受重大错误，并且 RMD 的性能受到限制。对于组装的成像系统，旋转中心的位置保持固定。利用这些先验知识，我们提出了一种基于几何的旋转中心识别方法并分析了其误差范围。此外，我们构建了一个 RMB 成像系统。实验表明，我们的方法沿单个轴（x 轴或 y 轴）实现小于 1 像素的误差。我们利用构建的成像系统捕捉真实的 RMB 图像，实验结果表明我们的方法可以帮助现有的 RMD 方法产生更好的 RMD 图像。]]></description>
      <guid>https://arxiv.org/abs/2408.04171</guid>
      <pubDate>Fri, 09 Aug 2024 06:18:11 GMT</pubDate>
    </item>
    <item>
      <title>多色：通过学习多种颜色空间对图像进行着色</title>
      <link>https://arxiv.org/abs/2408.04172</link>
      <description><![CDATA[arXiv:2408.04172v1 公告类型：新
摘要：深度网络在图像恢复任务（例如图像着色）中表现出色。然而，我们发现以前的方法在着色流程中依赖于具有特定映射函数（即颜色空间）的单一颜色模型的数字表示。在本文中，我们首先研究不同颜色空间的建模，发现每个颜色空间都表现出独特的特征和独特的颜色分布。多个颜色空间之间的互补性为图像着色任务带来了好处。
我们提出了一种新的基于学习的 MultiColor 方法，用于自动为灰度图像着色，该方法结合了来自多个颜色空间的线索。具体来说，我们为单个颜色空间采用了一组专用的着色模块。在每个模块中，首先使用变压器解码器来细化颜色查询嵌入，然后颜色映射器使用嵌入和语义特征生成颜色通道预测。利用这些代表各种颜色空间的预测颜色通道，设计了一个互补网络来利用互补性并生成令人愉悦且合理的彩色图像。我们对真实世界的数据集进行了大量的实验，结果证明了其优于最先进技术的性能。]]></description>
      <guid>https://arxiv.org/abs/2408.04172</guid>
      <pubDate>Fri, 09 Aug 2024 06:18:11 GMT</pubDate>
    </item>
    <item>
      <title>通过适配器去关联结构使集成学习适用于半监督学习</title>
      <link>https://arxiv.org/abs/2408.04150</link>
      <description><![CDATA[arXiv:2408.04150v1 公告类型：新
摘要：在计算机视觉中，传统的集成学习方法要么训练效率低，要么在增强深度神经网络可靠性方面性能有限。在本文中，我们提出了一种轻量级、无损失函数、与架构无关的集成学习方法，通过适配器去相关结构 (DSA) 来实现各种视觉任务。具体来说，所提出的 DSA 利用结构多样的适配器来去相关多个预测头，而无需任何尾部正则化或损失。这使得 DSA 可以轻松扩展到与架构无关的网络，以完成一系列计算机视觉任务。重要的是，理论分析表明，所提出的 DSA 比基于单头的方法（大多数最新方法都采用该方法）具有更低的偏差和方差。因此，DSA 使深度网络对于各种现实世界的挑战（例如数据损坏和标签噪声）变得可靠和稳健。大量实验表明，所提方法与 FreeMatch 结合在包含 40 个标记数据的 CIFAR-10 数据集上准确率提升了 5.35%，在包含 400 个标记数据的 CIFAR-100 数据集上准确率提升了 0.71%。此外，所提方法与 DualPose 结合在包含 100 个数据（30 个标记数据）的 Sniffing 数据集上准确率提升了 2.08%，在包含 100 个数据（包括 50 个标记数据）的 FLIC 数据集上准确率提升了 5.2%，在包含 200 个数据（100 个标记数据）的 LSP 数据集上准确率提升了 2.35%。]]></description>
      <guid>https://arxiv.org/abs/2408.04150</guid>
      <pubDate>Fri, 09 Aug 2024 06:18:10 GMT</pubDate>
    </item>
    <item>
      <title>M2EF-NNs：用于癌症生存预测的多模态多实例证据融合神经网络</title>
      <link>https://arxiv.org/abs/2408.04170</link>
      <description><![CDATA[arXiv:2408.04170v1 公告类型：新
摘要：准确的癌症生存预测对于协助临床医生制定治疗计划至关重要。多模态数据，包括组织病理学图像和基因组数据，提供了互补和全面的信息，可以大大提高这项任务的准确性。然而，尽管目前的方法取得了有希望的结果，但仍存在两个显着的局限性：它们没有有效地利用全局背景并且忽视了模态不确定性。在本研究中，我们提出了一种称为 M2EF-NNs 的神经网络模型，该模型利用多模态和多实例证据融合技术来准确预测癌症生存率。具体而言，为了捕获图像中的全局信息，我们使用预先训练的视觉变换器 (ViT) 模型来获得组织病理学图像的斑块特征嵌入。然后，我们引入一个多模态注意模块，该模块使用基因组嵌入作为查询并学习基因组和组织病理学图像之间的共同注意映射，以实现多模态信息的早期交互融合并更好地捕捉它们的相关性。随后，我们首次将Dempster-Shafer证据理论（DST）应用于癌症生存预测。我们使用处理后的多模态特征参数化类概率的分布，并引入主观逻辑来估计与不同模态相关的不确定性。结合Dempster-Shafer理论，我们可以在多模态融合后动态调整类概率的权重，实现可信的生存预测。最后，在TCGA数据集上的实验验证证实了我们提出的方法在癌症生存预测方面取得的显着改进，并提高了模型的可靠性。]]></description>
      <guid>https://arxiv.org/abs/2408.04170</guid>
      <pubDate>Fri, 09 Aug 2024 06:18:10 GMT</pubDate>
    </item>
    <item>
      <title>PaveCap：首个具有密集字幕和 PCI 估计功能的综合路面状况评估多模式框架</title>
      <link>https://arxiv.org/abs/2408.04110</link>
      <description><![CDATA[arXiv:2408.04110v1 公告类型：新 
摘要：本研究介绍了第一种用于路面状况评估的多模态方法，提供定量路面状况指数 (PCI) 预测和定性描述。我们介绍了 PaveCap，这是一种用于自动路面状况评估的新型框架。该框架由两个主要部分组成：单次 PCI 估计网络和密集字幕网络。PCI 估计网络使用 YOLOv8 进行对象检测，使用 Segment Anything 模型 (SAM) 进行零次分割，并使用四层卷积神经网络来预测 PCI。密集字幕网络使用 YOLOv8 主干、Transformer 编码器-解码器架构和卷积前馈模块来生成路面状况的详细描述。为了训练和评估这些网络，我们开发了一个带有边界框注释、文本注释和 PCI 值的路面数据集。我们的 PCI 估计网络的结果显示，预测 PCI 与实际 PCI 之间存在很强的正相关性 (0.70)，证明了其在自动进行状况评估方面的有效性。此外，密集字幕网络生成了准确的路面状况描述，BLEU (0.7445)、GLEU (0.5893) 和 METEOR (0.7252) 得分较高就是明证。此外，密集字幕模型可以很好地处理复杂场景，甚至可以纠正地面实况数据中的一些错误。这里开发的框架可以极大地改善基础设施管理和路面维护决策18。]]></description>
      <guid>https://arxiv.org/abs/2408.04110</guid>
      <pubDate>Fri, 09 Aug 2024 06:18:09 GMT</pubDate>
    </item>
    <item>
      <title>集成动态物候特征的遥感影像土地覆盖变化检测</title>
      <link>https://arxiv.org/abs/2408.04144</link>
      <description><![CDATA[arXiv:2408.04144v1 公告类型：新
摘要：遥感图像变化检测（CD）对于分析地表随时间的变化至关重要，其中的重大挑战是区分实际变化和复杂场景，同时滤除伪变化。这一挑战的主要因素是由于自然区域的物候特征导致的类内动态变化。为了克服这个问题，我们引入了 InPhea 模型，该模型将物候特征集成到遥感图像 CD 框架中。该模型具有一个带有差分注意模块的检测器，用于改进变化信息的特征表示，并结合高分辨率特征提取和空间金字塔块来提高性能。此外，还采用了具有四个约束模块的约束器和多阶段对比学习方法来帮助模型理解物候特征。在 HRSCD、SECD 和 PSCD-Wuhan 数据集上的实验表明，InPhea 优于其他模型，证实了其在解决物候伪变化方面的有效性及其整体模型优势。]]></description>
      <guid>https://arxiv.org/abs/2408.04144</guid>
      <pubDate>Fri, 09 Aug 2024 06:18:09 GMT</pubDate>
    </item>
    <item>
      <title>ComKD-CLIP：对比语言-图像预训练模型的综合知识提炼</title>
      <link>https://arxiv.org/abs/2408.04145</link>
      <description><![CDATA[arXiv:2408.04145v1 公告类型：新
摘要：对比语言-图像预训练（CLIP）擅长通过对比学习技术整合图像和文本之间的语义信息。它在各种多模态任务中取得了显著的表现。然而，在资源有限的环境中，大型 CLIP 模型的部署受到阻碍，而较小的模型往往达不到实际应用所需的性能基准。在本文中，我们提出了一种新方法，称为 ComKD-CLIP：对比语言-图像预训练模型的综合知识提炼，旨在将大型教师 CLIP 模型中的知识全面提炼到较小的学生模型中，确保在显著减少参数的情况下获得可比的性能。ComKD-CLIP 由两个关键机制组成：图像特征对齐（IFAlign）和教育注意（EduAttention）。IFAlign 使学生模型提取的图像特征与教师模型提取的图像特征紧密匹配，使学生能够学习教师提取图像特征的知识。 EduAttention 探索了教师模型提取的文本特征与学生模型提取的图像特征之间的交叉关系，使学生模型能够学习教师模型如何融合文本和图像特征。此外，ComKD-CLIP 可以利用教师模型的文本和图像特征融合结果来细化从 IFAlign 和 EduAttention 中提炼的知识，确保学生模型准确吸收教师模型的知识。在 11 个数据集上进行的大量实验证明了所提方法的优越性。]]></description>
      <guid>https://arxiv.org/abs/2408.04145</guid>
      <pubDate>Fri, 09 Aug 2024 06:18:09 GMT</pubDate>
    </item>
    <item>
      <title>ArtVLM：通过基于视觉的前缀语言建模进行属性识别</title>
      <link>https://arxiv.org/abs/2408.04102</link>
      <description><![CDATA[arXiv:2408.04102v1 公告类型：新
摘要：识别和分离物体的视觉属性是许多计算机视觉应用的基础。虽然像 CLIP 这样的大型视觉语言表示已经在很大程度上解决了零样本物体识别的任务，但零样本视觉属性识别仍然是一个挑战，因为 CLIP 的对比学习视觉语言表示无法有效捕获物体属性依赖关系。在本文中，我们针对这一弱点，提出了一种基于句子生成的属性识别检索公式，该公式的新颖之处在于 1) 将待测量和检索的物体属性关系显式建模为条件概率图，从而将识别问题转换为依赖敏感的语言建模问题，以及 2) 在此公式上应用大型预训练视觉语言模型 (VLM)，并自然地提炼其图像-物体-属性关系知识以用于属性识别。具体来说，对于图像上要识别的每个属性，我们测量生成一个简短句子的视觉条件概率，该句子编码了属性与图像上对象的关系。与对比检索不同，对比检索通过将句子的元素与图像全局对齐来测量可能性，而生成检索对句子中对象和属性的顺序和依赖性很敏感。我们通过实验证明，在两个视觉推理数据集上，即野外视觉属性 (VAW) 和我们新提出的视觉基因组属性排名 (VGARank)，生成检索始终优于对比检索。]]></description>
      <guid>https://arxiv.org/abs/2408.04102</guid>
      <pubDate>Fri, 09 Aug 2024 06:18:08 GMT</pubDate>
    </item>
    <item>
      <title>解读政治意象的视觉情感</title>
      <link>https://arxiv.org/abs/2408.04103</link>
      <description><![CDATA[arXiv:2408.04103v1 公告类型：新
摘要：当观众的观点存在系统性分歧时，我们如何定义视觉情绪？本研究通过将态度差异整合到视觉情绪分类中，引入了一种新颖的视觉情绪分析方法。认识到社会分歧（例如党派分歧）严重影响情绪标签，我们开发了一个反映这些分歧的数据集。然后，我们训练了一个深度学习多任务多类模型来预测来自不同意识形态观点的视觉情绪。应用于与移民相关的图像，我们的方法捕捉了民主党和共和党的观点。通过将不同的观点纳入标签和模型训练过程，我们的策略解决了标签模糊性的局限性，并提高了视觉情绪预测的准确性。总的来说，我们的研究主张在解码视觉情绪方面进行范式转变，以创建更准确地反映人类产生的情绪的分类器。]]></description>
      <guid>https://arxiv.org/abs/2408.04103</guid>
      <pubDate>Fri, 09 Aug 2024 06:18:08 GMT</pubDate>
    </item>
    <item>
      <title>AEye：图像数据集可视化工具</title>
      <link>https://arxiv.org/abs/2408.04072</link>
      <description><![CDATA[arXiv:2408.04072v1 公告类型：新
摘要：图像数据集是计算机视觉中机器学习模型的基础，除了架构考虑之外，还显著影响模型功能、性能和偏差。因此，了解这些数据集的组成和分布变得越来越重要。为了满足直观探索这些数据集的需求，我们提出了 AEye，这是一种可扩展且可扩展的可视化工具，专门针对图像数据集。AEye 利用对比训练的模型将图像嵌入到语义上有意义的高维表示中，促进数据聚类和组织。为了可视化高维表示，我们将它们投影到二维平面上并分层排列图像，以便用户可以无缝导航和交互探索它们。AEye 为文本和图像查询提供语义搜索功能，使用户能够搜索内容。我们为 AEye 开源代码库，并提供了一个简单的配置来添加数据集。]]></description>
      <guid>https://arxiv.org/abs/2408.04072</guid>
      <pubDate>Fri, 09 Aug 2024 06:18:07 GMT</pubDate>
    </item>
    <item>
      <title>PushPull-Net：抑制驱动的 ResNet，对图像损坏具有鲁棒性</title>
      <link>https://arxiv.org/abs/2408.04077</link>
      <description><![CDATA[arXiv:2408.04077v1 公告类型：新
摘要：我们在 ResNet 架构的第一层引入了一种称为 PushPull-Conv 的新型计算单元，其灵感来自在初级视觉皮层中观察到的反相抑制现象。该单元通过实现一对互补滤波器重新定义了传统的卷积层：可训练的推送内核及其对应物拉内核。推送内核（类似于传统卷积）学习对特定刺激做出反应，而拉内核对相同但对比度相反的刺激做出反应。这种配置增强了刺激选择性并有效抑制了缺乏首选刺激的区域的反应。这种效果归因于推送和拉内核，它们在这些区域产生相当幅度的响应，从而相互抵消。将 PushPull-Conv 纳入 ResNet 可显著提高其对图像损坏的鲁棒性。我们对基准腐败数据集的实验表明，PushPull-Conv 可以与其他数据增强技术相结合，以进一步提高模型的鲁棒性。我们在 ResNet50 上设定了一个新的鲁棒性基准，当将 PRIME 增强与 PushPull 抑制相结合时，在 ImageNet-C 上实现了 49.95$\%$ 的 $mCE$。]]></description>
      <guid>https://arxiv.org/abs/2408.04077</guid>
      <pubDate>Fri, 09 Aug 2024 06:18:07 GMT</pubDate>
    </item>
    <item>
      <title>基于基础模型特征聚合的组织病理学图像嵌入用于患者治疗反应预测</title>
      <link>https://arxiv.org/abs/2408.03954</link>
      <description><![CDATA[arXiv:2408.03954v1 公告类型：新
摘要：预测患者对癌症治疗的反应备受关注。尽管如此，从医学角度来看，这项任务仍然具有挑战性，因为患者机体与所考虑的治疗之间的相互作用非常复杂。最近在大规模未标记组织病理学数据集上使用自监督学习进行预训练的基础模型的研究为开发癌症诊断相关任务的新方法开辟了新的方向。在本文中，我们提出了一种从全幻灯片图像预测弥漫性大 B 细胞淋巴瘤患者治疗反应的新方法。我们的方法利用几个基础模型作为特征提取器来获得对应于组织小区域的图像的局部表示，然后，通过使用基于注意的多实例学习聚合这些局部表示来获得图像的全局表示。我们对 152 名患者的数据集进行的实验研究显示了我们的方法的良好结果，尤其是通过强调使用基础模型与传统 ImageNet 预训练相比的优势。此外，获得的结果清楚地证明了基础模型在表征组织病理学图像和为该任务生成更适合的语义表示方面的潜力。]]></description>
      <guid>https://arxiv.org/abs/2408.03954</guid>
      <pubDate>Fri, 09 Aug 2024 06:18:06 GMT</pubDate>
    </item>
    <item>
      <title>HiRISE：通过传感器内压缩和选择性 ROI 实现边缘 ML 的高分辨率图像缩放</title>
      <link>https://arxiv.org/abs/2408.03956</link>
      <description><![CDATA[arXiv:2408.03956v1 公告类型：新
摘要：随着由机器学习 (ML) 驱动的微型物联网设备的兴起，许多研究人员将重点转向压缩模型以适应微型边缘设备。最近的研究在压缩用于具有小内存（例如 512kB SRAM）的微控制器上的物体检测和图像分类的 ML 模型方面取得了显著成功。然而，仍有许多挑战阻碍了需要高分辨率图像的 ML 系统的部署。由于微型物联网设备内存容量的基本限制，如果没有外部硬件，存储大图像在物理上可能是不可能的。为此，我们提出了一种用于边缘 ML 的高分辨率图像缩放系统，称为 HiRISE，它配备了利用模拟传感器内图像缩放的选择性感兴趣区域 (ROI) 功能。我们的方法不仅显着降低了峰值内存需求，而且还实现了高达 17.7 倍的数据传输和能耗减少。]]></description>
      <guid>https://arxiv.org/abs/2408.03956</guid>
      <pubDate>Fri, 09 Aug 2024 06:18:06 GMT</pubDate>
    </item>
    <item>
      <title>3D场景中面向任务的顺序基础</title>
      <link>https://arxiv.org/abs/2408.04034</link>
      <description><![CDATA[arXiv:2408.04034v1 公告类型：新
摘要：将自然语言置于物理 3D 环境中对于具身人工智能的发展至关重要。当前的 3D 视觉基础数据集和模型主要侧重于从静态、以对象为中心的描述中识别和定位对象。这些方法不能充分解决实际应用所必需的面向任务的基础的动态和顺序性。在这项工作中，我们提出了一项新任务：3D 场景中的面向任务的顺序基础，其中代理必须遵循详细的分步说明，通过在室内场景中定位一系列目标对象来完成日常活动。为了促进这项任务，我们引入了 SG3D，这是一个大规模数据集，包含 4,895 个真实世界 3D 场景中的 22,346 个任务和 112,236 个步骤。该数据集由来自各种 3D 场景数据集的 RGB-D 扫描和自动任务生成流程组合而成，随后进行人工验证以确保质量。我们将三种最先进的 3D 视觉接地模型调整为顺序接地任务，并评估其在 SG3D 上的性能。我们的结果表明，虽然这些模型在传统基准上表现良好，但它们在面向任务的顺序接地方面面临重大挑战，这凸显了进一步研究该领域的必要性。]]></description>
      <guid>https://arxiv.org/abs/2408.04034</guid>
      <pubDate>Fri, 09 Aug 2024 06:18:06 GMT</pubDate>
    </item>
    <item>
      <title>分类法驱动的快速对抗训练</title>
      <link>https://arxiv.org/abs/2408.03944</link>
      <description><![CDATA[arXiv:2408.03944v1 公告类型：新
摘要：对抗训练（AT）是一种有效的防御基于梯度的攻击的方法，可以增强神经网络的鲁棒性。其中，单步AT由于其简单高效而成为热点话题，只需要一次梯度传播即可生成对抗样本。然而，导致训练崩溃的灾难性过拟合（CO）问题仍然不太清楚，单步和多步AT实现的鲁棒精度之间存在差距。在本文中，我们提出了一个令人惊讶的发现，即对抗样本的分类揭示了CO的真相。基于这一结论，我们提出了分类驱动的快速对抗训练（TDAT），它联合优化学习目标、损失函数和初始化方法，从而可以看作是单步AT的新范式。与其他快速 AT 方法相比，TDAT 可以提高神经网络的鲁棒性，减轻错误分类示例的影响，并在训练过程中防止 CO，同时几乎不需要额外的计算和内存资源。在针对扰动预算为 8/255 的投影梯度下降 PGD10 攻击时，我们的方法在 CIFAR-10、CIFAR-100、Tiny ImageNet 和 ImageNet-100 数据集上实现了 $1.59\%$、$1.62\%$、$0.71\%$ 和 $1.26\%$ 的稳健准确率提升。此外，我们提出的方法还实现了针对其他攻击的最先进的稳健准确率。代码可在 https://github.com/bookman233/TDAT 上找到。]]></description>
      <guid>https://arxiv.org/abs/2408.03944</guid>
      <pubDate>Fri, 09 Aug 2024 06:18:05 GMT</pubDate>
    </item>
    </channel>
</rss>