<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Wed, 11 Sep 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>具有潜在特征的 SpecGaussian：3D 高斯溅射的视图相关外观的高质量建模</title>
      <link>https://arxiv.org/abs/2409.05868</link>
      <description><![CDATA[arXiv:2409.05868v1 公告类型：新
摘要：最近，3D 高斯溅射 (3D-GS) 方法在新型视图合成中取得了巨大成功，在确保高质量渲染结果的同时提供实时渲染。然而，该方法在建模镜面反射和处理各向异性外观分量方面面临挑战，特别是在处理复杂光照条件下的视图相关颜色时。此外，3D-GS 使用球谐函数来学习颜色表示，这在表示复杂场景方面的能力有限。为了克服这些挑战，我们引入了 Lantent-SpecGS，这是一种在每个 3D 高斯中使用通用潜在神经描述符的方法。这使得 3D 特征场（包括外观和几何形状）的表示更加有效。此外，设计了两个并行的 CNN，分别将溅射特征图解码为漫反射颜色和镜面颜色。学习依赖于视点的掩码来合并这两种颜色，从而得到最终的渲染图像。实验结果表明，我们的方法在新颖的视图合成中获得了有竞争力的性能，并扩展了 3D-GS 处理具有镜面反射的复杂场景的能力。]]></description>
      <guid>https://arxiv.org/abs/2409.05868</guid>
      <pubDate>Wed, 11 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于息肉分割的 Transformer 增强迭代反馈机制</title>
      <link>https://arxiv.org/abs/2409.05875</link>
      <description><![CDATA[arXiv:2409.05875v1 公告类型：新
摘要：结直肠癌 (CRC) 是美国诊断出的第三大癌症原因，也是男女癌症相关死亡的第二大原因。值得注意的是，CRC 是 50 岁以下年轻男性癌症的主要原因。结肠镜检查被认为是早期诊断 CRC 的黄金标准。内镜医师的技能差异很大，并且报告的漏诊率很高。自动息肉分割可以降低漏诊率，并且可以在早期及时治疗。为了应对这一挑战，我们引入了 \textit{\textbf{\ac{FANetv2}}}，这是一种先进的编码器-解码器网络，旨在从结肠镜检查图像中准确分割息肉。利用 Otsu 阈值生成的初始输入掩码，FANetv2 通过一种新颖的反馈注意机制迭代地细化其二元分割掩码，该机制由先前时期的掩码预测提供信息。此外，它还采用了一种文本引导方法，整合了息肉数量（一个或多个）和大小（小、中、大）的基本信息，以进一步增强其特征表示能力。这种双任务方法有助于准确分割息肉，并有助于对息肉属性进行辅助分类，从而显著提高模型的性能。我们对公开的 BKAI-IGH 和 CVC-ClinicDB 数据集进行的全面评估证明了 FANetv2 的卓越性能，其骰子相似系数 (DSC) 分别为 0.9186 和 0.9481，而豪斯多夫距离分别为 2.83 和 3.19。FANetv2 的源代码可在 https://github.com/xxxxx/FANetv2 上找到。]]></description>
      <guid>https://arxiv.org/abs/2409.05875</guid>
      <pubDate>Wed, 11 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>进步与完善：无人机检测与分类技术的演变</title>
      <link>https://arxiv.org/abs/2409.05985</link>
      <description><![CDATA[arXiv:2409.05985v1 公告类型：新
摘要：本评论详细分析了 2020 年至今无人机 (UAV) 检测和分类系统的进步。它涵盖了雷达、射频、光学和声学传感器等各种检测方法，并强调了它们通过复杂的传感器融合技术的集成。彻底研究了推动无人机检测和分类的基本技术，重点关注其准确性和范围。此外，本文还讨论了人工智能和机器学习的最新创新，说明了它们对提高这些系统的准确性和效率的影响。评论最后预测了无人机检测的进一步技术发展，预计这些发展将提高性能和可靠性。]]></description>
      <guid>https://arxiv.org/abs/2409.05985</guid>
      <pubDate>Wed, 11 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过更强的指导增强语义分割的生成数据增强</title>
      <link>https://arxiv.org/abs/2409.06002</link>
      <description><![CDATA[arXiv:2409.06002v1 公告类型：新
摘要：数据增强是一种广泛使用的技术，用于为需要标记数据的任务（例如语义分割）创建训练数据。这种方法有利于需要大量精力和密集劳动的逐像素注释任务。传统的数据增强方法涉及简单的转换，例如旋转和翻转，以从现有图像创建新图像。但是，这些新图像可能缺乏数据中主要语义轴的多样性，并且不会改变高级语义属性。为了解决这个问题，生成模型已经成为通过生成合成图像来增强数据的有效解决方案。可控生成模型提供了一种使用原始图像中的提示和视觉参考来增强语义分割任务数据的方法。然而，直接使用这些模型会带来挑战，例如创建有效的提示和视觉参考来生成准确反映原始内容和结构的合成图像。在这项工作中，我们介绍了一种使用可控扩散模型进行语义分割的有效数据增强方法。我们提出的方法包括使用类提示附加和视觉先验组合来高效生成提示，以增强对真实图像中标记类的注意力。这些技术使我们能够生成准确描绘真实图像中分割类的图像。此外，我们采用类平衡算法来确保在合并合成图像和原始图像时高效生成训练数据集的平衡数据。我们在 PASCAL VOC 数据集上评估了我们的方法，发现它在语义分割中合成图像非常有效。]]></description>
      <guid>https://arxiv.org/abs/2409.06002</guid>
      <pubDate>Wed, 11 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>内窥镜视频中的在线三维重建和密集跟踪</title>
      <link>https://arxiv.org/abs/2409.06037</link>
      <description><![CDATA[arXiv:2409.06037v1 公告类型：新
摘要：从立体内窥镜视频数据进行 3D 场景重建对于推进外科手术至关重要。在这项工作中，我们提出了一个在线、密集 3D 场景重建和跟踪的在线框架，旨在增强对外科手术场景的理解和辅助干预。我们的方法使用高斯分布动态扩展规范场景表示，同时通过一组稀疏的控制点对组织变形进行建模。我们引入了一种高效的在线拟合算法，可以优化场景参数，实现一致的跟踪和准确的重建。通过在 StereoMIS 数据集上的实验，我们证明了我们的方法的有效性，其性能优于最先进的跟踪方法，并实现了与离线重建技术相当的性能。我们的工作支持各种下游应用，从而有助于提高外科手术辅助系统的能力。]]></description>
      <guid>https://arxiv.org/abs/2409.06037</guid>
      <pubDate>Wed, 11 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DiffusionPen：控制手写文本生成的风格</title>
      <link>https://arxiv.org/abs/2409.06065</link>
      <description><![CDATA[arXiv:2409.06065v1 公告类型：新
摘要：以文本和风格为条件的手写文本生成 (HTG) 是一项具有挑战性的任务，因为用户间特征的变化以及在训练期间形成新词的字符的无限组合。扩散模型最近在 HTG 中显示出有希望的结果，但仍未得到充分探索。我们提出了 DiffusionPen (DiffPen)，这是一种基于潜在扩散模型的 5 镜头式手写文本生成方法。通过利用结合度量学习和分类的混合风格提取器，我们的方法能够捕捉可见和不可见的单词和风格的文本和风格特征，从而生成逼真的手写样本。此外，我们探索了多种风格混合和噪声嵌入的数据变化策略，增强了生成数据的稳健性和多样性。使用 IAM 离线手写数据库进行的大量实验表明，我们的方法在质量和数量上都优于现有方法，并且其额外生成的数据可以提高手写文本识别 (HTR) 系统的性能。代码可在以下位置获取：https://github.com/koninik/DiffusionPen。]]></description>
      <guid>https://arxiv.org/abs/2409.06065</guid>
      <pubDate>Wed, 11 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SVS-GAN：利用 GAN 进行语义视频合成</title>
      <link>https://arxiv.org/abs/2409.06074</link>
      <description><![CDATA[arXiv:2409.06074v1 公告类型：新
摘要：近年来，人们对通过使用生成对抗网络 (GAN) 和扩散模型进行语义图像合成 (SIS) 的兴趣日益浓厚。该领域已经出现了创新，例如为这项任务量身定制的专门损失函数的实现，与图像到图像 (I2I) 翻译中更通用的方法不同。虽然语义视频合成 (SVS)$\unicode{x2013}$从语义图生成时间连贯、逼真的图像序列$\unicode{x2013}$的概念在本文中是新形式化的，但一些现有方法已经探索了该领域的某些方面。这些方法中的大多数依赖于为视频到视频转换设计的通用损失函数，或者需要额外的数据来实现时间连贯性。在本文中，我们介绍了 SVS-GAN，这是一个专门为 SVS 设计的框架，具有自定义架构和损失函数。我们的方法包括一个利用 SPADE 块的三金字塔生成器。此外，我们为图像鉴别器采用了基于 U-Net 的网络，该网络对 OASIS 损失执行语义分割。通过这种量身定制的架构和客观工程的结合，我们的框架旨在弥合 SIS 和 SVS 之间的现有差距，在 Cityscapes 和 KITTI-360 等数据集上的表现优于当前最先进的模型。]]></description>
      <guid>https://arxiv.org/abs/2409.06074</guid>
      <pubDate>Wed, 11 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LSE-NeRF：使用 RGB 事件立体技术学习去模糊神经辐射场的传感器建模误差</title>
      <link>https://arxiv.org/abs/2409.06104</link>
      <description><![CDATA[arXiv:2409.06104v1 公告类型：新
摘要：我们提出了一种即使在快速相机运动的情况下也能重建清晰的神经辐射场 (NeRF) 的方法。为了解决模糊伪影，我们利用双目配置中捕获的 (模糊) RGB 图像和事件相机数据。重要的是，在重建清晰的 NeRF 时，我们将简单针孔相机模型产生的相机建模缺陷视为每个相机测量的学习嵌入，并进一步学习将事件相机测量与 RGB 数据连接起来的映射器。由于我们的双目设置没有以前的数据集，我们引入了一个事件相机数据集，其中包含从 RGB 和事件相机之间的 3D 打印立体配置捕获的数据。从经验上讲，我们评估了我们引入的数据集和 EVIMOv2，并表明我们的方法可以改善重建。我们的代码和数据集可在 https://github.com/ubc-vision/LSENeRF 上找到。]]></description>
      <guid>https://arxiv.org/abs/2409.06104</guid>
      <pubDate>Wed, 11 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SGC-VQGAN：通过语义引导聚类码本实现复杂场景表示</title>
      <link>https://arxiv.org/abs/2409.06105</link>
      <description><![CDATA[arXiv:2409.06105v1 公告类型：新 
摘要：矢量量化 (VQ) 是一种通过离散码本表示确定性地学习特征的方法。最近的研究利用视觉标记器将视觉区域离散化，以进行自监督表示学习。然而，这些标记器的一个显着限制是缺乏语义，因为它们完全来自于在自动编码器范式中重建原始图像像素的借口任务。此外，由于码本利用率低下，码本分布不平衡和码本崩溃等问题会对性能产生不利影响。为了应对这些挑战，我们通过语义在线聚类方法引入 SGC-VQGAN，通过一致的语义学习增强标记语义。利用分割模型的推理结果，我们的方法构建了一个时空一致的语义码本，解决了码本崩溃和标记语义不平衡的问题。我们提出的金字塔特征学习流水线集成了多级特征，可同时捕获图像细节和语义。因此，SGC-VQGAN 在重建质量和各种下游任务方面都实现了 SOTA 性能。它的简单性，无需额外的参数学习，使其能够直接应用于下游任务，具有巨大的潜力。]]></description>
      <guid>https://arxiv.org/abs/2409.06105</guid>
      <pubDate>Wed, 11 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DECOLLAGE：通过可控制、局部化和学习几何增强实现 3D 细节化</title>
      <link>https://arxiv.org/abs/2409.06129</link>
      <description><![CDATA[arXiv:2409.06129v1 公告类型：新
摘要：我们提出了一种 3D 建模方法，使最终用户能够使用机器学习来细化或详细化 3D 形状，从而扩展了 AI 辅助 3D 内容创建的功能。给定一个粗体素形状（例如，使用简单的盒子挤压工具或通过生成建模生成的形状），用户可以直接从输入的样例形状在粗体形状的不同区域上“绘制”表示引人注目的几何细节的所需目标样式。然后将这些区域上采样为符合绘制样式的高分辨率几何图形。为了实现这种可控和局部化的 3D 细节化，我们在金字塔 GAN 之上构建，使其具有掩蔽感知能力。我们设计了新颖的结构损失和先验，以确保即使绘制的样式是从不同的来源借用的，例如不同的语义部分甚至不同的形状类别，我们的方法也能保留所需的粗结构和细粒度特征。通过大量实验，我们证明了我们定位细节的能力可以实现新颖的交互式创意工作流程和应用。我们的实验进一步表明，与基于全局细节的先前技术相比，我们的方法可以生成结构保留、高分辨率的风格化几何图形，具有更连贯的形状细节和风格过渡。]]></description>
      <guid>https://arxiv.org/abs/2409.06129</guid>
      <pubDate>Wed, 11 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>UniLearn：通过对图像和视频进行统一的预训练和微调来增强动态面部表情识别</title>
      <link>https://arxiv.org/abs/2409.06154</link>
      <description><![CDATA[arXiv:2409.06154v1 公告类型：新 
摘要：动态面部表情识别 (DFER) 对于理解人类的情绪和行为至关重要。然而，传统的 DFER 方法主要使用动态面部数据，通常未充分利用静态表情图像及其标签，从而限制了它们的性能和鲁棒性。为了解决这个问题，我们引入了 UniLearn，这是一种新颖的统一学习范式，它集成了静态面部表情识别 (SFER) 数据来增强 DFER 任务。UniLearn 采用双模态自监督预训练方法，利用面部表情图像和视频来增强 ViT 模型的时空表示能力。然后，使用联合微调策略在静态和动态表情数据集上对预训练模型进行微调。为了防止联合微调过程中的负迁移，我们引入了一个创新的混合适配器专家 (MoAE) 模块，该模块能够实现特定于任务的知识获取并有效地整合来自静态和动态表情数据的信息。大量实验表明，UniLearn 能够有效利用静态和动态面部数据的互补信息，从而实现更准确、更强大的 DFER。UniLearn 在 FERV39K、MAFW 和 DFEW 基准测试中始终保持最佳性能，加权平均召回率 (WAR) 分别为 53.65%、58.44% 和 76.68%。源代码和模型权重将在 \url{https://github.com/MSA-LMC/UniLearn} 上公开发布。]]></description>
      <guid>https://arxiv.org/abs/2409.06154</guid>
      <pubDate>Wed, 11 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>重新审视视觉语言模型的快速预训练</title>
      <link>https://arxiv.org/abs/2409.06166</link>
      <description><![CDATA[arXiv:2409.06166v1 公告类型：新 
摘要：提示学习是一种有效的方法，可用于为各种下游任务定制视觉语言模型 (VLM)，只需调整输入提示标记的极少量参数。最近，大规模数据集（例如 ImageNet-21K）中的提示预训练在通用视觉辨别的提示学习中发挥了至关重要的作用。然而，我们重新审视并观察到，由于提示预训练期间的图像量很大，有限的可学习提示可能会面临欠拟合风险，同时导致泛化能力差。为了解决上述问题，在本文中，我们提出了一个称为重新审视提示预训练 (RPP) 的通用框架，旨在从提示结构和提示监督两个方面提高拟合和泛化能力。对于提示结构，我们打破了常见实践中的限制，即查询、键和值向量来自共享的可学习提示标记。相反，我们引入了非共享的单个查询、键和值可学习提示，从而通过增加参数多样性来增强模型的拟合能力。对于提示监督，我们还利用了由预训练的对比语言图像预训练 (CLIP) 教师模型提供的零样本概率预测得出的软标签。这些软标签可以对类间关系提供更细致入微和更普遍的洞察，从而赋予预训练过程更好的泛化能力。RPP 产生更具弹性的提示初始化，增强了其在不同视觉识别任务中的稳健可转移性。跨各种基准的实验一致证实了我们预训练提示的最新 (SOTA) 性能。代码和模型将很快推出。]]></description>
      <guid>https://arxiv.org/abs/2409.06166</guid>
      <pubDate>Wed, 11 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过梯度匹配进行损失蒸馏，利用加权倒角距离完成点云</title>
      <link>https://arxiv.org/abs/2409.06171</link>
      <description><![CDATA[arXiv:2409.06171v1 公告类型：新
摘要：3D 点云增强了机器人感知环境几何信息的能力，使许多下游任务（如抓握姿势检测和场景理解）成为可能。然而，这些任务的性能在很大程度上依赖于数据输入的质量，因为不完整会导致结果不佳和失败的情况。最近为基于深度学习的点云完成设计的训练损失函数，如倒角距离 (CD) 及其变体（\eg HyperCD），意味着良好的梯度加权方案可以显著提高性能。然而，这些基于 CD 的损失函数通常需要与数据相关的参数调整，这对于数据密集型任务来说可能非常耗时。为了解决这个问题，我们的目标是找到一类不需要参数调整的加权训练损失（{\em 加权 CD}）。为此，我们提出了一种搜索方案，即通过梯度匹配进行损失蒸馏，通过模仿 HyperCD 和加权 CD 之间反向传播的学习行为来找到好的候选损失函数。完成此操作后，我们提出了一种新颖的双层优化公式来基于加权 CD 损失训练骨干网络。我们观察到：（1）使用适当的加权函数，加权 CD 始终可以实现与 HyperCD 相似的性能，并且（2）Landau 加权 CD，即 Landau CD，在点云补全方面可以胜过 HyperCD，并在多个基准数据集上产生新的最先进结果。{\it 我们的演示代码可在 \url{https://github.com/Zhang-VISLab/IROS2024-LossDistillationWeightedCD} 获得。}]]></description>
      <guid>https://arxiv.org/abs/2409.06171</guid>
      <pubDate>Wed, 11 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>EDADepth：用于单目深度估计的增强数据增强</title>
      <link>https://arxiv.org/abs/2409.06183</link>
      <description><![CDATA[arXiv:2409.06183v1 公告类型：新
摘要：由于其文本到图像的合成功能，扩散模型最近在视觉感知任务（例如深度估计）中得到了广泛的应用。缺乏高质量的数据集使得提取细粒度的语义上下文对于扩散模型来说具有挑战性。细节较少的语义上下文进一步恶化了创建有效文本嵌入的过程，这些文本嵌入将用作扩散模型的输入。在本文中，我们提出了一种新颖的 EDADepth，这是一种增强的数据增强方法，无需使用额外的训练数据即可估计单眼深度。我们使用超分辨率模型 Swin2SR 来提高输入图像的质量。我们采用 BEiT 预训练的语义分割模型来更好地提取文本嵌入。我们引入了 BLIP-2 标记器来从这些文本嵌入中生成标记。我们方法的创新之处在于在基于扩散的单目深度估计管道中引入了 Swin2SR、BEiT 模型和 BLIP-2 标记器。我们的模型在 NYUv2 和 KITTI 数据集的 {\delta}3 指标上取得了最佳结果 (SOTA)。它在 RMSE 和 REL 指标中也取得了与 SOTA 模型相当的结果。最后，与基于 SOTA 扩散的单目深度估计模型相比，我们还展示了估计深度可视化方面的改进。代码：https://github.com/edadepthmde/EDADepth_ICMLA。]]></description>
      <guid>https://arxiv.org/abs/2409.06183</guid>
      <pubDate>Wed, 11 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于瓶颈的编码器-解码器架构 (BEAR)，用于学习无偏消费者对消费者图像表征</title>
      <link>https://arxiv.org/abs/2409.06187</link>
      <description><![CDATA[arXiv:2409.06187v1 公告类型：新
摘要：在特定应用和背景下，无偏表示学习仍然是研究对象。通常使用基本部分的混合来设计新颖的架构来解决特定问题。本文介绍了不同的图像特征提取机制，这些机制与残差连接一起工作，以在自动编码器配置中对感知图像信息进行编码。我们使用的图像数据旨在支持更大的研究议程，该议程涉及消费者对消费者在线平台中的犯罪活动问题。初步结果表明，所提出的架构可以使用我们的和其他图像数据集来学习丰富的空间，从而解决已确定的重要挑战。]]></description>
      <guid>https://arxiv.org/abs/2409.06187</guid>
      <pubDate>Wed, 11 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>