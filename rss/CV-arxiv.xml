<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>CS.CV更新arxiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.cv更新arxiv.org e-print存档。</description>
    <lastBuildDate>Mon, 17 Mar 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>带有结构化关键字的视频异常检测</title>
      <link>https://arxiv.org/abs/2503.10653</link>
      <description><![CDATA[ARXIV：2503.10653V1公告类型：新 
摘要：本文通过利用基础模型的特征表示概括功能来检测监视视频中的异常。我们提出了一种使用关键字权重的新颖，轻巧的管道，以用于异常分类。我们的管道采用了两个阶段的过程：归纳，然后扣除。在归纳中，描述是从正常和异常框架生成的，以识别并将权重分配给相关的关键字。在推论中，推理框架描述使用诱导衍生的权重转换为关键字编码，以输入我们的神经网络进行异常分类。我们在三个基准UCSD PED2，上海理工学院和CUHK Avenue上取得了可比的性能，ROC AUC得分分别为0.865、0.745和0.742。这些结果是在没有时间上下文的情况下实现的，这使得如此适合实时应用程序的系统。我们的模型改善了边缘监视设备的实现设置，可解释性和推理速度，从而在其他视频异常检测系统上引入了性能权衡。随着开源基础模型的概括功能的改善，我们的模型表明，将文本用于特征表示是有效的实时可解释视频异常检测的有希望的方向。]]></description>
      <guid>https://arxiv.org/abs/2503.10653</guid>
      <pubDate>Mon, 17 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用Jensen-Shannon得分蒸馏的文本到3D</title>
      <link>https://arxiv.org/abs/2503.10660</link>
      <description><![CDATA[ARXIV：2503.10660V1公告类型：新 
摘要：得分蒸馏采样是一种有效的技术，可以利用预先训练的大规模文本对图像扩散模型作为指导，从文本提示中生成3D模型。但是，生产的3D资产往往过于饱和，过度光滑，多样性有限。这些问题是反向Kullback-Leibler（KL）差异目标的结果，这使得优化不稳定并导致寻求模式的行为。在本文中，我们基于Jensen-Shannon Divergence（JSD）得出一个有界的得分蒸馏目标，该目标稳定了优化过程并产生了高质量的3D代。 JSD可以匹配良好的生成和目标分布，从而减轻寻求模式。我们通过利用生成对抗网络的理论来定义发电机的近似目标函数，从而提供了JSD的实际实现，假设鉴别训练良好。通过假设遵循log-odds分类器的判别器，我们提出了少数族裔抽样算法来估计我们提出的目标的梯度，从而为JSD提供了实际实施。我们同时进行理论和经验研究来验证我们的方法。 T3Bench上的实验结果表明，我们的方法可以产生高质量和多样化的3D资产。]]></description>
      <guid>https://arxiv.org/abs/2503.10660</guid>
      <pubDate>Mon, 17 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CETAD：在视觉语言模型中迈向认证的毒性感知距离</title>
      <link>https://arxiv.org/abs/2503.10661</link>
      <description><![CDATA[ARXIV：2503.10661V1公告类型：新 
摘要：大型视觉模型（VLM）的最新进展已在广泛的视觉理解任务中取得了巨大的成功。但是，这些模型反对越狱攻击的鲁棒性仍然是一个公开挑战。在这项工作中，我们提出了一个通用认证的防御框架，以严格保护VLMS的潜在视觉越狱攻击。首先，我们提出了一个新颖的距离度量标准，以量化恶意和预期响应之间的语义差异，从而捕获经常被常规余弦相似性的度量忽略的细微差异。然后，我们设计了一种回归的认证方法，该方法采用随机平滑，即使在黑盒子设置下，也可以为对抗和结构扰动提供正式的鲁棒性保证。与此相辅相成，我们的功能空间防御将噪声分布（例如高斯，laplacian）引入了潜在的嵌入中，以维护免受像素级和结构级别的扰动。我们的结果突出了一种正式扎根，综合战略的潜力，以建立更具弹性和值得信赖的VLM。]]></description>
      <guid>https://arxiv.org/abs/2503.10661</guid>
      <pubDate>Mon, 17 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>小型视觉模型：关于紧凑型建筑和技术的调查</title>
      <link>https://arxiv.org/abs/2503.10665</link>
      <description><![CDATA[ARXIV：2503.10665V1公告类型：新 
摘要：小型视觉模型（SVLM）的出现标志着多模式AI的关键进步，从而有效地处理了资源约束环境中的视觉和文本数据。这项调查对SVLM开发进行了全面的探索，并介绍了基于变压器，基于MAMBA和混合动力的体系结构的分类法 - 重点介绍了紧凑的设计和计算效率的创新。讨论了诸如知识蒸馏，轻巧的注意机制和模态融合的技术，以减少资源需求的高性能推动者。通过对TinyGPT-V，Minigpt-4和VL-Mamba等模型的深入分析，我们确定了准确性，效率和可扩展性之间的权衡。持续的挑战，包括数据偏见和对复杂任务的概括，并通过提出的解决方案进行了严格的研究。通过巩固SVLM中的进步，这项工作强调了其可访问AI的变革潜力，为未来的有效多模式系统奠定了基础。]]></description>
      <guid>https://arxiv.org/abs/2503.10665</guid>
      <pubDate>Mon, 17 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>VRMDIFF：文本引导的视频参考散布的扩散产生</title>
      <link>https://arxiv.org/abs/2503.10678</link>
      <description><![CDATA[ARXIV：2503.10678V1公告类型：新 
摘要：我们提出了一项新任务，视频引用垫子，该任务通过输入引用标题来获得指定实例的alpha哑光。我们将密集的预测任务视为视频生成，并利用视频扩散模型的文本到视频对齐方式生成时间连贯且与相应的语义实例密切相关的alpha哑光。此外，我们提出了一种新的潜在构建损失，以进一步区分不同的实例，从而实现了更可控制的交互式效果。此外，我们介绍了带有10,000个视频的大型视频参考垫数据集。据我们所知，这是第一个同时包含标题，视频和实例级alpha哑光的数据集。广泛的实验证明了我们方法的有效性。该数据集和代码可在https://github.com/hansxssourse/vrmdiff上找到。]]></description>
      <guid>https://arxiv.org/abs/2503.10678</guid>
      <pubDate>Mon, 17 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从未分段的演示中发现的开放世界技能发现</title>
      <link>https://arxiv.org/abs/2503.10684</link>
      <description><![CDATA[ARXIV：2503.10684V1公告类型：新 
摘要：开放世界环境中的学习技能对于开发能够通过结合基本技能来处理各种任务的代理商至关重要。在线演示视频通常很长，但没有分割，因此很难用技能标识符进行细分和标记。与依赖序列采样或人类标签的现有方法不同，我们开发了一种基于学习的学习方法，以将这些长视频细分为一系列的语义感知和技能一致的段。从人类认知事件分割理论中汲取灵感，我们引入了技能边界检测（SBD），这是一种无注释的时间视频分割算法。 SBD通过利用预处理的无条件动作预测模型的预测错误来检测视频中的技能边界。这种方法基于这样的假设，即预测误差的显着增加表明所执行技能的转移。我们在Minecraft上评估了我们的方法，Minecraft是一个丰富的开放世界模拟器，并在线提供了广泛的游戏视频。在短期原子技能任务中，我们的SBD生成的细分市场将条件政策的平均绩效提高了63.7％和52.1％，在长期任务中，他们的相应分层剂及其相应的层次结构剂提高了11.3％和20.8％。我们的方法可以利用各种YouTube视频来训练跟随指导代理。项目页面可以在https://craftjarvis.github.io/skilldiscovery中找到。]]></description>
      <guid>https://arxiv.org/abs/2503.10684</guid>
      <pubDate>Mon, 17 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>VFM-UDA ++：改进无监督域自适应语义细分的网络体系结构和数据策略</title>
      <link>https://arxiv.org/abs/2503.10685</link>
      <description><![CDATA[ARXIV：2503.10685V1公告类型：新 
摘要：无监督的域适应性（UDA）显示出从标记的源域到未标记的目标域的强烈概括，同时需要相对较少的数据。同时，没有所谓视觉基础模型（VFM）标签的大规模预处理也显着改善了下游概括。这促使我们研究UDA如何最好地利用VFM的好处。 VFM-UDA的早期工作表明，可以通过在SOTA UDA方法中用VFM编码替换非VFM来获得超出最新的（SOTA）结果。在这项工作中，我们将其更进一步，并提高UDA架构和数据策略本身。我们观察到，当前的SOTA UDA方法VFM-UDA不使用多尺度的电感偏见或特征蒸馏损失，而已知这些可以改善概括。我们解决了VFM-UDA ++中的这两个局限性，并在高达+5.3 MIOU的标准UDA基准上获得了SOTA概括。受VFM微调的工作（例如Rein）的启发，我们还探索了使用易于访问的未标记目标数据添加更易于生成的合成源数据的好处，并实现了与当前SOTA相比+6.6 MIOU。对于较小的模型，VFM-UDA ++的改进最为重要，但是，我们表明，对于较大的模型，所获得的概括仅为2.8 miOU，而所有目标标签的全面监督学习的学习量仅为2.8 miOU。基于这些强大的结果，我们提供了基本见解，以帮助研究人员和从业者提高UDA。]]></description>
      <guid>https://arxiv.org/abs/2503.10685</guid>
      <pubDate>Mon, 17 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Maskattn-Unet：通用低分辨率图像分段的面具注意驱动框架</title>
      <link>https://arxiv.org/abs/2503.10686</link>
      <description><![CDATA[ARXIV：2503.10686V1公告类型：新 
摘要：低分辨率图像分割在现实世界应用中至关重要，例如机器人技术，增强现实和大规模的场景理解，由于计算约束，高分辨率数据通常无法使用。为了应对这一挑战，我们提出了Maskattn-Unet，这是一个新颖的分割框架，可通过掩盖注意机制增强传统的U-NET体系结构。我们的模型有选择地强调重要区域，同时抑制了无关紧要的背景，从而提高了混乱和复杂的场景的分割精度。与传统的U-NET变体不同，Maskattn-Unet有效地平衡了本地特征的提取与更广泛的上下文意识，这使其特别适合低分辨率输入。我们在三个基准数据集上评估了我们的方法，其中输入图像重新定制为128x128，并在语义，实例和泛型分割任务中展示了竞争性能。我们的结果表明，Maskattn-Unet具有与基于变压器的模型相比，其计算成本的准确性可与最先进的方法相媲美，这使其成为在资源约束场景中用于低分辨率分割的有效且可扩展的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2503.10686</guid>
      <pubDate>Mon, 17 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过扩散模型的上下文指导的负责任数据增强</title>
      <link>https://arxiv.org/abs/2503.10687</link>
      <description><![CDATA[ARXIV：2503.10687V1公告类型：新 
摘要：生成扩散模型在训练复杂的视觉模型时为数据增强提供了自然选择。但是，确保其生成含量作为增强样品的可靠性仍然是一个开放的挑战。尽管采用了许多利用生成图像来加强模型训练的技术，但尚不清楚如何利用自然图像和生成性图像作为丰富的监督信号的组合来有效地诱导模型。在这方面，我们提出了一种名为DiffCore-Mix的文本对图像（T2I）数据增强方法，该方法计算了一组具有明确约束的扩散模型的训练样本的生成性对应物，该模型利用了基于样本的上下文和负面的提示，并为可靠的增强样品产生了负面的提示。为了保留关键的语义轴，我们还会在增强过程中滤除不需要的生成样品。为此，我们在夹子的嵌入空间中提出了一种硬骨过滤。我们的方法系统地将天然和生成的图像混合在像素和斑块水平上。我们广泛评估了对Imagenet-1K，Tiny Imagenet-200，Cifar-100，Flowers102，Cub-Birds，Stanford Cars和Caltech数据集的技术，表明整个董事会的性能显着提高，可实现多达$ \％$ sim 3 \％$的绝对准确性，同时出现了与状态的准确性，同时又可以很好地衡量。我们的代码可在https://github.com/khawar-islam/diffcore-mix上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2503.10687</guid>
      <pubDate>Mon, 17 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>推理是您的视频概括所需的全部：一个反事实基准和子问题评估</title>
      <link>https://arxiv.org/abs/2503.10691</link>
      <description><![CDATA[ARXIV：2503.10691V1公告类型：新 
摘要：反事实推理对于强大的视频理解至关重要，但在现有的多模式基准中仍然没有被忽视。在本文中，我们介绍\ textbf {cover}（\ textbf {\ usewissline {co}} unterfactual \ textbf {\ textbf {\ undiseline {v}} id \ textbf {\ textbf {\ textbf {\ textbf {\ possionlline {e}}系统地评估在抽象混凝土和感知认知维度上的MLLM。除了先前的多模式基准外，覆盖物还将复杂的查询分解为结构化子问题，从而实现了细粒度的推理分析。商业和开源模型的实验揭示了子问题准确性与反事实推理性能之间的密切相关性，从而突出了结构化推断在视频理解中的作用。此外，我们的结果提出了一个关键的见解：增强模型的推理能力对于提高视频理解的鲁棒性至关重要。 Cover建立了一个新的标准，用于评估MLLM在动态环境中的逻辑推理能力。]]></description>
      <guid>https://arxiv.org/abs/2503.10691</guid>
      <pubDate>Mon, 17 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>探索在低海拔多视图观察条件下无人机视觉定位的最佳方法：一个基准测试</title>
      <link>https://arxiv.org/abs/2503.10692</link>
      <description><![CDATA[ARXIV：2503.10692V1公告类型：新 
摘要：绝对视觉定位（AVL）使无人机（UAV）通过在无人机图像和地理标记的参考图之间建立几何关系来确定其在GNSS贬低环境中的位置。尽管许多以前的作品都通过图像检索和匹配技术实现了AVL，但在低海拔多视图方案中的研究仍然有限。由于极端的观点变化，低空多视图条件提出了更大的挑战。为了在这种情况下探索最佳的无人机AVL方法，我们提出了这种基准。首先，构建了一个称为AnyVisloc的大型低空图数据集。该数据集包括在多个场景和高度捕获的18,000张图像，以及包含空中摄影测量图和历史卫星图的2.5D参考图。其次，提出了一个统一的框架，以整合最先进的AVL方法并全面测试其性能。选择最佳的组合方法作为基线，并根据其彻底分析影响定位精度的关键因素。在低海拔，多视图条件下，该基线在5M内达到了74.1％的定位精度。此外，引入了一个新颖的检索指标，称为PDM@K，以更好地与UAV AVL任务的特征保持一致。总体而言，该基准揭示了低空，多视图AVL的挑战，并为未来的研究提供了宝贵的指导。数据集和代码可从https://github.com/uav-avl/benchmark获得]]></description>
      <guid>https://arxiv.org/abs/2503.10692</guid>
      <pubDate>Mon, 17 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>半监督语义细分的知识咨询</title>
      <link>https://arxiv.org/abs/2503.10693</link>
      <description><![CDATA[ARXIV：2503.10693V1公告类型：新 
摘要：半监督语义分割通过使用未标记的数据和最先进的模型来改善整体性能，从而降低了对广泛注释的依赖。尽管深度共同训练方法取得了成功，但它们的潜在机制仍未得到充实。这项工作用双重异质骨架重新审视了伪造监督，并引入了知识咨询（SEGKC），以进一步提高细分性能。拟议的SEGKC在Pascal和CityScapes基准方面取得了重大改进，MIOU得分分别为87.1％，89.2％和89.8％的Pascal VOC，分别具有1/4、1/2，以及完整的分割分区，同时保持了紧凑的模型架构。]]></description>
      <guid>https://arxiv.org/abs/2503.10693</guid>
      <pubDate>Mon, 17 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>相邻的自回归建模，以进行有效的视觉生成</title>
      <link>https://arxiv.org/abs/2503.10696</link>
      <description><![CDATA[ARXIV：2503.10696V1公告类型：新 
摘要：视觉自动回调模型通常粘附在栅格上``下一步的预测&#39;&#39;范式，它忽略了视觉内容中固有的空间和时间局部性。具体而言，视觉令牌与它们在空间或时间上或时间上相邻的图表相比，视觉令牌与该论文相比，与之相邻的图表相比，具有更强的相关性。按照近来的``下一个邻居的预测&#39;&#39;机制，将自回归视觉生成作为渐进的支出程序进行了表达。从初始令牌开始，其余的令牌是从曼哈顿距离的上升顺序从空间空间中的初始令牌进行解码，从而逐渐扩展了解码区域的边界。为了使空间空间中的多个相邻令牌的平行预测，我们引入了一组面向维数的解码头，每个头部都可以预测沿相互正交尺寸的下一个令牌。在推断期间，与解码令牌相邻的所有代币都并行处理，从而大大降低了模型的前进步骤的生成步骤。 Imagenet上的实验$ 256 \ times 256 $和UCF101表明，NAR分别达到2.4 $ \ times $和8.6 $ \ times $更高的吞吐量，同时与PAR-4X方法相比，图像和视频生成任务都获得了出色的FID/FVD分数。在评估文本到图像生成基准的基准元音时，具有0.8B参数的NAR优于Chameleon-7b，而仅使用0.4的训练数据。代码可在https://github.com/thisisbillhe/nar上找到。]]></description>
      <guid>https://arxiv.org/abs/2503.10696</guid>
      <pubDate>Mon, 17 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用熵融合的零拍摄主题以创造性应用为中心</title>
      <link>https://arxiv.org/abs/2503.10697</link>
      <description><![CDATA[ARXIV：2503.10697V1公告类型：新 
摘要：生成模型广泛用于视觉内容创建中。但是，当前的文本到图像模型通常在实际应用中面临挑战，例如纺织模式设计和模因生成，并且存在不必要的元素，而这些元素很难用现有方法分开。同时，主题引用的产生已成为一种关键的研究趋势，突出了对可以产生清洁，高质量主题图像的技术的需求，同时有效地消除了多余的组件。为了应对这一挑战，我们引入了一个以可靠的以主题为中心的图像生成的框架。在这项工作中，我们提出了一种基于熵的特征加权融合方法，以合并从预算上的文本对图像模型通量的每个采样步骤中获得的信息性交叉注意特征，从而实现了精确的面具预测和以主题为中心的生成。此外，我们已经开发了一个基于大语言模型（LLM）的代理框架，该框架将用户的休闲输入转化为更具描述性的提示，从而导致图像生成高度详细。同时，代理提取提示的主要元素，以指导基于熵的特征融合，从而确保焦点的主要元素生成而无需外部组件。实验结果和用户研究表明，我们的方法会产生高质量的以主题为中心的图像，优于现有方法或其他可能的管道，从而突出了我们方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2503.10697</guid>
      <pubDate>Mon, 17 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过哈希记忆进行测试时间发现</title>
      <link>https://arxiv.org/abs/2503.10699</link>
      <description><![CDATA[ARXIV：2503.10699V1公告类型：新 
摘要：我们将测试时间发现（TTD）介绍为一个新的任务，该任务解决了测试过程中类变化的新任务，要求模型同时识别新兴类别，同时保留先前学到的类别。 TTD中的一个主要挑战是将新发现的类与已经确定的类别区分开来。为了解决这个问题，我们提出了一种基于培训的，基于哈希的记忆机制，该机制通过与过去的测试样本进行细粒度的比较来增强类发现。利用未知类别的特征，我们的方法基于特征量表和方向引入哈希表示，利用对局部敏感的哈希（LSH）进行有效分组相似的样品。与过去的实例相比，这使得测试样品可以轻松快速。此外，我们设计了一种协作分类策略，将已知类别的原型分类器与新颖的分类器相结合。为了提高可靠性，我们结合了一种自我纠正机制，该机制通过基于哈希的邻居检索来完善存储器标签，从而确保更稳定和准确的班级分配。实验结果表明，我们的方法在保持已知类别的性能的同时，可以很好地发现新的类别，并在模型测试中建立了新的范式。我们的代码可在https://github.com/fanlyu/ttd上找到。]]></description>
      <guid>https://arxiv.org/abs/2503.10699</guid>
      <pubDate>Mon, 17 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>