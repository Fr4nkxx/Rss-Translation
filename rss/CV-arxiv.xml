<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Wed, 31 Jul 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>通过多主体辩论解释和缓解 MLLM 中的幻觉</title>
      <link>https://arxiv.org/abs/2407.20505</link>
      <description><![CDATA[arXiv:2407.20505v1 Announce Type: new 
摘要：MLLM 通常会生成与视觉内容不一致的输出，这一挑战被称为幻觉。以前的方法侧重于确定生成的输出是否是幻觉，而没有确定哪个图像区域导致幻觉或解释为什么会出现这种幻觉。在本文中，我们认为 MLLM 中的幻觉部分是由于这些模型缺乏慢速思考和发散思维。为了解决这个问题，我们建议采用自我反思方案来促进慢速思考。此外，我们将消除幻觉视为一项复杂的推理任务，并提出一种多智能体辩论方法来鼓励发散思维。因此，我们的方法不仅可以减轻幻觉，还可以解释它们发生的原因并详细说明幻觉的具体情况。此外，我们建议在 MLLM 的背景下区分创造力和幻觉，并说明如何评估 MLLM 的创造力能力。在各种基准上进行的大量实验表明，我们的方法在多个 MLLM 中表现出了缓解广义幻觉的性能。]]></description>
      <guid>https://arxiv.org/abs/2407.20505</guid>
      <pubDate>Thu, 01 Aug 2024 03:14:29 GMT</pubDate>
    </item>
    <item>
      <title>不合作目标相对姿态估计的标记识别</title>
      <link>https://arxiv.org/abs/2407.20515</link>
      <description><![CDATA[arXiv:2407.20515v1 公告类型：新
摘要：本文介绍了一种使用追踪航天器图像处理和卷积神经网络 (CNN) 检测欧洲航天局 (ESA) 环境卫星 (ENVISAT) 上的结构标记以实现安全脱轨的新方法。采用先进的图像预处理技术（包括噪声添加和模糊）来提高标记检测的准确性和鲁棒性。初步结果显示，自主清除空间碎片具有良好的潜力，支持主动的太空可持续性战略。我们方法的有效性表明，通过在实际太空任务中实施更强大和自主的系统，我们的估计方法可以显着提高碎片清除操作的安全性和效率。]]></description>
      <guid>https://arxiv.org/abs/2407.20515</guid>
      <pubDate>Thu, 01 Aug 2024 03:14:29 GMT</pubDate>
    </item>
    <item>
      <title>使用 YOLO 和不确定性校正分割任意模型进行弱监督颅内出血分割</title>
      <link>https://arxiv.org/abs/2407.20461</link>
      <description><![CDATA[arXiv:2407.20461v2 公告类型：新
摘要：颅内出血 (ICH) 是一种危及生命的疾病，需要快速准确的诊断才能改善治疗效果和患者存活率。监督深度学习的最新进展极大地改善了医学图像的分析，但通常依赖于具有高质量注释的大量数据集，这些数据集成本高昂、耗时长，并且需要医学专业知识来准备。为了减轻对大量专家准备的分割数据的需求，我们开发了一种新颖的弱监督 ICH 分割方法，该方法利用 YOLO 对象检测模型和不确定性校正的 Segment Anything 模型 (SAM)。此外，我们为该模型提出了一种新颖的点提示生成器，以进一步通过 YOLO 预测的边界框提示改善分割结果。我们的方法在 ICH 检测中实现了 0.933 的高准确率和 0.796 的 AUC，以及 ICH 分割的平均 Dice 得分 0.629，优于现有的弱监督和流行的监督（UNet 和 Swin-UNETR）方法。总体而言，所提出的方法为 ICH 量化提供了一种可靠且准确的替代方案，可以替代更常用的监督技术，而无需在模型训练期间精确分割基本事实。]]></description>
      <guid>https://arxiv.org/abs/2407.20461</guid>
      <pubDate>Thu, 01 Aug 2024 03:14:28 GMT</pubDate>
    </item>
    <item>
      <title>恢复现实世界中退化的事件可提高去模糊质量</title>
      <link>https://arxiv.org/abs/2407.20502</link>
      <description><![CDATA[arXiv:2407.20502v1 公告类型：新
摘要：由于其速度快、延迟低，DVS 经常用于运动去模糊。理想情况下，高质量的事件会熟练地捕捉复杂的运动信息。然而，现实世界的事件通常会退化，从而在去模糊结果中引入显著的伪影。为了应对这一挑战，我们对事件的退化进行建模，并提出 RDNet 来提高图像去模糊的质量。具体来说，我们首先分析退化背后的机制，并在此基础上模拟配对事件。然后将这些配对事件输入到 RDNet 的第一阶段，以训练恢复模型。在此阶段恢复的事件可作为第二阶段去模糊过程的指导。为了更好地评估不同方法对现实世界退化事件的去模糊性能，我们提出了一个名为 DavisMCR 的新现实世界数据集。该数据集结合了通过操纵环境亮度和目标物体对比度收集的具有不同退化程度的事件。我们的实验是在合成数据集 (GOPRO)、真实世界数据集 (REBlur) 和建议的数据集 (DavisMCR) 上进行的。结果表明，RDNet 在事件恢复方面优于经典事件去噪方法。此外，与最先进的方法相比，RDNet 在去模糊任务中表现出更好的性能。DavisMCR 可在 https://github.com/Yeeesir/DVS_RDNet 上获得。]]></description>
      <guid>https://arxiv.org/abs/2407.20502</guid>
      <pubDate>Thu, 01 Aug 2024 03:14:28 GMT</pubDate>
    </item>
    <item>
      <title>BaseBoostDepth：利用更大的基线进行自监督单目深度估计</title>
      <link>https://arxiv.org/abs/2407.20437</link>
      <description><![CDATA[arXiv:2407.20437v1 公告类型：新
摘要：在多基线立体领域，传统的理解是，一般来说，增加基线分离会大大提高深度估计的准确性。然而，现行的自监督深度估计架构主要使用最小帧分离和受约束的立体基线。可以使用更大的帧分离；然而，我们表明，由于各种因素，包括亮度的显著变化和遮挡面积的增加，这会导致深度质量下降。为了应对这些挑战，我们提出的方法 BaseBoostDepth 结合了课程学习启发的优化策略，以有效利用更大的帧分离。然而，我们表明，仅靠我们的课程学习启发策略是不够的，因为更大的基线仍然会导致姿势估计漂移。因此，我们引入了增量姿势估计来提高姿势估计的准确性，从而显著改善了所有深度指标。此外，为了提高模型的稳健性，我们引入了误差诱导重建，通过增加姿势估计误差来优化重建。最终，我们的最终深度网络在基于图像、基于边缘和基于点云的指标上在 KITTI 和 SYNS-patches 数据集上实现了最先进的性能，而不会增加测试时的计算复杂度。项目网站可在 https://kieran514.github.io/BaseBoostDepth-Project 找到。]]></description>
      <guid>https://arxiv.org/abs/2407.20437</guid>
      <pubDate>Thu, 01 Aug 2024 03:14:27 GMT</pubDate>
    </item>
    <item>
      <title>MEVDT：基于多模式事件的车辆检测和跟踪数据集</title>
      <link>https://arxiv.org/abs/2407.20446</link>
      <description><![CDATA[arXiv:2407.20446v1 公告类型：新
摘要：在这篇数据文章中，我们介绍了基于多模式事件的车辆检测和跟踪 (MEVDT) 数据集。该数据集提供同步的事件数据流和交通场景的灰度图像，使用动态和有源像素视觉传感器 (DAVIS) 240c 混合事件相机捕获。MEVDT 包含 63 个多模式序列，大约有 13k 个图像、5M 个事件、10k 个对象标签和 85 个独特的对象跟踪轨迹。此外，MEVDT 还包括手动注释的地面真实标签 $\unicode{x2014}$，包括对象分类、像素精确的边界框和唯一对象 ID $\unicode{x2014}$，以 24 Hz 的标记频率提供。 MEVDT 旨在推动基于事件的视觉领域的研究，旨在满足对高质量、真实世界注释数据集的关键需求，从而开发和评估汽车环境中的对象检测和跟踪算法。]]></description>
      <guid>https://arxiv.org/abs/2407.20446</guid>
      <pubDate>Thu, 01 Aug 2024 03:14:27 GMT</pubDate>
    </item>
    <item>
      <title>从生成的图像对中学习保留特征的肖像编辑</title>
      <link>https://arxiv.org/abs/2407.20455</link>
      <description><![CDATA[arXiv:2407.20455v1 公告类型：新
摘要：由于难以保留身份等主题特征，肖像编辑对于现有技术而言具有挑战性。在本文中，我们提出了一种基于训练的方法，利用自动生成的配对数据来学习所需的编辑，同时确保保留不变的主题特征。具体而言，我们设计了一个数据生成过程，以低成本为所需编辑创建相当好的训练对。基于这些对，我们引入了一个多条件扩散模型来有效地学习编辑方向并保留主题特征。在推理过程中，我们的模型会生成准确的编辑蒙版，可以指导推理过程进一步保留详细的主题特征。服装编辑和卡通表情编辑的实验表明，我们的方法在数量和质量上都达到了最先进的质量。]]></description>
      <guid>https://arxiv.org/abs/2407.20455</guid>
      <pubDate>Thu, 01 Aug 2024 03:14:27 GMT</pubDate>
    </item>
    <item>
      <title>用于医学图像分割的密集自监督学习</title>
      <link>https://arxiv.org/abs/2407.20395</link>
      <description><![CDATA[arXiv:2407.20395v1 公告类型：新
摘要：深度学习彻底改变了医学图像分割，但它严重依赖于高质量的注释。每个新任务在像素级标记图像所需的时间、成本和专业知识减缓了该范式的广泛采用。我们提出了一种用于小样本分割的自监督学习 (SSL) 方法 Pix2Rep，它通过直接从未标记的图像中学习强大的像素级表示来减轻手动注释负担。Pix2Rep 是一种新颖的像素级损失和预训练范式，用于对整个图像进行对比 SSL。它适用于通用编码器-解码器深度学习主干（例如 U-Net）。虽然大多数 SSL 方法在强度和空间图像增强下强制学习的图像级表示不变，但 Pix2Rep 强制像素级表示的等方差。我们在心脏 MRI 分割任务上展示了该框架。结果表明，与现有的半监督和自监督方法相比，性能有所提高；与全监督 U-Net 基线相比，在同等性能​​下，注释负担减少了 5 倍。这包括线性探测（微调）下一次性分割的 30%（31%）DICE 改进。最后，我们还将新颖的 Pix2Rep 概念与 Barlow Twins 非对比 SSL 相结合，从而实现更好的分割性能。]]></description>
      <guid>https://arxiv.org/abs/2407.20395</guid>
      <pubDate>Thu, 01 Aug 2024 03:14:26 GMT</pubDate>
    </item>
    <item>
      <title>平均意见得分作为用户评估 XAI 方法的新指标</title>
      <link>https://arxiv.org/abs/2407.20427</link>
      <description><![CDATA[arXiv:2407.20427v1 公告类型：新 
摘要：本文研究了使用平均意见分数 (MOS)（一种常见的图像质量指标）作为 XAI 事后解释器的用户中心评估指标。为了测量 MOS，提出了一个用户实验，该实验使用故意扭曲的图像的解释图进行。特征归因方法系列中的三种方法 - 梯度加权类激活映射 (Grad-CAM)、多层特征解释方法 (MLFEM) 和特征解释方法 (FEM) - 与此指标进行了比较。此外，通过 Spearman 等级相关系数研究了这种新的以用户为中心的指标与自动指标的相关性。MLFEM 的 MOS 与插入曲线下面积 (IAUC) 和删除曲线下面积 (DAUC) 的自动指标显示出最高的相关性。然而，整体相关性有限，这凸显了自动指标和以用户为中心的指标之间缺乏共识。]]></description>
      <guid>https://arxiv.org/abs/2407.20427</guid>
      <pubDate>Thu, 01 Aug 2024 03:14:26 GMT</pubDate>
    </item>
    <item>
      <title>使用 COw LOcalization (COLO) 数据集定位室内奶牛的模型泛化研究</title>
      <link>https://arxiv.org/abs/2407.20372</link>
      <description><![CDATA[arXiv:2407.20372v1 公告类型：新
摘要：精准畜牧业 (PLF) 越来越依赖先进的物体定位技术来监测牲畜健康状况并优化资源管理。本研究调查了 YOLOv8 和 YOLOv9 模型在室内散栏牛棚环境中进行奶牛检测的泛化能力，重点关注不同的训练数据特征，例如视角和光照以及模型复杂性。利用新发布的公共数据集 COws LOcalization (COLO) 数据集，我们探索了三个关键假设：(1) 模型泛化同样受到光照条件和摄像机角度变化的影响；(2) 更高的模型复杂度保证了更好的泛化性能；(3) 使用在相关任务上训练的自定义初始权重进行微调总是会为检测任务带来优势。我们的研究结果揭示了在从侧视拍摄的图像中检测奶牛的巨大挑战，并强调了在构建检测模型时包含不同摄像机角度的重要性。此外，我们的结果强调，更高的模型复杂度并不一定会带来更好的性能。最佳模型配置在很大程度上取决于特定任务和数据集。最后，虽然使用在相关任务上训练的自定义初始权重进行微调可以为检测任务带来优势，但较简单的模型不会从这种方法中获得类似的好处。使用预先训练的权重训练简单模型而不依赖于先前的相关信息（这可能需要大量的劳动力）会更有效。未来的工作应侧重于自适应方法和高级数据增强，以提高泛化和鲁棒性。本研究为 PLF 研究人员提供了部署现有研究中的计算机视觉模型的实用指南，强调了泛化问题，并为进一步研究贡献了包含 1254 张图像和 11818 个奶牛实例的 COLO 数据集。]]></description>
      <guid>https://arxiv.org/abs/2407.20372</guid>
      <pubDate>Thu, 01 Aug 2024 03:14:25 GMT</pubDate>
    </item>
    <item>
      <title>对齐分数：多视图姿势准确度评估的稳健指标</title>
      <link>https://arxiv.org/abs/2407.20391</link>
      <description><![CDATA[arXiv:2407.20391v1 公告类型：新 
摘要：我们提出了三个新的指标来评估给定地面实况的一组估计相机姿势的准确性：平移对齐分数（TAS）、旋转对齐分数（RAS）和姿势对齐分数（PAS）。TAS 独立于旋转评估平移精度，RAS 独立于平移评估旋转精度。PAS 是两个分数的平均值，评估平移和旋转的综合精度。TAS 分四个步骤计算：（1）找到最近对距离的上四分位数 $d$。（2）使用稳健的配准方法将估计的轨迹与地面实况对齐。（3）收集所有距离误差并获得多个阈值的累积频率，范围从 $0.01d$ 到 $d$，分辨率为 $0.01d$。 (4) 将这些累积频率相加，并将它们归一化，使理论最大值为 1。TAS 比现有指标具有实际优势，因为 (1) 它对异常值和共线运动具有鲁棒性，并且 (2) 无需调整不同数据集上的参数。RAS 的计算方式与 TAS 类似，并且也显示出比现有旋转指标对异常值的鲁棒性更强。我们通过大量模拟验证了我们的说法，并深入讨论了所提出指标的优缺点。]]></description>
      <guid>https://arxiv.org/abs/2407.20391</guid>
      <pubDate>Thu, 01 Aug 2024 03:14:25 GMT</pubDate>
    </item>
    <item>
      <title>通过对比学习和全局-局部相似性对比 Deepfakes 的传播</title>
      <link>https://arxiv.org/abs/2407.20337</link>
      <description><![CDATA[arXiv:2407.20337v1 公告类型：新
摘要：辨别真实内容和高级 AI 方法生成的内容变得越来越具有挑战性。虽然以前的研究主要针对假脸检测，但生成的自然图像的识别最近才浮出水面。这促使人们最近探索采用基础视觉和语言模型（如 CLIP）的解决方案。然而，CLIP 嵌入空间针对全局图像到文本对齐进行了优化，并且本质上不是为深度伪造检测而设计的，忽略了定制训练和局部图像特征的潜在好处。在本研究中，我们提出了 CoDE（对比深度伪造嵌入），这是一种专为深度伪造检测而设计的新型嵌入空间。CoDE 通过对比学习进行训练，额外加强了全局-局部相似性。为了维持我们模型的训练，我们生成了一个全面的数据集，该数据集侧重于由扩散模型生成的图像，并包含使用四个不同生成器生成的 920 万张图像的集合。实验结果表明，CoDE 在新收集的数据集上实现了最佳准确率，同时还对未见过的图像生成器表现出了出色的泛化能力。我们的源代码、训练模型和收集的数据集均可在以下网址公开获取：https://github.com/aimagelab/CoDE。]]></description>
      <guid>https://arxiv.org/abs/2407.20337</guid>
      <pubDate>Thu, 01 Aug 2024 03:14:24 GMT</pubDate>
    </item>
    <item>
      <title>BRIDGE：通过更强的视觉提示弥补图像字幕评估中的差距</title>
      <link>https://arxiv.org/abs/2407.20341</link>
      <description><![CDATA[arXiv:2407.20341v1 公告类型：新
摘要：在评估机器生成的图像字幕时有效地与人类判断保持一致是一个复杂而有趣的挑战。现有的评估指标（如 CIDEr 或 CLIP-Score）在这方面存在不足，因为它们没有考虑相应的图像，或者缺乏编码细粒度细节和惩罚幻觉的能力。为了克服这些问题，在本文中，我们提出了 BRIDGE，这是一种新的可学习且无参考的图像字幕指标，它采用一种新颖的模块将视觉特征映射到密集向量中，并将它们集成到在评估过程中构建的多模态伪字幕中。这种方法产生了一种多模态指标，它可以正确地整合来自输入图像的信息，而无需依赖参考字幕，从而弥合人类判断与机器生成的图像字幕之间的差距。跨越多个数据集的实验表明，与现有的无参考评估分数相比，我们的提案取得了最先进的结果。我们的源代码和训练模型公开发布于：https://github.com/aimagelab/bridge-score。]]></description>
      <guid>https://arxiv.org/abs/2407.20341</guid>
      <pubDate>Thu, 01 Aug 2024 03:14:24 GMT</pubDate>
    </item>
    <item>
      <title>利用生成对抗网络对半导体晶圆切割引起的缺陷进行图像数据增强和分类</title>
      <link>https://arxiv.org/abs/2407.20268</link>
      <description><![CDATA[arXiv:2407.20268v1 公告类型：新
摘要：在半导体制造中，晶圆切割工艺至关重要，但容易受到缺陷的影响，从而严重影响产量——无缺陷芯片的比例。深度神经网络是目前（半）自动化视觉检测的最新技术。然而，众所周知，它们需要特别大量的数据进行模型训练。为了应对这些挑战，我们探索了生成对抗网络 (GAN) 在图像数据增强和半导体晶圆切割引起的缺陷分类中的应用，以增强视觉检测系统训练数据的多样性和平衡性。通过这种方法，可以生成模拟真实世界切割缺陷的合成但逼真的图像。我们采用三种不同的 GAN 变体进行高分辨率图像合成：深度卷积 GAN (DCGAN)、CycleGAN 和 StyleGAN3。我们正在进行的结果表明，可以获得更高的分类准确率，平衡准确率平均提高了 23.1%，从 65.1%（基线实验）到 88.2%（DCGAN 实验），这可能有助于生产中的产量优化。]]></description>
      <guid>https://arxiv.org/abs/2407.20268</guid>
      <pubDate>Thu, 01 Aug 2024 03:14:23 GMT</pubDate>
    </item>
    <item>
      <title>太阳关闭，灯光亮起：用于稳健语义感知的逼真单目夜间模拟</title>
      <link>https://arxiv.org/abs/2407.20336</link>
      <description><![CDATA[arXiv:2407.20336v1 公告类型：新
摘要：夜间场景很难用学习模型进行语义感知并为人类进行注释。因此，逼真的合成夜间数据对于学习夜间稳健的语义感知变得更加重要，这要归功于它们准确且廉价的语义注释。然而，现有的数据驱动或手工制作的从白天图像生成夜间图像的技术存在真实感差的问题。原因是高度空间变化的夜间照明与白天照明截然不同，场景中空间变化材质的物体在 3D 中发生复杂的相互作用，并且很难用这种 2D 方法捕捉。与雾或雨等其他条件相比，上述 3D 交互和照明偏移在文献中同样难以建模。我们的方法名为“太阳关闭，灯光打开”（SOLO），是第一个通过 3D 操作以照片般逼真的方式对单个图像进行夜间模拟的方法。它首先从输入的日间图像中明确估计场景的 3D 几何形状、材质和光源位置，然后通过概率性地实例化光源（以考虑其语义的方式）重新照亮场景，然后运行标准光线追踪。我们的夜间图像不仅在视觉质量和照片级真实感方面优于包括扩散模型在内的竞争方法，而且事实证明，前者的图像更有利于在日夜适应中进行语义夜间分割。代码和数据将公开提供。]]></description>
      <guid>https://arxiv.org/abs/2407.20336</guid>
      <pubDate>Thu, 01 Aug 2024 03:14:23 GMT</pubDate>
    </item>
    </channel>
</rss>