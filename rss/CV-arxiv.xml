<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Tue, 17 Dec 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>个性化和顺序的文本到图像生成</title>
      <link>https://arxiv.org/abs/2412.10419</link>
      <description><![CDATA[arXiv:2412.10419v1 公告类型：新
摘要：我们解决了个性化、交互式文本转图像 (T2I) 生成问题，设计了一个强化学习 (RL) 代理，它通过一系列提示扩展迭代地为用户改进一组生成的图像。使用人工评估者，我们创建了一个新颖的顺序偏好数据集，并将其与大规模开源（非顺序）数据集一起使用。我们使用 EM 策略构建用户偏好和用户选择模型，并识别不同的用户偏好类型。然后，我们利用大型多模态语言模型 (LMM) 和基于价值的 RL 方法向用户推荐个性化和多样化的提示扩展。我们的个性化和顺序文本转图像代理 (PASTA) 通过个性化的多轮功能扩展了 T2I 模型，促进了协作共同创造并解决了用户意图中的不确定性或未充分规范的问题。我们使用人工评估者评估 PASTA，与基线方法相比显示出显着的改进。我们还发布了顺序评估者数据集和模拟用户评估者交互，以支持未来个性化、多轮 T2I 生成的研究。]]></description>
      <guid>https://arxiv.org/abs/2412.10419</guid>
      <pubDate>Tue, 17 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CAP：具有说服力和创造性的图像生成评估</title>
      <link>https://arxiv.org/abs/2412.10426</link>
      <description><![CDATA[arXiv:2412.10426v1 公告类型：新
摘要：我们解决了广告图像生成的任务，并引入了三个评估指标来评估生成的广告图像中的创造力、提示对齐和说服力 (CAP)。尽管文本到图像 (T2I) 生成方面取得了最新进展，并且它们在生成用于明确描述的高质量图像方面表现出色，但评估这些模型仍然具有挑战性。现有的评估方法主要侧重于评估与明确、详细描述的对齐，但评估与视觉隐含提示的对齐仍然是一个悬而未决的问题。此外，创造力和说服力是增强广告图像有效性的基本品质，但很少被衡量。为了解决这个问题，我们提出了三个新的指标来评估生成的图像的创造力、对齐和说服力。我们的研究结果表明，当输入文本是隐含消息时，当前的 T2I 模型在创造力、说服力和对齐方面存在困难。我们进一步介绍了一种简单而有效的方法来增强 T2I 模型生成更一致、更具创造力和更具说服力的图像的能力。]]></description>
      <guid>https://arxiv.org/abs/2412.10426</guid>
      <pubDate>Tue, 17 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GPTDrawer：通过 ChatGPT 增强视觉合成</title>
      <link>https://arxiv.org/abs/2412.10429</link>
      <description><![CDATA[arXiv:2412.10429v1 公告类型：新
摘要：在人工智能驱动的图像生成这一新兴领域，对文本提示的精确度和相关性的追求仍然至关重要。本文介绍了 GPTDrawer，这是一种创新的流程，它利用基于 GPT 的模型的生成能力来增强视觉合成过程。我们的方法采用了一种新算法，该算法使用关键字提取、语义分析和图像文本一致性评估迭代地细化输入提示。通过集成 ChatGPT 用于自然语言处理和 Stable Diffusion 用于图像生成，GPTDrawer 生成一批图像，这些图像在余弦相似度指标的指导下经过连续的细化循环，直到达到语义对齐的阈值。结果表明，根据用户定义的提示生成的图像的保真度显着提高，展示了系统解释和可视化复杂语义结构的能力。这项工作的影响扩展到从创意艺术到设计自动化的各个应用领域，为人工智能辅助创意过程树立了新的标杆。]]></description>
      <guid>https://arxiv.org/abs/2412.10429</guid>
      <pubDate>Tue, 17 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于细粒度 3D 游戏角色重建的无监督跨域回归</title>
      <link>https://arxiv.org/abs/2412.10430</link>
      <description><![CDATA[arXiv:2412.10430v1 Announce Type: new 
摘要：随着“元宇宙”的兴起和游戏的快速发展，在虚拟世界中忠实地重建角色变得越来越重要。沉浸式体验是“元宇宙”最核心的主题之一，而虚拟角色的可还原性则是其中的关键点。同时，游戏是元宇宙的载体，玩家可以在其中自由编辑游戏角色的面部外观。在本文中，我们提出了一个简单但功能强大的跨领域框架，可以端到端地从单视图图像重建细粒度的3D游戏角色。与以前无法解决跨领域差距的方法不同，我们提出了一个有效的回归器，可以大大减少现实世界领域和游戏领域之间的差异。为了解决没有基本事实的弊端，我们的无监督框架完成了目标领域的知识转移。此外，我们还提出了一种创新的对比损失来解决实例视差问题，从而保留了重建角色的人物细节。相反，我们激活了一个辅助的 3D 身份感知提取器，使我们的模型的结果更加完美。然后，我们稳健而精致地生成了一大组具有物理意义的面部参数。实验表明，我们的方法在 3D 游戏角色重建中取得了最先进的性能。]]></description>
      <guid>https://arxiv.org/abs/2412.10430</guid>
      <pubDate>Tue, 17 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CUPS：利用共形深度不确定性改进人体姿势形状估计器</title>
      <link>https://arxiv.org/abs/2412.10431</link>
      <description><![CDATA[arXiv:2412.10431v1 公告类型：新
摘要：我们介绍了 CUPS，这是一种从 RGB 视频中学习序列到序列 3D 人体形状和姿势的新方法，具有不确定性量化。为了在先前的工作基础上改进，我们开发了一种在训练期间生成和评分多个假设的方法，有效地将不确定性量化整合到学习过程中。该过程会产生一个深度不确定性函数，该函数与 3D 姿势估计器进行端到端训练。训练后，学习到的深度不确定性模型用作一致性分数，可用于校准共形预测器以评估输出预测的质量。由于人体姿势形状学习中的数据不是完全可交换的，我们还提出了共形预测覆盖差距的两个实际界限，为我们模型的不确定性界限提供了理论支持。我们的结果表明，通过利用共形预测的深度不确定性，我们的方法在继承共形预测的概率保证的同时，在各种指标和数据集上实现了最先进的性能。]]></description>
      <guid>https://arxiv.org/abs/2412.10431</guid>
      <pubDate>Tue, 17 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>点云的隐式神经压缩</title>
      <link>https://arxiv.org/abs/2412.10433</link>
      <description><![CDATA[arXiv:2412.10433v1 公告类型：新
摘要：点云因其能够准确描绘 3D 对象和场景而在许多应用中占据重要地位。然而，有效压缩非结构化、高精度点云数据仍然是一项重大挑战。在本文中，我们提出了 NeRC$^{\textbf{3}}$，这是一种新颖的点云压缩框架，利用隐式神经表示来处理几何和属性。我们的方法采用两个基于坐标的神经网络来隐式表示体素化点云：第一个确定体素的占用状态，而第二个预测占用体素的属性。通过将体素坐标输入这些网络，接收器可以有效地重建原始点云的几何形状和属性。神经网络参数与重建所需的辅助信息一起被量化和压缩。此外，我们将我们的方法扩展到动态点云压缩，并使用减少时间冗余的技术，包括称为 4D-NeRC$^{\textbf{3}}$ 的 4D 时空表示。实验结果验证了我们方法的有效性：对于静态点云，NeRC$^{\textbf{3}}$ 在最新的 G-PCC 标准中优于基于八叉树的方法。对于动态点云，4D-NeRC$^{\textbf{3}}$ 与最先进的 G-PCC 和 V-PCC 标准相比表现出卓越的几何压缩性能，并在联合几何和属性压缩方面取得了有竞争力的结果。]]></description>
      <guid>https://arxiv.org/abs/2412.10433</guid>
      <pubDate>Tue, 17 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>COEF-VQ：通过级联多模式 LLM 框架实现经济高效的视频质量理解</title>
      <link>https://arxiv.org/abs/2412.10435</link>
      <description><![CDATA[arXiv:2412.10435v1 公告类型：新
摘要：最近，随着多模态大型语言模型 (MLLM) 技术的出现，利用其视频理解能力处理不同的分类任务成为可能。在实践中，如果我们需要在线部署 MLLM，我们将面临对 GPU 资源需求巨大的困难。在本文中，我们提出了一种新型的级联 MLLM 框架 COEF-VQ，用于更好地理解 TikTok 上的视频质量。为此，我们首先提出了一种融合所有视觉、文本和音频信号的 MLLM，然后开发了一个级联框架，其中轻量级模型作为预过滤阶段，MLLM 作为精细考虑阶段，显着减少了对 GPU 资源的需求，同时保留了 MLLM 独有的性能。为了证明 COEF-VQ 的有效性，我们将这个新框架部署到 TikTok 的视频管理平台 (VMP) 上，并对两个与视频质量理解相关的内部任务进行了一系列详细的实验。我们表明，COEF-VQ 在这两个任务中可以在限制资源消耗的情况下显著提高性能。]]></description>
      <guid>https://arxiv.org/abs/2412.10435</guid>
      <pubDate>Tue, 17 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>语义数据集的联邦学习基准测试：联邦场景图生成</title>
      <link>https://arxiv.org/abs/2412.10436</link>
      <description><![CDATA[arXiv:2412.10436v1 公告类型：新
摘要：联邦学习 (FL) 最近引起了人们的关注，它是一种数据分散的训练框架，能够从本地分布的样本中学习深度模型，同时保持数据隐私。在此框架的基础上，人们付出了巨大的努力来建立 FL 基准，这些基准提供了严格的评估设置，可以控制客户端之间的数据异质性。之前的努力主要集中在处理相对简单的分类任务上，其中每个样本都用一个独热标签注释，例如 MNIST、CIFAR、LEAF 基准等。然而，很少有人关注展示一个处理复杂语义的 FL 基准，其中每个样本包含来自多个标签的不同语义信息，例如全景场景图生成 (PSG)，其中包含对象、主题和它们之间的关系。由于现有基准旨在以单一语义的狭隘视角（例如独热标签）分发数据，因此在形式化 FL 基准时管理客户端之间的复杂语义异质性并非易事。在本文中，我们提出了一个基准测试流程，以建立具有可控跨客户端语义异构性的 FL 基准测试：两个关键步骤是 i) 使用语义进行数据聚类和 ii) 通过可控跨客户端语义异构性进行数据分发。作为概念证明，我们首先构建一个联邦 PSG 基准测试，展示现有 PSG 方法在具有可控场景图语义异构性的 FL 设置中的有效性。我们还通过将强大的联邦学习算法应用于数据异构性来展示我们基准测试的有效性，以显示性能的提高。我们的代码可在 https://github.com/Seung-B/FL-PSG 上找到。]]></description>
      <guid>https://arxiv.org/abs/2412.10436</guid>
      <pubDate>Tue, 17 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SVGFusion：通过向量空间扩散实现可扩展的文本到 SVG 生成</title>
      <link>https://arxiv.org/abs/2412.10437</link>
      <description><![CDATA[arXiv:2412.10437v1 公告类型：新
摘要：从文本数据生成可缩放矢量图形 (SVG) 资产仍然是一项重大挑战，这主要是由于高质量矢量数据集的稀缺以及建模复杂图形分布所需的可缩放矢量表示的局限性。这项工作介绍了 SVGFusion，这是一种文本到 SVG 模型，能够扩展到现实世界的 SVG 数据，而无需依赖基于文本的离散语言模型或长时间的 SDS 优化。SVGFusion 的本质是使用流行的文本到图像框架学习矢量图形的连续潜在空间。具体来说，SVGFusion 由两个模块组成：矢量像素融合变分自动编码器 (VP-VAE) 和矢量空间扩散变换器 (VS-DiT)。 VP-VAE 将 SVG 和相应的光栅化作为输入，并学习连续的潜在空间，而 VS-DiT 则根据文本提示学习在此空间内生成潜在代码。基于 VP-VAE，提出了一种新颖的渲染序列建模策略，使潜在空间能够嵌入 SVG 中的构造逻辑知识。这使模型能够在矢量图形中实现类似人类的设计能力，同时系统地防止复杂图形组合中的遮挡。此外，通过添加更多 VS-DiT 块，可以利用 VS-DiT 的可扩展性不断提高我们的 SVGFusion 的能力。收集了大规模 SVG 数据集来评估我们提出的方法的有效性。大量实验证实了我们的 SVGFusion 优于现有的 SVG 生成方法，实现了更高的质量和通用性，从而为 SVG 内容创建建立了一个新颖的框架。代码、模型和数据将发布在：\href{https://ximinng.github.io/SVGFusionProject/}https://ximinng.github.io/SVGFusionProject/}]]></description>
      <guid>https://arxiv.org/abs/2412.10437</guid>
      <pubDate>Tue, 17 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于映射特征检测的自动图像注释</title>
      <link>https://arxiv.org/abs/2412.10438</link>
      <description><![CDATA[arXiv:2412.10438v1 公告类型：新
摘要：检测道路特征是实现自动驾驶和定位的关键因素。例如，可靠地检测道路环境中广泛分布的电线杆可以改善定位。现代基于深度学习的感知系统需要大量带注释的数据。自动注释避免了耗时且昂贵的手动注释。由于自动方法容易出错，因此管理注释不确定性对于确保正确的学习过程至关重要。在同一数据集上融合多个注释源可以有效减少错误。这不仅可以提高注释的质量，还可以提高感知模型的学习能力。在本文中，我们考虑了三种图像自动注释方法的融合：高精度矢量地图与激光雷达相结合的特征投影、图像分割和激光雷达分割。我们的实验结果通过对手动注释图像的比较评估证明了多模态自动注释对电线杆检测的显著优势。最后，使用所得的多模态融合来微调使用未标记数据的杆基检测对象检测模型，显示通过增强网络专业化实现的整体改进。该数据集是公开可用的。]]></description>
      <guid>https://arxiv.org/abs/2412.10438</guid>
      <pubDate>Tue, 17 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CogNav：使用 LLM 进行对象目标导航的认知过程建模</title>
      <link>https://arxiv.org/abs/2412.10439</link>
      <description><![CDATA[arXiv:2412.10439v1 公告类型：新
摘要：对象目标导航 (ObjectNav) 是具身 AI 的一项基本任务，要求代理在看不见的环境中找到目标对象。这项任务特别具有挑战性，因为它需要感知和认知过程才能有效地感知和决策。虽然感知在快速发展的视觉基础模型的推动下取得了重大进展，但认知方面的进展仍然仅限于从大量导航演示中隐式学习或明确利用预定义的启发式规则。受神经科学证据的启发，即人类在看不见的环境中搜索物体时会不断更新其认知状态，我们提出了 CogNav，它试图在大型语言模型的帮助下对这一认知过程进行建模。具体来说，我们使用由从探索到识别的认知状态组成的有限状态机来建模认知过程。状态之间的转换由大型语言模型确定，该模型基于在线构建的异构认知图，其中包含正在探索的场景的空间和语义信息。在模拟和真实环境中进行的大量实验表明，我们的认知建模显著提高了 ObjectNav 的效率，并具有类似人类的导航行为。在开放词汇和零样本设置中，我们的方法将 HM3D 基准的 SOTA 从 69.3% 提高到 87.2%。代码和数据即将发布。]]></description>
      <guid>https://arxiv.org/abs/2412.10439</guid>
      <pubDate>Tue, 17 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于多模态实体链接的多级匹配网络</title>
      <link>https://arxiv.org/abs/2412.10440</link>
      <description><![CDATA[arXiv:2412.10440v1 公告类型：新 
摘要：多模态实体链接 (MEL) 旨在将多模态上下文中的模糊提及链接到多模态知识库中的相应实体。现有的 MEL 方法大多基于表示学习或视觉和语言预训练机制，以探索多种模态之间的互补作用。然而，这些方法有两个局限性。一方面，它们忽略了考虑来自同一模态的负样本的可能性。另一方面，它们缺乏捕获双向跨模态交互的机制。为了解决这些问题，我们提出了一种用于多模态实体链接的多级匹配网络 (M3EL)。具体而言，M3EL 由三个不同的模块组成：(i) 多模态特征提取模块，它使用多模态编码器提取特定于模态的表示，并引入模态内对比学习子模块以获得基于单模态差异的更好的判别嵌入； (ii) 模态内匹配网络模块，包含两个级别的匹配粒度：粗粒度全局到全局和细粒度全局到局部，以实现局部和全局级别的模态内交互；(iii) 跨模态匹配网络模块，应用双向策略，即文本到视觉和视觉到文本匹配，以实现双向跨模态交互。在 WikiMEL、RichpediaMEL 和 WikiDiverse 数据集上进行的大量实验表明，与最先进的基线相比，M3EL 具有出色的性能。]]></description>
      <guid>https://arxiv.org/abs/2412.10440</guid>
      <pubDate>Tue, 17 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SweetTokenizer：用于紧凑视觉离散化的语义感知时空标记器</title>
      <link>https://arxiv.org/abs/2412.10443</link>
      <description><![CDATA[arXiv:2412.10443v1 公告类型：新
摘要：本文介绍了 \textbf{S}emantic-a\textbf{W}ar\textbf{E} spatial-t\textbf{E}mporal \textbf{T}okenizer (SweetTokenizer)，这是一种紧凑而有效的视觉数据离散化方法。我们的目标是提高 tokenizer 的压缩率，同时保持 VQ-VAE 范式中的重建保真度。首先，为了获得紧凑的潜在表示，我们将图像或视频分解为时空维度，通过 \textbf{C}cross-attention \textbf{Q}uery \textbf{A}uto\textbf{E}ncoder (CQAE) 将视觉信息转换为可学习的查询空间和时间标记。其次，为了在压缩过程中补充视觉信息，我们通过从现成的 LLM 嵌入派生的专用码本量化这些标记，以利用语言模态的丰富语义。最后，为了增强训练稳定性和收敛性，我们还引入了课程学习策略，这对于有效的离散视觉表征学习至关重要。SweetTokenizer 仅使用以前最先进的视频标记器中使用的 \textbf{25\%} 的标记即可实现相当的视频重建保真度，并将视频生成结果提升 \textbf{32.9\%} w.r.t gFVD。当使用相同的标记数时，我们显著提高了视频和图像重建结果，在 UCF-101 上，w.r.t rFVD 提高了 \textbf{57.1\%}，在 ImageNet-1K 上，w.r.t rFID 提高了 \textbf{37.2\%}。此外，压缩标记还包含语义信息，从而支持下游应用程序中由 LLM 提供支持的少量识别功能。]]></description>
      <guid>https://arxiv.org/abs/2412.10443</guid>
      <pubDate>Tue, 17 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>3D 机器人扫描中次优视图策略的边界探索</title>
      <link>https://arxiv.org/abs/2412.10444</link>
      <description><![CDATA[arXiv:2412.10444v1 公告类型：新
摘要：下一个最佳视图 (NBV) 问题是 3D 机器人扫描中的一个关键挑战，有可能大大提高物体捕获和重建的效率。当前确定 NBV 的方法经常忽略视图重叠，假设相机焦点的虚拟原点，并依赖于 3D 数据的体素表示。为了解决这些问题并提高扫描未知物体的实用性，我们提出了一种 NBV 策略，其中下一个视图探索扫描点云的边界，并且本质上考虑重叠。扫描距离或相机工作距离是可调且灵活的。为此，提出了一种基于模型的方法，其中基于参考模型迭代搜索下一个传感器位置。通过考虑新扫描数据和现有数据之间的重叠以及最终收敛来计算分数。此外，遵循边界探索的思想，设计并提出了一种深度学习网络——边界探索 NBV 网络 (BENBV-Net)，该网络可用于直接从扫描数据预测 NBV，而无需参考模型。它预测给定边界的得分，并选择得分最高的边界作为下一个最佳视图的目标点。BENBV-Net 提高了 NBV 生成速度，同时保持了基于模型的方法的性能。在 ShapeNet、ModelNet 和 3D Repository 数据集上评估了我们提出的方法并与现有方法进行了比较。实验结果表明，我们的方法在扫描效率和重叠度方面优于其他方法，这两者对于实际的 3D 扫描应用至关重要。相关代码发布在 \url{github.com/leihui6/BENBV}。]]></description>
      <guid>https://arxiv.org/abs/2412.10444</guid>
      <pubDate>Tue, 17 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>变分自动编码器视觉模型中字母身份和字母位置的解缠和组合性</title>
      <link>https://arxiv.org/abs/2412.10446</link>
      <description><![CDATA[arXiv:2412.10446v1 公告类型：新
摘要：人类读者可以准确地计算一个单词中有多少个字母（例如，“buffalo”中有 7 个字母），从给定位置删除一个字母（例如，“bufflo”）或添加一个新字母。因此，读者的大脑必须学会解开与字母位置及其身份相关的信息。这种解开对于人类创建和解析新字符串的无限组合能力是必要的，其中任何字母组合都出现在任何位置。现代深度神经模型是否也具备这种关键的组合能力？在这里，我们测试了在视觉输入中实现特征解开的最先进神经模型在对书面单词图像进行训练时是否也可以解开字母位置和字母身份。具体来说，我们训练了 beta 变分自动编码器 ($\beta$-VAE) 来重建字母串图像，并使用 CompOrth 评估了它们的解缠性能 - CompOrth 是我们为研究正字法视觉模型中的组合学习和零样本泛化而创建的新基准。该基准建议进行一组复杂度不断增加的测试，以评估深度神经模型中书面单词的正字法特征之间的解缠程度。使用 CompOrth，我们进行了一系列实验来分析这些模型的泛化能力，特别是对未知单词长度以及未知字母身份和字母位置组合的泛化能力。我们发现，虽然模型可以有效地解缠表面特征，例如图像中单词的水平和垂直“视网膜”位置，但它们却无法解缠字母位置和字母身份，并且缺乏任何单词长度的概念。总之，这项研究展示了最先进的 $\beta$-VAE 模型与人类相比的缺点，并提出了一个新的挑战和相应的基准来评估神经模型。]]></description>
      <guid>https://arxiv.org/abs/2412.10446</guid>
      <pubDate>Tue, 17 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>