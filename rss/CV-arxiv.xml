<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Tue, 01 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>通过 Warp Composition 进行神经产品重要性采样</title>
      <link>https://arxiv.org/abs/2409.18974</link>
      <description><![CDATA[arXiv:2409.18974v1 公告类型：新
摘要：实现现代照片级真实感渲染的高效率取决于使用蒙特卡洛采样分布，该分布可以紧密近似每个像素估计的照明积分。样本通常由一组简单分布生成，每个分布针对被积函数中的不同因子，这些分布通过多重重要性采样组合在一起。得到的混合分布可能远离所有因子的实际乘积，导致即使对于直接照明估计也存在次优方差。我们提出了一种基于学习的方法，该方法使用规范化流来有效地对照明乘积积分进行重要性采样，例如环境照明和材料项的乘积。我们的采样器将流头部扭曲与发射器尾部扭曲组合在一起。小条件头部扭曲由神经样条流表示，而大无条件尾部则根据环境图进行离散化，并且其评估是即时的。如果条件是低维的，头部扭曲也可以离散化以实现更好的性能。我们证明了在包括复杂几何、材料和照明的一系列应用上，与先前的方法相比，方差减少了。]]></description>
      <guid>https://arxiv.org/abs/2409.18974</guid>
      <pubDate>Tue, 01 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>高斯遗产：集成对象分割的文化遗产 3D 数字化</title>
      <link>https://arxiv.org/abs/2409.19039</link>
      <description><![CDATA[arXiv:2409.19039v1 公告类型：新
摘要：创建实物的数字复制品对于有形文化遗产的保存和传播具有宝贵的应用价值。然而，现有的方法往往速度慢、成本高，而且需要专业知识。我们提出了一种流程，仅使用 RGB 图像（例如博物馆的照片）生成场景的 3D 复制品，然后为每个感兴趣的项目（例如展品中的物品）提取一个模型。我们通过利用新颖的视图合成和高斯溅射方面的进步来实现这一点，这些进步经过修改以实现高效的 3D 分割。这种方法不需要手动注释，可以使用标准智能手机捕获视觉输入，使其既经济实惠又易于部署。我们概述了该方法并对对象分割的准确性进行了基线评估。代码可在 https://mahtaabdn.github.io/gaussian_heritage.github.io/ 获得。]]></description>
      <guid>https://arxiv.org/abs/2409.19039</guid>
      <pubDate>Tue, 01 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于完成图形设计的多模式标记文档模型</title>
      <link>https://arxiv.org/abs/2409.19051</link>
      <description><![CDATA[arXiv:2409.19051v1 公告类型：新
摘要：本文介绍了多模态标记文档模型 (MarkupDM)，它可以在交错的多模态文档中生成标记语言和图像。与现有的视觉和语言多模态模型不同，我们的 MarkupDM 解决了图形设计任务中至关重要的独特挑战：生成有助于整体外观的部分图像，通常涉及透明度和不同大小，并理解标记语言的语法和语义，这些语言作为图形设计的表示格式起着根本性的作用。为了应对这些挑战，我们设计了一个图像量化器来标记具有透明度的不同大小的图像，并修改了一个代码语言模型来处理标记语言并合并图像模态。我们对三个图形设计完成任务的方法进行了深入评估：在图形设计模板中生成缺失的属性值、图像和文本。结果证实了我们的 MarkupDM 对图形设计任务的有效性。我们还详细讨论了优缺点，为未来多模态文档生成的研究提供了见解。]]></description>
      <guid>https://arxiv.org/abs/2409.19051</guid>
      <pubDate>Tue, 01 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>展示和指导：基于愿景和语言模型的教学计划</title>
      <link>https://arxiv.org/abs/2409.19074</link>
      <description><![CDATA[arXiv:2409.19074v1 公告类型：新
摘要：指导用户完成复杂的程序计划是一项本质上多模态的任务，其中以视觉方式说明计划步骤对于提供有效的计划指导至关重要。然而，现有的计划遵循语言模型 (LM) 工作通常不具备多模态输入和输出能力。在这项工作中，我们提出了 MM-PlanLLM，这是第一个多模态 LLM，旨在通过利用文本计划和视觉信息来帮助用户执行教学任务。具体来说，我们通过两个关键任务实现跨模态性：对话视频时刻检索，其中模型根据用户查询检索相关的步骤视频片段，以及视觉信息步骤生成，其中模型根据用户当前进度的图像生成计划中的下一步。 MM-PlanLLM 采用新颖的多任务多阶段方法进行训练，旨在逐步将模型暴露于多模态教学计划语义层，在以计划为基础的环境中在多模态和文本对话中均取得优异表现。此外，我们表明该模型提供跨模态时间和计划结构表征，这些表征在文本计划步骤和教学视频时刻之间保持一致。]]></description>
      <guid>https://arxiv.org/abs/2409.19074</guid>
      <pubDate>Tue, 01 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>融合就是你所需要的：面部融合，用于定制的身份保留图像合成</title>
      <link>https://arxiv.org/abs/2409.19111</link>
      <description><![CDATA[arXiv:2409.19111v1 公告类型：新
摘要：文本到图像 (T2I) 模型极大地推动了人工智能的发展，能够根据特定的文本提示在不同环境中生成高质量的图像。然而，现有的基于 T2I 的方法通常难以准确地从参考图像中重现个体的外观，也难以在各种环境中为这些个体创建新颖的表示。为了解决这个问题，我们利用 Stable Diffusion 中预先训练的 UNet 将目标人脸图像直接合并到生成过程中。我们的方法不同于以前依赖固定编码器或静态人脸嵌入的方法，后者通常无法弥补编码差距。相反，我们利用 UNet 复杂的编码功能来处理跨多个尺度的参考图像。通过创新地改变 UNet 的交叉注意层，我们有效地将个人身份融合到生成过程中。这种跨不同尺度的面部特征战略性整合不仅增强了生成图像的稳健性和一致性，还有助于高效地生成多参考和多身份。我们的方法为身份保留图像生成树立了新标杆，在相似性指标方面提供了最先进的结果，同时保持了快速对齐。]]></description>
      <guid>https://arxiv.org/abs/2409.19111</guid>
      <pubDate>Tue, 01 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>修剪然后重新加权：实现扩散模型的数据高效训练</title>
      <link>https://arxiv.org/abs/2409.19128</link>
      <description><![CDATA[arXiv:2409.19128v1 公告类型：新
摘要：尽管扩散模型 (DM) 具有出色的生成能力，但进行训练和推理仍然需要大量计算。以前的研究致力于加速扩散采样，但实现数据高效的扩散训练往往被忽视。在这项工作中，我们从数据集修剪的角度研究了高效的扩散训练。受到生成对抗网络 (GAN) 等生成模型的数据高效训练原则的启发，我们首先将 GAN 中使用的数据选择方案扩展到 DM 训练，其中数据特征由代理模型编码，然后应用分数标准来选择核心集。为了进一步提高生成性能，我们采用了一种逐类重新加权方法，该方法通过对预先训练的参考 DM 进行分布稳健优化 (DRO) 来得出类权重。对于 CIFAR-10 上的逐像素 DM (DDPM)，实验证明了我们的方法优于现有方法，其图像合成效果与原始全数据模型相当，同时实现了 2.34 倍至 8.32 倍的加速。此外，我们的方法可以推广到潜在 DM (LDM)，例如 Masked Diffusion Transformer (MDT) 和 Stable Diffusion (SD)，并在 ImageNet 上实现了具有竞争力的生成能力。代码可在此处获取 (https://github.com/Yeez-lee/Data-Selection-and-Reweighting-for-Diffusion-Models)。]]></description>
      <guid>https://arxiv.org/abs/2409.19128</guid>
      <pubDate>Tue, 01 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>语音驱动面部动画的多样化代码查询学习</title>
      <link>https://arxiv.org/abs/2409.19143</link>
      <description><![CDATA[arXiv:2409.19143v1 公告类型：新
摘要：语音驱动的面部动画旨在根据给定的语音信号合成口型同步的 3D 说话面部。此前完成这项任务的方法主要侧重于追求确定性系统的真实性，但迄今为止，对面部运动潜在随机性的描述却很少得到研究。虽然生成建模方法可以通过反复抽取样本轻松处理一对多映射，但确保在小规模数据集上对合理的面部运动进行多样化模式覆盖仍然具有挑战性，而且探索较少。在本文中，我们提出预测以相同音频信号为条件的多个样本，然后明确鼓励样本多样性来解决多样化的面部动画合成问题。我们的核心见解是引导我们的模型以促进多样性的损失探索富有表现力的面部潜在空间，从而可以理想地识别出所需的多样化潜在代码。为此，基于通过矢量量化变分自动编码机制学习到的丰富面部先验知识，我们的模型在时间上查询多个随机代码，这些代码可以灵活地解码为一组多样但合理的忠实语音面部动作。为了进一步允许在生成过程中控制不同的面部部位，所提出的模型旨在按顺序预测不同的面部感兴趣部分，并将它们组合起来最终形成整个面部动作。我们的范例以统一的公式实现了多样化和可控的面部动画合成。我们通过实验证明，我们的方法在数量和质量上都具有最先进的性能，尤其是在样本多样性方面。]]></description>
      <guid>https://arxiv.org/abs/2409.19143</guid>
      <pubDate>Tue, 01 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于稳健人群计数的边界紧缩网络</title>
      <link>https://arxiv.org/abs/2409.19146</link>
      <description><![CDATA[arXiv:2409.19146v1 公告类型：新
摘要：人群计数是一个基础课题，旨在估计监控摄像机提供的拥挤图像或视频中的人数。最近的研究侧重于提高计数准确性，而忽略了计数模型的认证稳健性。在本文中，我们提出了一种用于稳健人群计数的新型边界紧缩网络 (BTN)。它由三部分组成：基础模型、平滑正则化模块和认证边界模块。核心思想是通过基础模型（认证边界模块）传播区间边界，并利用层权重（平滑正则化模块）来指导网络学习。在不同的计数基准数据集上进行的实验证明了 BTN 的有效性和效率。]]></description>
      <guid>https://arxiv.org/abs/2409.19146</guid>
      <pubDate>Tue, 01 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>文本到图像模型上的多模式实用越狱</title>
      <link>https://arxiv.org/abs/2409.19149</link>
      <description><![CDATA[arXiv:2409.19149v1 公告类型：新 
摘要：扩散模型最近在图像质量和文本提示保真度方面取得了显着进步。同时，此类生成模型的安全性已成为日益令人担忧的领域。这项工作引入了一种新型的越狱，它触发 T2I 模型生成带有视觉文本的图像，其中图像和文本虽然单独被认为是安全的，但结合起来会形成不安全的内容。为了系统地探索这种现象，我们提出了一个数据集来评估这种越狱下的当前基于扩散的文本到图像 (T2I) 模型。我们对九个代表性的 T2I 模型进行了基准测试，其中包括两个闭源商业模型。实验结果揭示了一种令人担忧的产生不安全内容的趋势：所有测试模型都存在这种类型的越狱，不安全生成率从 8\% 到 74\%。在实际场景中，通常会使用各种过滤器（例如关键字屏蔽列表、自定义提示过滤器和 NSFW 图像过滤器）来减轻这些风险。我们评估了这些过滤器对我们的越狱的有效性，发现虽然当前的分类器可能对单模态检测有效，但它们无法对抗我们的越狱。我们的工作为进一步开发更安全、更可靠的 T2I 模型奠定了基础。]]></description>
      <guid>https://arxiv.org/abs/2409.19149</guid>
      <pubDate>Tue, 01 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MASt3R-SfM：不受约束的运动结构全集成解决方案</title>
      <link>https://arxiv.org/abs/2409.19152</link>
      <description><![CDATA[arXiv:2409.19152v1 公告类型：新
摘要：运动结构 (SfM) 是一项旨在根据一组图像联合恢复场景的相机姿势和 3D 几何形状的任务，尽管几十年来取得了重大进展，但仍然是一个难题，仍有许多未解决的挑战。SfM 的传统解决方案由一个复杂的最小求解器管道组成，当图像重叠不够、运动太少等时，它往往会传播错误并失败。最近的方法试图重新审视这一范式，但我们通过经验表明，它们无法解决这些核心问题。在本文中，我们建议在最近发布的 3D 视觉基础模型的基础上构建，该模型可以稳健地生成局部 3D 重建和精确匹配。我们引入了一种低内存方法来在全局坐标系中精确对齐这些局部重建。我们进一步表明，这种基础模型可以作为高效的图像检索器，没有任何开销，将整体复杂度从二次降低到线性。总体而言，我们新颖的 SfM 流程简单、可扩展、快速且真正不受约束，也就是说，它可以处理任何有序或无序的图像集合。在多个基准上进行的大量实验表明，我们的方法在不同设置中提供了稳定的性能，尤其是在中小型设置中优于现有方法。]]></description>
      <guid>https://arxiv.org/abs/2409.19152</guid>
      <pubDate>Tue, 01 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FLINT：基于学习的流估计和时间插值，用于科学集合可视化</title>
      <link>https://arxiv.org/abs/2409.19178</link>
      <description><![CDATA[arXiv:2409.19178v1 公告类型：新
摘要：我们提出了 FLINT（基于学习的流估计和时间插值），这是一种基于深度学习的新型方法，用于估计 2D+时间和 3D+时间科学集合数据的流场。FLINT 可以灵活地处理不同类型的场景，其中 (1) 流场对某些成员部分可用（例如，由于空间限制而省略）或 (2) 根本没有流场可用（例如，因为在实验期间无法获取）。我们的架构设计允许灵活地满足这两种情况，只需调整我们的模块化损失函数即可，有效地将不同的场景分别视为流监督和流无监督问题（关于存在或不存在地面真实流）。据我们所知，FLINT 是第一种从科学集合执行流估计的方法，即使在没有原始流信息的情况下，也可以为每个离散时间步生成相应的流场。此外，FLINT 可在标量场之间生成高质量的时间插值。FL​​INT 采用多个神经块，每个神经块都具有多个卷积层和反卷积层。我们通过模拟和实验的科学集成展示了不同使用场景的性能和准确性。]]></description>
      <guid>https://arxiv.org/abs/2409.19178</guid>
      <pubDate>Tue, 01 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>超像素分割的全面回顾和新分类</title>
      <link>https://arxiv.org/abs/2409.19179</link>
      <description><![CDATA[arXiv:2409.19179v1 公告类型：新
摘要：超像素分割包括将图像划分为由相似和连通像素组成的区域。它的方法已广泛应用于许多计算机视觉应用中，因为它可以减少工作量、删除冗余信息并保留具有有意义特征的区域。由于该领域的快速发展，文献未能赶上比较中的最新研究，也未能根据所有现有策略对方法进行分类。这项工作填补了这一空白，通过对超像素分割的新分类法进行全面回顾，其中方法根据其处理步骤和图像特征的处理级别进行分类。我们根据我们的分类法重新审视最近和流行的文献，并根据九个标准评估了 20 种策略：连通性、紧凑性、描绘、对超像素数量的控制、颜色均匀性、鲁棒性、运行时间、稳定性和视觉质量。我们的实验展示了像素聚类中每种方法的趋势，并讨论了各自的权衡。最后，我们提供了一个超像素评估的新基准，网址为 https://github.com/IMScience-PPGINF-PucMinas/superpixel-benchmark。]]></description>
      <guid>https://arxiv.org/abs/2409.19179</guid>
      <pubDate>Tue, 01 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>学习阻碍受限类别的少样本图像分类</title>
      <link>https://arxiv.org/abs/2409.19210</link>
      <description><![CDATA[arXiv:2409.19210v1 公告类型：新
摘要：开源预训练主干的进步使得针对新任务微调模型变得相对容易。然而，这种降低的进入门槛带来了潜在的风险，例如，不良行为者为有害应用开发模型。出现了一个问题：是否有可能开发一个难以针对某些下游任务进行微调的预训练模型？为了开始研究这个问题，我们专注于少样本分类 (FSC)。具体来说，我们研究了使 FSC 对一组受限类更具挑战性的方法，同时保持其他类的性能。我们建议以一种使其成为“初始化不佳”的方式对预训练的主干进行元学习。我们提出的学习阻碍 (LTO) 算法成功阻碍了三个数据集中的四种 FSC 方法，包括用于图像分类的 ImageNet 和 CIFAR100，以及用于属性分类的 CelebA。]]></description>
      <guid>https://arxiv.org/abs/2409.19210</guid>
      <pubDate>Tue, 01 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>第八届 HANDS 研讨会挑战赛第一名解决方案——ARCTIC 赛道：基于 3DGS 的双手类别无关交互重建</title>
      <link>https://arxiv.org/abs/2409.19215</link>
      <description><![CDATA[arXiv:2409.19215v1 公告类型：新
摘要：本报告介绍了我们在与 ECCV 2024 联合举办的第 8 届 HANDS 研讨会挑战赛（ARCTIC 轨道）中获得的第一名解决方案。在这个挑战中，我们解决了双手类别无关的手部-物体交互重建的任务，旨在从单目视频中生成双手和物体的 3D 重建，而不依赖于预定义的模板。这项任务特别具有挑战性，因为在双手操作过程中，手和物体之间存在严重的遮挡和动态接触。我们分别通过引入掩码损失和 3D 接触损失来解决这些问题。此外，我们将 3D 高斯溅射 (3DGS) 应用于此任务。结果，我们的方法在 ARCTIC 测试集上的主要指标 CD$_h$ 中获得了 38.69 的值。]]></description>
      <guid>https://arxiv.org/abs/2409.19215</guid>
      <pubDate>Tue, 01 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>扩展变焦多视角图像的景深</title>
      <link>https://arxiv.org/abs/2409.19220</link>
      <description><![CDATA[arXiv:2409.19220v1 公告类型：新
摘要：由于光学的性质，光学成像系统通常受到景深的限制。因此，扩展景深（EDoF）是满足新兴视觉应用要求的基本任务。为了解决这一任务，常见的做法是使用来自单一视点的多焦点图像。该方法可以在固定视场的情况下获得可接受的EDoF质量，但它仅适用于静态场景，并且视场有限且固定。作为一种新兴的数据类型，变焦多视图图像有可能成为解决EDoF的新范例，因为该数据比多焦点图像包含更多的视场信息。为了实现变焦多视图图像的EDoF，我们提出了一种端到端的EDoF方法，包括图像配准、图像优化和图像融合。实验结果证明了所提方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2409.19220</guid>
      <pubDate>Tue, 01 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>