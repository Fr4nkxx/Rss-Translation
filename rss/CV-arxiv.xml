<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Wed, 22 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>PhyDeformer：具有物理意识的高质量非刚性服装配准</title>
      <link>https://arxiv.org/abs/2501.10455</link>
      <description><![CDATA[arXiv:2501.10455v1 公告类型：新
摘要：我们提出了 PhyDeformer，一种用于高质量服装网格配准的新型变形方法。它分为两个阶段：在第一阶段，进行服装分级以实现网格模板和目标网格之间的粗略 3D 对齐，考虑比例缩放和拟合（例如长度、尺寸）。然后，通过优化结合基于雅可比矩阵的变形框架，对分级网格进行细化，使其与 3D 目标的细粒度细节对齐。对合成和真实服装的定量和定性评估都凸显了我们方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2501.10455</guid>
      <pubDate>Wed, 22 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>BloomScene：用于跨模式场景生成的轻量级结构化 3D 高斯溅射</title>
      <link>https://arxiv.org/abs/2501.10462</link>
      <description><![CDATA[arXiv:2501.10462v1 公告类型：新
摘要：随着虚拟现实应用的广泛使用，3D场景生成已成为一个新的具有挑战性的研究前沿。3D场景具有高度复杂的结构，需要确保输出是密集的、连贯的并包含所有必要的结构。许多当前的3D场景生成方法依赖于预先训练的文本到图像扩散模型和单目深度估计器。然而，生成的场景占用大量存储空间，并且通常缺乏有效的正则化方法，导致几何扭曲。为此，我们提出了BloomScene，一种用于跨模态场景生成的轻量级结构化3D高斯溅射，它从文本或图像输入创建多样化和高质量的3D场景。具体而言，提出了一种跨模态渐进式场景生成框架，利用增量点云重建和3D高斯溅射生成连贯的场景。此外，我们提出了一种基于深度先验的分层正则化机制，该机制利用深度精度和平滑度的多级约束来增强生成场景的真实感和连续性。最后，我们提出了一种结构化的上下文引导压缩机制，该机制利用结构化的哈希网格来模拟无组织锚点属性的上下文，从而显著消除结构冗余并降低存储开销。跨多个场景的综合实验证明了我们的框架与多个基线相比具有巨大的潜力和优势。]]></description>
      <guid>https://arxiv.org/abs/2501.10462</guid>
      <pubDate>Wed, 22 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>HyperCam：用于物联网摄像机的低功耗机载计算机视觉</title>
      <link>https://arxiv.org/abs/2501.10547</link>
      <description><![CDATA[arXiv:2501.10547v1 公告类型：新
摘要：我们介绍了 HyperCam，这是一种节能的图像分类管道，可在低功耗 IoT 摄像头系统上实现计算机视觉任务。HyperCam 利用超维计算在低功耗微控制器上高效地执行训练和推理。我们使用现成的硬件实现了一个低功耗无线摄像头平台，并证明 HyperCam 可以分别在 MNIST、Fashion-MNIST、人脸检测和人脸识别任务中实现 93.60%、84.06%、92.98% 和 72.79% 的准确率，同时在资源效率方面明显优于其他分类器。具体来说，它在峰值使用 42.91-63.00KB 闪存和 22.25KB RAM 时提供 0.08-0.27 秒的推理延迟。在 SVM、xgBoost、MicroNets、MobileNetV3 和 MCUNetV3 等其他机器学习分类器中，HyperCam 是唯一一款能够实现具有竞争力的准确度，同时保持具有竞争力的内存占用和推理延迟，满足低功耗摄像系统的资源要求的分类器。]]></description>
      <guid>https://arxiv.org/abs/2501.10547</guid>
      <pubDate>Wed, 22 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>实例分解在视频预测模型中的优势</title>
      <link>https://arxiv.org/abs/2501.10562</link>
      <description><![CDATA[arXiv:2501.10562v1 公告类型：新
摘要：视频预测是机器人和自动驾驶汽车等智能代理的一项关键任务，因为它使它们能够预测时间关键事件并尽早采取行动。最先进的视频预测方法通常联合且隐式地对场景的动态进行建模，而无需显式分解为单独的对象。这很有挑战性并且可能不是最优的，因为动态场景中的每个对象都有自己的运动模式，通常在某种程度上独立于其他对象。在本文中，我们研究了在潜在变压器视频预测模型的背景下分别显式建模动态场景中的对象的好处。我们对合成和现实世界数据集进行了详细且精心控制的实验；我们的结果表明，与缺乏这种分解的类似容量的模型相比，分解动态场景可以产生更高质量的预测。]]></description>
      <guid>https://arxiv.org/abs/2501.10562</guid>
      <pubDate>Wed, 22 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 Planet NICFI 图像和基于 LiDAR 的 U-Net 模型对亚马逊森林进行高分辨率树高测绘</title>
      <link>https://arxiv.org/abs/2501.10600</link>
      <description><![CDATA[arXiv:2501.10600v1 公告类型：新
摘要：树冠高度是森林生物量、生产力和生态系统结构的最重要指标之一，但从地面和太空进行准确测量具有挑战性。在这里，我们使用了一个适合回归的 U-Net 模型，从 Planet NICFI 图像中绘制了 2020-2024 年期间亚马逊森林的平均树冠高度，空间分辨率约为 4.78 米。U-Net 模型使用从航空激光雷达数据计算出的树冠高度模型作为参考，以及它们对应的 Planet NICFI 图像进行训练。验证样本上的树高预测平均误差为 3.68 米，并且在亚马逊森林中存在的整个树高范围内显示出相对较低的系统偏差。我们的模型成功地估计了高达 40-50 米的树冠高度，而没有太多饱和度，优于该地区现有的全球模型的树冠高度产品。我们确定亚马逊森林的平均树冠高度约为 22 米。伐木或砍伐森林等事件可以通过树高变化来检测，并且取得了令人鼓舞的结果来监测再生森林的高度。这些发现表明，使用 Planet NICFI 图像可以对亚马逊老森林和再生森林进行大规模测绘和树高监测。]]></description>
      <guid>https://arxiv.org/abs/2501.10600</guid>
      <pubDate>Wed, 22 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>当语言和视觉遇到道路安全：利用多模态大型语言模型进行基于视频的交通事故分析</title>
      <link>https://arxiv.org/abs/2501.10604</link>
      <description><![CDATA[arXiv:2501.10604v1 公告类型：新
摘要：24/7/365 时间尺度上不断增加的交通视频可用性具有增加交通事故时空覆盖范围的巨大潜力，这将有助于提高交通安全。然而，在 24/7/365 工作协议中分析数百甚至数千个交通摄像头的镜头仍然是一项极具挑战性的任务，因为当前基于视觉的方法主要侧重于提取原始信息，例如车辆轨迹或单个物体检测，但需要费力的后处理才能得出可行的见解。我们提出了 SeeUnsafe，这是一个集成多模态大型语言模型 (MLLM) 代理的新框架，可将基于视频的交通事故分析从传统的提取然后解释的工作流程转变为更具交互性的对话方法。这种转变通过自动执行视频分类和视觉基础等复杂任务显着提高了处理吞吐量，同时通过无缝调整以适应不同的交通场景和用户定义的查询来提高适应性。我们的框架采用基于严重程度的聚合策略来处理不同长度的视频，并采用新颖的多模态提示来生成结构化响应以供审查和评估，并实现细粒度的视觉基础。我们引入了 IMS（信息匹配分数），这是一种基于 MLLM 的新指标，用于将结构化响应与基本事实对齐。我们在丰田 Woven 交通安全数据集上进行了广泛的实验，证明 SeeUnsafe 通过利用现成的 MLLM 有效地执行事故感知视频分类和视觉基础。源代码将在 \url{https://github.com/ai4ce/SeeUnsafe} 上提供。]]></description>
      <guid>https://arxiv.org/abs/2501.10604</guid>
      <pubDate>Wed, 22 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于增强主动脉分割的分层 LoG 贝叶斯神经网络</title>
      <link>https://arxiv.org/abs/2501.10615</link>
      <description><![CDATA[arXiv:2501.10615v1 公告类型：新
摘要：准确分割主动脉及其相关的弓形分支对于诊断主动脉疾病至关重要。虽然深度学习技术已显着改善了主动脉分割，但由于复杂的多尺度结构和周围组织的复杂性，它们仍然具有挑战性。本文提出了一种使用基于贝叶斯神经网络的分层拉普拉斯高斯 (LoG) 模型增强主动脉分割的新方法。我们的模型由 3D U-Net 流和分层 LoG 流组成：前者提供初始主动脉分割，后者通过学习合适的 LoG 核增强不同尺度的血管检测，从而能够自适应处理具有显着尺度差异的主动脉血管的不同部分。我们采用贝叶斯方法参数化 LoG 流并为分割结果提供置信区间，确保血管医学图像分析师的预测稳健可靠。实验结果表明，我们的模型可以准确分割主动脉和主动脉上血管，在从两个主动脉数据集中提取的多个体积中，Dice 系数比最先进的方法至少高出 3%，并且可以为主动脉的不同部分提供可靠的置信区间。代码可在 https://github.com/adlsn/LoGBNet 上找到。]]></description>
      <guid>https://arxiv.org/abs/2501.10615</guid>
      <pubDate>Wed, 22 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一种资源高效的遥感文本图像检索训练框架</title>
      <link>https://arxiv.org/abs/2501.10638</link>
      <description><![CDATA[arXiv:2501.10638v1 公告类型：新
摘要：遥感文本-图像检索（RSTIR）旨在根据描述性文本从数据库中检索匹配的遥感（RS）图像。最近，大型视觉语言预训练模型的快速发展为 RSTIR 提供了新的见解。然而，随着 RSTIR 中模型的复杂性增加，先前的研究在迁移学习过程中存在资源效率不理想的问题。为了解决这个问题，我们为 RSTIR 提出了一种计算和内存高效的检索（CMER）框架。为了减少训练内存消耗，我们提出了采用侧支结构的 Focus-Adapter 模块。它的焦点层抑制了背景像素对小目标的干扰。同时，为了提高数据效率，我们将 RS 场景类别视为元数据并设计了一种简洁的增强技术。场景标签增强利用了土地覆盖类别的先验知识并缩小了搜索空间。我们提出了负样本回收策略，使负样本池与小批量大小脱钩。它在不引入额外编码器的情况下提高了泛化性能。我们在公共数据集上进行了定量和定性实验，并使用一些高级方法扩展了基准，证明了所提出的 CMER 的竞争力。与最近的先进方法相比，CMER 在 RSITMD 上的整体检索性能提高了 2%--5%。此外，我们提出的方法将内存消耗降低了 49%，并且在训练期间具有 1.4 倍的数据吞吐量。CMER 和数据集的代码将在 https://github.com/ZhangWeihang99/CMER 上发布。]]></description>
      <guid>https://arxiv.org/abs/2501.10638</guid>
      <pubDate>Wed, 22 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ClusterViG：通过图像分区实现高效的全局感知视觉 GNN</title>
      <link>https://arxiv.org/abs/2501.10640</link>
      <description><![CDATA[arXiv:2501.10640v1 公告类型：新
摘要：卷积神经网络 (CNN) 和视觉变换器 (ViT) 主导了计算机视觉 (CV) 领域。图神经网络 (GNN) 在不同领域表现非常出色，因为它们可以通过非结构化图表示复杂的关系。然而，直到引入视觉 GNN (ViG) 之前，GNN 在视觉任务中的适用性尚未得到探索。尽管 ViG 取得了成功，但由于昂贵的基于 $k$-最近邻 ($k$-NN) 的图构造，它们的性能受到严重瓶颈限制。最近解决这一瓶颈的研究对 GNN 构建非结构化图的灵活性施加了限制，削弱了它们的核心优势，同时引入了额外的低效率。为了解决这些问题，在本文中，我们提出了一种称为动态高效图卷积 (DEGC) 的新方法来设计高效且全局感知的 ViG。 DEGC 对输入图像进行分区并为每个分区并行构建图形，从而提高图形构建效率。此外，DEGC 集成了局部图内和全局图间特征学习，从而增强了全局上下文感知。使用 DEGC 作为构建块，我们为 CV 任务提出了一种新颖的 CNN-GNN 架构 ClusterViG。大量实验表明，与具有相似模型参数数量的 ViG、ViHGNN、PVG 和 GreedyViG 等一系列模型相比，ClusterViG 将视觉任务的端到端推理延迟减少了高达 $5\times$。此外，ClusterViG 在图像分类、对象检测和实例分割任务上达到了最先进的性能，证明了所提出的全局感知学习策略的有效性。最后，DEGC 执行的输入分区使 ClusterViG 能够在更高分辨率图像上进行有效训练，凸显了我们方法的可扩展性。]]></description>
      <guid>https://arxiv.org/abs/2501.10640</guid>
      <pubDate>Wed, 22 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多模态法学硕士能进行视觉时间理解和推理吗？答案是不能！</title>
      <link>https://arxiv.org/abs/2501.10674</link>
      <description><![CDATA[arXiv:2501.10674v1 公告类型：新 
摘要：多模态大型语言模型 (MLLM) 通过利用基础大型语言模型 (LLM) 在视觉问答 (VQA) 等任务中取得了重大进展。然而，它们在特定领域的能力，例如对于理解现实世界动态至关重要的时间理解，仍未得到充分探索。为了解决这个问题，我们提出了一个具有挑战性的评估基准，名为 TemporalVQA，由两部分组成：(1) 时间顺序理解和 (2) 延时估计。第一部分要求 MLLM 通过分析时间连续的视频帧来确定事件的顺序。第二部分以多项选择题的形式呈现具有不同时间差的图像对，要求 MLLM 估计图像之间的延时，选项范围从几秒到几年。我们对包括 GPT-4o 和 Gemini-1.5-Pro 等模型在内的高级 MLLM 的评估揭示了重大挑战：GPT-4o 在时间顺序任务中的平均一致准确率仅为 43.8%，在延时估计中的平均一致准确率仅为 70%，而开源模型的表现甚至更差。这些发现凸显了当前 MLLM 在视觉时间理解和推理方面的局限性，凸显了进一步改进其时间能力的必要性。我们的数据集可以在 https://huggingface.co/datasets/fazliimam/temporal-vqa 找到。]]></description>
      <guid>https://arxiv.org/abs/2501.10674</guid>
      <pubDate>Wed, 22 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>EMO2：末端执行器引导的音频驱动头像视频生成</title>
      <link>https://arxiv.org/abs/2501.10687</link>
      <description><![CDATA[arXiv:2501.10687v1 公告类型：新
摘要：在本文中，我们提出了一种新颖的音频驱动说话头部方法，能够同时生成富有表现力的面部表情和手势。与专注于生成全身或半身姿势的现有方法不同，我们研究了同步语音手势生成的挑战，并确定音频特征和全身手势之间的弱对应性是一个关键限制。为了解决这个问题，我们将任务重新定义为一个两阶段过程。在第一阶段，我们直接从音频输入生成手势，利用音频信号和手部动作之间的强相关性。在第二阶段，我们采用扩散模型来合成视频帧，结合第一阶段生成的手势来产生逼真的面部表情和身体动作。我们的实验结果表明，所提出的方法在视觉质量和同步精度方面都优于 Cyber​​Host 和 Vlogger 等最先进的方法。这项工作为音频驱动的手势生成提供了新的视角，并为创建富有表现力和自然的头部动画提供了强大的框架。]]></description>
      <guid>https://arxiv.org/abs/2501.10687</guid>
      <pubDate>Wed, 22 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于视频时刻检索和高亮检测的多模态融合和查询细化网络</title>
      <link>https://arxiv.org/abs/2501.10692</link>
      <description><![CDATA[arXiv:2501.10692v1 公告类型：新
摘要：给定一个视频和一个语言查询，视频时刻检索和亮点检测 (MR&amp;HD) 旨在定位所有相关跨度，同时预测显着性分数。大多数现有方法都使用 RGB 图像作为输入，忽略了固有的多模态视觉信号，如光流和深度。在本文中，我们提出了一种多模态融合和查询细化网络 (MRNet) 来从多模态线索中学习互补信息。具体来说，我们设计了一个多模态融合模块来动态组合 RGB、光流和深度图。此外，为了模拟人类对句子的理解，我们引入了一个查询细化模块，该模块合并不同粒度的文本，包含单词、短语和句子级别。在 QVHighlights 和 Charades 数据集上进行的综合实验表明，MRNet 的表现优于当前最先进的方法，在 QVHighlights 上的 MR-mAP@Avg (+3.41) 和 HD-HIT@1 (+3.46) 方面取得了显着的改进。]]></description>
      <guid>https://arxiv.org/abs/2501.10692</guid>
      <pubDate>Wed, 22 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>探索可迁移同质群用于组合零样本学习</title>
      <link>https://arxiv.org/abs/2501.10695</link>
      <description><![CDATA[arXiv:2501.10695v1 公告类型：新
摘要：条件依赖性是组合零样本学习中最棘手的问题之一，导致同一状态（对象）在不同对象（状态）之间的属性差异很大。为了解决这个问题，现有的方法通常采用一对一或一对一的表示范式。然而，这些极端情况造成了可转移性和可辨别性之间的不平衡，偏向一方而牺牲另一方。相比之下，人类擅长以层次聚类的方式进行类比和推理，直观地将具有相似属性的类别分组以形成有凝聚力的概念。受此启发，我们提出了同质组表示学习（HGRL），一种将状态（对象）表示学习表述为多个同质子组表示学习的新视角。 HGRL 力求通过自适应地发现和聚合具有共享属性的类别，学习保留特定于组的判别特征的分布式组中心，从而实现语义可迁移性和可判别性之间的平衡。我们的方法集成了三个核心组件，旨在同时增强模型的视觉和快速表示能力。在三个基准数据集上进行的大量实验验证了我们方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2501.10695</guid>
      <pubDate>Wed, 22 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>图片内容是:医学成像数据集、人工制品及其实时回顾</title>
      <link>https://arxiv.org/abs/2501.10727</link>
      <description><![CDATA[arXiv:2501.10727v1 公告类型：新
摘要：数据集在医学成像研究中起着关键作用，但标签质量、捷径和元数据等问题经常被忽视。这种缺乏关注可能会损害算法的普遍性，从而对患者结果产生负面影响。虽然现有的医学成像文献综述主要关注机器学习 (ML) 方法，只有少数关注特定应用的数据集，但这些综述仍然是静态的——它们只发布一次，之后就不会更新。这无法解释新出现的证据，例如偏见、捷径和其他研究人员在数据集发布后可能贡献的额外注释。我们将这些新发现的数据集结果称为研究成果。为了解决这一差距，我们提出了一种实时审查，它可以持续跟踪多个医学成像应用程序中的公共数据集及其相关的研究成果。我们的方法包括一个用于实时审查的框架来监控数据文档成果，以及一个 SQL 数据库来可视化研究成果和数​​据集之间的引用关系。最后，我们讨论了创建医学成像数据集的关键考虑因素，回顾了数据注释的最佳实践，讨论了捷径和人口多样性的重要性，并强调了在整个生命周期内管理数据集的重要性。我们的演示可在 http://130.226.140.142 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2501.10727</guid>
      <pubDate>Wed, 22 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于纵向 3D MRI 图像分类的 CNN-Transformer——肝细胞癌预测案例研究</title>
      <link>https://arxiv.org/abs/2501.10733</link>
      <description><![CDATA[arXiv:2501.10733v2 公告类型：新
摘要：纵向 MRI 分析对于预测疾病结果至关重要，特别是在肝细胞癌 (HCC) 等慢性疾病中，早期发现可以显著影响治疗策略和患者预后。然而，由于数据可用性有限、实质变化细微以及医疗筛查时间不规律等挑战，目前的方法迄今为止一直集中在横断面成像数据上。为了解决这个问题，我们提出了 HCCNet，这是一种新颖的模型架构，它将 ConvNeXt CNN 架构的 3D 改编与 Transformer 编码器相结合，可以捕获 3D MRI 的复杂空间特征和不同时间点的复杂时间依赖性。HCCNet 采用针对纵向 MRI 数据量身定制的两阶段预训练过程。 CNN 主干使用适用于 3D MRI 的自监督学习框架进行预训练，而 Transformer 编码器则使用序列顺序预测任务进行预训练，以增强其对疾病随时间进展的理解。我们将 HCCNet 应用于一组接受定期 MRI 筛查以监测 HCC 的肝硬化患者，证明了其有效性。我们的结果表明，与基线模型相比，HCCNet 显著提高了预测准确性和可靠性，为个性化 HCC 监测提供了强大的工具。本文提出的方法用途广泛，可适用于各种纵向 MRI 筛查应用。它能够处理不同的患者记录长度和不规则的筛查间隔，这使其成为监测慢性疾病的宝贵框架，及时准确的疾病预后对于有效的治疗计划至关重要。]]></description>
      <guid>https://arxiv.org/abs/2501.10733</guid>
      <pubDate>Wed, 22 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>