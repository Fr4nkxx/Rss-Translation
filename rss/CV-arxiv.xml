<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Wed, 20 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>DiHuR：扩散引导的可泛化人体重建</title>
      <link>https://arxiv.org/abs/2411.11903</link>
      <description><![CDATA[arXiv:2411.11903v1 公告类型：新
摘要：我们介绍了 DiHuR，这是一种新型的扩散引导模型，用于从稀疏、最小重叠图像中进行可推广的人体 3D 重建和视图合成。虽然现有的可推广的人体辐射场在新型视图合成方面表现出色，但它们往往难以进行全面的 3D 重建。同样，直接优化来自稀疏视图图像的隐式有符号距离函数 (SDF) 场通常会因重叠有限而产生较差的结果。为了提高 3D 重建质量，我们建议使用与 SMPL 顶点相关的可学习标记来聚合稀疏视图特征，然后指导 SDF 预测。这些标记在训练数据集中学习跨不同身份的可推广先验，利用 SMPL 顶点在不同人类身份之间的相似语义区域的一致投影。这种一致性使得在推理过程中能够有效地将知识转移到看不见的身份。认识到 SMPL 在捕捉服装细节方面的局限性，我们结合了扩散模型作为补充先验，以填补缺失信息，尤其是对于复杂的服装几何形状。我们的方法以连贯的方式整合了两个关键先验：来自可泛化前馈模型的先验和 2D 扩散先验，并且只需要多视图图像训练，而无需 3D 监督。与现有方法相比，DiHuR 在数据集内和跨数据集泛化设置中都表现出色，这在 THuman、ZJU-MoCap 和 HuMMan 数据集上得到了验证。]]></description>
      <guid>https://arxiv.org/abs/2411.11903</guid>
      <pubDate>Wed, 20 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GeoGround：统一的大型视觉语言模型。用于遥感视觉接地</title>
      <link>https://arxiv.org/abs/2411.11904</link>
      <description><![CDATA[arXiv:2411.11904v1 公告类型：新
摘要：遥感 (RS) 视觉接地旨在使用自然语言表达来定位 RS 图像中的特定对象（以边界框或分割掩码的形式），增强人机与智能 RS 解释系统的交互。该领域的早期研究主要基于水平边界框 (HBB)，但随着越来越多样化的 RS 数据集的出现，出现了涉及定向边界框 (OBB) 和分割掩码的任务。在实际应用中，不同的目标需要不同的接地类型：HBB 可以定位物体的位置，OBB 提供其方向，而掩码则描绘其形状。然而，现有的专门方法通常针对单一类型的 RS 视觉接地任务进行定制，很难在任务之间推广。相比之下，大型视觉语言模型 (VLM) 表现出强大的多任务学习能力，但难以处理像分割这样的密集预测任务。本文提出了 GeoGround，这是一个统一支持 HBB、OBB 和 mask RS 视觉接地任务的新框架，允许灵活地选择输出。我们的工作不是定制 VLM 的架构，而是旨在通过 Text-Mask 技术优雅地支持像素级视觉接地输出。我们定义了提示辅助和几何引导学习，以增强不同信号之间的一致性。为了支持模型训练，我们提出了 refGeo，这是一个包含 161k 个图像文本对的大规模 RS 视觉指令跟踪数据集。实验结果表明，GeoGround 在四个 RS 视觉接地任务中表现出色，在多个基准测试中达到或超越了专门方法的性能。代码可在 https://github.com/zytx121/GeoGround 上找到]]></description>
      <guid>https://arxiv.org/abs/2411.11904</guid>
      <pubDate>Wed, 20 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>$\text{S}^{3}$Mamba：通过可扩展状态空间模型实现任意尺度超分辨率</title>
      <link>https://arxiv.org/abs/2411.11906</link>
      <description><![CDATA[arXiv:2411.11906v1 公告类型：新
摘要：任意尺度超分辨率 (ASSR) 旨在使用单一模型将低分辨率图像超分辨率为任意尺度的高分辨率图像，解决传统超分辨率方法仅限于固定尺度因子（例如，$\times2$、$\times4$）的局限性。隐式神经表征 (INR) 的出现为 ASSR 带来了大量新方法，这些方法通过为坐标和像素值建模连续表示空间来促进原始连续信号的重建，从而实现任意尺度的超分辨率。因此，ASSR 的主要目标是构建一个来自低分辨率输入的连续表示空间。然而，现有的方法主要基于 CNN 和 Transformers，面临着计算复杂度高和长距离依赖关系建模不足等重大挑战，这阻碍了它们在实际应用中的有效性。为了克服这些限制，我们提出了一种新的任意尺度超分辨率方法，称为$\text{S}^{3}$Mamba，以构建可扩展的连续表示空间。具体而言，我们提出了一个可扩展状态空间模型（SSSM）来在离散化过程中调节状态转移矩阵和步长的采样矩阵，实现具有线性计算复杂度的可扩展连续表示建模。此外，我们提出了一种新颖的尺度感知自注意力机制，进一步增强网络感知不同尺度全局重要特征的能力，从而构建了$\text{S}^{3}$Mamba 以实现卓越的任意尺度超分辨率。在合成和真实基准上进行的大量实验表明，我们的方法在任意超分辨率尺度上实现了最先进的性能和卓越的泛化能力。]]></description>
      <guid>https://arxiv.org/abs/2411.11906</guid>
      <pubDate>Wed, 20 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SymDPO：通过符号演示直接偏好优化促进大型多模态模型的上下文学习</title>
      <link>https://arxiv.org/abs/2411.11909</link>
      <description><![CDATA[arXiv:2411.11909v1 公告类型：新
摘要：随着语言模型的不断扩展，大型语言模型 (LLM) 在上下文学习 (ICL) 中展现出了新兴的能力，使它们能够通过将一些上下文演示 (ICD) 作为上下文前缀来解决语言任务。受这些进步的启发，研究人员扩展了这些技术以开发具有 ICL 功能的大型多模态模型 (LMM)。然而，现有的 LMM 面临一个关键问题：它们通常无法有效利用多模态演示中的视觉上下文，而是简单地遵循文本模式。这表明 LMM 无法实现多模态演示和模型输出之间的有效对齐。为了解决这个问题，我们提出了符号演示直接偏好优化 (SymDPO)。具体来说，SymDPO 旨在打破传统的构建多模态演示的范式，即使用随机符号替换实例中的文本答案。这迫使模型仔细理解演示图像并建立图像和符号之间的关系以正确回答问题。我们在多个基准上验证了该方法的有效性，表明借助 SymDPO，LMM 可以更有效地理解示例中的多模式上下文，并利用这些知识更好地回答问题。]]></description>
      <guid>https://arxiv.org/abs/2411.11909</guid>
      <pubDate>Wed, 20 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>F$^3$OCUS——通过多目标元启发式方法对视觉语言基础模型进行联合微调，实现最佳客户端层更新策略</title>
      <link>https://arxiv.org/abs/2411.11912</link>
      <description><![CDATA[arXiv:2411.11912v1 公告类型：新
摘要：在联邦学习 (FL) 中，在资源受限的客户端设备上有效训练大型视觉语言模型 (VLM) 需要使用参数高效的微调 (PEFT) 策略。为此，我们展示了两个因素的影响，即客户端特定的层重要性得分，用于选择最重要的 VLM 层进行微调，以及客户端间层多样性得分，用于鼓励跨客户端进行多样化的层选择以实现最佳 VLM 层选择。我们首先从理论上激发和利用分层神经切线核的主特征值幅度，并展示其作为客户端特定的层重要性得分的有效性。接下来，我们提出了一种称为 F$^3$OCUS 的新型层更新策略，通过在服务器上采用无数据、多目标、元启发式优化来联合优化层重要性和多样性因素。我们探索了 5 种不同的元启发式算法，并比较了它们在选择 PEFT-FL 的模型层和适配器层方面的有效性。此外，我们发布了一个新的 MedVQA-FL 数据集，该数据集涉及总共 707,962 个 VQA 三元组和 9 个特定于模态的客户端，并利用它来训练和评估我们的方法。总体而言，我们在 6 个视觉语言 FL 任务设置上进行了 10,000 多次客户端级实验，涉及 58 个医学图像数据集和 4 种不同大小的 VLM 架构，以证明所提方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2411.11912</guid>
      <pubDate>Wed, 20 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FCC：用于小样本分割的全连接相关</title>
      <link>https://arxiv.org/abs/2411.11917</link>
      <description><![CDATA[arXiv:2411.11917v1 公告类型：新
摘要：小样本分割 (FSS) 旨在仅使用一小组支持图像和蒙版来分割查询图像中的目标对象。因此，使用支持集获得目标对象的强大先验信息对于指导 FSS 的初始训练至关重要，这可以在具有挑战性的情况下成功实现小样本分割，例如当目标对象在支持和查询图像中的外观、纹理或尺度表现出相当大的变化时。以前的方法试图通过从最终层或同一层特征的像素级相关性创建相关性图来获取先验信息。然而，我们发现当使用 Vision Transformers 等高级模型作为主干时，这些方法只能提供有限的部分信息。Vision Transformer 编码器具有多层结构，中间层形状相同。利用编码器中所有层的特征比较可以提高小样本分割的性能。我们引入了 FCC（全连接相关）来整合支持和查询特征之间的像素级相关性，捕获揭示同层和跨层中目标特定模式和对应关系的关联。FCC 捕获以前无法访问的目标信息，有效地解决了支持掩码的局限性。我们的方法在 PASCAL、COCO 和域移位测试中始终表现出最先进的性能。我们进行了消融研究和跨层相关性分析，以验证 FCC 的核心方法。这些发现揭示了 FCC 在增强先验信息和整体模型性能方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2411.11917</guid>
      <pubDate>Wed, 20 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>VL-Uncertainty：通过不确定性估计检测大型视觉语言模型中的幻觉</title>
      <link>https://arxiv.org/abs/2411.11919</link>
      <description><![CDATA[arXiv:2411.11919v1 公告类型：新 
摘要：与单模 LLM 相比，大型视觉语言模型 (LVLM) 处理的信息量更大，检测 LVLM 幻觉需要更多的人力和时间投入，从而引发更广泛的安全问题。在本文中，我们介绍了 VL-Uncertainty，这是第一个基于不确定性的 LVLM 幻觉检测框架。与大多数需要基本事实或伪注释的现有方法不同，VL-Uncertainty 利用不确定性作为内在度量。我们通过分析语义等效但受干扰的提示（包括视觉和文本数据）之间的预测方差来衡量不确定性。当 LVLM 高度自信时，它们会对语义等效的查询提供一致的响应。但是，当不确定时，目标 LVLM 的响应会变得更加随机。考虑到语义相似但措辞不同的答案，我们根据语义内容对 LVLM 响应进行聚类，然后计算聚类分布熵作为检测幻觉的不确定性度量。我们在四个基准上对 10 个 LVLM 进行了广泛的实验，涵盖了自由形式和多项选择任务，结果表明 VL-Uncertainty 在幻觉检测方面的表现明显优于强大的基线方法。]]></description>
      <guid>https://arxiv.org/abs/2411.11919</guid>
      <pubDate>Wed, 20 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DeSiRe-GS：用于城市驾驶场景静态动态分解和表面重建的 4D 街道高斯</title>
      <link>https://arxiv.org/abs/2411.11921</link>
      <description><![CDATA[arXiv:2411.11921v1 公告类型：新
摘要：我们提出了一种自监督高斯分布表示法 DeSiRe-GS，能够在复杂的驾驶场景中实现有效的静态动态分解和高保真表面重建。我们的方法采用了动态街道高斯的两阶段优化流程。在第一阶段，我们根据 3D 高斯分布本身只能重建动态环境中的静态区域的观察结果提取 2D 运动蒙版。然后，这些提取的 2D 运动先验以可微分的方式映射到高斯空间中，在第二阶段利用动态高斯的有效公式。结合引入的几何正则化，我们的方法能够解决自动驾驶中数据稀疏引起的过度拟合问题，重建与物体表面对齐而不是漂浮在空中的物理上合理的高斯。此外，我们引入了时间跨视图一致性，以确保跨时间和视点的一致性，从而实现高质量的表面重建。全面的实验证明了 DeSiRe-GS 的效率和有效性，超越了之前的自监督技术，并达到了与依赖外部 3D 边界框注释的方法相当的准确度。代码可在 \url{https://github.com/chengweialan/DeSiRe-GS} 获得]]></description>
      <guid>https://arxiv.org/abs/2411.11921</guid>
      <pubDate>Wed, 20 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SAMURAI：利用运动感知记忆技术，调整任意片段模型，实现零样本视觉追踪</title>
      <link>https://arxiv.org/abs/2411.11922</link>
      <description><![CDATA[arXiv:2411.11922v1 公告类型：新
摘要：Segment Anything Model 2 (SAM 2) 在对象分割任务中表现出色，但在视觉对象跟踪方面面临挑战，特别是在管理具有快速移动或自我遮挡对象的拥挤场景时。此外，原始模型中的固定窗口记忆方法没有考虑选择用于调节下一帧图像特征的记忆的质量，导致视频中的错误传播。本文介绍了 SAMURAI，这是专为视觉对象跟踪而设计的 SAM 2 的增强版。通过将时间运动线索与所提出的运动感知记忆选择机制相结合，SAMURAI 可以有效地预测物体运动并改进掩码选择，实现稳健、准确的跟踪，而无需重新训练或微调。SAMURAI 实时运行，并在各种基准数据集中表现出强大的零样本性能，展示了其无需微调即可泛化的能力。在评估中，SAMURAI 的成功率和精度显著高于现有跟踪器，在 LaSOT$_{\text{ext}}$ 上 AUC 增益为 7.1%，在 GOT-10k 上 AO 增益为 3.5%。此外，与 LaSOT 上的全监督方法相比，它取得了具有竞争力的结果，凸显了其在复杂跟踪场景中的稳健性以及在动态环境中的实际应用潜力。代码和结果可在 https://github.com/yangchris11/samurai 上找到。]]></description>
      <guid>https://arxiv.org/abs/2411.11922</guid>
      <pubDate>Wed, 20 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于自回归图像生成的连续推测解码</title>
      <link>https://arxiv.org/abs/2411.11925</link>
      <description><![CDATA[arXiv:2411.11925v1 公告类型：新
摘要：连续值自回归 (AR) 图像生成模型已显示出比离散标记模型显著的优势，展示了相当好的重建质量和更高的生成保真度。然而，自回归框架的计算需求导致了显著的推理开销。虽然推测解码已被证明可以有效加速大型语言模型 (LLM)，但它们对连续值视觉自回归模型的适应性仍未得到探索。这项工作将推测解码算法从离散标记推广到连续空间。通过分析输出分布的内在属性，我们为此类模型中普遍存在的扩散分布建立了量身定制的接受标准。为了克服推测解码输出分布中出现的不一致性，我们引入了去噪轨迹对齐和标记预填充方法。此外，我们在拒绝阶段确定了难以采样的分布。为了缓解这个问题，我们提出了一种细致的接受-拒绝采样方法，并设置了适当的上限，从而避免了复杂的集成。实验结果表明，我们的连续推测解码在现成的模型上实现了显著的 2.33 倍加速，同时保持了输出分布。代码将在 https://github.com/MarkXCloud/CSpD 上提供]]></description>
      <guid>https://arxiv.org/abs/2411.11925</guid>
      <pubDate>Wed, 20 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>KAN-Mamba FusionNet：利用非线性建模重新定义医学图像分割</title>
      <link>https://arxiv.org/abs/2411.11926</link>
      <description><![CDATA[arXiv:2411.11926v1 公告类型：新
摘要：医学图像分割在机器人手术、疾病诊断和治疗计划中至关重要。本研究提出了一种创新方法，将 Kolmogorov-Arnold 网络 (KAN) 与经过调整的 Mamba 层相结合，用于医学图像分割。提出的 KAN-Mamba FusionNet 框架通过将注意力驱动机制与卷积并行训练和自回归部署相结合来改进图像分割，同时保留可解释性，这与最先进的技术完全依赖 Mamba 进行疾病定位和准确诊断形成鲜明对比。我们在三个不同的医学图像分割数据集 BUSI、Kvasir-Seg 和 GlaS 上评估了我们提出的 KAN-Mamba FusionNet 模型。结果表明，与最先进的方法相比，KAN-Mamba FusionNet 始终能产生更好的 IoU 和 F1 分数。此外，我们通过消融研究深入了解模型的行为，检查各种组件的影响并评估它们对所提模型整体性能的贡献。研究结果表明，这种方法在可靠的医学图像分割方面具有优势和有效性，为解决医疗保健领域复杂的视觉数据问题提供了一种独特的方法。]]></description>
      <guid>https://arxiv.org/abs/2411.11926</guid>
      <pubDate>Wed, 20 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FLAME：冻结大型语言模型可实现数据高效的语言图像预训练</title>
      <link>https://arxiv.org/abs/2411.11927</link>
      <description><![CDATA[arXiv:2411.11927v1 公告类型：新 
摘要：由于特定格式的数据有限以及文本编码器的容量受限，语言图像预训练面临重大挑战。虽然现行方法试图通过数据增强和架构修改来解决这些问题，但它们在处理长格式文本输入方面仍然举步维艰，而传统 CLIP 文本编码器的固有局限性导致下游泛化不理想。在本文中，我们提出了 FLAME（冻结的大型语言模型实现数据高效的语言图像预训练），它利用冻结的大型语言模型作为文本编码器，自然处理长文本输入并展示令人印象深刻的多语言泛化。FLAME 包含两个关键组件：1）一种多方面的快速蒸馏技术，用于从长标题中提取不同的语义表示，这更好地符合图像的多面性；2）一种方面解耦的注意机制，辅以离线嵌入策略，以确保高效计算。大量的实证评估证明了 FLAME 的卓越性能。在 CC3M 上进行训练时，FLAME 在 ImageNet top-1 准确率上比之前最先进的技术高出 4.9\%。在 YFCC15M 上，FLAME 在 36 种语言的平均图像到文本召回率@1 上比 WIT-400M 训练的 CLIP 高出 44.4\%，在 Urban-1k 上的长上下文检索的文本到图像召回率@1 上比 WIT-400M 训练的 CLIP 高出 34.6\%。代码可在 \url{https://github.com/MIV-XJTU/FLAME} 上找到。]]></description>
      <guid>https://arxiv.org/abs/2411.11927</guid>
      <pubDate>Wed, 20 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AtomThink：多模态数学推理的慢思考框架</title>
      <link>https://arxiv.org/abs/2411.11930</link>
      <description><![CDATA[arXiv:2411.11930v1 公告类型：新
摘要：在本文中，我们通过将“慢思考”的能力融入多模态大型语言模型（MLLM）来解决多模态数学推理这一具有挑战性的任务。与依赖直接或快速思考的现有方法相反，我们的关键思想是逐步构建由原子动作组成的长思维链（CoT），引导 MLLM 进行复杂的推理。为此，我们设计了一个由三个关键模块组成的新型 AtomThink 框架：（i）CoT 注释引擎，可自动生成高质量的 CoT 注释，以解决缺乏高质量视觉数学数据的问题；（ii）原子步骤微调策略，联合优化 MLLM 和策略奖励模型（PRM）进行分步推理；（iii）可与 PRM 一起应用以完成推理的四种不同的搜索策略。此外，我们提出了 AtomMATH，一个大规模的长 CoT 多模态数据集，以及数学任务的原子能力评估指标。大量实验结果表明，所提出的 AtomThink 显著提高了基线 MLLM 的性能，在 MathVista 上实现了约 50% 的相对准确率提升，在 MathVerse 上实现了 120% 的相对准确率提升。为了支持多模态慢思考模型的进步，我们将在 https://github.com/Quinn777/AtomThink 上公开我们的代码和数据集。]]></description>
      <guid>https://arxiv.org/abs/2411.11930</guid>
      <pubDate>Wed, 20 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SpatialDreamer：基于单目输入的自监督立体视频合成</title>
      <link>https://arxiv.org/abs/2411.11934</link>
      <description><![CDATA[arXiv:2411.11934v1 公告类型：新
摘要：从单目输入合成立体视频是空间计算和虚拟现实领域的一项艰巨任务。这项任务的主要挑战在于训练所需的高质量配对立体视频不足以及难以保持帧之间的时空一致性。现有方法主要通过将新颖的视图合成 (NVS) 技术直接应用于视频来解决这些问题，同时面临着无法有效表示动态场景和需要大量训练数据等限制。在本文中，我们通过视频扩散模型引入了一种新颖的自监督立体视频合成范式，称为 SpatialDreamer，它正面应对了挑战。首先，为了解决立体视频数据不足的问题，我们提出了一个基于深度的视频生成模块 DVG，它采用前向后向渲染机制来生成具有几何和时间先验的配对视频。利用 DVG 生成的数据，我们提出了 RefinerNet 以及一个自监督合成框架，旨在促进高效而专注的训练。更重要的是，我们设计了一个一致性控制模块，该模块由立体偏差强度度量和时间交互学习模块 TIL 组成，分别用于确保几何和时间一致性。我们根据各种基准方法对所提出的方法进行了评估，结果显示了其卓越的性能。]]></description>
      <guid>https://arxiv.org/abs/2411.11934</guid>
      <pubDate>Wed, 20 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于 LiDAR 场景语义分割的校准且高效的无采样置信度估计</title>
      <link>https://arxiv.org/abs/2411.11935</link>
      <description><![CDATA[arXiv:2411.11935v1 公告类型：新
摘要：可靠的深度学习模型不仅需要准确的预测，还需要经过良好校准的置信度估计，以确保可靠的不确定性估计。这对于自动驾驶等安全关键型应用至关重要，这些应用依赖于快速精确的 LiDAR 点云语义分割来实现实时 3D 场景理解。在这项工作中，我们引入了一种无采样方法来估计分类任务的经过良好校准的置信度值，实现与真实分类精度的对齐，并与基于采样的方法相比显着缩短推理时间。我们使用自适应校准误差 (ACE) 指标对 LiDAR 语义分割进行的评估表明，与采样基线相比，我们的方法在实现更快的处理速度的同时，保持了良好校准的置信度值。此外，可靠性图显示，我们的方法产生的是信心不足而不是过度自信的预测，这对安全关键型应用来说是一个优势。我们的无采样方法为 LiDAR 场景语义分割提供了经过良好校准且省时的预测。]]></description>
      <guid>https://arxiv.org/abs/2411.11935</guid>
      <pubDate>Wed, 20 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>