<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Mon, 22 Jul 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>通过学习音频和视觉重现实现无监督视频亮点检测</title>
      <link>https://arxiv.org/abs/2407.13933</link>
      <description><![CDATA[arXiv:2407.13933v1 公告类型：新
摘要：随着视频内容的指数级增长，对自动视频亮点检测的需求越来越迫切，以便从长视频中提取关键时刻或亮点。这项技术有可能通过允许快速访问不同领域的相关内容来显着增强用户体验。现有方法通常依赖于昂贵的手动标记的帧级注释，或依赖于通过类别信息进行弱监督的大量外部视频数据集。为了克服这个问题，我们专注于无监督视频亮点检测，从而无需手动注释。我们提出了一种创新的无监督方法，该方法利用了重要时刻往往会在音频和视觉模态中相似类别的多个视频中重复出现的前提。令人惊讶的是，尽管音频具有检测关键时刻的潜力，但其仍然未得到充分探索，尤其是在无监督算法中。通过聚类技术，我们识别视频的伪类别，并通过测量每个伪类别中所有视频的音频片段之间的音频特征相似性来计算每个视频的音频伪亮点分数。同样，我们还使用视觉特征计算每个视频的视觉伪亮点分数。随后，我们结合音频和视觉伪亮点来创建每个视频的视听伪真实亮点，以训练视听亮点检测网络。在三个亮点检测基准上进行的大量实验和消融研究表明，我们的方法比以前的工作具有更优越的性能。]]></description>
      <guid>https://arxiv.org/abs/2407.13933</guid>
      <pubDate>Mon, 22 Jul 2024 06:17:55 GMT</pubDate>
    </item>
    <item>
      <title>通过摄像头-雷达交叉检查增强在线 3D 多目标跟踪</title>
      <link>https://arxiv.org/abs/2407.13937</link>
      <description><![CDATA[arXiv:2407.13937v1 公告类型：新
摘要：在自动驾驶领域，基于来自不同传感器的数据的多模态感知技术的集成已取得实质性进展。通过传感器融合有效超越最先进的单模态检测器的能力仍然是一个活跃的挑战。这项工作利用了透视图中的摄像机和鸟瞰图 (BEV) 中的雷达各自的优势，大大提高了整体检测和跟踪性能。我们的方法，即摄像机-雷达关联融合跟踪助推器 (CRAFTBooster)，代表了在跟踪阶段增强雷达-摄像机融合的开创性努力，有助于提高 3D MOT 精度。在 K-Radaar 数据集上的优异实验结果，IDF1 跟踪性能增益为 5-6%，验证了有效的传感器融合在推进自动驾驶方面的潜力。]]></description>
      <guid>https://arxiv.org/abs/2407.13937</guid>
      <pubDate>Mon, 22 Jul 2024 06:17:55 GMT</pubDate>
    </item>
    <item>
      <title>合成反事实面孔</title>
      <link>https://arxiv.org/abs/2407.13922</link>
      <description><![CDATA[arXiv:2407.13922v1 公告类型：新
摘要：计算机视觉系统已部署在涉及人脸等生物识别技术的各种应用中。这些系统可以识别社交媒体用户、搜索失踪人员并验证个人身份。虽然计算机视觉模型通常会根据可用基准来评估其准确性，但需要更多带注释的数据来了解它们对输入数据（尤其是人脸数据）中语义分布变化的稳健性和公平性。在带注释的数据中，反事实示例具有很强的可解释性。由于收集自然人脸数据的成本过高，我们提出了一个基于 AI 的生成框架来构建有针对性的、反事实的、高质量的合成人脸数据。我们的合成数据管道有许多用例，包括人脸识别系统敏感性评估和图像理解系统探测。该管道通过多项用户研究进行了验证。我们在领先的商业视觉模型上展示了我们的人脸生成管道的有效性。我们识别导致视觉系统失败的面部属性。]]></description>
      <guid>https://arxiv.org/abs/2407.13922</guid>
      <pubDate>Mon, 22 Jul 2024 06:17:54 GMT</pubDate>
    </item>
    <item>
      <title>RT-Pose：基于 4D 雷达张量的 3D 人体姿势估计和定位基准</title>
      <link>https://arxiv.org/abs/2407.13930</link>
      <description><![CDATA[arXiv:2407.13930v1 公告类型：新
摘要：传统的人体定位和姿势估计 (HPE) 方法主要依靠 RGB 图像作为输入模态，由于隐私问题，在实际应用中面临很大的限制。相比之下，基于雷达的 HPE 方法成为一种有前途的替代方案，其特点是穿墙识别和隐私保护等独特属性，使该方法更有利于实际部署。本文介绍了一种基于雷达张量的人体姿势 (RT-Pose) 数据集和一个开源基准测试框架。RT-Pose 数据集包括 4D 雷达张量、LiDAR 点云和 RGB 图像，共收集了 240 个序列中的 72k 帧，具有六种不同的复杂度级别动作。4D 雷达张量提供原始时空信息，使其有别于其他基于雷达点云的数据集。我们开发了一种注释流程，使用 RGB 图像和 LiDAR 点云来准确标记 3D 人体骨骼。此外，我们提出了 HRRadarPose，这是第一个单阶段架构，可在 3D 空间中提取 4D 雷达张量的高分辨率表示，以帮助估计人体关键点。HRRadarPose 在 RT-Pose 基准上的表现优于之前基于雷达的 HPE 工作。HRRadarPose 在 RT-Pose 数据集上的整体性能（平均每个关节位置误差 (MPJPE) 为 9.91 厘米）表明，在复杂的现实场景中实现准确的 HPE 仍面临挑战。RT-Pose 可在 https://huggingface.co/datasets/uwipl/RT-Pose 上找到。]]></description>
      <guid>https://arxiv.org/abs/2407.13930</guid>
      <pubDate>Mon, 22 Jul 2024 06:17:54 GMT</pubDate>
    </item>
    <item>
      <title>关键点感知蒙版图像建模</title>
      <link>https://arxiv.org/abs/2407.13873</link>
      <description><![CDATA[arXiv:2407.13873v1 公告类型：新
摘要：SimMIM 是一种广泛使用的使用蒙版图像建模预训练视觉变换器的方法。然而，尽管它在微调性能方面取得了成功，但在用于线性探测时，它已被证明表现不佳。我们提出了一种从关键点特征中衍生的高效块状加权，它可以捕获局部信息并在 SimMIM 的重建阶段提供更好的上下文。我们的方法 KAMIM 在 ImageNet-1K 数据集上使用 ViT-B 进行测试时，在训练相同数量的 epoch 的情况下，将 top-1 线性探测准确率从 16.12% 提高到 33.97%，将微调准确率从 76.78% 提高到 77.3%。我们对不同的数据集、关键点提取器和模型架构进行了广泛的测试，并观察到块状加权增强了较大预训练数据集的线性探测性能。我们还分析了使用 KAMIM 训练的 ViT-B 的学习表征，并观察到它们的行为类似于对比学习，具有更长的注意距离和跨层的同质自注意力。我们的代码可在 https://github.com/madhava20217/KAMIM 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2407.13873</guid>
      <pubDate>Mon, 22 Jul 2024 06:17:53 GMT</pubDate>
    </item>
    <item>
      <title>持续蒸馏学习</title>
      <link>https://arxiv.org/abs/2407.13911</link>
      <description><![CDATA[arXiv:2407.13911v1 公告类型：新
摘要：我们研究了持续学习 (CL) 设置中考虑知识蒸馏 (KD) 的持续蒸馏学习 (CDL) 问题。教师模型和学生模型需要学习一系列任务，教师模型的知识将被蒸馏给学生以改进学生模型。我们介绍了一种名为 CDL-Prompt 的新方法，该方法利用基于提示的持续学习模型来构建师生模型。我们研究如何在学生模型中利用教师模型的提示进行知识蒸馏，并提出了一种基于注意力的提示映射方案，以将教师提示用于学生。我们证明我们的方法可以应用于不同的基于提示的持续学习模型，例如 L2P、DualPrompt 和 CODA-Prompt，以使用强大的教师模型来提高它们的性能。尽管最近的 CL 方法侧重于提示学习，但我们表明，我们的方法可用于使用基于提示的知识提炼来构建高效的 CL 模型。]]></description>
      <guid>https://arxiv.org/abs/2407.13911</guid>
      <pubDate>Mon, 22 Jul 2024 06:17:53 GMT</pubDate>
    </item>
    <item>
      <title>DuoFormer：利用局部和全局注意力实现分层视觉表征</title>
      <link>https://arxiv.org/abs/2407.13920</link>
      <description><![CDATA[arXiv:2407.13920v1 公告类型：新
摘要：我们在此提出了一种新颖的分层变压器模型，该模型巧妙地将卷积神经网络 (CNN) 的特征提取功能与视觉变压器 (ViT) 的高级表示潜力相结合。针对 ViT 中缺乏归纳偏差和对大量训练数据集的依赖，我们的模型采用 CNN 主干来生成分层视觉表示。然后通过创新的补丁标记化将这些表示调整为变压器输入。我们还引入了一种“尺度注意”机制，可以捕获跨尺度依赖关系，补充补丁注意以增强空间理解并保持全局感知。我们的方法在小型和中型医疗数据集上的表现明显优于基线模型，证明了其效率和通用性。这些组件设计为适用于不同 CNN 架构的即插即用，可以适用于多种应用。代码可在 https://github.com/xiaoyatang/DuoFormer.git 上找到。]]></description>
      <guid>https://arxiv.org/abs/2407.13920</guid>
      <pubDate>Mon, 22 Jul 2024 06:17:53 GMT</pubDate>
    </item>
    <item>
      <title>通过集成卫星地面属性预测器增强全球图像地理定位</title>
      <link>https://arxiv.org/abs/2407.13862</link>
      <description><![CDATA[arXiv:2407.13862v1 公告类型：新
摘要：在没有 GPS 或其他位置元数据的情况下，对地面场景的图像进行地理定位需要估计拍摄照片的地球上的位置。通常，通过测量预测位置和地面真实位置之间的大圆距离 (GCD) 来评估方法。但是，这种测量是有限的，因为它只评估一个点，而不是区域或分数热图的估计值。这在农村、荒野和欠采样地区的应用中尤其重要，因为在这些地区可能无法找到确切的位置，并且在逐步缩小位置范围的聚合系统中使用时尤其重要。
在本文中，我们引入了一种新颖的度量标准，即召回率与面积 (RvA)，它衡量位置估计分布的准确性。RvA 将图像地理定位结果与文档检索类似，将召回率作为面积的函数进行测量：对于（可能不连续的）预测区域的排序列表，我们测量该区域包含地面真实坐标所需的累积面积。这会产生一条类似于精确度-召回率曲线的曲线，其中“精确度”被平方公里面积取代，从而可以评估不同下游搜索区域预算的性能。
直接从这个问题的角度来看，我们随后研究了一种简单的集成方法，用于全球规模的图像地理定位，该方法结合了来自多个来源的信息来帮助解决域转移问题，并且可以轻松整合多个模型、属性预测器和数据源。我们通过将地理定位模型 GeoEstimation 和当前的 SOTA GeoCLIP 与基于 ORNL LandScan 和 ESA-CCI Land Cover 的属性预测器相结合来研究其有效性。我们发现，在 Im2GPS3k 和街景图像上，对于训练集中代表性不足的区域（尤其是非城市区域），图像地理定位得到了显着改善。]]></description>
      <guid>https://arxiv.org/abs/2407.13862</guid>
      <pubDate>Mon, 22 Jul 2024 06:17:52 GMT</pubDate>
    </item>
    <item>
      <title>深入研究 GAN 先验：利用中间特征进行增强模型反演攻击</title>
      <link>https://arxiv.org/abs/2407.13863</link>
      <description><![CDATA[arXiv:2407.13863v1 公告类型：新
摘要：模型反转 (MI) 攻击旨在利用输出信息从已发布的模型中重建隐私敏感的训练数据，这引起了人们对深度神经网络 (DNN) 安全性的广泛担忧。生成对抗网络 (GAN) 的最新进展为 MI 攻击性能的提高做出了重大贡献，因为它们具有生成具有高保真度和适当语义的真实图像的强大能力。然而，以前的 MI 攻击仅在 GAN 先验的潜在空间中泄露了私人信息，限制了它们在多个目标模型和数据集之间的语义提取和可转移性。为了应对这一挑战，我们提出了一种新方法，即中间特征增强的生成模型反转 (IF-GMI)，它分解了 GAN 结构并利用中间块之间的特征。这使我们能够将优化空间从潜在代码扩展到具有增强表达能力的中间特征。为了防止 GAN 先验生成不切实际的图像，我们对优化过程应用了 L1 球约束。在多个基准测试中的实验表明，我们的方法明显优于以前的方法，并在各种设置下取得了最先进的结果，尤其是在分布外 (OOD) 场景中。我们的代码可在以下位置获取：https://github.com/final-solution/IF-GMI]]></description>
      <guid>https://arxiv.org/abs/2407.13863</guid>
      <pubDate>Mon, 22 Jul 2024 06:17:52 GMT</pubDate>
    </item>
    <item>
      <title>许多感知任务是其输入数据的高度冗余函数</title>
      <link>https://arxiv.org/abs/2407.13841</link>
      <description><![CDATA[arXiv:2407.13841v1 公告类型：新
摘要：我们表明，从视觉识别、语义分割、光流、深度估计到发声辨别，许多感知任务都是输入数据的高度冗余函数。图像或频谱图被投影到由像素、傅里叶或小波域中的正交基形成的不同子空间中，无论是数据变化最大的顶部子空间，还是具有中等变化性的中间子空间，还是数据变化最小的底部子空间，都可以很好地解决这些任务。出现这种现象的原因是不同的子空间具有与任务相关的大量冗余信息。]]></description>
      <guid>https://arxiv.org/abs/2407.13841</guid>
      <pubDate>Mon, 22 Jul 2024 06:17:51 GMT</pubDate>
    </item>
    <item>
      <title>X-Former：统一 MLLM 的对比学习和重构学习</title>
      <link>https://arxiv.org/abs/2407.13851</link>
      <description><![CDATA[arXiv:2407.13851v1 公告类型：新
摘要：多模态大型语言模型 (MLLM) 的最新进展通过将视觉感知功能集成到大型语言模型 (LLM) 中，彻底改变了视觉语言理解领域。该领域的主流趋势涉及使用源自视觉语言对比学习 (CL) 的视觉编码器，该编码器在捕捉整体表征方面表现出专业知识，但在捕捉详细的局部模式方面面临困难。在这项工作中，我们专注于通过将通过掩蔽图像建模 (MIM) 获得的高频和详细视觉表征与 CL 捕获的语义丰富的低频表征相结合来增强 MLLM 的视觉表征。为了实现这一目标，我们引入了 X-Former，这是一个轻量级的转换器模块，旨在通过创新的交互机制利用 CL 和 MIM 的互补优势。具体来说，X-Former 首先从两个冻结视觉编码器（即 CLIP-ViT（基于 CL）和 MAE-ViT（基于 MIM））引导视觉语言表征学习和多模态到多模态生成学习。它进一步从冻结的 LLM 引导视觉到语言生成学习，以确保来自 X-Former 的视觉特征可以被 LLM 解释。为了证明我们方法的有效性，我们评估了它在需要详细视觉理解的任务上的表现。广泛的评估表明，X-Former 在 GQA 数据集中涉及结构和语义类别的视觉推理任务中表现出色。对细粒度视觉感知基准的评估进一步证实了其在视觉理解方面的卓越能力。]]></description>
      <guid>https://arxiv.org/abs/2407.13851</guid>
      <pubDate>Mon, 22 Jul 2024 06:17:51 GMT</pubDate>
    </item>
    <item>
      <title>评估 PlanetScope 卫星图像估计颗粒物氧化潜力的潜力</title>
      <link>https://arxiv.org/abs/2407.13778</link>
      <description><![CDATA[arXiv:2407.13778v1 公告类型：新
摘要：氧化电位 (OP) 可测量颗粒物 (PM) 在肺部诱发氧化应激的能力，越来越多地被认为是 PM 毒性的指标。由于 OP 不是常规监测对象，因此很难估计暴露量和健康影响。遥感数据通常用于估计 PM 质量浓度，但从未用于估计 OP。在本研究中，我们评估了卫星图像估计 OP 的潜力，测量方法是通过对法国格勒诺布尔周围三个地点五年内定期采样的 24 小时 PM10 进行无细胞抗坏血酸 (OP AA) 和二硫苏糖醇 (OP DTT) 测定来测量。我们使用深度卷积神经网络提取每日 3 m/像素 PlanetScope 卫星图像的特征，并训练多层感知器根据图像特征和常见气象变量估计 1 公里空间分辨率的 OP。该模型捕捉到了 OP AA 中超过一半的变化和 OP DTT 中几乎一半的变化（测试集 R2 分别为 0.62 和 0.48），相对平均绝对误差 (MAE) 约为 32%。仅使用卫星图像，该模型仍可捕捉到 OP AA 中大约一半的变化和 OP DTT 中三分之一的变化（测试集 R2 分别为 0.49 和 0.36），相对 MAE 约为 37%。如果在其他领域得到证实，我们的方法可以代表一种扩大 OP 估计的时间或空间覆盖范围的低成本方法。]]></description>
      <guid>https://arxiv.org/abs/2407.13778</guid>
      <pubDate>Mon, 22 Jul 2024 06:17:50 GMT</pubDate>
    </item>
    <item>
      <title>CoAPT：用于快速调整的上下文属性词</title>
      <link>https://arxiv.org/abs/2407.13808</link>
      <description><![CDATA[arXiv:2407.13808v1 公告类型：新
摘要：我们提出了一种新的提示调整方法，称为 CoAPT（提示调整中的上下文属性词），用于少样本/零样本图像分类。核心动机是属性是具有关于给定概念的丰富信息的描述性词语。因此，我们旨在丰富现有提示调整方法的文本查询，改善 CLIP 嵌入空间中文本和图像嵌入之间的对齐。为此，CoAPT 将属性词作为可学习提示调整中的附加提示进行集成，并且可以轻松地合并到各种现有的提示调整方法中。为了便于将属性合并到文本嵌入中并与图像嵌入对齐，软提示与附加元网络一起训练，该元网络从图像文本组合查询的连接特征编码中生成输入图像特征偏差。我们的实验表明，CoAPT 可显著改善现有基线方法在几个少样本/零样本图像分类任务上的表现，包括从基础到新颖的泛化、跨数据集迁移和领域泛化。我们的研究结果强调了结合硬提示和软提示的重要性，并为未来在预训练模型中研究文本和图像潜在空间之间的相互作用铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2407.13808</guid>
      <pubDate>Mon, 22 Jul 2024 06:17:50 GMT</pubDate>
    </item>
    <item>
      <title>哪些物品能帮助我有效地行动？推理基于物理基础的可供性</title>
      <link>https://arxiv.org/abs/2407.13811</link>
      <description><![CDATA[arXiv:2407.13811v1 公告类型：新
摘要：为了与开放世界进行有效互动，机器人应该了解与已知和新物体的互动如何帮助它们实现目标。这种理解的一个关键方面在于检测物体的可供性，这代表了通过各种方式操纵物体可以实现的潜在效果。我们的方法利用大型语言模型 (LLM) 和视觉语言模型 (VLM) 的对话来实现开放世界可供性检测。给定预期动作和效果的开放词汇描述，可以找到环境中的有用物体。通过将我们的系统置于物理世界中，我们考虑了机器人的体现及其遇到的物体的内在属性。在我们的实验中，我们已经证明我们的方法可以根据不同的体现或预期效果产生定制的输出。该方法能够从一组干扰项中选择一个有用的对象。对 VLM 的物理属性进行微调可以提高整体性能。这些结果强调了在物理世界中进行可供性搜索的重要性，通过考虑机器人的体现和物体的物理属性。]]></description>
      <guid>https://arxiv.org/abs/2407.13811</guid>
      <pubDate>Mon, 22 Jul 2024 06:17:50 GMT</pubDate>
    </item>
    <item>
      <title>DIR-BHRNet：用于智能手机上基于视觉的实时多人姿态估计的轻量级网络</title>
      <link>https://arxiv.org/abs/2407.13777</link>
      <description><![CDATA[arXiv:2407.13777v1 公告类型：新 
摘要：人体姿势估计（HPE），特别是多人姿势估计（MPPE），已应用于人机系统等许多领域。然而，目前的 MPPE 方法通常在强大的 GPU 系统上运行，并且需要大量的计算成本。在低性能计算的移动设备上实时进行 MPPE 是一项具有挑战性的任务。在本文中，我们提出了一种轻量级神经网络 DIR-BHRNet，用于智能手机上的实时 MPPE。在 DIR-BHRNet 中，我们设计了一种新颖的轻量级卷积模块密集倒置残差（DIR），通过在众所周知的倒置残差中添加深度卷积和快捷连接来提高准确性，并设计了一种新颖的高效神经网络结构平衡 HRNet（BHRNet），通过在每个分支上重新配置适当数量的卷积块来降低计算成本。我们在著名的 COCO 和 CrowdPose 数据集上评估了 DIR-BHRNet。结果表明，DIR-BHRNet 在实时计算成本方面优于最先进的方法。最后，我们在当前主流的 Android 智能手机上实现了 DIR-BHRNet，其性能超过 10 FPS。免费使用的可执行文件（Android 10）、源代码和本作品的视频描述在第 1 页上公开提供，以促进智能手机上实时 MPPE 的开发。]]></description>
      <guid>https://arxiv.org/abs/2407.13777</guid>
      <pubDate>Mon, 22 Jul 2024 06:17:49 GMT</pubDate>
    </item>
    </channel>
</rss>