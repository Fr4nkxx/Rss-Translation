<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Wed, 11 Dec 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>更好的压缩：通用且稳定的无损压缩框架</title>
      <link>https://arxiv.org/abs/2412.06868</link>
      <description><![CDATA[arXiv:2412.06868v1 Announce Type: new 
摘要：本研究重点研究如何稳定和无损模型压缩，旨在降低模型复杂度并提高效率，而不会因压缩误差而牺牲性能。一个关键挑战是有效地利用压缩误差并定义无损压缩的边界以最大限度地减少模型损失。即压缩更好。目前，没有系统的方法来决定这个错误边界或理解它对模型性能的具体影响。我们提出了一个通用的\textbf{L}oss\textbf{L}ess \textbf{C}ompression理论框架（\textbf{LLC}），它通过全微分进一步描绘压缩邻域和高阶分析边界，从而指定模型可以在无损的情况下压缩的误差范围。为了验证LLC的有效性，我们应用了各种压缩技术，包括量化和分解。具体来说，对于量化，我们将经典的量化搜索问题重新表述为无损邻域内的分组背包问题，实现无损量化的同时提高计算效率。对于分解，LLC 解决了低秩约束下的近似问题，自动确定每层的秩并生成无损低秩模型。我们在不同的数据集上对多种神经网络架构进行了广泛的实验。结果表明，无需花哨的技巧，LLC 就可以有效实现无损模型压缩。我们的代码将公开。]]></description>
      <guid>https://arxiv.org/abs/2412.06868</guid>
      <pubDate>Wed, 11 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SafeWatch：一种具有透明解释的高效安全策略跟踪视频护栏模型</title>
      <link>https://arxiv.org/abs/2412.06878</link>
      <description><![CDATA[arXiv:2412.06878v1 公告类型：新
摘要：随着生成式人工智能的兴起和高质量视频生成的快速增长，视频护栏对于确保跨平台的安全比以往任何时候都更加重要。然而，当前的视频护栏要么过于简单，依赖于在具有有限不安全类别的简单策略上训练的纯分类模型，缺乏详细的解释，要么促使使用具有冗长安全指南的多模态大型语言模型 (MLLM)，这对于保护现实世界的内容来说是低效和不切实际的。为了弥补这一差距，我们提出了 SafeWatch，这是一种基于 MLLM 的高效视频护栏模型，旨在遵循定制的安全策略并以零样本方式提供具有内容特定解释的多标签视频护栏输出。具体来说，与传统的基于 MLLM 的护栏不同，传统护栏会对所有安全策略进行自回归编码，从而导致效率低下和偏见，而 SafeWatch 则会对每个策略块进行唯一地并行编码并消除它们的位置偏见，从而让所有策略同时得到同等重要的关注。此外，为了提高效率和准确性，SafeWatch 采用了策略感知的视觉标记修剪算法，该算法可以自适应地为每项策略选择最相关的视频标记，丢弃嘈杂或不相关的信息。这样就可以实现更有针对性的、符合策略的护栏，同时显著降低计算开销。考虑到现有视频护栏基准的局限性，我们提出了 SafeWatch-Bench，这是一个大规模视频护栏基准，包含超过 200 万个视频，涵盖六个安全类别，涵盖 30 多个任务，以确保全面覆盖所有潜在的安全场景。SafeWatch 在 SafeWatch-Bench 上的表现比 SOTA 高出 28.2%，在基准上高出 13.6%，成本降低了 10%，并提供了经 LLM 和人工审核验证的顶级解释。]]></description>
      <guid>https://arxiv.org/abs/2412.06878</guid>
      <pubDate>Wed, 11 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SphereUFormer：用于球形 360 感知的 U 形变压器</title>
      <link>https://arxiv.org/abs/2412.06968</link>
      <description><![CDATA[arXiv:2412.06968v1 公告类型：新
摘要：本文提出了一种用于全向 360° 感知的新方法。以前最常见的方法依赖于等距矩形投影。这种表示很容易应用于 2D 操作层，但会在图像中引入失真。其他方法试图通过保持球体表示来消除失真，但依赖于复杂的卷积核，无法显示出有竞争力的结果。在这项工作中，我们引入了一种基于 Transformer 的架构，通过结合新颖的“球面局部自注意力”和其他球面导向模块，成功地在球面域中运行，并在深度估计和语义分割的 360° 感知基准中超越了最先进的技术。]]></description>
      <guid>https://arxiv.org/abs/2412.06968</guid>
      <pubDate>Wed, 11 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MV-DUSt3R+：2 秒内从稀疏视图进行单阶段场景重建</title>
      <link>https://arxiv.org/abs/2412.06974</link>
      <description><![CDATA[arXiv:2412.06974v1 公告类型：新
摘要：最近的稀疏多视图场景重建进展，如 DUSt3R 和 MASt3R，不再需要相机校准和相机姿势估计。但是，它们一次只处理一对视图来推断像素对齐的点图。处理两个以上的视图时，通常会进行大量容易出错的成对重建，然后进行昂贵的全局优化，这通常无法纠正成对重建错误。为了处理更多视图、减少错误并缩短推理时间，我们提出了快速单级前馈网络 MV-DUSt3R。其核心是多视图解码器块，它们在考虑一个参考视图的同时在任意数量的视图之间交换信息。为了使我们的方法对参考视图选择具有鲁棒性，我们进一步提出了 MV-DUSt3R+，它采用交叉参考视图块来融合不同参考视图选择的信息。为了进一步实现新颖视图合成，我们通过添加和联合训练高斯溅射头来扩展两者。多视图立体重建、多视图姿势估计和新颖视图合成的实验证实，我们的方法比现有技术有了显著改进。代码即将发布。]]></description>
      <guid>https://arxiv.org/abs/2412.06974</guid>
      <pubDate>Wed, 11 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Edge-SD-SR：通过双向调节实现稳定扩散的低延迟和参数高效的设备超分辨率</title>
      <link>https://arxiv.org/abs/2412.06978</link>
      <description><![CDATA[arXiv:2412.06978v1 公告类型：新
摘要：最近，基于稳定扩散的超分辨率 (SD-SR) 的视觉质量取得了巨大进步。然而，由于模型尺寸大且延迟高，在计算受限的设备（如移动电话）上部署大型扩散模型仍然不切实际。这对于 SR 来说更加复杂，因为它通常以高分辨率（例如 4Kx3K）运行。在这项工作中，我们引入了 Edge-SD-SR，这是第一个用于图像超分辨率的参数高效且低延迟的扩散模型。Edge-SD-SR 由 ~169M 个参数组成，包括 UNet、编码器和解码器，复杂度仅为 ~142 GFLOP。为了在如此低的计算预算下保持高视觉质量，我们引入了许多训练策略：(i) 一种针对低分辨率输入的新型调节机制，称为双向调节，它为 SR 任务定制 SD 模型。 (ii) 对 UNet 和编码器进行联合训练，同时将 HR 和 LR 图像的编码分离并使用专用的时间表。(iii) 使用 UNet 的输出对解码器进行微调，以直接根据推理时获得的潜在信息定制解码器。Edge-SD-SR 在设备上高效运行，例如，在 Samsung S24 DSP 上运行时，它可以在 38 毫秒内将 128x128 补丁升级到 512x512，并在短短 1.1 秒内将 512x512 升级到 2048x2048（需要 25 次模型评估）。此外，我们表明 Edge-SD-SR 在最成熟的 SR 基准测试中与最先进的 SR 方法相媲美甚至优于最先进的 SR 方法。]]></description>
      <guid>https://arxiv.org/abs/2412.06978</guid>
      <pubDate>Wed, 11 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>扩散可微分表示</title>
      <link>https://arxiv.org/abs/2412.06981</link>
      <description><![CDATA[arXiv:2412.06981v1 公告类型：新
摘要：我们引入了一种新颖的、无需训练的方法，使用预训练的扩散模型对可微分表示 (diffreps) 进行采样。我们的方法不仅仅是模式搜索，而是通过“拉回”逆时过程的动态（从图像空间到 diffrep 参数空间）并根据此拉回过程更新参数来实现采样。我们确定了 diffrep 引起的样本的隐式约束，并证明解决此约束可显著提高生成对象的一致性和细节。与现有技术相比，我们的方法可以显着提高图像、全景图和 3D NeRF 的质量和多样性。我们的方法是一种用于采样 diffreps 的通用方法，扩大了扩散模型可以解决的问题范围。]]></description>
      <guid>https://arxiv.org/abs/2412.06981</guid>
      <pubDate>Wed, 11 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LUIEO：集成水下图像增强与物体检测的轻量级模型</title>
      <link>https://arxiv.org/abs/2412.07009</link>
      <description><![CDATA[arXiv:2412.07009v1 Announce Type: new 
摘要：水下光学图像不可避免地会受到模糊、对比度低、色彩失真等各种退化因素的影响，阻碍目标检测任务的准确性。由于缺乏配对的水下/干净图像，大多数研究方法采用先增强后检测的策略，导致两个学习任务之间缺乏特征交流。另一方面，由于水下图像退化因素多样与样本数量有限的矛盾，现有的水下增强方法难以有效增强未知水体的退化图像，从而限制了目标检测精度的提升。因此大多数水下目标检测结果仍然显示在退化图像上，难以直观判断检测结果的正确性。针对上述问题，本文提出了一种同时增强水下图像和提高检测精度的多任务学习方法。与单任务学习相比，集成模型允许动态调整不同任务之间的信息通信和共享。由于真实水下图像只能提供带标注的物体标签，本文引入物理约束，保证物体检测任务不干扰图像增强任务。因此本文引入物理模块将水下图像分解为干净图像、背景光和透射图像，并使用物理模型计算水下图像进行自监督。数值实验表明，与最先进的比较方法相比，所提出的模型在视觉性能、物体检测精度和检测效率方面取得了令人满意的结果。]]></description>
      <guid>https://arxiv.org/abs/2412.07009</guid>
      <pubDate>Wed, 11 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ProVision：以编程方式扩展以视觉为中心的多模态语言模型教学数据</title>
      <link>https://arxiv.org/abs/2412.07012</link>
      <description><![CDATA[arXiv:2412.07012v2 公告类型：新
摘要：随着多模态应用的兴起，指令数据对于训练能够理解复杂基于图像的查询的多模态语言模型至关重要。现有的实践依赖于功能强大但成本高昂的大型语言模型 (LLM) 或多模态语言模型 (MLM) 来生成指令数据。这些通常容易产生幻觉、许可问题，并且生成过程通常难以扩展和解释。在这项工作中，我们提出了一种编程方法，该方法使用场景图作为图像的符号表示和人工编写的程序来系统地合成以视觉为中心的指令数据。我们的方法确保了数据生成过程的可解释性和可控性，并在保持事实准确性的同时有效扩展。通过实施一套 24 个单图像、14 个多图像指令生成器和一个场景图生成管道，我们构建了一个可扩展、经济高效的系统：ProVision，它可以为任何给定的图像生成有关对象、属性、关系、深度等的不同问答对。应用于 Visual Genome 和 DataComp 数据集后，我们生成了超过 1000 万个指令数据点 ProVision-10M，并在 MLM 的预训练和指令调整阶段中利用它们。在指令调整阶段采用时，我们的单图像指令数据在 CVBench 的 2D 分割上可提高 7%，在 3D 分割上可提高 8%，同时在 QBench2、RealWorldQA 和 MMMU 上的性能可提高 3%。我们的多图像指令数据在 Mantis-Eval 上可提高 8%。在 xGen-MM-4B 的预训练和微调阶段结合我们的数据，可在 11 个基准测试中平均提高 1.6%。]]></description>
      <guid>https://arxiv.org/abs/2412.07012</guid>
      <pubDate>Wed, 11 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>密集交叉连接集成卷积神经网络增强模型鲁棒性</title>
      <link>https://arxiv.org/abs/2412.07022</link>
      <description><![CDATA[arXiv:2412.07022v1 公告类型：新
摘要：卷积神经网络对输入变化和对抗性攻击的弹性仍然是图像识别任务中的一项重大挑战。出于对更强大和更可靠的图像识别系统的需求，我们提出了密集交叉连接集成卷积神经网络 (DCC-ECNN)。这种新颖的架构将 DenseNet 的密集连接原理与集成学习策略相结合，在不同的 DenseNet 路径之间加入中间交叉连接，以促进广泛的特征共享和集成。DCC-ECNN 架构利用 DenseNet 高效的参数使用和深度，同时受益于集成学习的稳健性，确保更丰富、更具弹性的特征表示。]]></description>
      <guid>https://arxiv.org/abs/2412.07022</guid>
      <pubDate>Wed, 11 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>视觉中的静态关键注意力机制</title>
      <link>https://arxiv.org/abs/2412.07049</link>
      <description><![CDATA[arXiv:2412.07049v1 公告类型：新
摘要：视觉变换器的成功被广泛归因于其动态参数化的多头自注意力机制的表达能力。我们研究了在视觉变换器的标准注意力机制中用静态键替换动态参数化键的影响。我们的研究结果表明，静态键注意力机制可以匹配甚至超过标准自注意力的性能。将静态键注意力模块集成到 Metaformer 主干中，我们发现它在分层混合架构中充当更好的中间阶段，平衡了深度卷积和自注意力的优势。在几个视觉任务上的实验强调了静态键机制的有效性，表明注意力中典型的两步动态参数化可以简化为一步，而不会在某些情况下影响性能。]]></description>
      <guid>https://arxiv.org/abs/2412.07049</guid>
      <pubDate>Wed, 11 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于半监督视频动作检测的稳定均值教师</title>
      <link>https://arxiv.org/abs/2412.07072</link>
      <description><![CDATA[arXiv:2412.07072v1 公告类型：新
摘要：在这项工作中，我们专注于视频动作检测的半监督学习。视频动作检测除了分类之外还需要时空定位，而有限的标签数量使得模型容易出现不可靠的预测。我们提出了稳定平均教师，这是一个简单的端到端基于教师的框架，它受益于改进的和时间一致的伪标签。它依赖于一种新颖的错误恢复 (EoR) 模块，该模块从学生在标记样本上的错误中学习并将这些知识传递给老师，以改进未标记样本的伪标签。此外，现有的时空损失没有考虑时间连贯性，容易出现时间不一致。为了解决这个问题，我们提出了像素差异 (DoP)，这是一种简单而新颖的约束，专注于时间一致性，从而实现连贯的时间检测。我们在四个不同的时空检测基准上评估了我们的方法：UCF101-24、JHMDB21、AVA 和 YouTube-VOS。我们的方法在动作检测方面的表现优于监督基线，UCF101-24 上的平均优势为 23.5%，JHMDB21 上的优势为 16%，AVA 上的优势为 3.3%。仅使用 10% 和 20% 的数据，它就与分别在 UCF101-24 和 JHMDB21 上对 100% 注释进行训练的监督基线相比提供了具有竞争力的性能。我们进一步评估了它在 AVA 上扩展到大规模数据集的有效性以及 YouTube-VOS 上的视频对象分割，展示了它在视频领域其他任务中的泛化能力。代码和模型是公开的。]]></description>
      <guid>https://arxiv.org/abs/2412.07072</guid>
      <pubDate>Wed, 11 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用快速集成保留和增强视觉语言模型中的预训练知识</title>
      <link>https://arxiv.org/abs/2412.07077</link>
      <description><![CDATA[arXiv:2412.07077v1 公告类型：新 
摘要：视觉语言模型的进步，特别是对比语言图像预训练 (CLIP) 模型，通过实现强大的零样本学习能力，彻底改变了机器学习领域。这些功能使模型无需特定于任务的训练即可理解和响应以前未见过的数据。然而，调整 CLIP 以整合来自各个领域的专业知识，同时保留其零样本能力仍然是一个重大挑战。为了解决这个问题，我们引入了一种称为分组提示集成 (GPE) 的新型提示集成学习方法。该方法旨在通过整合新的领域知识来增强 CLIP 的零样本能力，同时提高其对数据分布变化的适应性和鲁棒性。我们的方法取决于三个主要策略：使用掩蔽注意力进行提示分组以优化 CLIP 的适应性，同时保护其零样本能力；加入辅助提示，无缝集成新的领域见解，而不会破坏原始模型的表示；以及有效融合新旧知识的集成学习策略。通过严格的实验，包括更具挑战性的跨数据集迁移评估，我们的 GPE 方法重新定义了视觉语言模型的适应性和效率基准，在各种场景下超越了现有的模型。]]></description>
      <guid>https://arxiv.org/abs/2412.07077</guid>
      <pubDate>Wed, 11 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>EvRepSL：通过自监督学习实现基于事件的视觉事件流表示</title>
      <link>https://arxiv.org/abs/2412.07080</link>
      <description><![CDATA[arXiv:2412.07080v1 公告类型：新
摘要：事件流表示是使用事件相机执行许多计算机视觉任务的第一步。它将异步事件流转换为格式化的结构，以便可以轻松应用传统的机器学习模型。然而，大多数最先进的事件流表示都是手动设计的，由于事件流的噪声性质，这些表示的质量无法得到保证。在本文中，我们介绍了一种数据驱动的方法，旨在提高事件流表示的质量。我们的方法首先引入一种基于时空统计的新事件流表示，表示为 EvRep。随后，我们从理论上推导出异步事件流和同步视频帧之间的内在关系。基于这种理论关系，我们以自监督学习的方式训练表示生成器 RepGen，接受 EvRep 作为输入。最后，通过学习到的 RepGen（无需微调或重新训练），将事件流转换为高质量表示，称为 EvRepSL。我们的方法通过对各种主流基于事件的分类和光流数据集（使用各种类型的事件摄像机捕获）进行大量评估得到了严格验证。实验结果不仅突出了我们的方法优于现有事件流表示的性能，还突出了它的多功能性，与不同的事件摄像机和任务无关。]]></description>
      <guid>https://arxiv.org/abs/2412.07080</guid>
      <pubDate>Wed, 11 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>创意肖像：探索创意对抗网络和条件创意对抗网络</title>
      <link>https://arxiv.org/abs/2412.07091</link>
      <description><![CDATA[arXiv:2412.07091v1 公告类型：新
摘要：卷积神经网络 (CNN) 与生成对抗网络 (GAN) 相结合，创建了深度卷积生成对抗网络 (DCGAN)，并取得了巨大成功。DCGAN 已用于生成时装设计和绘画等创意领域的图像和视频。对 DCGAN 在创意应用中的使用的一个常见批评是，它们在生成创意产品方面的能力有限，因为生成器只是学习复制训练分布。我们探索了 DCGAN 的扩展，即创意对抗网络 (CAN)。使用 CAN，我们生成新颖的创意肖像，并使用 WikiArt 数据集训练网络。此外，我们介绍了 CAN 的扩展，即条件创意对抗网络 (CCAN)，并展示了它们根据风格标签生成创意肖像的潜力。我们认为，生产受风格标签制约或启发的产品密切模拟了真实的创作过程，在这一过程中，人类创造出的富有想象力的作品仍然植根于以前的风格。]]></description>
      <guid>https://arxiv.org/abs/2412.07091</guid>
      <pubDate>Wed, 11 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Maya：一种指令微调的多语言多模式模型</title>
      <link>https://arxiv.org/abs/2412.07112</link>
      <description><![CDATA[arXiv:2412.07112v1 公告类型：新 
摘要：大型视觉语言模型 (VLM) 的快速发展在学术基准上取得了令人瞩目的成果，主要是在广泛使用的语言中。然而，当前的 VLM 在处理低资源语言和不同文化背景的能力方面仍然存在很大差距，这主要是由于缺乏高质量、多样化和安全审查过的数据。因此，这些模型通常难以以无毒性的方式理解低资源语言和文化细微差别。为了解决这些限制，我们引入了 Maya，一个开源多模态多语言模型。我们的贡献有三方面：1) 基于 LLaVA 预训练数据集的八种语言多语言图像文本预训练数据集；2) 对 LLaVA 数据集内的毒性进行彻底分析，然后创建一个跨八种语言的新型无毒性版本； 3) 支持这些语言的多语言图像文本模型，增强视觉语言任务中的文化和语言理解能力。代码可在 https://github.com/nahidalam/maya 上找到。]]></description>
      <guid>https://arxiv.org/abs/2412.07112</guid>
      <pubDate>Wed, 11 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>