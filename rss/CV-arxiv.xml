<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Fri, 04 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>隐私保护 SAM 量化，实现医疗保健领域的高效边缘智能</title>
      <link>https://arxiv.org/abs/2410.01813</link>
      <description><![CDATA[arXiv:2410.01813v1 公告类型：新 
摘要：世界各地区医护人员专业知识和医疗资源的差异是一个紧迫的社会问题。人工智能技术为缓解这一问题提供了新的机会。擅长智能图像分割的任意分割模型 (SAM) 在医疗监测和辅助诊断方面表现出色。不幸的是，SAM 巨大的计算和存储开销对于在资源有限的边缘设备上部署构成了重大挑战。量化是模型压缩的有效解决方案；然而，传统方法严重依赖原始数据进行校准，这引起了人们对医疗数据隐私和安全的广泛担忧。在本文中，我们提出了一种无数据的 SAM 量化框架，称为 DFQ-SAM，它在没有任何原始数据的情况下学习和校准量化参数，从而有效地在模型压缩过程中保护数据隐私。具体来说，我们提出了用于分割的伪正标签进化，结合块相似性，充分利用预训练模型中的语义和分布先验，这有助于高质量数据合成作为真实数据的替代品。此外，我们引入了尺度重参数化以确保低比特量化的准确性。我们对各种数据集进行了广泛的分割实验，DFQ-SAM 在低比特量化上始终提供显著的性能。DFQ-SAM 消除了云边协作中数据传输的需要，从而保护敏感数据免受潜在攻击。它在边缘实现了安全、快速和个性化的医疗服务，提高了系统效率并优化了资源配置，从而促进了人工智能在全球医疗保健领域的广泛应用。]]></description>
      <guid>https://arxiv.org/abs/2410.01813</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>自动场景生成：最新技术、模型、数据集、挑战和未来前景</title>
      <link>https://arxiv.org/abs/2410.01816</link>
      <description><![CDATA[arXiv:2410.01816v1 公告类型：新
摘要：自动场景生成是一个重要的研究领域，可应用于机器人技术、娱乐、视觉表现、训练和模拟、教育等。本综述全面回顾了自动场景生成的当前最新技术，重点关注利用机器学习、深度学习、嵌入式系统和自然语言处理 (NLP) 的技术。我们将模型分为四种主要类型：变分自动编码器 (VAE)、生成对抗网络 (GAN)、Transformers 和扩散模型。每个类别都进行了详细探讨，讨论了各种子模型及其对该领域的贡献。
我们还回顾了最常用的数据集，例如 COCO-Stuff、Visual Genome 和 MS-COCO，这些数据集对于训练和评估这些模型至关重要。研究了场景生成的方法，包括图像到 3D 的转换、文本到 3D 的生成、UI/布局设计、基于图形的方法和交互式场景生成。在评估模型性能的背景下，讨论了 Frechet 初始距离 (FID)、Kullback-Leibler (KL) 散度、初始分数 (IS)、交集比并集 (IoU) 和平均精度 (mAP) 等评估指标。
该调查确定了该领域的主要挑战和局限性，例如保持真实感、处理具有多个对象的复杂场景以及确保对象关系和空间排列的一致性。通过总结最近的进展并指出需要改进的领域，本调查旨在为从事自动场景生成的研究人员和从业者提供宝贵的资源。]]></description>
      <guid>https://arxiv.org/abs/2410.01816</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从专家到公众：管理政治敏感视频分析中的多模态语言模型</title>
      <link>https://arxiv.org/abs/2410.01817</link>
      <description><![CDATA[arXiv:2410.01817v1 公告类型：新
摘要：本文通过个人和集体审议研究了多模态大型语言模型 (MM-LLM) 的治理，重点关注政治敏感视频的分析。我们进行了一项两步研究：首先，对 10 名记者的采访建立了对专家视频解释的基本了解；其次，114 名来自普通公众的个人使用 Inclusive.AI 进行了审议，这是一个通过去中心化自治组织 (DAO) 机制促进民主决策的平台。我们的研究结果表明，虽然专家强调情感和叙事，但普通公众优先考虑事实清晰度、情况的客观性和情感中立性。此外，我们还探讨了不同治理机制的影响：二次与加权排名投票以及平等与 20-80 权力分配对用户决策 AI 行为的影响。具体而言，二次投票增强了人们对自由民主和政治平等的认知，对人工智能持更乐观态度的参与者认为投票过程具有更高程度的参与式民主。我们的结果表明，应用 DAO 机制有助于实现人工智能治理的民主化。]]></description>
      <guid>https://arxiv.org/abs/2410.01817</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PixelBytes：捕捉多模态生成的统一表征</title>
      <link>https://arxiv.org/abs/2410.01820</link>
      <description><![CDATA[arXiv:2410.01820v1 公告类型：新
摘要：本报告介绍了一种统一多模态表示学习的新方法 PixelBytes。受现有序列模型（如 Image Transformers、PixelCNN 和 Mamba-Bytes）的启发，我们的方法旨在以有凝聚力的表示形式捕获不同的输入，探索不同数据类型的集成，特别是文本、音频和像素化图像（精灵）。我们在专门的 PixelBytes Pok{\&#39;e}mon 数据集上进行了实验。最初，我们研究了各种模型架构，包括循环神经网络 (RNN)、状态空间模型 (SSM) 和基于注意力的模型，重点关注双向处理和我们的卷积 PxBy 嵌入技术。随后，我们根据数据缩减策略和自回归学习的有效性评估了模型。我们专门在预测和自回归模式下检查了长短期记忆 (LSTM) 网络，作为我们的主要实验。我们的研究结果表明，在这种情况下，自回归模型的表现优于预测模型。通过采用灵活的多模态建模方法，PixelBytes 为能够理解和生成多模态数据的基础模型的持续开发做出了贡献。完整的 PixelBytes 项目（包括代码、模型和数据集）可在线获取。]]></description>
      <guid>https://arxiv.org/abs/2410.01820</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于卷积神经网络的图像分类分析：水稻叶病预测和农民建议的多功能应用</title>
      <link>https://arxiv.org/abs/2410.01827</link>
      <description><![CDATA[arXiv:2410.01827v1 公告类型：新
摘要：本研究提出了一种使用 8 种不同的卷积神经网络 (CNN) 算法改进水稻病害分类的新方法，这将进一步推动精准农业领域的发展。基于 Tkinter 的应用程序为农民提供了功能丰富的界面。借助这一尖端应用程序，农民将能够通过实时疾病预测和提供个性化建议做出及时和明智的决策。结合用户友好的 Tkinter 界面，包括 ResNet-50、InceptionV3、VGG16 和 MobileNetv2 在内的尖端 CNN 迁移学习算法技术与 UCI 数据集的顺利集成代表了朝着现代化农业实践和保证可持续作物管理迈出的重大进步。显著的成果包括 ResNet-50 的准确率 75%、DenseNet121 的准确率 90%、VGG16 的准确率 84%、MobileNetV2 的准确率 95.83%、DenseNet169 的准确率 91.61% 以及 InceptionV3 的准确率 86%。这些结果简明扼要地概括了模型的能力，有助于研究人员选择适当的策略，以精确、成功地识别水稻作物病害。在准确率为 70% 的 VGG19 和准确率为 80.02% 的 Nasnet 上出现了严重的过度拟合。在 Renset101 上，只能达到 54% 的准确率，而在 efficientNetB0 上也只能达到 33%。MobileNetV2 训练的模型已成功部署在 TKinter GUI 应用程序上，以使用图像或实时视频捕获进行预测。]]></description>
      <guid>https://arxiv.org/abs/2410.01827</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>EgoAvatar：以自我为中心的视图驱动和逼真的全身虚拟形象</title>
      <link>https://arxiv.org/abs/2410.01835</link>
      <description><![CDATA[arXiv:2410.01835v1 公告类型：新
摘要：沉浸式 VR 远程呈现理想情况下意味着能够与数字化身进行交互和交流，这些数字化身与真实对应者的行为无法区分且准确反映其行为。核心技术挑战有两个方面：创建一个忠实反映真实人类的数字替身，并仅通过重量轻且能耗低的自我中心传感设备（例如单个 RGB 摄像头）跟踪真实人类。到目前为止，还没有统一的解决方案来解决这个问题，因为最近的研究只关注自我中心的动作捕捉，只建模头部，或者从多视角捕捉中构建化身。在这项工作中，我们首次在文献中提出了一种以人为本的自我中心远程呈现方法，该方法联合建模照片级真实的数字化身，同时从单个自我中心视频驱动它。我们首先介绍一个可动画化的角色模型，即可以仅由骨骼运动驱动，同时能够对几何形状和外观进行建模。然后，我们引入一个个性化的自我中心动作捕捉组件，该组件可从自我中心视频中恢复全身动作。最后，我们将恢复的姿势应用于我们的角色模型并执行测试时网格细化，以使几何形状忠实地投射到自我中心视图上。为了验证我们的设计选择，我们提出了一个新的具有挑战性的基准，它提供了真实人类执行各种动作的成对自我中心和密集多视图视频。我们的实验表明，我们朝着自我中心和照片级真实的远程呈现迈出了明显的一步，因为我们的方法优于基线和竞争方法。有关更多详细信息、代码和数据，请参阅我们的项目页面。]]></description>
      <guid>https://arxiv.org/abs/2410.01835</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于可解释深度面部表情识别的空间动作单元线索</title>
      <link>https://arxiv.org/abs/2410.01848</link>
      <description><![CDATA[arXiv:2410.01848v1 公告类型：新
摘要：尽管最先进的面部表情识别 (FER) 分类器可以达到很高的准确度，但它们缺乏可解释性，这对最终用户来说是一个重要特性。专家通常将码本中的空间动作单元 (AU) 与面部区域关联起来，以便对表情进行视觉解释。在本文中，遵循相同的专家步骤。提出了一种新的学习策略，将 AU 线索明确纳入分类器训练中，从而可以训练深度可解释模型。在训练期间，使用此 AU 码本以及输入图像表情标签和面部标志来构建 AU 热图，该热图指示与面部表情相关的最具辨别力的图像区域。利用这个有价值的空间线索来训练 FER 的深度可解释分类器。这是通过限制分类器的空间层特征与 AU 热图相关来实现的。使用复合损失，分类器经过训练可以正确分类图像，同时产生与 AU 图相关的可解释的视觉分层注意力，从而模拟专家决策过程。我们的策略仅依靠图像类别表达进行监督，无需额外的手动注释。我们的新策略是通用的，可以应用于任何基于深度 CNN 或 Transformer 的分类器，而无需任何架构更改或大量额外的训练时间。我们对两个公共基准 RAF-DB 和 AffectNet 数据集的广泛评估表明，我们提出的策略可以提高分层可解释性而不会降低分类性能。此外，我们探索了一种依赖于类激活映射 (CAM) 方法的常见可解释分类器，并表明我们的方法也可以提高 CAM 可解释性。]]></description>
      <guid>https://arxiv.org/abs/2410.01848</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>OCC-MLLM-Alpha：通过自监督测试时间学习增强多模态大型语言模型以理解遮挡物体</title>
      <link>https://arxiv.org/abs/2410.01861</link>
      <description><![CDATA[arXiv:2410.01861v1 公告类型：新
摘要：现有的大型视觉语言多模态模型对遮挡物体的理解存在差距。当前最先进的多模态模型无法通过通用视觉编码器和监督学习策略提供令人满意的描述遮挡物体的结果。因此，我们引入了一个支持 3D 生成的多模态大型语言框架和相应的自监督学习策略。我们在大型数据集 SOMVideo [18] 的评估中开始与最先进的模型进行比较的实验。初步结果表明，与最先进的 VLM 模型相比，其性能提高了 16.92%。]]></description>
      <guid>https://arxiv.org/abs/2410.01861</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用半脆弱隐形图像水印进行社交媒体身份验证和打击深度伪造</title>
      <link>https://arxiv.org/abs/2410.01906</link>
      <description><![CDATA[arXiv:2410.01906v1 公告类型：新
摘要：随着用于图像和视频合成的深度生成模型的重大进步，Deepfakes 和被操纵的媒体引起了严重的社会关注。用于深度伪造检测的传统机器学习分类器通常无法应对不断发展的深度伪造生成技术，并且容易受到对抗性攻击。或者，隐形图像水印正在被研究作为一种主动防御技术，通过验证嵌入在图像像素中的隐形秘密信息来实现媒体身份验证。为媒体认证而引入的少数隐形图像水印技术已被证明容易受到基本图像处理操作和水印去除攻击的攻击。作为回应，我们提出了一种半脆弱图像水印技术，该技术将隐形秘密信息嵌入真实图像中进行媒体认证。我们提出的水印框架旨在对面部操纵或篡改具有脆弱性，同时对良性图像处理操作和水印去除攻击具有鲁棒性。这是通过我们提出的技术的独特架构实现的，该架构由批评者和对抗网络组成，它们分别强制执行高图像质量和对水印去除工作的弹性，以及骨干编码器-解码器和鉴别器网络。对 SOTA 面部 Deepfake 数据集的彻底实验调查表明，我们提出的模型可以嵌入一个 $64$ 位秘密作为不可察觉的图像水印，当应用良性图像处理操作时可以以高位恢复精度恢复，而当应用看不见的 Deepfake 操作时则无法恢复。此外，我们提出的水印技术对几种白盒和黑盒水印去除攻击表现出很高的弹性。从而获得最先进的性能。]]></description>
      <guid>https://arxiv.org/abs/2410.01906</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>视觉语言智能的火花：用于高效细粒度图像生成的二维自回归变换器</title>
      <link>https://arxiv.org/abs/2410.01912</link>
      <description><![CDATA[arXiv:2410.01912v1 公告类型：新
摘要：这项工作通过引入一种称为二维自回归 (DnD) Transformer 的新型模型架构来解决矢量量化 (VQ) 自回归图像生成的信息丢失瓶颈。DnD-Transformer 通过引入新的自回归方向 \textit{模型深度} 以及序列长度方向来预测图像的更多代码。与传统的 1D 自回归和以前使用类似 2D 图像分解（如 RQ-Transformer）的工作相比，DnD-Transformer 是一个端到端模型，可以在相同的主干模型大小和序列长度下生成更高质量的图像，为自回归图像生成开辟了新的优化视角。此外，我们的实验表明，DnD-Transformer 的潜力不仅限于生成自然图像。它甚至可以以自我监督的方式生成具有丰富文本和图形元素的图像，展示对这些组合模式的理解。此前，流行的视觉生成模型（如扩散模型）从未展示过这一点，当仅对图像进行训练时，它就展现出了视觉语言智能的火花。代码、数据集和模型在 https://github.com/chenllliang/DnD-Transformer 上开放。]]></description>
      <guid>https://arxiv.org/abs/2410.01912</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>深度学习辅助高分辨率显微镜图像处理用于功能复合材料的相分割</title>
      <link>https://arxiv.org/abs/2410.01928</link>
      <description><![CDATA[arXiv:2410.01928v1 公告类型：新
摘要：在电池研究领域，高分辨率显微镜图像的处理是一项具有挑战性的任务，因为它涉及处理复杂的图像并且需要事先了解所涉及的组件。近年来，深度学习方法在图像分析中的应用引起了人们的极大兴趣，在电池研究领域，有多项研究采用此类技术进行图像分割和分析。然而，自动分析高分辨率显微镜图像以检测复合材料中的相和成分仍然是一个尚未充分探索的领域。这项工作提出了一种新颖的工作流程，用于使用训练有素的 U-Net 分割模型从原始高分辨率透射电子显微镜 (TEM) 图像中检测成分和相位分割。开发的模型可以加快成分和相位分割的检测，减少与仔细检查大量 TEM 图像相关的时间和认知需求，从而降低人为错误的可能性。该方法提出了一种新颖、高效的图像分析方法，具有广泛的适用性，超出了电池领域，并具有在以相和成分分布为特征的其他相关领域（如合金生产）中应用的潜力。]]></description>
      <guid>https://arxiv.org/abs/2410.01928</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一步消除标签噪音</title>
      <link>https://arxiv.org/abs/2410.01944</link>
      <description><![CDATA[arXiv:2410.01944v1 公告类型：新
摘要：减轻噪声标签对训练过程的不利影响变得越来越重要，因为获取完全干净或人工注释的样本用于大规模预训练任务通常是不切实际的。尽管如此，现有的噪声缓解方法由于其特定于任务的设计、模型依赖性和显着的计算开销，在实际应用中经常遇到限制。在这项工作中，我们利用高维正交性的特性来识别锥空间中用于分离干净和噪声样本的稳健有效边界。在此基础上，我们提出了一步抗噪（OSA），这是一种与模型无关的噪声标签缓解范例，它采用估计模型和评分函数通过一步推理来评估输入对的噪声水平，这是一种经济高效的过程。我们通过实证研究证明了 OSA 的优越性，突出了其增强的训练稳健性、改进的任务可转移性、易于部署以及降低的计算成本，涵盖各种基准、模型和任务。我们的代码发布于 https://github.com/leolee99/OSA。]]></description>
      <guid>https://arxiv.org/abs/2410.01944</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有显著融合的语言监督人类动作识别：以建筑工人动作识别为例</title>
      <link>https://arxiv.org/abs/2410.01962</link>
      <description><![CDATA[arXiv:2410.01962v1 公告类型：新
摘要：检测人类行为是自主机器人和车辆的一项关键任务，通常需要集成各种数据模态以提高准确性。在本研究中，我们介绍了一种基于骨架和视觉提示的人类行为识别 (HAR) 的新方法。我们的方法利用语言模型来指导骨架编码器中的特征提取过程。具体而言，我们采用可学习的提示来优化以骨架模态为条件的语言模型的特征表示。此外，我们提出了一种融合机制，该机制使用显着融合模块结合双模态特征，结合注意力和变压器机制来解决模态的高维性。该融合过程优先考虑信息丰富的视频帧和身体关节，从而提高人类动作的识别准确性。此外，我们引入了一个针对建筑工地中现实世界的机器人应用量身定制的新数据集，具有视觉、骨架和深度数据模态，名为 VolvoConstAct。该数据集用于促进机器学习模型的训练和评估，以指导自动施工机器在现实世界的施工区域中执行必要的任务。为了评估我们的方法，我们对我们的数据集以及三个广泛使用的公共数据集 NTU-RGB+D、NTU-RGB+D120 和 NW-UCLA 进行了实验。结果表明，我们提出的方法在所有数据集上都取得了良好的性能，证明了其稳健性和在各种应用中的潜力。代码和数据集可在以下位置获得：https://mmahdavian.github.io/ls_har/]]></description>
      <guid>https://arxiv.org/abs/2410.01962</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用多视角视觉语言模型和屏幕时间跟踪器增强儿童的屏幕时间识别能力</title>
      <link>https://arxiv.org/abs/2410.01966</link>
      <description><![CDATA[arXiv:2410.01966v1 公告类型：新
摘要：能够准确监测幼儿的屏幕暴露对于研究与屏幕使用相关的现象（例如儿童肥胖、身体活动和社交互动）非常重要。大多数现有研究依赖于笨重的可穿戴传感器的自我报告或手动测量，因此在捕获定量屏幕暴露数据方面缺乏效率和准确性。在这项工作中，我们开发了一种新颖的传感器信息学框架，该框架利用可穿戴传感器的自我中心图像（称为屏幕时间跟踪器 (STT)）和视觉语言模型 (VLM)。特别是，我们设计了一个多视图 VLM，它从自我中心图像序列中获取多个视图并动态解释屏幕暴露。我们使用儿童自由生活活动的数据集验证了我们的方法，结果表明，与现有的普通视觉语言模型和物体检测模型相比，该方法有显着改进。结果支持了这种监测方法的前景，它可以优化儿童自然环境中屏幕暴露的行为研究。]]></description>
      <guid>https://arxiv.org/abs/2410.01966</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>UlcerGPT：一种利用大型语言和视觉模型进行糖尿病足溃疡图像转录的多模式方法</title>
      <link>https://arxiv.org/abs/2410.01989</link>
      <description><![CDATA[arXiv:2410.01989v1 公告类型：新
摘要：糖尿病足溃疡 (DFU) 是住院和下肢截肢的主要原因，给患者和医疗保健系统带来了沉重的负担。早期发现和准确分类 DFU 对于预防严重并发症至关重要，但由于无法获得专门服务，许多患者在接受治疗时会遇到延误。远程医疗已成为一种有前途的解决方案，它改善了获得护理的机会并减少了亲自就诊的需求。人工智能和模式识别与远程医疗的整合进一步增强了 DFU 管理，实现了从图像中自动检测、分类和监控。尽管人工智能驱动的 DFU 图像分析方法取得了进展，但大型语言模型在 DFU 图像转录中的应用尚未得到探索。为了解决这一差距，我们引入了 UlcerGPT，这是一种利用大型语言和视觉模型进行 DFU 图像转录的新型多模态方法。该框架结合了先进的视觉和语言模型，例如大型语言和视觉助手以及聊天生成预训练 Transformer，通过联合检测、分类和定位感兴趣的区域来转录 DFU 图像。通过对公共数据集进行详细实验并由专业临床医生评估，UlcerGPT 在 DFU 转录的准确性和效率方面表现出令人鼓舞的结果，为临床医生通过远程医疗及时提供护理提供了潜在支持。]]></description>
      <guid>https://arxiv.org/abs/2410.01989</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>