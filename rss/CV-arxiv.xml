<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Thu, 19 Sep 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>通过高阶运动流持续学习共轭视觉表征</title>
      <link>https://arxiv.org/abs/2409.11441</link>
      <description><![CDATA[arXiv:2409.11441v1 公告类型：新
摘要：由于数据的非独立同分布性质，使用神经网络从连续的视觉信息流中学习带来了一些挑战。然而，它也提供了开发与信息流一致的表示的新机会。在本文中，我们研究了受多个运动诱导约束的像素特征无监督连续学习的情况，因此称为运动共轭特征表示。与现有方法不同，运动不是给定信号（无论是地面实况还是由外部模块估计的），而是渐进和自主学习过程的结果，发生在特征层次结构的各个级别。使用神经网络估计多个运动流，并以不同级别的抽象为特征，从传统的光流到源自更高级别特征的其他潜在信号，因此称为高阶运动。持续学习以开发一致的多阶流和表示容易产生简单的解决方案，我们通过引入自监督对比损失来抵消这种损失，该损失具有空间感知性并基于流诱导相似性。我们在照片级逼真的合成流和真实世界视频上评估了我们的模型，并与预先训练的最先进的特征提取器（也基于 Transformers）和最近的无监督学习模型进行了比较，结果发现我们的模型明显优于这些替代方案。]]></description>
      <guid>https://arxiv.org/abs/2409.11441</guid>
      <pubDate>Thu, 19 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 PocketNet 对宫颈肿瘤进行两阶段分割</title>
      <link>https://arxiv.org/abs/2409.11456</link>
      <description><![CDATA[arXiv:2409.11456v1 公告类型：新
摘要：宫颈癌仍然是全球女性中第四大常见恶性肿瘤。1 同步放化疗 (CRT) 是局部晚期宫颈癌的主要治疗方案，包括外照射和近距离放射治疗。2 放射治疗计划不可或缺的部分是常规勾画宫颈水平的目标肿瘤、相关妇科解剖结构和邻近的危及器官 (OAR)。然而，手动勾画这些结构既费时又费力，并且与已知的观察者间差异有关，这可能会影响治疗结果。虽然已经开发了多种使用计算机断层扫描 (CT) 图像自动分割 OAR 和高危临床肿瘤体积 (HR-CTV) 的工具，3,4,5,6 但使用常规 T2 加权 (T2w) 磁共振成像 (MRI) 开发基于深度学习的肿瘤分割工具解决了尚未满足的临床需求，即改善解剖结构和宫颈癌的常规勾画，从而提高放射治疗计划的质量和一致性。这项工作应用了一种新颖的深度学习模型 (PocketNet) 在 T2w MRI 上分割宫颈、阴道、子宫和肿瘤。通过 5 倍交叉验证对数据进行训练时，对 PocketNet 架构的性能进行了评估。PocketNet 在肿瘤分割中实现了超过 70% 的平均 Dice-Sorensen 相似系数 (DSC)，在器官分割中实现了超过 80% 的平均 Dice-Sorensen 相似系数 (DSC)。这些结果表明 PocketNet 对造影剂方案的变化具有鲁棒性，可提供可靠的 ROI 分割。]]></description>
      <guid>https://arxiv.org/abs/2409.11456</guid>
      <pubDate>Thu, 19 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Mamba Fusion：通过提问学习行动</title>
      <link>https://arxiv.org/abs/2409.11513</link>
      <description><![CDATA[arXiv:2409.11513v1 公告类型：新
摘要：视频语言模型 (VLM) 对于跨不同任务进行概括和使用语言线索来增强学习至关重要。虽然基于 Transformer 的架构已成为视觉语言训练的事实标准，但它们面临着诸如二次计算复杂度、高 GPU 内存使用率和长期依赖性困难等挑战。为了解决这些限制，我们引入了 MambaVL，这是一种新模型，它利用选择性状态空间模态融合的最新进展来有效捕获远程依赖关系并学习视觉和语言数据的联合表示。MambaVL 在两种模态中使用共享的状态转换矩阵，允许模型从场景中的多个角度捕获有关动作的信息。此外，我们提出了一个问答任务，帮助引导模型找到相关线索。这些问题提供了有关动作、对象和环境背景的关键信息，从而提高了性能。因此，MambaVL 在 Epic-Kitchens-100 数据集上的动作识别方面实现了最先进的性能，并且在动作预测方面优于基线方法。]]></description>
      <guid>https://arxiv.org/abs/2409.11513</guid>
      <pubDate>Thu, 19 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于混淆的隐私保护表示可使用邻域信息恢复</title>
      <link>https://arxiv.org/abs/2409.11536</link>
      <description><![CDATA[arXiv:2409.11536v1 公告类型：新
摘要：AR/VR/MR 应用程序和基于云的视觉定位系统的普及度迅速增长，导致人们越来越关注定位过程中用户内容的隐私。
深度神经网络能够从一组稀疏的 3D 或 2D 点及其描述符中恢复场景的详细图像（即所谓的反转攻击），这进一步加剧了人们对隐私的担忧。
因此，隐私保护定位的研究重点是防止对查询图像关键点和场景地图的 3D 点的这些反转攻击。
为此，提出了几种几何混淆技术，将点提升到更高维空间，即线或平面，或在点之间交换坐标 % 。
在本文中，我们指出了这些混淆的一个共同弱点，即允许在已知邻域的假设下恢复原始点位置的近似值。
我们进一步表明，这些邻域可以通过学习识别邻域中同时出现的描述符来计算。
大量实验表明，我们的点恢复方法实际上适用于所有现有的几何混淆方案。
我们的结果表明，这些方案不应被视为隐私保护方案，即使它们声称是隐私保护方案。
代码将在 \url{https://github.com/kunalchelani/RecoverPointsNeighborhood} 提供。]]></description>
      <guid>https://arxiv.org/abs/2409.11536</guid>
      <pubDate>Thu, 19 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>VALO：基于 LiDAR 的物体检测深度神经网络的多功能随时框架</title>
      <link>https://arxiv.org/abs/2409.11542</link>
      <description><![CDATA[arXiv:2409.11542v1 公告类型：新
摘要：这项工作解决了适应 LiDAR 物体检测深度神经网络 (DNN) 的动态截止期限要求的挑战。物体检测的计算延迟对于确保安全高效的导航至关重要。然而，最先进的 LiDAR 物体检测 DNN 通常会表现出显著的延迟，阻碍其在资源受限的边缘平台上的实时性能。因此，应在运行时动态管理检测精度和延迟之间的权衡，以实现最佳结果。
在本文中，我们介绍了 VALO（用于 LiDAR 物体检测的多功能随时算法），这是一种新颖的以数据为中心的方法，可以随时计算 3D LiDAR 物体检测 DNN。VALO 采用截止期限感知调度程序来选择性地处理输入区域，在不修改架构的情况下进行执行时间和准确性权衡。此外，它利用对过去检测结果的有效预测来减轻由于部分处理输入而可能造成的准确性损失。最后，它在检测头内采用了一种新颖的输入减少技术，在不牺牲准确性的情况下显著加快了执行速度。
我们在最先进的 3D LiDAR 物体检测网络（即 CenterPoint 和 VoxelNext）上实现了 VALO，并展示了其对各种时间限制的动态适应性，同时实现了比之前最先进的技术更高的准确性。代码可在 https://github.com/CSL-KU/VALO}{github.com/CSL-KU/VALO 获得。]]></description>
      <guid>https://arxiv.org/abs/2409.11542</guid>
      <pubDate>Thu, 19 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多模态广义类别发现</title>
      <link>https://arxiv.org/abs/2409.11624</link>
      <description><![CDATA[arXiv:2409.11624v1 公告类型：新
摘要：广义类别发现 (GCD) 旨在将输入分为已知和新类别，这是一项对于开放世界科学发现至关重要的任务。然而，当前的 GCD 方法仅限于单模态数据，忽略了大多数现实世界数据固有的多模态性质。在这项工作中，我们将 GCD 扩展到多模态设置，其中来自不同模态的输入提供更丰富和互补的信息。通过理论分析和实证验证，我们发现多模态 GCD 的关键挑战在于有效地跨模态对齐异构信息。为了解决这个问题，我们提出了 MM-GCD，这是一个新颖的框架，它使用对比学习和蒸馏技术对齐不同模态的特征和输出空间。 MM-GCD 在 UPMC-Food101 和 N24News 数据集上取得了新的最佳性能，分别比以前的方法提高了 11.5% 和 4.7%。]]></description>
      <guid>https://arxiv.org/abs/2409.11624</guid>
      <pubDate>Thu, 19 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PainDiffusion：机器人可以表达痛苦吗？</title>
      <link>https://arxiv.org/abs/2409.11635</link>
      <description><![CDATA[arXiv:2409.11635v1 公告类型：新
摘要：疼痛是一种更直观、更人性化的问题交流方式，在康复护士培训机器人中特别有用。虽然大多数以前的方法都侧重于对疼痛表情进行分类或识别，但这些方法往往会导致机器人面部不自然、抖动。我们引入了 PainDiffusion，这是一种响应疼痛刺激而产生面部表情的模型，具有可控的疼痛表现力和情绪状态。PainDiffusion 利用扩散强制，使用条件时间 U-Net 在任意长度上推出预测。它在 EMOCA 的面部表情潜在空间中作为潜在扩散模型运行，确保紧凑的数据表示和快速的渲染时间。对于训练数据，我们处理 BioVid Heatpain 数据库，提取表情代码和主题身份配置。我们还提出了一套新的指标来评估疼痛表情，重点关注表现力、多样性和模型生成输出的适当性。最后，我们证明 PainDiffusion 在定性和定量方面都优于自回归方法。代码、视频和进一步分析可从以下网址获取：\href{https://damtien444.github.io/paindf/}。]]></description>
      <guid>https://arxiv.org/abs/2409.11635</guid>
      <pubDate>Thu, 19 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DAF-Net：用于红外和可见光图像融合的域自适应双分支特征分解融合网络</title>
      <link>https://arxiv.org/abs/2409.11642</link>
      <description><![CDATA[arXiv:2409.11642v1 公告类型：新
摘要：红外和可见光图像融合旨在结合两种模态的互补信息，以提供更全面的场景理解。然而，由于两种模态之间存在显著差异，在融合过程中保留关键特征仍然是一个挑战。为了解决这个问题，我们提出了一种具有领域自适应的双分支特征分解融合网络（DAF-Net），将多核最大均值差异（MK-MMD）引入基础编码器，并设计了一种适用于红外和可见光图像融合的混合核函数。基于 Restormer 网络构建的基础编码器捕获全局结构信息，而基于可逆神经网络（INN）的细节编码器专注于提取细节纹理信息。通过结合 MK-MMD，DAF-Net 有效地对齐了可见光和红外图像的潜在特征空间，从而提高了融合图像的质量。实验结果表明，该方法在多个数据集上的表现优于现有技术，显著提高了视觉质量和融合性能。相关 Python 代码可从 https://github.com/xujian000/DAF-Net 获取。]]></description>
      <guid>https://arxiv.org/abs/2409.11642</guid>
      <pubDate>Thu, 19 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Relax DARTS：放宽可区分架构搜索的限制以实现眼动识别</title>
      <link>https://arxiv.org/abs/2409.11652</link>
      <description><![CDATA[arXiv:2409.11652v1 公告类型：新
摘要：眼动生物识别是一种安全且创新的识别方法。深度学习方法表现出良好的性能，但其网络架构依赖于手动设计和组合先验知识。为了解决这些问题，我们将自动网络搜索（NAS）算法引入眼动识别领域，并提出 Relax DARTS，它是可微分架构搜索（DARTS）的改进，以实现更高效的网络搜索和训练。关键思想是通过独立训练架构参数$\alpha$来规避权重共享问题，以实现更精确的目标架构。此外，模块输入权重$\beta$的引入使单元可以灵活地选择输入，以缓解过度拟合现象并提高模型性能。在四个公共数据库上的结果表明，Relax DARTS 实现了最先进的识别性能。值得注意的是，Relax DARTS 表现出对其他多特征时间分类任务的适应性。]]></description>
      <guid>https://arxiv.org/abs/2409.11652</guid>
      <pubDate>Thu, 19 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>VL-Reader：视觉和语言重建器是一种有效的场景文本识别器</title>
      <link>https://arxiv.org/abs/2409.11656</link>
      <description><![CDATA[arXiv:2409.11656v1 公告类型：新
摘要：文本识别是视觉和语言的内在集成，包括笔画模式中的视觉纹理和字符序列中的语义上下文。对于高级文本识别，有三个关键挑战：（1）能够表示视觉和语义分布的编码器；（2）确保视觉和语义一致的解码器；（3）在预训练（如果存在）和微调期间框架的一致性。受掩蔽自动编码（视觉和语言中成功的预训练策略）的启发，我们提出了一种创新的场景文本识别方法，称为 VL-Reader。VL-Reader 的新颖之处在于整个过程中视觉和语言之间的普遍相互作用。具体而言，我们首先引入了一个掩蔽视觉语言重建 (MVLR) 目标，旨在同时对视觉和语言信息进行建模。然后，我们设计了一个掩蔽视觉语言解码器 (MVLD)，以进一步利用掩蔽的视觉语言上下文并实现双模态特征交互。VL-Reader 的架构从预训练到微调保持一致性。在预训练阶段，VL-Reader 重建掩蔽的视觉和文本标记，而在微调阶段，网络降级以从没有任何掩蔽区域的图像中重建所有字符。VL-reader 在六个典型数据集上实现了 97.1% 的平均准确率，比 SOTA 高出 1.1%。在具有挑战性的数据集上，改进更为显著。结果表明，视觉和语言重建器可以作为有效的场景文本识别器。]]></description>
      <guid>https://arxiv.org/abs/2409.11656</guid>
      <pubDate>Thu, 19 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>弥合领域差距，实现飞行就绪的太空视觉</title>
      <link>https://arxiv.org/abs/2409.11661</link>
      <description><![CDATA[arXiv:2409.11661v1 公告类型：新
摘要：本研究介绍了航天器姿态网络 v3 (SPNv3)，这是一种用于对已知非合作目标航天器进行单目姿态估计的神经网络 (NN)。与现有文献相反，SPNv3 的设计和训练具有计算效率，同时为离线训练和地面验证期间未观察到的星载图像提供鲁棒性。这些特性对于在太空级边缘设备上部署 NN 至关重要。它们是通过仔细的 NN 设计选择实现的，广泛的权衡分析揭示了数据增强、迁移学习和视觉转换器架构等功能，这些功能有助于同时最大化鲁棒性和最小化计算开销。实验表明，最终的 SPNv3 可以在机器人测试台的硬件在环图像上实现最先进的姿势精度，同时专门对计算机生成的合成图像进行训练，有效地弥合了合成图像和真实图像之间的领域差距。同时，在具有飞行历史的代表性图形处理单元系统上进行测试时，SPNv3 的运行频率远高于现代卫星导航滤波器的更新频率。总体而言，SPNv3 是一种高效、可飞行的 NN 模型，可轻松应用于与目标驻留空间物体进行的各种近距离会合和近距离操作。SPNv3 的代码实现将公开发布。]]></description>
      <guid>https://arxiv.org/abs/2409.11661</guid>
      <pubDate>Thu, 19 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有掩模去噪机制的代理聚合器，用于组织病理学全幻灯片图像分析</title>
      <link>https://arxiv.org/abs/2409.11664</link>
      <description><![CDATA[arXiv:2409.11664v1 公告类型：新
摘要：组织病理学分析是医学诊断的黄金标准。准确分类整个幻灯片图像 (WSI) 和感兴趣区域 (ROI) 定位可以帮助病理学家进行诊断。WSI 的千兆像素分辨率和缺乏细粒度注释使得直接分类和分析具有挑战性。在弱监督学习中，多实例学习 (MIL) 为 WSI 分类提供了一种有前途的方法。流行的策略是使用注意机制来衡量实例对分类的重要性。然而，注意机制无法捕获实例间信息，而自注意力会导致二次计算复杂度。为了应对这些挑战，我们提出了 AMD-MIL，一种具有掩码去噪机制的代理聚合器。代理令牌充当查询和密钥之间的中间变量，用于计算实例重要性。从代理聚合值映射的掩码和去噪矩阵动态屏蔽低贡献表示并消除噪音。 AMD-MIL 通过调整特征表示、捕捉癌症中的微转移和提高可解释性来实现更好的注意力分配。在 CAMELYON-16、CAMELYON-17、TCGA-KIDNEY 和 TCGA-LUNG 上进行的大量实验表明 AMD-MIL 优于最先进的方法。]]></description>
      <guid>https://arxiv.org/abs/2409.11664</guid>
      <pubDate>Thu, 19 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 2D 掩模进行高斯溅射中的梯度驱动 3D 分割和可供性转移</title>
      <link>https://arxiv.org/abs/2409.11681</link>
      <description><![CDATA[arXiv:2409.11681v1 公告类型：新
摘要：3D 高斯 Splatting 已成为一种强大的 3D 场景表示技术，可以高效地捕捉精细细节。在本文中，我们介绍了一种基于投票的新型方法，将 2D 分割模型扩展为 3D 高斯 Splat。我们的方法利用了掩码梯度，其中梯度由输入的 2D 掩码过滤，并且这些梯度用作投票以实现精确分割。作为副产品，我们发现推理时间梯度也可用于修剪高斯，从而实现高达 21% 的压缩。此外，我们探索了少量可供性转移，允许将 2D 图像中的注释有效地转移到 3D 高斯 Splat 上。这种方法所依赖的强大而直接的数学公式使其成为众多下游应用（例如增强现实 (AR)、对象编辑和机器人技术）的高效工具。项目代码和其他资源可在https://jojijoseph.github.io/3dgs-segmentation上找到。]]></description>
      <guid>https://arxiv.org/abs/2409.11681</guid>
      <pubDate>Thu, 19 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SRIF：基于扩散的图像变形和流量估计的语义形状配准</title>
      <link>https://arxiv.org/abs/2409.11682</link>
      <description><![CDATA[arXiv:2409.11682v1 公告类型：新
摘要：在本文中，我们提出了 SRIF，一种基于扩散图像变形和流量估计的新型语义形状配准框架。更具体地说，给定一对外部对齐的形状，我们首先从多视图渲染它们，然后利用基于扩散模型的图像插值框架在它们之间生成中间图像序列。随后将图像输入动态 3D 高斯溅射框架，我们利用该框架重建中间点云并对其进行后处理，以尊重图像变形处理。最后，针对上述情况，我们提出了一种新颖的配准模块来估计连续正则化流量，该模块以中间点云作为弱引导，将源形状始终朝着目标变形。我们的主要见解是利用大型视觉模型 (LVM) 来关联形状，从而获得比临时特征提取和对齐更丰富的形状关系语义信息。因此，SRIF 不仅在具有挑战性的形状对上实现了高质量的密集对应，而且还提供了平滑、语义上有意义的插值。经验证据证明了我们的方法的有效性和优越性以及特定的设计选择。代码发布于 https://github.com/rqhuang88/SRIF。]]></description>
      <guid>https://arxiv.org/abs/2409.11682</guid>
      <pubDate>Thu, 19 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用基于深度学习的机会性 CT 成像检测未确诊的疾病</title>
      <link>https://arxiv.org/abs/2409.11686</link>
      <description><![CDATA[arXiv:2409.11686v1 公告类型：新
摘要：腹部计算机断层扫描 (CT) 扫描经常在临床环境中进行。机会性 CT 涉及重新利用常规 CT 图像来提取诊断信息，并且是检测肌肉减少症、肝脂肪变性和腹水等未确诊疾病的新兴工具。本研究利用深度学习方法促进准确诊断和临床记录。我们分析了 2,674 次住院 CT 扫描，以确定成像表型（源自机会性 CT 扫描的特征）与其在放射学报告和 ICD 编码中的相应记录之间的差异。通过我们的分析，我们发现通过机会性成像或放射学报告诊断为肌肉减少症、肝脂肪变性和腹水（分别）的扫描中只有 0.5%、3.2% 和 30.7% 是 ICD 编码的。我们的研究结果表明机会性 CT 具有提高诊断精度和风险调整模型准确性的潜力，从而为精准医疗带来进步。]]></description>
      <guid>https://arxiv.org/abs/2409.11686</guid>
      <pubDate>Thu, 19 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>