<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Wed, 30 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>TV-3DG：通过视觉提示掌握文本到 3D 定制生成</title>
      <link>https://arxiv.org/abs/2410.21299</link>
      <description><![CDATA[arXiv:2410.21299v1 公告类型：新
摘要：近年来，生成模型的进步大大扩展了文本到3D生成的能力。许多方法依赖于分数蒸馏采样（SDS）技术。然而，SDS难以在定制生成任务中适应多条件输入，例如文本和视觉提示。为了探究核心原因，我们将SDS分解为差异项和无分类器的指导项。我们的分析发现核心问题是由优化过程中的差异项和随机噪声添加引起的，这两者都导致了蒸馏过程中偏离目标模式。为了解决这个问题，我们提出了一种新的算法，分类器分数匹配（CSM），它删除了SDS中的差异项，并使用确定性的噪声添加过程来降低优化过程中的噪声，有效地克服了我们定制生成框架中SDS的低质量限制。基于 CSM，我们将视觉提示信息与注意力融合机制和采样引导技术相结合，形成了视觉提示 CSM (VPCSM) 算法。此外，我们引入了语义几何校准 (SGC) 模块，通过改进文本信息集成来提高质量。我们将我们的方法称为 TV-3DG，并通过大量实验证明了其能够实现稳定、高质量、定制的 3D 生成。项目页面：\url{https://yjhboy.github.io/TV-3DG}]]></description>
      <guid>https://arxiv.org/abs/2410.21299</guid>
      <pubDate>Wed, 30 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>对比学习与辅助用户检测用于识别活动</title>
      <link>https://arxiv.org/abs/2410.21300</link>
      <description><![CDATA[arXiv:2410.21300v1 公告类型：新
摘要：人类活动识别 (HAR) 在普适计算中至关重要，具有深远的实际应用。虽然最近的 SOTA HAR 研究表现出令人印象深刻的性能，但一些关键方面仍未得到充分探索。首先，HAR 可以高度情境化和个性化。然而，之前的工作主要集中在情境感知 (CA) 上，而很大程度上忽略了用户感知 (UA) 的必要性。我们认为，解决先天用户动作执行差异的影响与考虑 HAR 任务中的外部情境环境设置同样重要。其次，用户感知使模型承认用户差异，但并不一定保证减轻这些差异，即在相同活动下统一预测。需要一种明确强制更接近（不同用户，相同活动）表示的方法。为了弥补这一差距，我们引入了 CLAUDIA，这是一个旨在解决这些问题的新框架。具体来说，我们通过在 CA-HAR 框架内集成用户识别 (UI) 来扩展 CA-HAR 任务的上下文范围，在名为用户和上下文感知 HAR (UCA-HAR) 的新任务中联合预测 CA-HAR 和 UI。这种方法通过联合学习用户不变和用户特定模式来丰富个性化和上下文理解。受到视觉领域 SOTA 设计的启发，我们在实例对上引入了监督对比损失目标，以提高模型效率并提高学习到的特征质量。对三个真实世界的 CA-HAR 数据集的评估显示性能显着增强，马修相关系数的平均改进范围为 5.8% 至 14.1%，宏 F1 分数的平均改进范围为 3.0% 至 7.2%。]]></description>
      <guid>https://arxiv.org/abs/2410.21300</guid>
      <pubDate>Wed, 30 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于胃肠内窥镜医学图像分类的自监督基础模型的领域自适应预训练</title>
      <link>https://arxiv.org/abs/2410.21302</link>
      <description><![CDATA[arXiv:2410.21302v1 公告类型：新
摘要：视频胶囊内窥镜通过提供一种非侵入性方法来捕捉胃肠道的详细图像，从而实现早期疾病检测，从而改变了胃肠内窥镜 (GIE) 诊断。然而，其潜力受到成像过程中生成的图像数量庞大的限制，该过程可能需要 6-8 小时，并且通常会产生多达 100 万张图像，因此需要自动分析。此外，这些图像的多变性，加上对专家注释的需求以及大量高质量标记数据集的稀缺性，限制了当前医学图像分析模型的有效性。为了解决这个问题，我们引入了一种新型的大型胃肠内窥镜数据集，称为 EndoExtend24，它是通过合并和重新分层十个现有公共和私人数据集的训练/测试分割而创建的，确保患者数据在分割之间不会重叠。 EndoExtend24 包含超过 226,000 张带标签的图像以及动态类映射，可跨具有不同标签粒度的数据集进行统一训练，支持多达 123 种不同的病理发现。此外，我们建议利用在通用图像数据上进行自监督训练的计算机视觉基础模型的领域自适应预训练，使其适应 GIE 医学诊断任务。具体来说，EVA-02 模型基于视觉转换器架构，并在 ImageNet-22k 上使用蒙版图像建模进行训练（使用 EVA-CLIP 作为 MIM 老师），在新的 EndoExtend24 数据集上进行预训练以实现领域自适应，最后在 Capsule Endoscopy 2024 Challenge 数据集上进行训练。实验结果在挑战验证集上显示出令人满意的结果，AUC Macro 得分为 0.993，平衡准确率为 89.3%。]]></description>
      <guid>https://arxiv.org/abs/2410.21302</guid>
      <pubDate>Wed, 30 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>VEMOCLAP：视频情绪分类 Web 应用程序</title>
      <link>https://arxiv.org/abs/2410.21303</link>
      <description><![CDATA[arXiv:2410.21303v1 公告类型：新
摘要：我们推出了 VEMOCLAP：使用预训练特征的视频情绪分类器，这是第一个随时可用的开源 Web 应用程序，可分析任何用户提供的视频的情感内容。我们改进了以前的工作，该工作利用了对视频帧和音频进行处理的开源预训练模型，然后使用多头交叉注意有效地融合了生成的预训练特征。我们的方法将 Ekman-6 视频情绪数据集上最先进的分类准确率提高了 4.3%，并为用户提供了一个在线应用程序，让他们可以在自己的视频或 YouTube 视频上运行我们的模型。我们邀请读者在 serkansulun.com/app 上试用我们的应用程序。]]></description>
      <guid>https://arxiv.org/abs/2410.21303</guid>
      <pubDate>Wed, 30 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>VideoSAM：用于高速视频分割的大型视觉基础模型</title>
      <link>https://arxiv.org/abs/2410.21304</link>
      <description><![CDATA[arXiv:2410.21304v1 公告类型：新
摘要：高速视频 (HSV) 分割对于分析科学和工业应用中的动态物理过程（例如沸腾传热）至关重要。现有的模型（如 U-Net）在泛化和准确分割复杂气泡形成方面存在困难。我们提出了 VideoSAM，这是 Segment Anything Model (SAM) 的专门改编版，针对多样化的 HSV 数据集进行了微调以进行相位检测。通过各种实验，VideoSAM 在四种流体环境（水、FC-72、氮气和氩气）中表现出色，在复杂的分割任务中明显优于 U-Net。除了介绍 VideoSAM 之外，我们还贡献了一个专为相位检测而设计的开源 HSV 分割数据集，为该领域的未来研究提供支持。我们的研究结果强调了 VideoSAM 在稳健和准确的 HSV 分割方面树立新标准的潜力。本研究中使用的代码和数据集可在线获取，网址为 https://github.com/chikap421/videosam。]]></description>
      <guid>https://arxiv.org/abs/2410.21304</guid>
      <pubDate>Wed, 30 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一种基于锚点的稳健多摄像机行人定位方法</title>
      <link>https://arxiv.org/abs/2410.21308</link>
      <description><![CDATA[arXiv:2410.21308v1 公告类型：新
摘要：本文解决了基于视觉的行人定位问题，即使用图像和相机参数估计行人的位置。然而，在实践中，校准后的相机参数往往偏离地面实况，导致定位不准确。为了解决这个问题，我们提出了一种基于锚点的方法，利用固定位置的锚点来减少相机参数误差的影响。我们提供了一个理论分析来证明我们的方法的稳健性。在模拟、现实世界和公共数据集上进行的实验表明，与没有锚点的方法相比，我们的方法显著提高了定位精度，并且对相机参数的噪声具有很强的弹性。]]></description>
      <guid>https://arxiv.org/abs/2410.21308</guid>
      <pubDate>Wed, 30 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ArCSEM：通过高斯溅射对 SEM 图像进行艺术着色</title>
      <link>https://arxiv.org/abs/2410.21310</link>
      <description><![CDATA[arXiv:2410.21310v1 公告类型：新
摘要：扫描电子显微镜 (SEM) 因其分析微观物体表面结构的能力而广为人知，能够捕获高度详细但仅为灰度的图像。为了创建更具表现力和逼真的插图，这些图像通常由艺术家在图像编辑软件的支持下手动着色。当扫描对象的多幅图像需要着色时，这项任务变得非常费力。我们建议通过使用微观场景的底层 3D 结构将颜色信息传播到所有捕获的图像（从一个彩色视图开始）来促进这一过程。我们探索了几种场景表示技术，并实现了 SEM 场景的高质量彩色新视图合成。与之前的工作相比，获取 3D 表示不需要人工干预或标记。这使艺术家能够为序列中的单个或几个视图着色，并自动检索全彩色的场景或视频。项目页面：https://ronly2460.github.io/ArCSEM]]></description>
      <guid>https://arxiv.org/abs/2410.21310</guid>
      <pubDate>Wed, 30 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MMDocBench：对大型视觉语言模型进行基准测试，以实现细粒度的视觉文档理解</title>
      <link>https://arxiv.org/abs/2410.21311</link>
      <description><![CDATA[arXiv:2410.21311v1 公告类型：新
摘要：大型视觉语言模型 (LVLM) 在许多视觉语言任务中取得了显著的表现，但它们在细粒度视觉理解方面的能力仍未得到充分评估。现有的基准要么包含与其他数据混合的有限的细粒度评估样本，要么局限于自然图像中的对象级评估。为了全面评估 LVLM 的细粒度视觉理解能力，我们建议使用具有多粒度和多模态信息的文档图像来补充自然图像。有鉴于此，我们构建了 MMDocBench，这是一个具有各种无 OCR 文档理解任务的基准，用于评估细粒度的视觉感知和推理能力。MMDocBench 定义了 15 个主要任务，4,338 个 QA 对和 11,353 个支持区域，涵盖各种文档图像，例如研究论文、收据、财务报告、维基百科表格、图表和信息图表。基于 MMDocBench，我们使用 13 个开源和 3 个专有的高级 LVLM 进行了广泛的实验，评估了它们在不同任务和文档图像类型中的优势和劣势。基准、任务说明和评估代码将公开发布。]]></description>
      <guid>https://arxiv.org/abs/2410.21311</guid>
      <pubDate>Wed, 30 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>迈向稳健的非分布泛化：数据增强和神经架构搜索方法</title>
      <link>https://arxiv.org/abs/2410.21313</link>
      <description><![CDATA[arXiv:2410.21313v1 公告类型：新
摘要：近年来，深度学习取得了巨大成功。尽管如此，当遇到分布外 (OoD) 数据时，其实际性能往往会急剧下降，即训练和测试数据是从不同的分布中采样的。在本文中，我们研究了深度学习的稳健 OoD 泛化方法，即其性能不受测试数据分布变化的影响。
我们首先提出了一种新颖有效的方法来解开对识别不重要的特征之间的虚假相关性。它通过正交化类别和上下文分支的两个损失梯度来采用分解特征表示。此外，我们对与上下文相关的特征（例如，目标对象的样式、背景或场景）执行基于梯度的增强，以提高学习表示的鲁棒性。结果表明，我们的方法对不同的分布变化具有很好的泛化能力。
然后，我们研究了在 OoD 场景中加强神经架构搜索的问题。我们建议优化架构参数，以最小化合成 OoD 数据的验证损失，前提是相应的网络参数最小化训练损失。此外，为了获得合适的验证集，我们通过最大化由不同神经架构计算的损失来学习条件生成器。结果表明，我们的方法有效地发现了在 OoD 泛化方面表现良好的稳健架构。]]></description>
      <guid>https://arxiv.org/abs/2410.21313</guid>
      <pubDate>Wed, 30 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>文本到图像人物检索的多路径探索与反馈调整</title>
      <link>https://arxiv.org/abs/2410.21318</link>
      <description><![CDATA[arXiv:2410.21318v1 公告类型：新
摘要：基于文本的人物检索旨在使用文本描述作为查询来识别特定的人物。现有的先进方法通常依赖于视觉语言预训练 (VLP) 模型来促进有效的跨模态对齐。然而，VLP 模型的固有限制，包括全局对齐偏差和自我反馈调节不足，阻碍了最佳检索性能。在本文中，我们提出了一个多途径探索、反馈和调整框架 MeFa，它深入探索模态内和模态间的内在反馈以进行有针对性的调整，从而实现更精确的人-文本关联。具体而言，我们首先设计一个模态内推理路径，为跨模态数据生成硬负样本，利用这些样本的反馈来改进模态内推理，从而增强对细微差异的敏感性。随后，我们引入了一种跨模态细化路径，利用全局信息和模态间反馈来细化局部信息，从而增强其全局语义表示。最后，判别线索校正路径将次级相似性的细粒度特征作为判别线索，以进一步缓解由这些特征的差异导致的检索失败。在三个公共基准上的实验结果表明，MeFa 无需额外的数据或复杂的结构即可实现出色的人物检索性能。]]></description>
      <guid>https://arxiv.org/abs/2410.21318</guid>
      <pubDate>Wed, 30 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有单一视觉语言嵌入的领域自适应</title>
      <link>https://arxiv.org/abs/2410.21361</link>
      <description><![CDATA[arXiv:2410.21361v1 公告类型：新 
摘要：领域自适应在计算机视觉中得到了广泛的研究，但仍然需要在训练时访问目标数据，这在某些不常见的情况下可能难以获得。在本文中，我们提出了一个依赖于单个视觉语言 (VL) 潜在嵌入而不是完整目标数据的领域自适应新框架。首先，利用对比语言图像预训练模型 (CLIP)，我们提出了提示/照片驱动的实例规范化 (PIN)。PIN 是一种特征增强方法，它使用单个目标 VL 潜在嵌入挖掘多种视觉风格，通过优化低级源特征的仿射变换。VL 嵌入可以来自描述目标域的语言提示、部分优化的语言提示或单个未标记的目标图像。其次，我们展示了这些挖掘出的风格（即增强）可用于零样本（即无目标）和单样本无监督域自适应。语义分割实验证明了所提方法的有效性，其在零样本和单样本设置中的表现优于相关基线。]]></description>
      <guid>https://arxiv.org/abs/2410.21361</guid>
      <pubDate>Wed, 30 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SocialGPT：通过贪婪段优化促进法学硕士进行社会关系推理</title>
      <link>https://arxiv.org/abs/2410.21411</link>
      <description><![CDATA[arXiv:2410.21411v1 公告类型：新
摘要：社会关系推理旨在从图像中识别出朋友、配偶和同事等关系类别。虽然当前的方法采用使用标记图像数据端到端训练专用网络的范式，但它们在通用性和可解释性方面受到限制。为了解决这些问题，我们首先提出了一个简单但精心设计的框架，名为 {\name}，它将视觉基础模型 (VFM) 的感知能力和大型语言模型 (LLM) 的推理能力结合在一个模块化框架中，为社会关系识别提供了强大的基础。具体来说，我们指示 VFM 将图像内容转换为文本社交故事，然后利用 LLM 进行基于文本的推理。{\name} 引入了系统设计原则，以分别适应 VFM 和 LLM 并弥合它们的差距。无需额外的模型训练，它就可以在两个数据库上实现具有竞争力的零样本结果，同时提供可解释的答案，因为 LLM 可以为决策生成基于语言的解释。在推理阶段，LLM 的手动提示设计过程非常繁琐，需要一种自动提示优化方法。由于我们本质上将视觉分类任务转换为 LLM 的生成任务，因此自动提示优化遇到了一个独特的长提示优化问题。为了解决这个问题，我们进一步提出了贪婪片段提示优化 (GSPO)，它利用片段级别的梯度信息执行贪婪搜索。实验结果表明，GSPO 显著提高了性能，而且我们的方法也可以推广到不同的图像风格。代码可在 https://github.com/Mengzibin/SocialGPT 获得。]]></description>
      <guid>https://arxiv.org/abs/2410.21411</guid>
      <pubDate>Wed, 30 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>TACO：卡车对抗性伪装优化，用于欺骗物体探测器</title>
      <link>https://arxiv.org/abs/2410.21443</link>
      <description><![CDATA[arXiv:2410.21443v1 公告类型：新
摘要：对抗性攻击威胁着自动驾驶汽车和防御系统等关键应用中机器学习模型的可靠性。随着物体检测器通过 YOLOv8 等模型变得更加稳健，开发有效的对抗方法变得越来越具有挑战性。我们提出了卡车对抗性伪装优化 (TACO)，这是一种新颖的框架，可在 3D 车辆模型上生成对抗性伪装图案，以欺骗最先进的物体检测器。TACO 采用虚幻引擎 5，将可微分渲染与真实感渲染网络相结合，以优化针对 YOLOv8 的对抗性纹理。为了确保生成的纹理既能有效欺骗检测器又在视觉上合理，我们引入了卷积平滑损失函数，这是一种广义平滑损失函数。实验评估表明，TACO 显著降低了 YOLOv8 的检测性能，在未见测试数据上实现了 0.0099 的 AP@0.5。此外，这些对抗模式表现出很强的可迁移性，可应用于其他物体检测模型，例如 Faster R-CNN 和早期版本的 YOLO。]]></description>
      <guid>https://arxiv.org/abs/2410.21443</guid>
      <pubDate>Wed, 30 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于约束变压器的多孔介质生成对岩石特性的空间分布</title>
      <link>https://arxiv.org/abs/2410.21462</link>
      <description><![CDATA[arXiv:2410.21462v1 公告类型：新 
摘要：基于 3D 微型计算机断层扫描数据中的信息对岩石图像进行孔隙尺度建模对于研究地质碳储存 (GCS) 期间复杂的地下过程（例如 CO2 和盐水多相流）至关重要。虽然深度学习模型可以生成与静态岩石特性相匹配的 3D 岩石微结构，但它们有两个关键限制：它们不考虑可能对岩石的流动和传输特性（例如渗透性和相对渗透性）产生重要影响的岩石特性的空间分布，并且它们会生成低于这些传输特性的代表性基本体积 (REV) 尺度的结构。解决这些问题对于在孔隙尺度分析和现场尺度建模之间建立一致的工作流程至关重要。为了应对这些挑战，我们提出了一个两阶段建模框架，该框架结合了矢量量化变分自动编码器 (VQVAE) 和变压器模型，以自回归方式进行空间上采样和任意尺寸的 3D 多孔介质重建。 VQVAE 首先将子体积训练图像压缩并量化为低维标记，同时我们训练一个转换器，按照特定的空间顺序将这些标记在空间上组装成更大的图像。通过采用多标记生成策略，我们的方法既保留了子体积的完整性，也保留了这些子图像块之间的空间关系。我们证明了多标记转换器生成方法的有效性，并使用来自测试井的真实数据对其进行了验证，展示了它仅使用空间孔隙度模型就能在井尺度上生成多孔介质模型的潜力。反映现场地质特性的插值代表性多孔介质可以准确地模拟传输特性，包括二氧化碳和盐水的渗透率和多相流相对渗透率。]]></description>
      <guid>https://arxiv.org/abs/2410.21462</guid>
      <pubDate>Wed, 30 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AdvI2I：针对图像到图像扩散模型的对抗性图像攻击</title>
      <link>https://arxiv.org/abs/2410.21471</link>
      <description><![CDATA[arXiv:2410.21471v1 公告类型：新
摘要：扩散模型的最新进展显著提高了图像合成的质量，但也带来了严重的安全隐患，尤其是生成不适合工作 (NSFW) 的内容。先前的研究表明，对抗性提示可用于生成 NSFW 内容。然而，这种对抗性文本提示通常很容易被基于文本的过滤器检测到，从而限制了它们的有效性。在本文中，我们揭示了一个以前被忽视的漏洞：针对图像到图像 (I2I) 扩散模型的对抗性图像攻击。我们提出了 AdvI2I，这是一个新颖的框架，它可以操纵输入图像以诱导扩散模型生成 NSFW 内容。通过优化生成器来制作对抗性图像，AdvI2I 可以绕过现有的防御机制，例如安全潜在扩散 (SLD)，而无需更改文本提示。此外，我们引入了 AdvI2I-Adaptive，这是一个增强版本，可以适应潜在的对策并最大限度地减少对抗性图像与 NSFW 概念嵌入之间的相似性，从而使攻击更能抵御防御。通过大量实验，我们证明 AdvI2I 和 AdvI2I-Adaptive 都可以有效绕过当前的安全措施，这凸显了迫切需要更强大的安全措施来解决 I2I 扩散模型的滥用问题。]]></description>
      <guid>https://arxiv.org/abs/2410.21471</guid>
      <pubDate>Wed, 30 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>