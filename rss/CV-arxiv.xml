<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Tue, 08 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>利用神经广义圆柱进行可控形状建模</title>
      <link>https://arxiv.org/abs/2410.03675</link>
      <description><![CDATA[arXiv:2410.03675v1 公告类型：新
摘要：神经形状表示，例如神经符号距离场 (NSDF)，在形状建模中越来越受欢迎，因为它能够处理复杂的拓扑和任意分辨率。由于使用特征进行形状表示的方式是隐式的，因此操纵形状面临着固有的不便挑战，因为无法直观地编辑特征。在这项工作中，我们提出了神经广义圆柱 (NGC) 来显式操纵 NSDF，它是传统广义圆柱 (GC) 的扩展。具体来说，我们首先定义一条中心曲线，并沿曲线分配神经特征来表示轮廓。然后在具有椭圆形轮廓的专用 GC 的相对坐标上定义 NSDF。通过使用相对坐标，可以通过操纵 GC 明确控制 NSDF。为此，我们将 NGC 应用于许多非刚性变形任务，如复杂的曲线变形、形状的局部缩放和扭曲。形状变形与其他方法的比较证明了 NGC 的有效性和效率。此外，NGC 可以通过简单的神经特征插值利用神经特征进行形状混合。]]></description>
      <guid>https://arxiv.org/abs/2410.03675</guid>
      <pubDate>Tue, 08 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LCM：用于稳健表示的对数共形映射学习以减轻透视失真</title>
      <link>https://arxiv.org/abs/2410.03686</link>
      <description><![CDATA[arXiv:2410.03686v1 公告类型：新
摘要：透视失真 (PD) 会导致图像中视觉元素的形状、大小、方向、角度和空间关系发生重大改变。准确确定相机内在和外在参数具有挑战性，因此很难有效地合成透视失真。当前的失真校正方法涉及消除失真和学习视觉任务，因此使其成为一个多步骤的过程，通常会影响性能。最近的研究利用 M\&quot;obius 变换来减轻透视失真 (MPD) 来合成透视失真而无需估计相机参数。使用 M\&quot;obius 变换的一个基本缺点是它需要调整多个相互依赖和相互关联的参数并涉及复杂的算术运算，从而导致相当大的计算复杂度。为了应对这些挑战，我们提出了对数共形图 (LCM)，这是一种利用对数函数以更少的参数和更低的计算复杂度来近似透视失真的方法。我们提供了理论基础并辅以实验，以证明具有较少参数的 LCM 可以近似 MPD。我们表明 LCM 可以很好地与监督和自监督表示学习相结合，优于标准模型，并且在多个基准（即 Imagenet-PD、Imagenet-E 和 Imagenet-X）上减轻透视失真方面与最先进的性能相匹配。此外，LCM 还展示了与行人重新识别的无缝集成并提高了性能。源代码即将发布。]]></description>
      <guid>https://arxiv.org/abs/2410.03686</guid>
      <pubDate>Tue, 08 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于 SGW 的视觉任务多任务学习</title>
      <link>https://arxiv.org/abs/2410.03778</link>
      <description><![CDATA[arXiv:2410.03778v1 公告类型：新
摘要：多任务学习（MTL）是一项多目标优化任务。神经网络试图使用 MTL 中的共享解释空间来实现每个目标。然而，随着数据集规模的扩大和任务复杂性的增加，知识共享变得越来越具有挑战性。在本文中，我们首先从噪声的角度重新审视以前的交叉注意 MTL 方法。我们从理论上分析了这个问题，并将其确定为交叉注意机制的一个缺陷。为了解决这个问题，我们提出了一个信息瓶颈知识提取模块（KEM）。该模块旨在通过限制信息流来减少任务间干扰，从而降低计算复杂度。此外，我们采用了神经崩溃来稳定知识选择过程。也就是说，在输入 KEM 之前，我们将特征投影到 ETF 空间中。这种映射使我们的方法更加健壮。我们在多个数据集上实施了该方法并进行了比较实验。结果表明，我们的方法在多任务学习中明显优于现有方法。]]></description>
      <guid>https://arxiv.org/abs/2410.03778</guid>
      <pubDate>Tue, 08 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>EvenNICER-SLAM：基于事件的神经隐式编码 SLAM</title>
      <link>https://arxiv.org/abs/2410.03812</link>
      <description><![CDATA[arXiv:2410.03812v1 公告类型：新
摘要：神经隐式表征的出现极大地促进了密集视觉同步定位与地图构建 (SLAM) 的发展。神经隐式编码 SLAM 的一个典型例子是 NICE-SLAM，最近在大型室内场景中表现出良好的效果。然而，这些方法通常依赖于时间密集的 RGB-D 图像流作为输入才能正常运行。当输入源不支持高帧速率或相机移动太快时，这些方法通常会崩溃或跟踪和地图构建精度显著下降。在本文中，我们提出了 EvenNICER-SLAM，这是一种通过结合事件相机来解决此问题的新方法。事件相机是受生物启发的相机，它对强度变化而不是绝对亮度做出反应。具体来说，我们将事件丢失反向传播流集成到 NICE-SLAM 管道中，以在 RGB-D 输入不足的情况下增强相机跟踪。我们通过定量评估发现，EvenNICER-SLAM 包含更高频率的事件图像输入，其性能明显优于 RGB-D 输入频率较低的 NICE-SLAM。我们的结果表明，事件摄像机有可能提高密集 SLAM 系统在现实世界场景中对抗快速摄像机运动的鲁棒性。]]></description>
      <guid>https://arxiv.org/abs/2410.03812</guid>
      <pubDate>Tue, 08 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于 MSTAR 数据的 SAR 成像时空杂波统计建模与分析</title>
      <link>https://arxiv.org/abs/2410.03816</link>
      <description><![CDATA[arXiv:2410.03816v1 公告类型：新
摘要：合成孔径雷达 (SAR) 成像的地面杂波统计分析已成为研究和调查日益重要的课题。设计能够在背景杂波中执行目标检测任务的稳健算法也绝对必要。任何试图从地面杂波中提取所需目标能量的尝试都需要完全了解背景杂波的统计特性。本文研究了地面杂波的空间和时间特性。由于每幅图像的数据都是基于不同的方位角收集的；因此，时间分析包含方位角的变化。因此，时间分析包括雷达截面相对于收集数据的方位角的特征。为了进行统计分析，几个众所周知的相关分布，即威布尔分布、对数正态分布、伽马分布和瑞利分布被视为模拟地面杂波的主要候选者。拟合优度检验基于 Kullback-Leibler (KL) 散度度量。本文提供的详细分析表明，威布尔分布更适合时间方位角统计分析，而瑞利分布可以更准确地模拟背景杂波的空间特征。最后，基于上述统计分析，利用恒定误报率 (CFAR) 算法，我们在陆地杂波中进行目标检测。利用在 X 波段以聚光灯模式收集的移动和静止目标获取与识别 (MSTAR) 数据集对分析进行了全面验证，并给出了结果。]]></description>
      <guid>https://arxiv.org/abs/2410.03816</guid>
      <pubDate>Tue, 08 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MonST3R：在运动情况下估计几何形状的简单方法</title>
      <link>https://arxiv.org/abs/2410.03825</link>
      <description><![CDATA[arXiv:2410.03825v1 公告类型：新
摘要：从动态场景中估计几何形状（其中物体随时间移动和变形）仍然是计算机视觉的核心挑战。当前的方法通常依赖于多阶段管道或全局优化，将问题分解为子任务，例如深度和流量，从而导致系统复杂且容易出错。在本文中，我们提出了 Motion DUSt3R (MonST3R)，这是一种新颖的几何优先方法，可直接从动态场景中估计每个时间步的几何形状。我们的主要见解是，通过简单地估计每个时间步的点图，我们可以有效地将 DUST3R 的表示（以前仅用于静态场景）适应动态场景。然而，这种方法带来了一个重大挑战：缺乏合适的训练数据，即具有深度标签的动态、摆拍视频。尽管如此，我们表明，通过将问题作为微调任务，确定几个合适的数据集，并策略性地在这些有限的数据上训练模型，我们可以令人惊讶地使模型能够处理动态，即使没有显式运动表示。基于此，我们为几个下游视频特定任务引入了新的优化，并在视频深度和相机姿势估计方面表现出色，在稳健性和效率方面优于以前的工作。此外，MonST3R 在主要前馈 4D 重建方面显示出有希望的结果。]]></description>
      <guid>https://arxiv.org/abs/2410.03825</guid>
      <pubDate>Tue, 08 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>无监督先验学习：从视频中发现分类姿势先验</title>
      <link>https://arxiv.org/abs/2410.03858</link>
      <description><![CDATA[arXiv:2410.03858v1 公告类型：新
摘要：先验表示一组关于系统的信念或假设，有助于推理和决策。在这项工作中，我们介绍了姿势估计中无监督先验学习的挑战，其中人工智能模型以自我监督的方式从视频中学习动画对象的姿势先验。这些视频展示了执行各种动作的对象，提供了有关其关键点和连通性的关键信息。虽然先验在姿势估计中很有效，但获取它们可能很困难。我们提出了一种名为姿势先验学习器 (PPL) 的新方法来学习适用于任何对象类别的一般姿势先验。PPL 使用分层内存来存储原型姿势的组成部分，从中我们可以提取一般姿势先验。该先验通过模板转换和图像重建提高了姿势估计的准确性。PPL 无需任何额外的人工注释或干预即可学习有意义的姿势先验，在人类和动物姿势估计数据集上的表现均优于竞争基线。值得注意的是，我们的实验结果揭示了 PPL 使用学习先验知识对遮挡图像进行姿势估计的有效性。通过迭代推理，PPL 利用先验知识来优化估计的姿势，将它们回归到存储在内存中的任何原型姿势。我们的代码、模型和数据将公开提供。]]></description>
      <guid>https://arxiv.org/abs/2410.03858</guid>
      <pubDate>Tue, 08 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MDMP：用于不确定性监督运动预测的多模态扩散</title>
      <link>https://arxiv.org/abs/2410.03860</link>
      <description><![CDATA[arXiv:2410.03860v1 公告类型：新
摘要：本文介绍了一种用于运动预测的多模态扩散模型 (MDMP)，该模型集成并同步骨骼数据和动作的文本描述，以生成具有可量化不确定性的精确长期运动预测。现有的运动预测或运动生成方法仅依赖于先前的运动或文本提示，在精度或控制方面受到限制，尤其是在长时间内。我们方法的多模态特性增强了对人体运动的情境理解，而我们基于图的变换器框架有效地捕捉了空间和时间运动动态。因此，我们的模型在准确预测长期运动方面始终优于现有的生成技术。此外，通过利用扩散模型捕捉不同预测模式的能力，我们可以估计不确定性，通过结合每个身体关节具有不同置信度的存在区域，显着提高人机交互中的空间意识。]]></description>
      <guid>https://arxiv.org/abs/2410.03860</guid>
      <pubDate>Tue, 08 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过多视图可区分渲染细化单目深度图</title>
      <link>https://arxiv.org/abs/2410.03861</link>
      <description><![CDATA[arXiv:2410.03861v1 公告类型：新
摘要：精确重建图像的每像素深度对于计算机图形学、计算机视觉和机器人技术中的许多任务至关重要。在本文中，我们提出了一种新方法，从多个摆姿势的图像中生成视图一致且详细的深度图。我们利用单目深度估计的进步，生成拓扑完整但度量不准确的深度图，并在基于可微渲染器的两阶段优化过程中对其进行细化。以单目深度图作为输入，我们首先根据运动结构将该图缩放为绝对距离，然后将深度转换为三角形表面网格。然后，我们在局部优化中细化该深度网格，以增强光度和几何一致性。
我们的评估表明，我们的方法能够生成密集、详细、高质量的深度图，即使在具有挑战性的室内场景中也是如此，并且优于最先进的深度重建方法。该项目的概述和补充材料可以在https://lorafib.github.io/ref_depth/找到。]]></description>
      <guid>https://arxiv.org/abs/2410.03861</guid>
      <pubDate>Tue, 08 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SPARTUN3D：大型语言模型中 3D 世界的空间理解</title>
      <link>https://arxiv.org/abs/2410.03878</link>
      <description><![CDATA[arXiv:2410.03878v1 公告类型：新
摘要：将 3D 世界集成到大型语言模型（基于 3D 的 LLM）中一直是 3D 场景理解的一个有前途的研究方向。然而，目前基于 3D 的 LLM 在情境理解方面存在不足，原因是两个关键限制：1）现有的 3D 数据集是从 3D 场景的全局视角构建的，缺乏情境背景。2）现有基于 3D 的 LLM 的架构缺乏 3D 场景的空间表示与自然语言之间的明确对齐，限制了它们在需要精确空间推理的任务中的表现。我们通过引入一个可扩展的情境 3D 数据集 Spartun3D 来解决这些问题，该数据集包含各种情境空间推理任务。此外，我们提出了 Spartun3D-LLM，它建立在现有的基于 3D 的 LLM 上，但集成了一个新颖的情境空间对齐模块，旨在增强 3D 视觉表示与其相应文本描述之间的对齐。实验结果表明，我们提出的数据集和对齐模块都显著增强了基于 3D 的 LLM 的空间理解。]]></description>
      <guid>https://arxiv.org/abs/2410.03878</guid>
      <pubDate>Tue, 08 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>墙纸很丑：使用视觉和语言进行室内定位</title>
      <link>https://arxiv.org/abs/2410.03900</link>
      <description><![CDATA[arXiv:2410.03900v1 公告类型：新
摘要：我们研究使用自然语言查询和来自环境的图像在映射的室内环境中定位用户的任务。
基于最近预训练的视觉语言模型，我们学习了文本描述和环境中位置图像之间的相似度得分。
此分数使我们能够识别与语言查询最匹配的位置，从而估计用户的位置。
我们的方法能够定位在训练期间未见过的环境、文本和图像上。
在我们的评估中，一个经过微调的 CLIP 模型表现优于人类。]]></description>
      <guid>https://arxiv.org/abs/2410.03900</guid>
      <pubDate>Tue, 08 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>STONE：用于主动 3D 物体检测的子模块优化框架</title>
      <link>https://arxiv.org/abs/2410.03918</link>
      <description><![CDATA[arXiv:2410.03918v1 公告类型：新 
摘要：3D 物体检测对于各种新兴应用至关重要，包括自动驾驶和机器人技术。训练准确的 3D 物体检测器的一个关键要求是拥有大量基于 LiDAR 的点云数据。不幸的是，标记点云数据极具挑战性，因为每个潜在物体都需要准确的 3D 边界框和语义标签。本文提出了一个统一的主动 3D 物体检测框架，大大降低了训练 3D 物体检测器的标记成本。我们的框架基于一种新颖的子模优化公式，专门针对主动 3D 物体检测问题。具体来说，我们解决了与主动 3D 物体检测相关的两个基本挑战：数据不平衡和需要覆盖数据分布，包括不同难度级别的基于 LiDAR 的点云数据。大量实验表明，与现有的主动学习方法相比，我们的方法以高计算效率实现了最先进的性能。]]></description>
      <guid>https://arxiv.org/abs/2410.03918</guid>
      <pubDate>Tue, 08 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于视频修复的学习截断因果历史模型</title>
      <link>https://arxiv.org/abs/2410.03936</link>
      <description><![CDATA[arXiv:2410.03936v1 公告类型：新
摘要：视频修复的一个关键挑战是模拟受运动控制的视频帧的过渡动态。在这项工作中，我们提出了 TURTLE 来学习截断因果历史模型，以实现高效、高性能的视频修复。与并行处理一系列上下文帧的传统方法不同，TURTLE 通过将输入帧潜在表示的截断历史存储并汇总为不断发展的历史状态来提高效率。这是通过一种复杂的基于相似性的检索机制实现的，该机制隐式地考虑了帧间运动和对齐。TURTLE 中的因果设计通过状态记忆的历史特征实现推理的重复，同时允许通过采样截断的视频片段进行并行训练。我们报告了在众多视频修复基准任务上的最新成果，包括视频去雪、夜间视频去雨、视频雨滴和雨纹去除、视频超分辨率、真实世界和合成视频去模糊以及盲视频去噪，同时与所有这些任务上现有的最佳上下文方法相比降低了计算成本。]]></description>
      <guid>https://arxiv.org/abs/2410.03936</guid>
      <pubDate>Tue, 08 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AutoLoRA：AutoGuidance 满足扩散模型的低秩自适应要求</title>
      <link>https://arxiv.org/abs/2410.03941</link>
      <description><![CDATA[arXiv:2410.03941v1 公告类型：新
摘要：低秩自适应 (LoRA) 是一种可应用于条件生成扩散模型的微调技术。LoRA 利用少量上下文示例将模型适应特定领域、特征、风格或概念。然而，由于训练期间使用的数据有限，微调后的模型性能通常具有强烈的上下文偏差和生成的图像的低变异度。为了解决这个问题，我们引入了 AutoLoRA，这是一种使用 LoRA 方法微调的扩散模型的新型指导技术。受其他指导技术的启发，AutoLoRA 在 LoRA 权重所代表的域中的一致性和来自基本条件扩散模型的样本多样性之间寻找一种平衡。此外，我们表明，将无分类器指导结合到 LoRA 微调和基本模型中可以生成具有更高多样性和更好质量的样本。几个微调的 LoRA 域的实验结果显示，在选定的指标上优于现有的指导技术。]]></description>
      <guid>https://arxiv.org/abs/2410.03941</guid>
      <pubDate>Tue, 08 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>学习平衡：换衣服后人员重新识别的多样化规范化</title>
      <link>https://arxiv.org/abs/2410.03977</link>
      <description><![CDATA[arXiv:2410.03977v1 公告类型：新
摘要：衣服变化行人重新识别 (CC-ReID) 涉及识别图像中的个人，无论其着装状态如何。在本文中，我们通过经验和实验证明，完全消除或完全保留服装特征对任务是有害的。现有的工作，无论是依靠服装标签、轮廓还是其他辅助数据，从根本上都旨在平衡服装和身份特征的学习。然而，我们实际上发现实现这种平衡具有挑战性和微妙性。在这项研究中，我们引入了一个名为 Diverse Norm 的新模块，它将个人特征扩展到正交空间，并利用通道注意力来分离服装和身份特征。还引入了一个样本重新加权优化策略来保证相反的优化方向。Diverse Norm 提出了一种简单而有效的方法，不需要额外的数据。此外，Diverse Norm 可以无缝集成 ResNet50，并且明显优于最先进的方法。]]></description>
      <guid>https://arxiv.org/abs/2410.03977</guid>
      <pubDate>Tue, 08 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>