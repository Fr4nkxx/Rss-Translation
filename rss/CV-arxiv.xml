<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Thu, 14 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>TIPO：文本转图像，使用文本预采样进行快速优化</title>
      <link>https://arxiv.org/abs/2411.08127</link>
      <description><![CDATA[arXiv:2411.08127v1 公告类型：新 
摘要：TIPO（带有文本预采样的文本到图像提示优化）是一个创新框架，旨在增强语言模型（LM）的文本到图像（T2I）生成，以实现自动提示工程。通过改进和扩展用户提供的提示，TIPO 弥合了简单输入与高质量图像生成所需的详细提示之间的差距。与以前依赖大型语言模型（LLM）或强化学习（RL）的方法不同，TIPO 使用训练有素的提示数据集的分布来调整用户输入提示，从而无需通过轻量级模型来提高复杂的运行时成本。这种预采样方法基于模型的训练分布，实现了高效且可扩展的提示优化。实验结果表明，TIPO 在提高美学分数、减少图像损坏以及更好地将生成的图像与数据集分布对齐方面的有效性。这些发现强调了提示工程在 T2I 系统中的关键作用，并为自动提示细化的更广泛应用开辟了道路。]]></description>
      <guid>https://arxiv.org/abs/2411.08127</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CameraHMR：将人物与视角对齐</title>
      <link>https://arxiv.org/abs/2411.08128</link>
      <description><![CDATA[arXiv:2411.08128v1 公告类型：新
摘要：我们解决了从单目图像中准确估计 3D 人体姿势和形状的挑战。准确性和稳健性的关键在于高质量的训练数据。现有的包含具有伪地面实况 (pGT) 的真实图像的训练数据集使用 SMPLify 将 SMPL 拟合到稀疏的 2D 关节位置，假设具有默认内在函数的简化相机。我们做出了两项贡献来提高 pGT 的准确性。首先，为了估计相机内在函数，我们开发了一个在包含人物的图像数据集上训练的视场预测模型 (HumanFoV)。我们使用估计的内在函数通过在 SMPLify 拟合期间合并全透视相机模型来增强 4D-Humans 数据集。其次，2D 关节对 3D 体形的约束有限，导致身体看起来很普通。为了解决这个问题，我们使用 BEDLAM 数据集来训练密集的表面关键点检测器。我们将此检测器应用于 4D-Humans 数据集，并修改 SMPLify 以适应检测到的关键点，从而产生更加逼真的身体形状。最后，我们升级 HMR2.0 架构以包含估计的相机参数。我们迭代模型训练和使用之前训练的模型初始化的 SMPLify 拟合。这导致更准确的 pGT 和具有最先进精度的新模型 CameraHMR。代码和 pGT 可用于研究目的。]]></description>
      <guid>https://arxiv.org/abs/2411.08128</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于野火检测的迁移学习与定制 VGG 和 CNN-SVM 模型的全面比较分析</title>
      <link>https://arxiv.org/abs/2411.08171</link>
      <description><![CDATA[arXiv:2411.08171v1 公告类型：新
摘要：当代人工智能 (AI) 和机器学习 (ML) 研究非常重视迁移学习，展示了其在提高不同领域模型性能方面的变革潜力。本文研究了迁移学习在野火检测方面的效率和有效性。三个专用模型——视觉几何组 (VGG)-7、VGG-10 和卷积神经网络 (CNN)-支持向量机 (SVM) CNN-SVM——与三个预训练模型——VGG-16、VGG-19 和残差神经网络 (ResNet) ResNet101 进行了严格比较。我们使用一个数据集来训练和评估这些模型，该数据集捕捉了野火的复杂性，结合了不同的光照条件、一天中的时间和不同的地形等变量。目的是辨别迁移学习在解决野火检测问题的复杂性方面与从头开始训练的模型相比的表现如何。通过评估准确度、精确度、召回率和 F1 分数等性能指标，可以全面了解迁移学习在此特定领域的优缺点。这项研究为正在进行的讨论提供了宝贵的见解，指导了 AI 和 ML 研究的未来方向。关键词：野火预测、深度学习、机器学习火灾、检测]]></description>
      <guid>https://arxiv.org/abs/2411.08171</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>TractoEmbed：用于白质束分割的模块化多级嵌入框架</title>
      <link>https://arxiv.org/abs/2411.08187</link>
      <description><![CDATA[arXiv:2411.08187v1 公告类型：新
摘要：白质束分割对于研究大脑结构连接和神经外科规划至关重要。然而，由于主要束和次要束之间的类别不平衡、结构相似性、受试者变异性、半球之间的对称流线等问题，分割仍然具有挑战性。为了应对这些挑战，我们提出了 TractoEmbed，这是一个模块化多级嵌入框架，它通过各自编码器中的学习任务对局部表示进行编码。在本文中，TractoEmbed 引入了一种新颖的分层流线数据表示，它可以捕获每个级别（即单个流线、簇和斑块）的最大空间信息。实验表明，TractoEmbed 在不同数据集和不同年龄组的白质束分割方面优于最先进的方法。模块化框架直接允许在未来的工作中集成额外的嵌入。]]></description>
      <guid>https://arxiv.org/abs/2411.08187</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一种可解释的机器学习方法，利用牙科生物特征估计活体个体的年龄和性别</title>
      <link>https://arxiv.org/abs/2411.08195</link>
      <description><![CDATA[arXiv:2411.08195v1 公告类型：新
摘要：目标：年龄和性别估计对于各种应用都至关重要，包括法医调查和人类学研究。本研究旨在开发一种用于估计活体个体年龄和性别的预测系统，利用牙冠高度 (CH)、牙髓腔冠高度 (CPCH) 和牙冠指数 (TCI) 等牙科测量值。方法：我们的研究采用了机器学习模型，包括 Cat Boost 分类器 (Catboost)、梯度提升机 (GBM)、Ada Boost 分类器 (AdaBoost)、随机森林 (RF)、eXtreme 梯度提升 (XGB)、轻梯度提升机 (LGB) 和额外树分类器 (ETC)，以分析来自 862 名活体个体（459 名男性和 403 名女性）的牙科数据。具体来说，我们使用了每个个体六颗牙齿的根尖 X 光片，包括上颌和下颌的前磨牙和磨牙。我们开发了一种新颖的集成学习技术，该技术使用多个针对不同牙科指标量身定制的模型来准确估计年龄和性别。此外，我们还利用 SHAP 创建了一个可解释的 AI 模型，使牙科专家能够根据可理解的见解做出明智的决定。结果：RF 和 XGB 模型特别有效，在年龄和性别估计方面获得了最高的 F1 分数。值得注意的是，XGB 模型在年龄估计方面的表现略好，F1 得分为 73.26%。RF 模型在性别估计方面也观察到了类似的趋势，F1 得分为 77.53%。结论：这项研究标志着牙科法医方法的重大进步，展示了机器学习在自动化年龄和性别估计过程并以更高的准确性进行的潜力。]]></description>
      <guid>https://arxiv.org/abs/2411.08195</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>扩散变换器中的潜在空间解缠可实现精确的零样本语义编辑</title>
      <link>https://arxiv.org/abs/2411.08196</link>
      <description><![CDATA[arXiv:2411.08196v1 公告类型：新
摘要：扩散变换器 (DiT) 最近在文本引导的图像生成方面取得了显著的成功。在图像编辑中，DiT 将文本和图像输入投射到联合潜在空间，从中解码并合成新图像。然而，多模态信息如何共同形成这个联合空间以及它们如何指导合成图像的语义，这在很大程度上仍未得到探索。在本文中，我们研究了 DiT 模型的潜在空间并发现了两个关键属性：首先，DiT 的潜在空间本质上是语义解缠的，其中不同的语义属性可以通过特定的编辑方向来控制。其次，一致的语义编辑需要利用整个联合潜在空间，因为编码图像或文本本身都不包含足够的语义信息。我们表明这些编辑方向可以直接从文本提示中获得，从而无需额外的训练或掩码注释即可实现精确的语义控制。基于这些见解，我们提出了一个简单但有效的编码-识别-操作 (EIM) 框架，用于零样本细粒度图像编辑。具体来说，我们首先对给定的源图像和描述图像的文本提示进行编码，以获得联合潜在嵌入。然后，使用我们提出的 Hessian 分数蒸馏采样 (HSDS) 方法，我们识别控制特定目标属性同时保留其他图像特征的编辑方向。这些方向由文本提示引导并用于操作潜在嵌入。此外，我们提出了一个新的指标来量化扩散模型潜在空间的解缠结程度。我们在新策划的基准数据集和分析上进行的大量实验结果证明了 DiT 的解缠结特性和 EIM 框架的有效性。]]></description>
      <guid>https://arxiv.org/abs/2411.08196</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GTA：运动多目标追踪全球 Tracklet 协会</title>
      <link>https://arxiv.org/abs/2411.08216</link>
      <description><![CDATA[arXiv:2411.08216v1 公告类型：新
摘要：运动场景中的多目标跟踪已成为计算机视觉的焦点之一，通过整合深度学习技术取得了重大进展。尽管取得了这些突破，但挑战仍然存在，例如在重新进入场景时准确重新识别玩家并尽量减少 ID 切换。在本文中，我们提出了一种基于外观的全局轨迹关联算法，旨在通过拆分包含多个身份的轨迹并连接看似来自同一身份的轨迹来提高跟踪性能。此方法可以作为任何多目标跟踪器的即插即用细化工具，以进一步提高其性能。所提出的方法在 SportsMOT 数据集上取得了新的最佳性能，HOTA 得分为 81.04%。同样，在 SoccerNet 数据集上，我们的方法增强了多个跟踪器的性能，将 HOTA 得分从 79.41% 持续提高到 83.11%。这些跨不同追踪器和数据集的显著且一致的改进凸显了我们提出的方法对运动员追踪应用的潜在影响。我们在 https://github.com/sjc042/gta-link.git 上开源了我们的项目代码库。]]></description>
      <guid>https://arxiv.org/abs/2411.08216</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DPU：用于多模态分布检测的动态原型更新</title>
      <link>https://arxiv.org/abs/2411.08227</link>
      <description><![CDATA[arXiv:2411.08227v1 公告类型：新
摘要：分布外 (OOD) 检测通过识别偏离训练分布的样本，对于确保机器学习模型的稳健性至关重要。虽然传统的 OOD 检测主要关注单模态输入，例如图像，但多模态模型的最新进展已经证明了利用多种模态（例如视频、光流、音频）来增强检测性能的潜力。然而，现有的方法往往忽略了分布内 (ID) 数据中的类内变异性，假设同一类的样本是完全内聚和一致的。这种假设可能会导致性能下降，尤其是当预测差异在所有样本中均匀放大时。为了解决这个问题，我们提出了动态原型更新 (DPU)，这是一种用于多模态 OOD 检测的新型即插即用框架，可以考虑类内变化。我们的方法通过测量每个批次中相似样本的方差来动态更新每个类的类中心表示，从而实现自适应调整。这种方法使我们能够根据更新后的类中心放大预测差异，从而提高模型在不同模态中的稳健性和泛化能力。对两个任务、五个数据集和九种基本 OOD 算法进行的大量实验表明，DPU 显著提高了 OOD 检测性能，在多模态 OOD 检测中创下了新的最高水平，Far-OOD 检测的改进率高达 80%。为了方便访问和可重复性，我们的代码在 GitHub 上公开提供。]]></description>
      <guid>https://arxiv.org/abs/2411.08227</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LBONet：用于形状分析的监督光谱描述符</title>
      <link>https://arxiv.org/abs/2411.08272</link>
      <description><![CDATA[arXiv:2411.08272v1 公告类型：新
摘要：拉普拉斯-贝尔特拉米算子因其许多有用的特性而在非刚性形状分析领域确立了自己的地位，例如在等距变换下不变、具有可数特征系统形成正交基以及完全表征流形的测地线距离。然而，这种不变性仅适用于等距变形，这导致许多实际应用中的性能下降。近年来，人们一直强调使用深度学习方法提取最佳特征，但光谱特征起着至关重要的作用，仍然具有价值。在本文中，我们退后一步，重新审视 LBO，并提出一种监督的方式来学习流形上的几个运算符。根据任务，通过应用这些函数，我们可以训练 LBO 特征基，使其更加针对任务。 LBO 的优化极大地改进了已建立的描述符，例如检索、分类、分割和对应等各种任务中的热核特征，证明了 LBO 特征基对全局和高度局部学习设置的适应性。]]></description>
      <guid>https://arxiv.org/abs/2411.08272</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MBA-SLAM：具有辐射场表示的运动模糊感知密集视觉 SLAM</title>
      <link>https://arxiv.org/abs/2411.08279</link>
      <description><![CDATA[arXiv:2411.08279v1 公告类型：新 
摘要：新兴的 3D 场景表示，例如神经辐射场 (NeRF) 和 3D 高斯溅射 (3DGS)，已证明其在照片级逼真渲染的同步定位和映射 (SLAM) 中的有效性，尤其是在使用高质量视频序列作为输入时。然而，现有的方法难以处理运动模糊帧，这在现实世界场景中很常见，例如低光或长时间曝光条件。这通常会导致相机定位精度和地图重建质量显着降低。为了应对这一挑战，我们提出了一种密集的视觉 SLAM 管道（即 MBA-SLAM）来处理严重的运动模糊输入。我们的方法将高效的运动模糊感知跟踪器与神经辐射场或基于高斯溅射的映射器相结合。通过精确建模运动模糊图像的物理成像过程，我们的方法可以同时学习 3D 场景表示并估计相机在曝光时间内的局部轨迹，从而能够主动补偿相机移动造成的运动模糊。在我们的实验中，我们证明了 MBA-SLAM 在相机定位和地图重建方面都超越了之前最先进的方法，在一系列数据集上展示了卓越的性能，包括具有清晰图像的合成和真实数据集以及受运动模糊影响的数据集，凸显了我们方法的多功能性和稳健性。代码可在 https://github.com/WU-CVGL/MBA-SLAM 上找到。]]></description>
      <guid>https://arxiv.org/abs/2411.08279</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>主动成像仪复原算法及系统性能评估</title>
      <link>https://arxiv.org/abs/2411.08291</link>
      <description><![CDATA[arXiv:2411.08291v1 公告类型：新
摘要：本文涉及与主动成像系统相关的两个领域。首先，我们开始探索图像处理算法，以恢复由大气湍流引起的斑点、闪烁和图像舞动等伪影。接下来，我们研究如何评估此类系统的性能。为了完成这项任务，我们提出了德国 TRM3 度量的修改版本，该版本允许获得类似 MTF 的测量值。我们使用在 NATO-TG40 现场试验期间获得的数据库进行测试。]]></description>
      <guid>https://arxiv.org/abs/2411.08291</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>噪声图像分解：基于局部自适应的新结构、纹理和噪声模型</title>
      <link>https://arxiv.org/abs/2411.08292</link>
      <description><![CDATA[arXiv:2411.08292v1 公告类型：新
摘要：最近几年，人们提出了图像分解算法，将图像分成两部分：结构和纹理。这些算法不适用于噪声图像的情况，因为纹理被噪声破坏。在本文中，我们提出了一种新模型，该模型基于局部正则化方案将图像分解为三个部分（结构、纹理和噪声）。我们将我们的结果与 Aujol 和 Chambolle 的最新工作进行了比较。最后，我们给出了另一个模型，该模型结合了前两个模型的优点。]]></description>
      <guid>https://arxiv.org/abs/2411.08292</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>选择适合道路网络检测的图像表示空间</title>
      <link>https://arxiv.org/abs/2411.08293</link>
      <description><![CDATA[arXiv:2411.08293v1 公告类型：新
摘要：近年来，出现了允许将图像分解为其结构和纹理成分的算法。在本文中，我们介绍了这种分解类型在航空或卫星图像中道路网络检测问题中的应用。算法过程涉及图像分解（使用独特属性）、基于格式塔理论的对齐检测步骤以及使用统计活动轮廓的细化步骤。]]></description>
      <guid>https://arxiv.org/abs/2411.08293</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>运动控制增强复杂动作视频生成</title>
      <link>https://arxiv.org/abs/2411.08328</link>
      <description><![CDATA[arXiv:2411.08328v1 公告类型：新
摘要：现有的文本转视频 (T2V) 模型通常难以生成具有足够明显或复杂动作的视频。一个关键的限制在于文本提示无法精确传达复杂的运动细节。为了解决这个问题，我们提出了一个新颖的框架 MVideo，旨在制作具有精确流畅动作的长时间视频。MVideo 通过将掩码序列作为额外的运动条件输入来克服文本提示的局限性，从而提供更清晰、更准确的预期动作表示。利用 GroundingDINO 和 SAM2 等基础视觉模型，MVideo 可自动生成掩码序列，从而提高效率和稳健性。我们的结果表明，经过训练后，MVideo 可以有效地将文本提示与运动条件对齐，从而生成同时满足两个标准的视频。这种双重控制机制允许通过独立更改文本提示或运动条件或同时更改两者来生成更动态的视频。此外，MVideo 支持运动条件编辑和合成，有助于生成具有更复杂动作的视频。MVideo 从而推动了 T2V 运动生成，为当前视频传播模型中改进动作描绘树立了强有力的标杆。我们的项目页面位于 https://mvideo-v1.github.io/。]]></description>
      <guid>https://arxiv.org/abs/2411.08328</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SASE：一种用于挤压和激励操作的搜索架构</title>
      <link>https://arxiv.org/abs/2411.08333</link>
      <description><![CDATA[arXiv:2411.08333v1 公告类型：新
摘要：在过去的几年中，通道和空间注意模块已被广泛用作深度神经网络的补充模块，在降低复杂度的同时增强了网络的表示能力。大多数注意模块遵循挤压和激励范式。然而，设计这样的注意模块需要大量的实验和计算资源。同时，神经架构搜索 (NAS) 能够自动化神经网络的设计，并省去了优化架构所需的大量实验。这促使我们设计一种搜索架构，可以通过 NAS 自动找到接近最优的注意模块。我们提出了 SASE，一种用于挤压和激励操作的搜索架构，通过在特定的搜索空间内搜索来形成即插即用的注意模块。搜索空间分为 4 个不同的集合，每个集合对应于沿通道或空间维度的挤压或激励操​​作。此外，搜索集不仅包括现有的注意模块，还包括以前未在注意机制中使用过的其他操作。据我们所知，SASE 是首次尝试细分注意力搜索空间并搜索超出当前已知注意力模块的架构。搜索到的注意力模块在一系列视觉任务中经过了大量实验测试。实验结果表明，与使用当前最先进的注意力模块的网络相比，使用 SASE 注意力模块的视觉主干网络 (ResNet-50/101) 实现了最佳性能。代码包含在补充材料中，稍后将公开。]]></description>
      <guid>https://arxiv.org/abs/2411.08333</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>