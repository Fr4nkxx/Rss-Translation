<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Wed, 29 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>DynAlign：用于跨域分割的无监督动态分类对齐</title>
      <link>https://arxiv.org/abs/2501.16410</link>
      <description><![CDATA[arXiv:2501.16410v1 公告类型：新
摘要：当前用于语义分割的无监督域自适应 (UDA) 方法通常假设源域和目标域之间的类标签相同。这种假设忽略了标签级域差距，这在现实世界场景中很常见，从而限制了它们在不需要大量手动注释的情况下识别更细粒度或新类别的能力。解决这一限制的一个有希望的方向在于基础模型的最新进展，这些模型由于其丰富的先验知识而表现出强大的泛化能力。然而，这些模型往往难以处理特定领域的细微差别和代表性不足的细粒度类别。
为了应对这些挑战，我们引入了 DynAlign，这是一个将 UDA 与基础模型集成的框架，以弥合图像级和标签级域差距。我们的方法利用先验语义知识将源类别与目标类别对齐，这些目标类别可以是新颖的、更细粒度的或以不同方式命名的（例如，车辆到{汽车、卡车、公共汽车}）。然后使用基础模型进行精确分割和类别重新分配。为了进一步提高准确性，我们提出了一种知识融合方法，可以动态适应不同的场景环境。DynAlign 在新的目标标签空间中生成准确的预测，而无需任何手动注释，从而可以通过模型再训练或直接推理无缝适应新的分类法。
在街景语义分割基准 GTA 到 Mapillary Vistas 和 GTA 到 IDD 上进行的实验验证了我们方法的有效性，并实现了对现有方法的显着改进。我们的代码将公开提供。]]></description>
      <guid>https://arxiv.org/abs/2501.16410</guid>
      <pubDate>Wed, 29 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PhysBench：为物理世界理解而对视觉语言模型进行基准测试和增强</title>
      <link>https://arxiv.org/abs/2501.16411</link>
      <description><![CDATA[arXiv:2501.16411v1 公告类型：新
摘要：理解物理世界是具身人工智能的一个基本挑战，对于使代理能够执行复杂任务并在现实环境中安全运行至关重要。虽然视觉语言模型 (VLM) 在具身代理的推理和任务规划方面表现出巨大的潜力，但它们理解物理现象的能力仍然极其有限。为了弥补这一差距，我们推出了 PhysBench，这是一个全面的基准，旨在评估 VLM 在一系列不同任务中的物理世界理解能力。PhysBench 包含 100,000 个交错的视频图像文本数据条目，分为四个主要领域：物理对象属性、物理对象关系、物理场景理解和基于物理的动力学，进一步分为 19 个子类和 8 个不同的能力维度。我们对 75 个具有代表性的 VLM 进行了广泛的实验，结果表明，虽然这些模型在常识推理方面表现出色，但它们在理解物理世界方面却举步维艰——这可能是由于训练数据中缺乏物理知识，并且缺乏嵌入式物理先验。为了解决这一不足，我们引入了 PhysAgent，这是一个新颖的框架，它将 VLM 的泛化优势与视觉模型的专业知识相结合，显著增强了 VLM 在各种任务中的物理理解能力，包括在 GPT-4o 上提高了 18.4%。此外，我们的结果表明，增强 VLM 的物理世界理解能力可以帮助 MOKA 等具身代理。我们相信 PhysBench 和 PhysAgent 提供了宝贵的见解，并有助于弥合 VLM 与物理世界理解之间的差距。]]></description>
      <guid>https://arxiv.org/abs/2501.16411</guid>
      <pubDate>Wed, 29 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用大型语言模型辅助描述符生成进行跨域语义分割</title>
      <link>https://arxiv.org/abs/2501.16467</link>
      <description><![CDATA[arXiv:2501.16467v1 公告类型：新
摘要：语义分割在使机器能够在像素级别理解和解释视觉场景方面起着至关重要的作用。虽然传统的分割方法取得了显著的成功，但它们对不同场景和看不见的对象类别的推广仍然有限。大型语言模型 (LLM) 的最新进展为连接视觉和文本模态提供了一条有希望的途径，提供了对语义关系的更深入的理解。在本文中，我们提出了 LangSeg，一种新颖的 LLM 引导语义分割方法，它利用 LLM 生成的上下文敏感、细粒度的子类描述符。我们的框架将这些描述符与预先训练的视觉变换器 (ViT) 相结合，以实现卓越的分割性能，而无需大量的模型再训练。我们在两个具有挑战性的数据集 ADE20K 和 COCO-Stuff 上对 LangSeg 进行了评估，结果表明它的表现优于最先进的模型，平均交并比 (mIoU) 提高了 6.1%。此外，我们还进行了全面的消融研究和人工评估，以验证我们的方法在现实场景中的有效性。结果表明，LangSeg 不仅在语义理解和上下文对齐方面表现出色，而且还为语言引导的分割任务提供了灵活高效的框架。这种方法为交互式和特定领域的分割应用开辟了新的可能性。]]></description>
      <guid>https://arxiv.org/abs/2501.16467</guid>
      <pubDate>Wed, 29 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>医学图像分析中的物体检测：来自 RT-DETR 模型的见解</title>
      <link>https://arxiv.org/abs/2501.16469</link>
      <description><![CDATA[arXiv:2501.16469v1 公告类型：新
摘要：深度学习已成为解决复杂模式识别和物体检测挑战的一种变革性方法。本文重点介绍了基于 RT-DETR 模型的新型检测框架在分析复杂图像数据中的应用，特别是在糖尿病视网膜病变检测等领域。糖尿病视网膜病变是全球视力丧失的主要原因，需要准确有效的图像分析来识别早期病变。所提出的 RT-DETR 模型建立在基于 Transformer 的架构上，擅长处理高维和复杂的视觉数据，具有增强的鲁棒性和准确性。与 YOLOv5、YOLOv8、SSD 和 DETR 等模型的比较评估表明，RT-DETR 在精度、召回率、mAP50 和 mAP50-95 指标方面均取得了优异的性能，尤其是在检测小规模物体和密集目标方面。这项研究强调了 RT-DETR 等基于 Transformer 的模型在推进物体检测任务方面的潜力，为医学成像等领域提供了有前景的应用。]]></description>
      <guid>https://arxiv.org/abs/2501.16469</guid>
      <pubDate>Wed, 29 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 LLM 为零样本稀有事件医学图像分类生成定制提示</title>
      <link>https://arxiv.org/abs/2501.16481</link>
      <description><![CDATA[arXiv:2501.16481v1 公告类型：新
摘要：罕见事件由于发生频率低，数据量不大，因此深度学习技术无法估计此类数据的分布。开放词汇模型代表了一种创新的图像分类方法。与传统模型不同，这些模型在推理过程中将图像分类到用自然语言提示指定的任何类别集合中。这些提示通常包括手工制作的模板（例如，“{} 的照片”），其中填写了每个类别的名称。本文介绍了一种简单而有效的方法，用于生成包含判别特征的高度准确和上下文描述性提示。罕见事件检测，尤其是在医学领域，由于类间差异低和类内差异高而更具挑战性。为了解决这些问题，我们提出了一种新方法，该方法使用罕见事件领域的专家知识来生成定制的和上下文相关的提示，然后由大型语言模型使用这些提示进行图像分类。我们的零样本、隐私保护方法无需额外训练即可增强罕见事件分类，其表现优于最先进的技术。]]></description>
      <guid>https://arxiv.org/abs/2501.16481</guid>
      <pubDate>Wed, 29 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于多目标深度学习的生物力学可变形图像配准与 MOREA</title>
      <link>https://arxiv.org/abs/2501.16525</link>
      <description><![CDATA[arXiv:2501.16525v1 公告类型：新
摘要：当为具有较大变形和内容不匹配的图像选择可变形图像配准 (DIR) 方法时，通常需要在找到的变换的真实性与所需的运行时间之间进行权衡。使用深度学习 (DL) 技术的 DIR 方法在即时预测变换方面表现出了非凡的前景。然而，在困难的配准问题上，这些变换的真实性可能会不足。使用生物力学有限元建模 (FEM) 技术的 DIR 方法可以找到更真实的变换，但往往需要更长的运行时间。这项工作提出了第一种将它们结合起来的混合方法，目的是兼顾两全其美。这种称为 DL-MOREA 的混合方法结合了最近推出的基于 DL 的多目标 DIR 方法，该方法利用 VoxelMorph 框架，称为 DL-MODIR，以及 MOREA，这是一种基于进化算法的多目标 DIR 方法，其中使用类似 FEM 的生物力学网格变换模型。在我们提出的混合方法中，DL 结果用于智能初始化 MOREA，目的是更有效地优化其网格变换模型。我们在 CT 扫描对上对 DL-MOREA 与其组件 DL-MODIR 和 MOREA 进行了实证比较，这些 CT 扫描对捕捉到了 15 名宫颈癌患者的膀胱充盈差异较大。虽然 MOREA 需要 45 分钟的中位运行时间，但 DL-MOREA 在 5 分钟后已经可以找到高质量的变换。与 DL-MODIR 变换相比，DL-MOREA 发现的变换表现出更少的折叠，并改善或保留了膀胱轮廓距离误差。]]></description>
      <guid>https://arxiv.org/abs/2501.16525</guid>
      <pubDate>Wed, 29 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PackDiT：通过相互提示实现人体运动与文本的联合生成</title>
      <link>https://arxiv.org/abs/2501.16551</link>
      <description><![CDATA[arXiv:2501.16551v1 公告类型：新
摘要：随着扩散模型的出现，人体运动生成取得了显著进展。最近的研究集中在基于文本提示生成运动序列，通常称为文本到运动生成。然而，运动和文本的双向生成，使诸如运动到文本和文本到运动等任务成为可能，在很大程度上尚未得到探索。这种能力对于协调不同的模态至关重要，并支持无条件生成。在本文中，我们介绍了 PackDiT，这是第一个基于扩散的生成模型，能够同时执行各种任务，包括运动生成、运动预测、文本生成、文本到运动、运动到文本和联合运动文本生成。我们的核心创新利用相互块无缝集成不同模态的多个扩散变换器 (DiT)。我们在 HumanML3D 数据集上训练了 PackDiT，实现了最佳的文本转运动性能，FID 得分为 0.106，并且在运动预测和中间任务中也取得了优异的成绩。我们的实验进一步证明了扩散模型对于运动转文本生成是有效的，其性能可与自回归模型相媲美。]]></description>
      <guid>https://arxiv.org/abs/2501.16551</guid>
      <pubDate>Wed, 29 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LoRA-X：桥接基础模型与免训练跨模型自适应</title>
      <link>https://arxiv.org/abs/2501.16559</link>
      <description><![CDATA[arXiv:2501.16559v1 公告类型：新
摘要：大型基础模型的日益普及导致对参数高效微调方法的需求日益增加，例如低秩自适应 (LoRA)，它提供与完整模型微调相当的性能，同时只需要针对特定​​基础模型定制一些额外的参数。当此类基础模型被弃用和替换时，所有相关的 LoRA 模块都必须重新训练，需要访问原始训练数据或大量反映原始分布的合成数据。然而，由于隐私或许可问题，原始数据通常无法访问，而生成合成数据可能不切实际且代表性不足。这些因素使微调过程变得相当复杂。为了应对这一挑战，我们引入了一个新的适配器，即跨模型低秩自适应 (LoRA-X)，它能够在源和目标模型之间进行无需训练的 LoRA 参数传输，从而无需原始或合成训练数据。我们的方法要求适配器在源基础模型的子空间内运行。这种约束是必要的，因为我们对目标模型的先验知识仅限于其权重，而确保适配器可转移性的标准仅限于目标基础模型的权重和子空间。为了便于将源模型的 LoRA 参数转移到目标模型，我们仅在目标模型中表现出可接受的子空间相似度的层中使用适配器。我们进行了广泛的实验，证明了 LoRA-X 在文本到图像生成方面的有效性，包括 Stable Diffusion v1.5 和 Stable Diffusion XL。]]></description>
      <guid>https://arxiv.org/abs/2501.16559</guid>
      <pubDate>Wed, 29 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用修剪后的 YOLO 模型高效检测海洋垃圾</title>
      <link>https://arxiv.org/abs/2501.16571</link>
      <description><![CDATA[arXiv:2501.16571v1 公告类型：新
摘要：海洋垃圾对海洋生物造成重大危害，因为诸如微塑料、多氯联苯和杀虫剂等物质会破坏栖息地并毒害生物。潜水等人类解决方案在解决这一问题方面越来越无效。自主水下航行器 (AUV) 正在开发中，以高效收集海上垃圾，而物体检测架构的选择至关重要。本研究采用 YOLOv4 模型，使用 Trash-ICRA 19 数据集实时检测海洋垃圾，该数据集包含 7683 张 480x320 像素的图像。对各种修改（预训练模型、从头开始训练、马赛克增强、层冻结、YOLOv4-tiny 和通道修剪）进行了比较，以提高架构效率。通道修剪显著提高了检测速度，将基本 YOLOv4 帧率从 15.19 FPS 提高到 19.4 FPS，而平均精度仅下降 1.2%，从 97.6% 下降到 96.4%。]]></description>
      <guid>https://arxiv.org/abs/2501.16571</guid>
      <pubDate>Wed, 29 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>将 Mamba 引导至复杂纹理：用于图像恢复的高效纹理感知状态空间模型</title>
      <link>https://arxiv.org/abs/2501.16583</link>
      <description><![CDATA[arXiv:2501.16583v1 公告类型：新
摘要：图像恢复旨在恢复细节并增强退化图像中的对比度。随着对高质量成像（\textit{例如}，4K 和 8K）的需求不断增长，实现恢复质量和计算效率之间的平衡变得越来越重要。现有方法主要基于 CNN、Transformers 或它们的混合方法，在整个图像上应用统一的深度表示提取。然而，这些方法通常难以有效地模拟长距离依赖关系，并且在很大程度上忽略了图像退化的空间特征（纹理更丰富的区域往往遭受更严重的损坏），因此很难在恢复质量和效率之间实现最佳平衡。为了解决这些问题，我们提出了一种新颖的纹理感知图像恢复方法 TAMambaIR，它可以同时感知图像纹理并在性能和效率之间实现平衡。具体来说，我们引入了一种新颖的纹理感知状态空间模型，该模型通过调节状态空间方程的转换矩阵并关注具有复杂纹理的区域来增强纹理感知并提高效率。此外，我们设计了一个{多方向感知块}来改善多方向感受野，同时保持较低的计算开销。在图像超分辨率、去雨和低光图像增强的基准上进行的大量实验表明，TAMambaIR 实现了最先进的性能，效率显著提高，使其成为一个强大而高效的图像恢复框架。]]></description>
      <guid>https://arxiv.org/abs/2501.16583</guid>
      <pubDate>Wed, 29 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于步态识别的无监督域自适应动态聚类和对比细化</title>
      <link>https://arxiv.org/abs/2501.16608</link>
      <description><![CDATA[arXiv:2501.16608v1 公告类型：新 
摘要：步态识别是一种新兴的身份识别技术，通过分析个体的步行模式来区分远距离的个体。传统技术严重依赖大规模标记数据集，成本高昂，标记挑战巨大。最近，研究人员探索了基于聚类的无监督领域自适应方法进行无监督步态识别并取得了显著的成功。然而，这些方法直接使用聚类产生的伪标签，忽略了领域差异引起的伪标签噪声，影响了模型训练过程的效果。为了缓解这些问题，我们提出了一种名为 GaitDCCR 的新模型，旨在减少嘈杂的伪标签对聚类和模型训练的影响。我们的方法可以分为两个主要阶段：聚类和训练阶段。在聚类阶段，我们提出动态聚类参数 (DCP) 和动态权重质心 (DWC) 来提高聚类效率并获得可靠的聚类质心。在训练阶段，我们采用经典的师生结构，并提出基于置信度的伪标签细化 (CPR) 和对比教师模块 (CTM)，以鼓励嘈杂样本向包含其真实身份的聚类收敛。在公共步态数据集上的大量实验表明，我们简单有效的方法显著提高了无监督步态识别的性能，为其在现实世界中的应用奠定了基础。代码可在 https://github.com/YanSun-github/GaitDCCR 获得]]></description>
      <guid>https://arxiv.org/abs/2501.16608</guid>
      <pubDate>Wed, 29 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CascadeV：用于视频生成的Wurstchen 架构的实现</title>
      <link>https://arxiv.org/abs/2501.16612</link>
      <description><![CDATA[arXiv:2501.16612v1 公告类型：新
摘要：最近，随着扩散模型在文本到图像 (T2I) 生成领域取得巨大成功，人们越来越关注它们在文本到视频 (T2V) 应用中的潜力。然而，扩散模型的计算需求带来了重大挑战，特别是在生成高帧率的高分辨率视频时。在本文中，我们提出了 CascadeV，一种级联潜在扩散模型 (LDM)，能够生成最先进的 2K 分辨率视频。实验表明，我们的级联模型实现了更高的压缩比，大大降低了与高质量视频生成相关的计算挑战。我们还实现了时空交替网格 3D 注意机制，有效地整合了空间和时间信息，确保生成的视频帧之间的卓越一致性。此外，我们的模型可以与现有的 T2V 模型级联，理论上无需进行任何微调即可将分辨率或每秒帧数提高 4 倍。我们的代码可在 https://github.com/bytedance/CascadeV 上找到。]]></description>
      <guid>https://arxiv.org/abs/2501.16612</guid>
      <pubDate>Wed, 29 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>预测动态场景的 3D 表示</title>
      <link>https://arxiv.org/abs/2501.16617</link>
      <description><![CDATA[arXiv:2501.16617v1 公告类型：新
摘要：我们提出了一种新的框架，用于给定单目视频流的动态辐射场预测。与以前主要关注预测未来帧的方法不同，我们的方法更进一步，生成动态场景的明确 3D 表示。该框架建立在两个核心设计之上。首先，我们采用以自我为中心的无界三平面来明确表示动态物理世界。其次，我们开发了一个 4D 感知转换器来聚合单目视频的特征以更新三平面。结合这两种设计使我们能够以自监督的方式使用大规模单目视频训练所提出的模型。我们的模型在 NVIDIA 动态场景的动态辐射场预测中取得了最佳结果，展示了其在 4D 物理世界建模方面的强大性能。此外，我们的模型对看不见的场景表现出卓越的通用性。值得注意的是，我们发现我们的方法具有几何和语义学习的能力。]]></description>
      <guid>https://arxiv.org/abs/2501.16617</guid>
      <pubDate>Wed, 29 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>肿瘤病理学的分子驱动基础模型</title>
      <link>https://arxiv.org/abs/2501.16652</link>
      <description><![CDATA[arXiv:2501.16652v1 公告类型：新
摘要：基础模型正在通过实现迁移学习重塑计算病理学，其中在大量数据集上预先训练的模型可以适应下游诊断、预后和治疗反应任务。尽管取得了这些进展，但基础模型在编码整个千兆像素全幻灯片图像的能力方面仍然有限，而无需额外的训练，并且通常缺乏互补的多模态数据。在这里，我们介绍了 Threads，这是一个幻灯片级基础模型，能够生成任何大小的全幻灯片图像的通用表示。Threads 使用多模态学习方法在 47,171 个苏木精和伊红 (H&amp;E) 染色的组织切片的不同队列上进行了预训练，并与相应的基因组和转录组图谱配对 - 这是迄今为止用于基础模型开发的最大此类配对数据集。这种独特的训练模式使 Threads 能够捕捉组织的潜在分子组成，从而产生适用于各种下游任务的强大表示。在对 54 项肿瘤学任务（包括临床亚型、分级、突变预测、免疫组织化学状态确定、治疗反应预测和生存预测）进行广泛的基准测试中，Threads 的表现优于所有基线，同时表现出卓越的通用性和标签效率。它特别适合预测罕见事件，进一步强调了其临床实用性。我们打算向更广泛的社区公开该模型。]]></description>
      <guid>https://arxiv.org/abs/2501.16652</guid>
      <pubDate>Wed, 29 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用数据驱动方法的基于视觉的自主结构损伤检测</title>
      <link>https://arxiv.org/abs/2501.16662</link>
      <description><![CDATA[arXiv:2501.16662v1 公告类型：新
摘要：本研究解决了风力涡轮机结构中高效、准确损伤检测的迫切需求，风力涡轮机结构是可再生能源基础设施的重要组成部分。传统的检查方法，例如人工评估和无损检测 (NDT)，通常成本高昂、耗时且容易出现人为错误。为了应对这些挑战，本研究调查了基于视觉的结构健康监测 (SHM) 的先进深度学习算法。准备并增强了一个风力涡轮机表面图像数据集，其中包含各种损伤类型和污染，以增强模型训练。采用三种算法 - YOLOv7、其轻量级变体和 Faster R-CNN - 来检测和分类表面损伤。对模型进行了训练和评估，数据集分为训练、测试和评估子集（80%-10%-10%）。结果表明，YOLOv7 的表现优于其他算法，实现了 82.4% 的 mAP@50 和高处理速度，使其适合实时检查。通过优化学习率和批处理大小等超参数，模型的准确性和效率进一步提高。YOLOv7 在检测精度和执行速度方面表现出显著的进步，尤其是对于实时应用而言。然而，数据集限制和环境变化等挑战也值得关注，这表明未来需要研究分割方法和更大的数据集。这项研究强调了基于视觉的深度学习技术通过降低成本、提高安全性和提高可靠性来改变 SHM 实践的潜力，从而有助于关键基础设施的可持续维护并支持风能系统的长寿。]]></description>
      <guid>https://arxiv.org/abs/2501.16662</guid>
      <pubDate>Wed, 29 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>