<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Fri, 17 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>Social-LLaVA：通过社交空间中的人类语言推理增强机器人导航</title>
      <link>https://arxiv.org/abs/2501.09024</link>
      <description><![CDATA[arXiv:2501.09024v1 公告类型：新
摘要：大多数现有的社交机器人导航技术要么利用手工制定的规则，要么利用人类演示将机器人感知与符合社交的行为联系起来。然而，在有效地将感知转化为符合社交的行为方面仍然存在很大的差距，就像人类推理在动态环境中自然发生的方式一样。考虑到视觉语言模型 (VLM) 最近的成功，我们建议使用语言来弥合感知和社交意识机器人动作之间类人推理的差距。我们创建了一个视觉语言数据集，通过可解释的交互进行社交机器人导航 (SNEI)，其中包含 40K 个人类注释的视觉问答 (VQA)，基于非结构化、拥挤的公共空间中的 2K 人机社交互动，涵盖感知、预测、思路推理、行动和解释。我们使用 SNEI 对 VLM Social-LLaVA 进行微调，以展示我们数据集的实际应用。根据 50 个 VQA 中 15 个不同人类评判评分的平均值，Social-LLaVA 的表现优于 GPT-4V 和 Gemini 等最先进的模型。Social-LLaVA 部署在移动机器人上，可实现类似人类的推理，标志着通过语言推理在动态公共空间中实现符合社交要求的机器人导航迈出了有希望的一步。]]></description>
      <guid>https://arxiv.org/abs/2501.09024</guid>
      <pubDate>Fri, 17 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>生成视频模型是否通过观看视频来学习物理原理？</title>
      <link>https://arxiv.org/abs/2501.09038</link>
      <description><![CDATA[arXiv:2501.09038v1 公告类型：新
摘要：人工智能视频生成正在经历一场革命，质量和真实感正在迅速提高。这些进步引发了一场激烈的科学争论：视频模型是否学习了发现物理定律的“世界模型”——或者，它们仅仅是复杂的像素预测器，可以在不了解现实物理原理的情况下实现视觉真实感？我们通过开发 Physics-IQ 来解决这个问题，这是一个全面的基准数据集，只有通过深入了解各种物理原理（如流体动力学、光学、固体力学、磁学和热力学）才能解决。我们发现，在一系列当前模型（Sora、Runway、Pika、Lumiere、Stable Video Diffusion 和 VideoPoet）中，物理理解受到严重限制，并且与视觉真实感无关。同时，一些测试用例已经可以成功解决。这表明仅从观察中获取某些物理原理可能是可能的，但仍存在重大挑战。虽然我们预计未来会取得快速进展，但我们的工作表明，视觉真实性并不意味着物理理解。我们的项目页面位于 https://physics-iq.github.io；代码位于 https://github.com/google-deepmind/physics-IQ-benchmark。]]></description>
      <guid>https://arxiv.org/abs/2501.09038</guid>
      <pubDate>Fri, 17 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于域自适应语义分割的伪标签引导像素对比</title>
      <link>https://arxiv.org/abs/2501.09040</link>
      <description><![CDATA[arXiv:2501.09040v1 公告类型：新
摘要：语义分割对于理解图像至关重要，但该过程需要大量像素级的详细注释。在现实世界中，获取此类注释的成本可能很高。用于语义分割的无监督域自适应 (UDA) 是一种使用带有标签的虚拟数据来训练模型并将其适应没有标签的真实数据的技术。最近的一些研究使用对比学习来帮助实现这种技术，这是一种强大的自监督学习方法。然而，这些研究在使用对比学习时没有考虑到每个类内特征的多样性，这会导致类预测错误。我们分析了这些研究的局限性，并提出了一种称为伪标签引导像素对比 (PGPC) 的新框架，它克服了以前方法的缺点。我们还研究了如何在不添加伪标签噪声的情况下使用目标图像中的更多信息。我们在两个标准 UDA 基准上测试了我们的方法，并表明它优于现有方法。具体来说，我们基于 DAFormer 在 Grand Theft Auto V (GTA5) to Cityscapes 和 SYNTHIA to Cityscapes 任务上分别实现了 5.1% mIoU 和 4.6% mIoU 的相对改进。此外，我们的方法可以在不增加模型复杂性的情况下提高其他 UDA 方法的性能。代码可在 https://github.com/embar111/pgpc 上找到]]></description>
      <guid>https://arxiv.org/abs/2501.09040</guid>
      <pubDate>Fri, 17 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>生成式视觉常识问答与解释及生成式场景图构建</title>
      <link>https://arxiv.org/abs/2501.09041</link>
      <description><![CDATA[arXiv:2501.09041v1 公告类型：新
摘要：视觉常识推理被视为追求高级视觉场景理解的一项具有挑战性的任务，已用于诊断人工智能系统的推理能力。然而，可靠的推理需要对场景的细节有很好的把握。现有的工作未能有效地利用场景中存在的真实世界对象关系信息，而是过度依赖来自训练记忆的知识。基于这些观察，我们提出了一种新的场景图增强的视觉常识推理生成方法 \textit{\textbf{G2}}，该方法首先利用图像块和 LLM 构建一个位置无关的场景图，然后根据场景图的信息进行回答和解释。我们还提出了自动场景图过滤和选择策略，以在训练过程中吸收有价值的场景图信息。对场景图构建任务和视觉常识回答和解释数据集分别进行了广泛的实验。实验结果和消融分析证明了我们提出的框架的有效性。]]></description>
      <guid>https://arxiv.org/abs/2501.09041</guid>
      <pubDate>Fri, 17 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CookingDiffusion：通过稳定扩散生成烹饪程序图像</title>
      <link>https://arxiv.org/abs/2501.09042</link>
      <description><![CDATA[arXiv:2501.09042v1 公告类型：新
摘要：文本到图像生成模型的最新进展在创建多样化和逼真的图像方面表现出色。这一成功延伸到食物图像，其中利用了各种条件输入，如烹饪风格、配料和食谱。然而，一个尚未探索的挑战是根据食谱中的烹饪步骤生成一系列程序图像。这可以通过视觉指导增强烹饪体验，并可能导致智能烹饪模拟系统。为了填补这一空白，我们引入了一项名为 \textbf{烹饪程序图像生成} 的新任务。这项任务本质上是苛刻的，因为它努力创建与烹饪步骤一致的照片般逼真的图像，同时保持顺序一致性。为了共同应对这些挑战，我们提出了 \textbf{CookingDiffusion}，这是一种利用稳定扩散和三个创新记忆网络来模拟程序提示的新方法。这些提示包括文本提示（代表烹饪步骤）、图像提示（对应于烹饪图像）和多模式提示（混合烹饪步骤和图像），确保一致地生成烹饪过程图像。为了验证我们方法的有效性，我们对 YouCookII 数据集进行了预处理，建立了一个新的基准。我们的实验结果表明，我们的模型擅长生成高质量的烹饪过程图像，并且在连续烹饪步骤中具有显著的一致性，这通过 FID 和提出的平均程序一致性指标来衡量。此外，CookingDiffusion 展示了在食谱中操纵配料和烹饪方法的能力。我们将公开我们的代码、模型和数据集。]]></description>
      <guid>https://arxiv.org/abs/2501.09042</guid>
      <pubDate>Fri, 17 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>TCMM：用于无监督行人重新识别的标记约束和对比学习多尺度记忆库</title>
      <link>https://arxiv.org/abs/2501.09044</link>
      <description><![CDATA[arXiv:2501.09044v1 公告类型：新
摘要：本文提出了 ViT Token Constraint 和多尺度记忆库 (TCMM) 方法来解决无监督行人重识别工作中的补丁噪声和特征不一致问题。许多优秀的方法使用 ViT 特征来获取伪标签和聚类原型，然后使用对比学习训练模型。然而，ViT 通过执行补丁嵌入来处理图像，这不可避免地会在补丁中引入噪声，并可能损害重识别模型的性能。另一方面，由于批次大小的限制，以前的基于记忆库的对比方法可能导致数据不一致。此外，现有的伪标签方法通常会丢弃难以聚类的异常样本。它牺牲了异常样本的潜在价值，导致模型多样性和鲁棒性有限。本文引入了 ViT Token Constraint 来减轻补丁噪声对 ViT 架构造成的损害。所提出的多尺度记忆增强了对异常样本的探索并保持了特征一致性。实验结果表明，我们的系统在常见基准上实现了最先进的性能。该项目可在 \href{https://github.com/andy412510/TCMM}{https://github.com/andy412510/TCMM} 上找到。]]></description>
      <guid>https://arxiv.org/abs/2501.09044</guid>
      <pubDate>Fri, 17 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>时空基础模型：愿景、挑战和机遇</title>
      <link>https://arxiv.org/abs/2501.09045</link>
      <description><![CDATA[arXiv:2501.09045v1 公告类型：新
摘要：基础模型彻底改变了人工智能，在性能方面设定了新的基准，并在广泛的视觉和语言任务中实现了变革能力。然而，尽管时空数据在交通、公共卫生和环境监测等关键领域普遍存在，但时空基础模型 (STFM) 尚未取得类似的成功。在本文中，我们阐述了 STFM 未来的愿景，概述了它们的基本特征和广泛适用所必需的泛化能力。我们批判性地评估了当前的研究状况，确定了与这些理想特征相关的差距，并强调了阻碍其进展的关键挑战。最后，我们探索了潜在的机会和方向，以推动研究朝着有效和广泛适用的 STFM 的目标迈进。]]></description>
      <guid>https://arxiv.org/abs/2501.09045</guid>
      <pubDate>Fri, 17 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在线签名的拟人化特征</title>
      <link>https://arxiv.org/abs/2501.09048</link>
      <description><![CDATA[arXiv:2501.09048v1 公告类型：新
摘要：在线签名验证中提出了许多特征。通常，这些特征依赖于平板电脑记录的在线签名样本的位置及其动态特性。本文提出了一种新颖的特征空间来有效地描述在线签名。由于制作签名需要骨骼臂系统及其相关肌肉，因此新的特征空间基于表征签名时肩部、肘部和腕关节的运动。由于此运动不是直接从数字平板电脑获得的，因此新特征是通过虚拟骨骼臂 (VSA) 模型计算的，该模型模拟了真实手臂和前臂的结构。具体而言，VSA 运动由其 3D 关节位置和关节角度描述。这些拟人化特征是通过 VSA 正向和正向运动模型从笔的位置和方向计算出来的。通过在第三方签名数据库上使用多个验证器和多个基准测试实现最先进的性能，证明了拟人特征的稳健性，这些数据库是使用不同的设备、不同的语言和脚本收集的。]]></description>
      <guid>https://arxiv.org/abs/2501.09048</guid>
      <pubDate>Fri, 17 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用深度学习为扩展现实生成逼真的合成头部旋转数据</title>
      <link>https://arxiv.org/abs/2501.09050</link>
      <description><![CDATA[arXiv:2501.09050v1 公告类型：新
摘要：扩展现实是一种向用户提供多媒体内容的革命性方法。其受欢迎程度的一个重要因素是通过将真实世界的运动准确而即时地反映在虚拟体验中，实现沉浸感和互动性。这种用户运动主要由头部旋转引起，带来了一些技术挑战。例如，生成和传输哪些内容在很大程度上取决于用户正在看哪里。因此，无缝系统会主动考虑用户运动，需要准确预测即将到来的旋转。训练和评估此类预测器需要大量的方向输入数据，这些数据的收集成本很高，因为它需要人类测试对象。更可行的方法是通过测试对象收集适度的数据集，然后使用合成数据生成方法将其扩展到更大的集合。在这项工作中，我们提出了一个基于 TimeGAN 的头部旋转时间序列生成器，TimeGAN 是著名的生成对抗网络的扩展，专门用于生成时间序列。这种方法能够使用与测量时间序列的分布紧密匹配的新样本来扩展头部旋转数据集。]]></description>
      <guid>https://arxiv.org/abs/2501.09050</guid>
      <pubDate>Fri, 17 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 YOLOv11 检测结肠镜检查图像中的息肉</title>
      <link>https://arxiv.org/abs/2501.09051</link>
      <description><![CDATA[arXiv:2501.09051v1 公告类型：新
摘要：结直肠癌 (CRC) 是全世界最常见的癌症之一。它始于结肠内壁的息肉。为了预防 CRC，需要尽早检测息肉。结肠镜检查用于检查结肠。通常，专家会手动分析放置在内窥镜尖端的摄像机拍摄的图像。随着机器学习的兴起，各种传统的机器学习模型已被使用。最近，深度学习模型在息肉检测中表现出更有效的效果，因为它们在泛化和学习小特征方面具有优势。这些用于对象检测的深度学习模型可以分为两种不同类型：单阶段和双阶段。通常，两阶段模型比单阶段模型具有更高的准确度，但单阶段模型的推理时间较短。因此，单阶段模型易于用于快速物体检测。YOLO 是成功用于息肉检测的单阶段模型之一。由于其推理时间较短，它引起了研究人员的关注。到目前为止，研究人员已经使用了不同版本的 YOLO，随着每个新版本的推出，模型的准确性都在提高。本文旨在了解最近发布的 YOLOv11 在检测息肉方面的有效性。我们使用 Kvasir 数据集进行训练和测试，分析了 YOLOv11 的所有五种模型（YOLO11n、YOLO11s、YOLO11m、YOLO11l、YOLO11x）的性能。使用了两个不同版本的数据集。第一个由原始数据集组成，另一个是使用增强技术创建的。已经分析了使用这两个版本数据集的所有模型的性能。]]></description>
      <guid>https://arxiv.org/abs/2501.09051</guid>
      <pubDate>Fri, 17 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SHYI：高保真文本到图像生成中对比学习的动作支持</title>
      <link>https://arxiv.org/abs/2501.09055</link>
      <description><![CDATA[arXiv:2501.09055v1 公告类型：新
摘要：在这个项目中，我们解决了文本到图像生成中的不忠实问题，特别是涉及多个对象的动作。为此，我们在 CONFORM 框架的基础上构建了对比学习，以提高针对多个对象生成的图像的准确性。然而，涉及多个不同对象的动作的描述仍然有很大的改进空间。为了改进，我们采用了语义超图对比邻接学习、增强对比结构的理解和“对比但链接”技术。我们通过 InteractDiffusion 进一步修改了 Stable Diffusion 对动作的理解。我们使用图像文本相似度 CLIP 和 TIFA 作为评估指标。此外，我们还进行了用户研究。
即使对于 Stable Diffusion 理解一般的动词，我们的方法也显示出有希望的结果。然后，我们通过分析结果提供未来的方向。
我们的代码库可以在 polybox 上的以下链接找到：https://polybox.ethz.ch/index.php/s/dJm3SWyRohUrFxn]]></description>
      <guid>https://arxiv.org/abs/2501.09055</guid>
      <pubDate>Fri, 17 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>保留显著信息的对抗性训练可提高清洁度和稳健性</title>
      <link>https://arxiv.org/abs/2501.09086</link>
      <description><![CDATA[arXiv:2501.09086v1 公告类型：新
摘要：在这项工作中，我们引入了显着信息保留对抗训练 (SIP-AT)，这是一种直观的方法，可以缓解传统对抗训练带来的稳健性-准确性权衡。SIP-AT 使用显着的图像区域来指导对抗训练过程，使得注释者认为有意义的脆弱特征在训练期间保持不受干扰，从而使模型能够学习高度预测的非稳健特征而不会牺牲整体稳健性。该技术与基于人类和自动生成的显着性估计兼容，允许 SIP-AT 用作人为驱动的模型开发的一部分，而无需强迫 SIP-AT 依赖于额外的人类数据。我们在多个数据集和架构上进行实验，并证明 SIP-AT 能够提高模型的干净准确性，同时保持对多个 epsilon 级别攻击的高度稳健性。我们通过一项观察性研究来补充我们的核心实验，该研究测量了人类受试者成功识别受干扰图像的速率。这项研究有助于更直观地了解对抗攻击的强度，并证明了低 epsilon 鲁棒性的重要性。我们的结果证明了 SIP-AT 的有效性，并为不同强度的对抗样本带来的风险提供了宝贵的见解。]]></description>
      <guid>https://arxiv.org/abs/2501.09086</guid>
      <pubDate>Fri, 17 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于潜在代码投影和优化的生成医学图像匿名化</title>
      <link>https://arxiv.org/abs/2501.09114</link>
      <description><![CDATA[arXiv:2501.09114v1 公告类型：新
摘要：医学图像匿名化旨在通过删除身份信息来保护患者隐私，同时保留数据实用性以解决下游任务。在本文中，我们使用两阶段解决方案解决医学图像匿名化问题：潜在代码投影和优化。在投影阶段，我们设计了一个精简的编码器将输入图像投影到潜在空间，并提出了一种共同训练方案来增强投影过程。在优化阶段，我们使用两个深度损失函数来改进潜在代码，这两个函数旨在解决医学图像专用的身份保护和数据实用性之间的权衡。通过一套全面的定性和定量实验，我们通过生成可作为检测肺部病变的训练集的匿名合成图像，展示了我们的方法在 MIMIC-CXR 胸部 X 光数据集上的有效性。源代码可在 https://github.com/Huiyu-Li/GMIA 获得。]]></description>
      <guid>https://arxiv.org/abs/2501.09114</guid>
      <pubDate>Fri, 17 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 OPERA Sentinel-1 辐射地形校正 SAR 后向散射产品进行深度自监督扰动映射</title>
      <link>https://arxiv.org/abs/2501.09129</link>
      <description><![CDATA[arXiv:2501.09129v1 公告类型：新 
摘要：绘制陆地表面扰动图有助于应对灾害、资源和生态系统管理以及气候适应工作。合成孔径雷达 (SAR) 是扰动图绘制的宝贵工具，无论天气或照明条件如何，都能提供一致的地面时间序列图像。尽管 SAR 具有进行扰动图绘制的潜力，但将 SAR 数据处理为可供分析的格式需要专业知识和大量计算资源，尤其是对于大规模全球分析而言。2023 年 10 月，NASA 的遥感分析终端用户观测产品 (OPERA) 项目发布了近全球辐射地形校正 SAR 后向散射 Sentinel-1 (RTC-S1) 数据集，提供公开可用、可供分析的 SAR 图像。在这项工作中，我们利用这个新数据集系统地分析陆地表面扰动。由于标记 SAR 数据通常非常耗时，我们在 OPERA RTC-S1 数据上训练了一个自监督视觉转换器（无需标记即可训练），以从基线图像集中估计每个像素的分布，并在与模型分布有显著偏差时评估干扰。为了测试我们模型的能力和通用性，我们评估了来自世界三个不同地区的三种不同的自然灾害（代表高强度、突然的干扰）。在各个事件中，我们的方法都能产生高质量的描绘：F1 分数超过 0.6，精确召回曲线下面积超过 0.65，始终优于现有的 SAR 干扰方法。我们的研究结果表明，自监督视觉转换器非常适合全局干扰映射，并且可以成为操作性、近全局干扰监测的有力工具，尤其是在不存在标记数据的情况下。]]></description>
      <guid>https://arxiv.org/abs/2501.09129</guid>
      <pubDate>Fri, 17 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>医学图像报告检索对比学习模型的基准稳健性</title>
      <link>https://arxiv.org/abs/2501.09134</link>
      <description><![CDATA[arXiv:2501.09134v1 公告类型：新
摘要：医学图像和报告为患者健康提供了宝贵的见解。这些数据的异质性和复杂性阻碍了有效的分析。为了弥补这一差距，我们研究了跨领域检索的对比学习模型，该模型将医学图像与其相应的临床报告相关联。本研究对四种最先进的对比学习模型的稳健性进行了基准测试：CLIP、CXR-RePaiR、MedCLIP 和 CXR-CLIP。我们引入了一个遮挡检索任务来评估模型在不同程度的图像损坏下的性能。我们的研究结果表明，所有评估的模型都对分布外的数据高度敏感，这可以从性能随着遮挡水平的增加而成比例下降中看出。虽然 MedCLIP 表现出略高的稳健性，但其整体性能仍然远远落后于 CXR-CLIP 和 CXR-RePaiR。 CLIP 是在通用数据集上训练的，但在医学图像报告检索方面表现不佳，这凸显了领域特定训练数据的重要性。对这项工作的评估表明，需要投入更多精力来提高这些模型的稳健性。通过解决这些限制，我们可以为医疗应用开发更可靠的跨领域检索模型。]]></description>
      <guid>https://arxiv.org/abs/2501.09134</guid>
      <pubDate>Fri, 17 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>