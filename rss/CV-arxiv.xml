<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Wed, 23 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>2024 ECCV 第二届感知测试挑战赛点跟踪任务解决方案</title>
      <link>https://arxiv.org/abs/2410.16286</link>
      <description><![CDATA[arXiv:2410.16286v1 公告类型：新
摘要：本报告介绍了一种改进的跟踪任意点（TAP）方法，重点是监控视频片段中的物理表面。尽管 TAP 方法在短序列场景中取得了成功，但在长序列情况下仍然面临性能下降和资源开销的问题。为了解决这些问题，我们提出了一种简单而有效的方法，称为细粒度点鉴别（\textbf{FPD}），该方法侧重于以零镜头方式感知和纠正多粒度的点跟踪，特别是对于静态摄像机拍摄的视频中的静态点。所提出的 FPD 包含两个关键组件：（1）多粒度点感知，可以检测视频中的静态序列和点。（2）动态轨迹校正，根据跟踪点的类型替换点轨迹。我们的方法在最终测试中以 0.4720 的成绩获得了第二高的分数。]]></description>
      <guid>https://arxiv.org/abs/2410.16286</guid>
      <pubDate>Wed, 23 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>OOD-CV UNICORN Challenge 2024 解决方案 物体检测辅助 LLM 计数能力提升</title>
      <link>https://arxiv.org/abs/2410.16287</link>
      <description><![CDATA[arXiv:2410.16287v1 公告类型：新
摘要：本报告详细描述了我们在 ECCV OOD-CV UNICORN Challenge 2024 中探索和提出的方法，该方法侧重于大型语言模型响应的鲁棒性。本次比赛的数据集是 OODCA-VQA 和 SketchyQA。为了测试模型的鲁棒性。组织者扩展了数据集 OODCV-Counterfactual 和 Sketchy-Challenging 的两个变体。这些数据集有几个难点。首先，Sketchy-Challenging 数据集使用一些比较罕见的项目类别来测试模型的泛化能力。其次，在 OODCV-Counterfactual 数据集中，给定的问题通常具有拐点和计算步骤，需要模型在推理过程中识别它们。为了解决这个问题，我们提出了一种简单而有效的方法，称为对象检测辅助大型语言模型 (LLM) 计数能力改进 (ODAC)，该方法专注于使用对象检测模型来辅助 LLM。需要说明的是，我们的方法包含两个主要模块：(1) 对象检测辅助。(2) 反事实特定提示。我们的方法在最终测试中排名第二，得分为 0.86。]]></description>
      <guid>https://arxiv.org/abs/2410.16287</guid>
      <pubDate>Wed, 23 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用并行积分图像技术加速生物空间聚类分析</title>
      <link>https://arxiv.org/abs/2410.16291</link>
      <description><![CDATA[arXiv:2410.16291v1 公告类型：新
摘要：空间聚类分析 (SCA) 为生物图像提供了宝贵的见解；一种常见的 SCA 技术是滑动窗口分析 (SWA)。不幸的是，SWA 的计算成本阻碍了它应用于更大的图像，限制了它只能用于小规模图像。随着高分辨率显微镜的进步，图像现在超过了以前的 SWA 方法的能力，尺寸高达 70,000 x 85,000 像素。为了克服这些限制，本文介绍了 SWA 的并行积分图像方法，超越了以前的方法。我们在小规模图像上实现了 131,806 倍的显着加速，并在各种大规模显微镜图像上实现了超过 10,000 倍的持续加速。我们分析了并行积分图像方法的计算复杂性优势，并给出了实验结果，验证了基于积分图像的方法的卓越性能。我们的方法以开源 Python PIP 包的形式提供，可在 https://github.com/OckermanSethGVSU/BioPII 上找到。]]></description>
      <guid>https://arxiv.org/abs/2410.16291</guid>
      <pubDate>Wed, 23 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 YOLOv4 加速实时应用的对象检测</title>
      <link>https://arxiv.org/abs/2410.16320</link>
      <description><![CDATA[arXiv:2410.16320v1 公告类型：新
摘要：对象检测与计算机视觉相关。对象检测可以检测图像和视频中的对象实例。由于其在监控、安全跟踪系统和许多其他应用中的使用率不断提高，促使研究人员不断推导出更高效、更具竞争力的算法。然而，由于动态环境和对象检测中使用的复杂算法，在实时实现它时会出现问题。在过去的几年里，卷积神经网络 (CNN) 已经成为识别图像内容和解决大多数问题的计算机视觉方法的强大工具。在本文中，我们重新开始简要介绍深度学习和对象检测框架，如卷积神经网络 (CNN)、你只看一次 - 版本 4 (YOLOv4)。然后我们重点介绍我们提出的对象检测架构以及一些修改。传统模型检测图像中的小物体。我们对模型进行了一些修改。我们提出的方法可以准确地给出正确的结果。]]></description>
      <guid>https://arxiv.org/abs/2410.16320</guid>
      <pubDate>Wed, 23 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>2024 感知测试挑战赛单目标跟踪任务解决方案</title>
      <link>https://arxiv.org/abs/2410.16329</link>
      <description><![CDATA[arXiv:2410.16329v1 公告类型：新
摘要：本报告介绍了我们的单对象跟踪 (SOT) 方法，该方法旨在在整个视频序列中跟踪指定对象。我们采用 LoRAT 方法。这项工作的本质在于将 LoRA（一种在不增加推理延迟的情况下微调一小部分模型参数的技术）应用于视觉跟踪领域。我们使用广泛的 LaSOT 和 GOT-10k 数据集训练我们的模型，这为稳健的性能提供了坚实的基础。此外，我们实施了 alpha-refine 技术来对边界框输出进行后处理。虽然 alpha-refine 方法没有产生预期的结果，但我们的整体方法获得了 0.813 的分数，在比赛中名列第一。]]></description>
      <guid>https://arxiv.org/abs/2410.16329</guid>
      <pubDate>Wed, 23 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用时空变换器消除 3D 着装人体单目重建歧义</title>
      <link>https://arxiv.org/abs/2410.16337</link>
      <description><![CDATA[arXiv:2410.16337v1 公告类型：新 
摘要：由于视点限制和图像模糊性，从单目相机数据重建 3D 着装人体极具挑战性。虽然基于隐函数的方法与参数模型的先验知识相结合取得了重大进展，但仍然存在两个值得注意的问题。首先，由于视点不可见，人体模型的背部细节模糊不清。背部细节的质量取决于卷积神经网络 (CNN) 预测的背部法线图。然而，CNN 缺乏理解背部纹理的全局信息意识，导致背部细节过于平滑。其次，由于光照条件和身体运动，单个图像会出现局部模糊。然而，隐函数对模​​糊区域中的像素变化高度敏感。为了解决这些模糊性，我们提出了用于 3D 着装人体重建的时空变换器 (STT) 网络。空间变换器用于提取法线图预测的全局信息。全局相关性的建立有助于网络理解人体的整体纹理和形状。同时，为了补偿图像中的局部模糊性，使用时间变换器从相邻帧中提取时间特征。时间特征的结合可以提高隐式网络中输入特征的准确性。此外，为了获得更准确的时间特征，使用联合标记来建立帧之间的局部对应关系。在 Adob​​e 和 MonoPerfCap 数据集上的实验结果表明，我们的方法优于最先进的方法，即使在低光户外条件下也能保持稳健的泛化。]]></description>
      <guid>https://arxiv.org/abs/2410.16337</guid>
      <pubDate>Wed, 23 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Joker：具有极端面部表情的条件性 3D 头部合成</title>
      <link>https://arxiv.org/abs/2410.16395</link>
      <description><![CDATA[arXiv:2410.16395v1 公告类型：新
摘要：我们介绍了 Joker，一种用于条件合成具有极端表情的 3D 人头的新方法。给定一个人的单个参考图像，我们合成具有参考身份和新表情的体积人头。我们通过 3D 可变形模型 (3DMM) 和文本输入提供对表情的控制。这种多模态调节信号至关重要，因为仅靠 3DMM 无法定义细微的情绪变化和极端表情，包括涉及口腔和舌头发音的表情。我们的方法建立在基于 2D 扩散的先验之上，可以很好地推广到域外样本，例如雕塑、浓妆和绘画，同时实现高水平的表现力。为了提高视图一致性，我们提出了一种新的 3D 蒸馏技术，将我们的 2D 先验预测转换为神经辐射场 (NeRF)。 2D 先验和我们的提炼技术都产生了最先进的结果，这已得到我们广泛评估的证实。此外，据我们所知，我们的方法是第一个实现视图一致的极端舌头发音的方法。]]></description>
      <guid>https://arxiv.org/abs/2410.16395</guid>
      <pubDate>Wed, 23 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AttentionPainter：一种高效且自适应的场景绘画笔触预测器</title>
      <link>https://arxiv.org/abs/2410.16418</link>
      <description><![CDATA[arXiv:2410.16418v1 公告类型：新
摘要：基于笔触的渲染（SBR）旨在将输入图像分解为一系列参数化的笔触，这些笔触可以渲染成与输入图像相似的绘画。最近，利用深度学习和强化学习模型来预测笔触序列的神经绘画方法已经得到开发，但存在推理时间较长或训练不稳定的问题。为了解决这些问题，我们提出了 AttentionPainter，一种高效且自适应的单步神经绘画模型。首先，我们提出了一种新颖的可扩展笔触预测器，它在单个前向过程中预测大量笔触参数，而不是以前的强化学习或自回归方法的迭代预测，这使得 AttentionPainter 比以前的神经绘画方法更快。为了进一步提高训练效率，我们提出了一种快速笔触堆叠算法，为训练带来了 13 倍的加速。此外，我们提出了笔画密度损失，鼓励模型使用小笔画来获取详细信息，以帮助提高重建质量。最后，我们提出了一种新的笔画扩散模型，用于有条件和无条件的基于笔画的生成，该模型在笔画参数空间中去噪，并促进基于笔画的修复和编辑应用程序，有助于人类艺术家的设计。大量实验表明，AttentionPainter 的表现优于最先进的神经绘画方法。]]></description>
      <guid>https://arxiv.org/abs/2410.16418</guid>
      <pubDate>Wed, 23 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>HaHeAE：学习扩展现实中人类手部和头部运动的通用联合表征</title>
      <link>https://arxiv.org/abs/2410.16430</link>
      <description><![CDATA[arXiv:2410.16430v1 公告类型：新
摘要：人类的手部和头部运动是扩展现实 (XR) 中最普遍的输入模式，对广泛的应用具有重要意义。然而，之前关于 XR 中手部和头部建模的研究仅探索了单一模式或专注于特定应用。我们提出了 HaHeAE——一种用于学习 XR 中手部和头部运动的通用联合表示的新型自监督方法。我们方法的核心是一个自动编码器 (AE)，它使用基于图卷积网络的语义编码器和基于扩散的随机编码器来学习手部头部运动的联合语义和随机表示。它还具有基于扩散的解码器来重建原始信号。通过对三个公共 XR 数据集进行大量评估，我们表明我们的方法 1) 在重建质量方面明显优于常用的自监督方法，最高可达 74.0%，并且适用于所有用户、活动和 XR 环境；2) 支持新应用，包括可解释的手头簇识别和可变的手头运动生成；3) 可作为下游任务的有效特征提取器。总之，这些结果证明了我们方法的有效性，并强调了自监督方法在扩展现实中联合建模手头行为的潜力。]]></description>
      <guid>https://arxiv.org/abs/2410.16430</guid>
      <pubDate>Wed, 23 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GenGMM：基于广义高斯混合的语义分割领域自适应模型</title>
      <link>https://arxiv.org/abs/2410.16485</link>
      <description><![CDATA[arXiv:2410.16485v1 公告类型：新
摘要：域自适应语义分割是使用在标记源域上训练的模型为未标记的目标域生成精确且密集的预测的任务。虽然已经投入了大量精力来改进此任务的无监督域自适应，但必须注意的是，许多模型都依赖于一个强有力的假设，即源数据是完全准确标记的，而目标数据是未标记的。然而，在现实世界中，我们经常在源域和目标域中遇到部分或嘈杂的标记数据，称为广义域自适应（GDA）。在这种情况下，我们建议利用来自两个域的弱或未标记数据来缩小它们之间的差距，从而实现有效的自适应。我们引入了基于广义高斯混合（GenGMM）的域自适应模型，该模型利用两个域中的底层数据分布来细化嘈杂的弱标签和伪标签。实验证明了我们方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2410.16485</guid>
      <pubDate>Wed, 23 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>新加坡：单一图像控制生成物体中的关节部件</title>
      <link>https://arxiv.org/abs/2410.16499</link>
      <description><![CDATA[arXiv:2410.16499v1 公告类型：新
摘要：我们解决了从单个图像创建家用铰接式物体的 3D 资产的挑战。关于铰接式物体创建的先前工作要么需要多视图多状态输入，要么仅允许对生成过程进行粗略控制。这些限制阻碍了铰接式物体建模的可扩展性和实用性。在这项工作中，我们提出了一种从单个图像生成铰接式物体的方法。从任意视图观察处于静止状态的物体，我们的方法会生成与输入图像在视觉上一致的铰接式物体。为了捕捉物体的单一视图造成的部分形状和运动的模糊性，我们设计了一个扩散模型，该模型可以学习物体在几何和运动学方面的合理变化。为了解决生成具有多个域属性的结构化数据的复杂性，我们设计了一个管道，以从粗到细的方式生成从高级结构到几何细节的铰接对象，其中我们使用部分连接图和部分抽象作为代理。我们的实验表明，在生成的对象真实性、与输入图像的相似性和重建质量方面，我们的方法远远优于最先进的铰接对象创建方法。]]></description>
      <guid>https://arxiv.org/abs/2410.16499</guid>
      <pubDate>Wed, 23 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>提示：利用空间感知进行文本图像预训练</title>
      <link>https://arxiv.org/abs/2410.16512</link>
      <description><![CDATA[arXiv:2410.16512v1 公告类型：新
摘要：虽然图像文本表示学习近年来变得非常流行，但现有模型往往缺乏空间意识，并且对密集理解任务的直接适用性有限。因此，尽管缺乏明确的监督信号，但自监督的纯图像预训练仍然是许多密集视觉应用（例如深度估计、语义分割）的首选方法。在本文中，我们通过提出一种新颖的通用图像文本模型来缩小图像文本和自监督学习之间的差距，该模型可以有效地用于密集和全局视觉任务。我们的方法，我们称之为具有空间意识的文本图像预训练（TIPS），利用了两个简单有效的见解。首先，在文本监督方面：我们发现用合成生成的文本描述替换嘈杂的网络图像标题可以显着提高密集理解性能，因为学习空间感知表示的信号更加丰富。我们提出了一种改进的训练方法，将嘈杂字幕和合成字幕结合起来，从而提高了密集理解和全局理解任务的性能。其次，在学习技术方面：我们建议将对比性图像文本学习与自监督蒙版图像建模相结合，以促进空间连贯性，为下游应用带来实质性的增强。基于这两个想法，我们使用 Transformer 架构扩展了我们的模型，并在一组精选的公共图像上进行了训练。我们的实验针对 8 个任务进行了研究，总共涉及 16 个数据集，在多个纯图像和图像文本任务中，在密集理解和全局理解方面都表现出了强大的现成性能。]]></description>
      <guid>https://arxiv.org/abs/2410.16512</guid>
      <pubDate>Wed, 23 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用脉冲时间依赖性可塑性的无梯度监督学习进行图像识别</title>
      <link>https://arxiv.org/abs/2410.16524</link>
      <description><![CDATA[arXiv:2410.16524v1 公告类型：新
摘要：提出了一种使用无梯度方法结合脉冲时间相关可塑性进行图像识别的脉冲神经网络监督学习方法。所提出的网络架构可扩展到多层，从而能够开发更复杂、更深层的 SNN 模型。该方法的有效性已通过将其应用于 MNIST 数据集得到证实，显示出良好的学习准确性。所提出的方法为监督学习中基于反向传播的方法提供了一种稳健而有效的替代方案。]]></description>
      <guid>https://arxiv.org/abs/2410.16524</guid>
      <pubDate>Wed, 23 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PlaneSAM：使用 Segment Anything 模型进行多模态平面实例分割</title>
      <link>https://arxiv.org/abs/2410.16545</link>
      <description><![CDATA[arXiv:2410.16545v1 公告类型：新
摘要：从 RGB-D 数据中进行平面实例分割是许多下游任务的重要研究课题。然而，大多数现有的基于深度学习的方法仅利用 RGB 波段内的信息，而忽略了深度波段在平面实例分割中的重要作用。基于 SAM 的快速版本 EfficientSAM，我们提出了一个名为 PlaneSAM 的平面实例分割网络，它可以充分整合 RGB 波段（光谱波段）和 D 波段（几何波段）的信息，从而以多模态方式提高平面实例分割的有效性。具体来说，我们使用双复杂度主干，主要使用较简单的分支学习 D 波段特征，而较复杂的分支主要学习 RGB 波段特征。因此，即使在 D 波段训练数据规模有限的情况下，主干网络也可以有效地学习 D 波段特征表示，保留 EfficientSAM 强大的 RGB 波段特征表示，并允许原始主干分支针对当前任务进行微调。为了增强我们的 PlaneSAM 对 RGB-D 领域的适应性，我们通过基于不完善伪标签的自监督预训练策略，在大规模 RGB-D 数据上使用分割任何任务对我们的双复杂度主干网络进行预训练。为了支持大平面的分割，我们优化了 EfficientSAM 的损失函数组合比。此外，Faster R-CNN 被用作平面检测器，并将其预测的边界框作为提示输入到我们的双复杂度网络中，从而实现全自动平面实例分割。实验结果表明，所提出的 PlaneSAM 在 ScanNet 数据集上创下了新的 SOTA 性能，并且在 2D-3D-S、Matterport3D 和 ICL-NUIM RGB-D 数据集上的零样本传输中优于之前的 SOTA 方法，同时与 EfficientSAM 相比仅增加 10% 的计算开销。]]></description>
      <guid>https://arxiv.org/abs/2410.16545</guid>
      <pubDate>Wed, 23 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>遥感和地球观测的基础模型：综述</title>
      <link>https://arxiv.org/abs/2410.16602</link>
      <description><![CDATA[arXiv:2410.16602v1 公告类型：新
摘要：遥感 (RS) 是观察、监测和解释地球的关键技术，广泛应用于地球科学、经济学、人道主义领域等。虽然人工智能 (AI)，特别是深度学习，在 RS 方面取得了重大进展，但在开发更智能的 RS 系统方面仍然存在独特的挑战，包括地球环境的复杂性、多样化的传感器模式、独特的特征模式、不同的空间和光谱分辨率以及时间动态。与此同时，大型基础模型 (FM) 的最新突破因其出色的通用性和零样本传输能力而扩展了 AI 在许多领域的潜力。然而，它们的成功主要局限于图像和视频等自然数据，各种非光学模态的 RS 数据的性能下降甚至失败。这激发了人们对开发遥感基础模型 (RSFM) 的兴趣，以满足地球观测 (EO) 任务的复杂需求，这些任务涵盖地表、大气和海洋。本综述系统地回顾了 RSFM 这一新兴领域。它首先概述了它们的动机和背景，然后介绍了它们的基础概念。然后，它对现有的 RSFM 研究进行分类和回顾，包括它们的数据集和对视觉基础模型 (VFM)、视觉语言模型 (VLM)、大型语言模型 (LLM) 等的技术贡献。此外，我们根据公开可用的数据集对这些模型进行了基准测试，讨论了现有的挑战，并提出了这一快速发展领域的未来研究方向。]]></description>
      <guid>https://arxiv.org/abs/2410.16602</guid>
      <pubDate>Wed, 23 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>