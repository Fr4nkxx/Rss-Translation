<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Mon, 12 Aug 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>用于增强显著特征的聚类友好型表征学习</title>
      <link>https://arxiv.org/abs/2408.04891</link>
      <description><![CDATA[arXiv:2408.04891v1 公告类型：新
摘要：最近，使用对比学习算法的表示学习已成功应用于具有挑战性的未标记数据集。然而，这些方法无法在简单的无监督设置下区分重要特征和不重要特征，并且重要性的定义根据下游任务或分析目标的类型而有所不同，例如识别对象或背景。在本文中，我们专注于无监督图像聚类作为下游任务，并提出了一种增强对聚类任务至关重要的特征的表示学习方法。我们扩展了一种聚类友好的对比学习方法，并将对比分析方法（利用参考数据集将重要特征与不重要特征分离）纳入损失函数的设计中。对具有特征背景的三个数据集进行图像聚类的实验评估，我们表明，对于所有数据集，与传统的对比分析和深度聚类方法相比，我们的方法获得了更高的聚类分数。]]></description>
      <guid>https://arxiv.org/abs/2408.04891</guid>
      <pubDate>Tue, 13 Aug 2024 03:14:22 GMT</pubDate>
    </item>
    <item>
      <title>mPLUG-Owl3：面向多模态大型语言模型的长图像序列理解</title>
      <link>https://arxiv.org/abs/2408.04840</link>
      <description><![CDATA[arXiv:2408.04840v1 公告类型：新
摘要：多模态大型语言模型 (MLLM) 在执行各种单图像任务的指令方面表现出了卓越的能力。尽管取得了这些进展，但在对长图像序列进行建模方面仍然存在重大挑战。在这项工作中，我们引入了多功能多模态大型语言模型 mPLUG-Owl3，它增强了在结合检索到的图像文本知识、交错的图像文本和长视频的场景中对长图像序列的理解能力。具体来说，我们提出了新颖的超注意力模块，以有效地将视觉和语言整合到一个共同的语言引导的语义空间中，从而促进扩展的多图像场景的处理。大量的实验结果表明，mPLUG-Owl3 在单图像、多图像和视频基准上实现了与尺寸相似的模型中最先进的性能。此外，我们提出了一种具有挑战性的长视觉序列评估，称为干扰抵抗力，以评估模型在干扰中保持专注的能力。最后，基于所提出的架构，mPLUG-Owl3 在超长视觉序列输入上表现出色。我们希望 mPLUG-Owl3 能够为开发更高效、更强大的多模态大型语言模型做出贡献。]]></description>
      <guid>https://arxiv.org/abs/2408.04840</guid>
      <pubDate>Tue, 13 Aug 2024 03:14:21 GMT</pubDate>
    </item>
    <item>
      <title>ChatGPT 与虹膜生物识别技术相结合</title>
      <link>https://arxiv.org/abs/2408.04868</link>
      <description><![CDATA[arXiv:2408.04868v1 公告类型：新
摘要：本研究利用 GPT-4 多模态大型语言模型 (LLM) 的高级功能探索其在虹膜识别中的潜力 - 虹膜识别是一个比人脸识别更不常见且更专业的领域。通过专注于这个小众但关键的领域，我们研究了像 ChatGPT 这样的人工智能工具如何很好地理解和分析虹膜图像。通过一系列精心设计的实验，采用零样本学习方法，在各种具有挑战性的条件下评估了 ChatGPT-4 的能力，包括不同的数据集、演示攻击、眼镜等遮挡和其他现实世界的变化。研究结果传达了 ChatGPT-4 非凡的适应性和精确度，揭示了它在识别独特虹膜特征方面的能力，同时还能检测到化妆等细微影响虹膜识别。与谷歌的人工智能模型 Gemini Advanced 的比较分析突出了 ChatGPT-4 在复杂的虹膜分析任务中更好的性能和用户体验。这项研究不仅证实了 LLM 在专门的生物识别应用中的用途，还强调了细致入微的查询框架和交互设计在从生物识别数据中提取重要见解方面的重要性。我们的研究结果为未来的研究和开发更具适应性、效率更高、更强大和互动性的生物识别安全解决方案指明了一条有希望的道路。]]></description>
      <guid>https://arxiv.org/abs/2408.04868</guid>
      <pubDate>Tue, 13 Aug 2024 03:14:21 GMT</pubDate>
    </item>
    <item>
      <title>零样本图像识别中的元素表示与推理：系统综述</title>
      <link>https://arxiv.org/abs/2408.04879</link>
      <description><![CDATA[arXiv:2408.04879v1 公告类型：新
摘要：零样本图像识别（ZSIR）旨在通过从可见域中的有限数据中学习广义知识，使模型能够在看不见的领域进行识别和推理。ZSIR 的要点是从输入视觉空间到目标语义空间执行元素表示和推理，这是一种自下而上的建模范式，灵感来自人类观察世界的过程，即通过学习和组合基本组成部分或共同特征来捕捉新概念。近年来，元素学习技术在 ZSIR 中取得了重大进展，并得到了广泛的应用。然而，据我们所知，对这一主题仍然缺乏系统的概述。为了丰富文献并为其未来发展提供坚实的基础，本文对元素 ZSIR 的最新进展进行了广泛的回顾。具体来说，我们首先尝试将 ZSIR 的三个基本任务（即对象识别、组合识别和基于基础模型的开放世界识别）整合到统一的元素视角中，并对主要研究方法进行详细的分类和分析。然后，我们收集并总结一些关键信息和基准，例如详细的技术实现和通用数据集。最后，我们概述了其广泛的相关应用，讨论了重大挑战，并提出了潜在的未来方向。]]></description>
      <guid>https://arxiv.org/abs/2408.04879</guid>
      <pubDate>Tue, 13 Aug 2024 03:14:21 GMT</pubDate>
    </item>
    <item>
      <title>ProxyCLIP：代理注意力机制改进 CLIP 的开放词汇分割</title>
      <link>https://arxiv.org/abs/2408.04883</link>
      <description><![CDATA[arXiv:2408.04883v1 公告类型：新
摘要：开放词汇语义分割需要模型有效地将视觉表示与开放词汇语义标签相结合。虽然对比语言-图像预训练 (CLIP) 模型在从文本中识别视觉概念方面表现出色，但由于其有限的定位能力，它们往往难以实现片段连贯性。相比之下，视觉基础模型 (VFM) 擅长获取空间一致的局部视觉表征，但在语义理解方面却存在不足。本文介绍了 ProxyCLIP，这是一个创新框架，旨在协调 CLIP 和 VFM 的优势，促进增强开放词汇语义分割。ProxyCLIP 利用来自 VFM 的空间特征对应关系作为代理注意的一种形式来增强 CLIP，从而继承 VFM 强大的局部一致性并保持 CLIP 出色的零样本传输能力。我们提出了一种自适应规范化和掩蔽策略，以从 VFM 中获得代理注意力，从而实现跨不同 VFM 的适应性。值得注意的是，作为一种无需训练的方法，ProxyCLIP 显著提高了八个基准测试的平均交并比 (mIoU)，从 40.3 提高到 44.4，展示了其在弥合开放词汇分割任务的空间精度和语义丰富度之间的差距方面的卓越功效。]]></description>
      <guid>https://arxiv.org/abs/2408.04883</guid>
      <pubDate>Tue, 13 Aug 2024 03:14:21 GMT</pubDate>
    </item>
    <item>
      <title>UniBench：视觉推理需要重新思考视觉语言，而不仅仅是扩展</title>
      <link>https://arxiv.org/abs/2408.04810</link>
      <description><![CDATA[arXiv:2408.04810v1 公告类型：新
摘要：已经进行了大量研究来扩展和改进视觉语言模型 (VLM) 训练方法。然而，随着基准测试数量的不断增长，研究人员肩负着实施每项协议的沉重负担，承担着不小的计算成本，并弄清楚所有这些基准测试如何转化为有意义的进展轴。为了促进对 VLM 进展的系统评估，我们推出了 UniBench：统一实施 50 多个 VLM 基准测试，涵盖从对象识别到空间感知、计数等一系列精心分类的功能。我们通过评估近 60 个公开可用的视觉语言模型来展示 UniBench 在衡量进展方面的实用性，这些模型在高达 12.8B 的样本规模上进行了训练。我们发现，虽然扩展训练数据或模型大小可以提高许多视觉语言模型的能力，但扩展对推理或关系几乎没有好处。令人惊讶的是，我们还发现当今最好的 VLM 在简单的数字识别和计数任务（例如 MNIST）上表现不佳，而更简单的网络可以解决这些问题。在规模不足的情况下，我们发现更精确的干预措施（例如数据质量或量身定制的学习目标）更有希望。对于从业者，我们还提供选择适合特定应用的 VLM 的指导。最后，我们发布了一个易于运行的 UniBench 代码库，其中包含 59 个模型的 50 多个基准测试和比较的完整集合，以及一组精简的代表性基准测试，可在单个 GPU 上运行 5 分钟。]]></description>
      <guid>https://arxiv.org/abs/2408.04810</guid>
      <pubDate>Tue, 13 Aug 2024 03:14:20 GMT</pubDate>
    </item>
    <item>
      <title>重新思考多实例学习：通过弱监督自训练开发实例级分类器</title>
      <link>https://arxiv.org/abs/2408.04813</link>
      <description><![CDATA[arXiv:2408.04813v1 公告类型：新
摘要：目前，多实例学习 (MIL) 问题要么从包分类角度解决，要么从实例分类角度解决，这两种方法都忽略了某些实例中包含的重要信息，导致性能受限。例如，现有方法在学习困难正实例时经常面临困难。在本文中，我们将 MIL 公式化为半监督实例分类问题，以便可以充分利用所有标记和未标记的实例来训练更好的分类器。这种公式化的难点在于，在 MIL 中所有标记实例都是负的，而半监督学习中使用的传统自训练技术在这种情况下往往会在为未标记实例生成伪标签时退化。为了解决这个问题，我们提出了一种弱监督的自训练方法，其中我们利用正包标签在伪标签上构建全局约束和局部约束，以防止它们退化并强制分类器学习困难正实例。值得注意的是，容易的正例是分类过程中远离决策边界的实例，而困难的正例是靠近决策边界的实例。通过迭代优化，伪标签可以逐渐接近真实标签。在两个 MNIST 合成数据集、五个传统 MIL 基准数据集和两个组织病理学全幻灯片图像数据集上进行的大量实验表明，我们的方法在所有这些数据集上都实现了新的 SOTA 性能。代码将公开。]]></description>
      <guid>https://arxiv.org/abs/2408.04813</guid>
      <pubDate>Tue, 13 Aug 2024 03:14:20 GMT</pubDate>
    </item>
    <item>
      <title>一次拍摄即可完成连续红外小目标分割</title>
      <link>https://arxiv.org/abs/2408.04823</link>
      <description><![CDATA[arXiv:2408.04823v1 公告类型：新 
摘要：红外小目标序列在帧间表现出很强的相似性并包含丰富的上下文信息，这激励我们用最少的数据实现顺序红外小目标分割。受到以 Segment Anything Model (SAM) 为首的大型分割模型在各种下游任务中的成功的启发，我们提出了一种一次性且无需训练的方法，将 SAM 的零样本泛化能力完美地应用于顺序红外小目标分割。给定一个带注释的帧作为参考，我们的方法可以准确地分割序列中其他帧中的小目标。具体而言，我们首先通过参考图像和测试图像之间的局部特征匹配获得置信度图。然后，以置信度图中的最高点作为提示，我们设计了点提示中心聚焦 (PPCF) 模块来解决边界模糊的小目标的过度分割。随后，为了防止漏检和误检，我们引入了三级集成 (TLE) 模块，该模块集成了前两个步骤在不同级别获得的掩码以生成最终掩码。实验表明，我们的方法只需要一次拍摄就能实现与基于传统多次拍摄监督的最新方法相当的性能，甚至在几次拍摄设置中也能获得更好的性能。此外，消融研究证实了我们的方法对单次拍摄样本变化、场景变化和多个目标存在的稳健性。]]></description>
      <guid>https://arxiv.org/abs/2408.04823</guid>
      <pubDate>Tue, 13 Aug 2024 03:14:20 GMT</pubDate>
    </item>
    <item>
      <title>具有结构感知掩模的自增强高斯溅射用于稀疏视图 3D 重建</title>
      <link>https://arxiv.org/abs/2408.04831</link>
      <description><![CDATA[arXiv:2408.04831v1 公告类型：新
摘要：稀疏视图 3D 重建是计算机视觉领域的一项艰巨挑战，旨在从有限的视角构建完整的三维模型。这项任务面临几个困难：1）输入图像数量有限且缺乏一致信息；2）依赖输入图像的质量；3）模型参数的大小很大。为了应对这些挑战，我们提出了一种自增强的粗到细高斯分层范式，并通过结构感知掩模进行增强，用于稀疏视图 3D 重建。具体而言，我们的方法最初采用粗高斯模型从稀疏视图输入中获得基本的 3D 表示。随后，我们开发了一个精细高斯网络，通过 3D 几何增强和感知视图增强来增强输出的一致性和详细表示。在训练过程中，我们设计了一种结构感知的掩蔽策略，以进一步提高模型对稀疏输入和噪声的鲁棒性。在 MipNeRF360 和 OmniObject3D 数据集上的实验结果表明，所提出的方法在感知质量和效率方面都对稀疏输入视图实现了最先进的性能。]]></description>
      <guid>https://arxiv.org/abs/2408.04831</guid>
      <pubDate>Tue, 13 Aug 2024 03:14:20 GMT</pubDate>
    </item>
    <item>
      <title>BRAT：与架构无关的文本反转的奖励正交令牌</title>
      <link>https://arxiv.org/abs/2408.04785</link>
      <description><![CDATA[arXiv:2408.04785v1 公告类型：新
摘要：文本反转仍然是一种流行的个性化传播模型的方法，目的是向模型传授新的主题和风格。我们注意到，使用 UNet 的替代方案对文本反转的探索不足，并使用视觉转换器尝试文本反转。我们还寻求使用不需要明确使用 UNet 及其特殊层的策略来优化文本反转，因此我们添加了奖励标记并强制正交性。我们发现使用奖励标记可以提高对源图像的遵守度，而使用视觉转换器可以提高对提示的遵守度。代码可在 https://github.com/jamesBaker361/tex_inv_plus 上找到。]]></description>
      <guid>https://arxiv.org/abs/2408.04785</guid>
      <pubDate>Tue, 13 Aug 2024 03:14:19 GMT</pubDate>
    </item>
    <item>
      <title>SOD-YOLOv8——增强 YOLOv8 在交通场景中的小物体检测</title>
      <link>https://arxiv.org/abs/2408.04786</link>
      <description><![CDATA[arXiv:2408.04786v1 公告类型：新
摘要：作为计算机视觉的一部分，物体检测对于交通管理、应急响应、自动驾驶汽车和智慧城市至关重要。尽管物体检测取得了重大进展，但由于物体的大小、与相机的距离、形状各异以及背景杂乱，在远距离摄像机拍摄的图像中检测小物体仍然具有挑战性。为了应对这些挑战，我们提出了小物体检测 YOLOv8 (SOD-YOLOv8)，这是一种专门为涉及大量小物体的场景设计的新模型。受高效广义特征金字塔网络 (GFPN) 的启发，我们增强了 YOLOv8 中的多路径融合以集成不同级别的特征，保留较浅层的细节并提高小物体检测的准确性。此外，还添加了第四个检测层以有效利用高分辨率空间信息。C2f-EMA 模块中的高效多尺度注意模块 (EMA) 通过重新分配权重和优先考虑相关特征来增强特征提取。我们引入了 Powerful-IoU (PIoU) 来替代 CIoU，重点关注中等质量的锚框，并根据预测和地面真实边界框角之间的差异添加惩罚。这种方法简化了计算，加快了收敛速度，并提高了检测精度。SOD-YOLOv8 显著改善了小物体检测，在各种指标上都超越了广泛使用的模型，而与 YOLOv8s 相比，计算成本或延迟没有显著增加。具体来说，它将召回率从 40.1\% 提高到 43.9\%，将准确率从 51.2\% 提高到 53.9\%，将 $\text{mAP}_{0.5}$ 从 40.6\% 提高到 45.1\%，将 $\text{mAP}_{0.5:0.95}$ 从 24\% 提高到 26.6\%。在动态的真实交通场景中，SOD-YOLOv8 在不同条件下表现出显着的改进，证明了其即使在具有挑战性的环境中检测小物体的可靠性和有效性。]]></description>
      <guid>https://arxiv.org/abs/2408.04786</guid>
      <pubDate>Tue, 13 Aug 2024 03:14:19 GMT</pubDate>
    </item>
    <item>
      <title>FewShotNeRF：基于元学习的新型视图合成，用于快速场景特定适应</title>
      <link>https://arxiv.org/abs/2408.04803</link>
      <description><![CDATA[arXiv:2408.04803v1 公告类型：新
摘要：在本文中，我们通过我们提出的方法 FewShotNeRF 解决了使用有限的多视图图像生成现实世界物体的新视图的挑战。我们的方法利用元学习来获得最佳初始化，从而促进神经辐射场 (NeRF) 快速适应特定场景。我们的元学习过程的重点是捕获类别中的共享几何和纹理，嵌入在权重初始化中。这种方法加快了 NeRF 的学习过程，并利用位置编码方面的最新进展来减少将 NeRF 拟合到场景所需的时间，从而加速元学习的内循环优化。值得注意的是，我们的方法能够在大量 3D 场景上进行元学习，从而为各种类别建立强大的 3D 先验。通过对 3D 开源数据集中的常见对象进行广泛的评估，我们通过实验证明了元学习在生成高质量新对象视图方面的有效性和潜力。]]></description>
      <guid>https://arxiv.org/abs/2408.04803</guid>
      <pubDate>Tue, 13 Aug 2024 03:14:19 GMT</pubDate>
    </item>
    <item>
      <title>Hyper-YOLO：当视觉物体检测遇到超图计算</title>
      <link>https://arxiv.org/abs/2408.04804</link>
      <description><![CDATA[arXiv:2408.04804v1 公告类型：新
摘要：我们介绍了一种新的物体检测方法 Hyper-YOLO，它集成了超图计算来捕获视觉特征之间复杂的高阶相关性。传统的 YOLO 模型虽然功能强大，但其颈部设计存在局限性，限制了跨级特征的集成和高阶特征相互关系的利用。为了应对这些挑战，我们提出了超图计算赋能语义收集和散射 (HGC-SCS) 框架，它将视觉特征图转置到语义空间并构建用于高阶消息传播的超图。这使模型能够获取语义和结构信息，超越了传统的以特征为中心的学习。Hyper-YOLO 在其主干中结合了所提出的混合聚合网络 (MANet) 以增强特征提取，并在其颈部引入了基于超图的跨级别和跨位置表示网络 (HyperC2Net)。 HyperC2Net 跨越五个尺度，摆脱了传统的网格结构，允许跨层级和位置进行复杂的高阶交互。这些组件的协同作用使 Hyper-YOLO 成为各种尺度模型中最先进的架构，这一点从其在 COCO 数据集上的出色表现可见一斑。具体而言，Hyper-YOLO-N 的表现明显优于先进的 YOLOv8-N 和 YOLOv9-T，改进幅度分别为 12% 和 9%。源代码位于 ttps://github.com/iMoonLab/Hyper-YOLO。]]></description>
      <guid>https://arxiv.org/abs/2408.04804</guid>
      <pubDate>Tue, 13 Aug 2024 03:14:19 GMT</pubDate>
    </item>
    <item>
      <title>视频分割对 3D MRI 的全新适应：使用 SAM2 进行高效的零样本膝关节分割</title>
      <link>https://arxiv.org/abs/2408.04762</link>
      <description><![CDATA[arXiv:2408.04762v1 公告类型：新
摘要：智能医学图像分割方法正在迅速发展并得到越来越广泛的应用，但它们面临着域转移的挑战，由于源域和目标域之间的数据分布不同，算法性能会下降。为了解决这个问题，我们引入了一种零样本、单提示分割 3D 膝关节 MRI 的方法，通过采用 Segment Anything Model 2 (SAM2)，这是一种通用分割模型，旨在接受提示并保留视频帧间的记忆。通过将 3D 医疗体积的切片视为单独的视频帧，我们利用 SAM2 的高级功能来生成运动和空间感知预测。我们证明 SAM2 可以高效地以零样本方式执行分割任务，无需额外的训练或微调，仅使用一个提示即可准确描绘膝关节 MRI 扫描中的结构。我们对柏林 Zuse 研究所骨关节炎倡议 (OAI-ZIB) 数据集进行的实验表明，SAM2 在 3D 膝骨分割方面实现了高精度，胫骨的测试 Dice 相似系数为 0.9643。我们还展示了使用不同 SAM2 模型大小、不同提示方案生成的结果，以及部署在同一数据集上的 SAM1 模型的比较结果。这一突破有可能通过提供可扩展、经济高效的自动分割解决方案来彻底改变医学图像分析，为更广泛的临床应用和简化的工作流程铺平道路。]]></description>
      <guid>https://arxiv.org/abs/2408.04762</guid>
      <pubDate>Tue, 13 Aug 2024 03:14:18 GMT</pubDate>
    </item>
    <item>
      <title>数据驱动的像素控制：挑战与前景</title>
      <link>https://arxiv.org/abs/2408.04767</link>
      <description><![CDATA[arXiv:2408.04767v1 公告类型：新 
摘要：传感器的最新进展带来了像素级的高分辨率和高数据吞吐量。同时，越来越大的（深度）神经网络（NN）的采用也带来了计算机视觉的重大进展。目前，视觉智能的计算复杂度、能耗和延迟越来越高。我们研究了一种数据驱动系统，该系统将像素级的动态感知与视频级的计算机视觉分析相结合，并提出了一种反馈控制回路，以最大限度地减少传感器前端和计算后端之间的数据移动，同时不影响检测和跟踪精度。我们的贡献有三方面：（1）我们引入了预期注意力，并表明它可以通过稀疏激活像素实现高精度预测；（2）利用反馈控制，我们表明，随着稀疏度的增加，学习到的特征向量的维数可以显著降低； （3）我们模拟模拟设计选择（例如改变 RGB 或 Bayer 像素格式和模拟噪声），并研究它们对数据驱动系统关键指标的影响。与传统像素和深度学习模型的比较分析表明性能显著增强。我们的系统在仅激活 30% 的像素时，带宽减少了 10 倍，能量延迟积 (EDP) 提高了 15-30 倍，而物体检测和跟踪精度略有降低。基于模拟仿真，我们的系统可以实现 205 兆像素/秒 (MP/s) 的吞吐量，功耗仅为每 MP 110 mW，即 EDP 理论上提高了约 30 倍。]]></description>
      <guid>https://arxiv.org/abs/2408.04767</guid>
      <pubDate>Tue, 13 Aug 2024 03:14:18 GMT</pubDate>
    </item>
    </channel>
</rss>