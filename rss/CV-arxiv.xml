<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>CS.CV更新arxiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.cv更新arxiv.org e-print存档。</description>
    <lastBuildDate>Fri, 11 Apr 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>反思中的感知</title>
      <link>https://arxiv.org/abs/2504.07165</link>
      <description><![CDATA[ARXIV：2504.07165V1公告类型：新 
摘要：我们介绍了旨在超越当前大型视觉模型（LVLM）的局限性的反思范式的看法，这些模型（LVLMS）最初通常无法获得完美的感知。具体而言，我们提出了反思感（REPER），这是一种在政策和评论家模型之间系统地交替的双模型反射机制，可以迭代地进行视觉感知。该框架由反思性感知学习（RPL）提供动力，该框架通过有条不紊的视觉反射数据集和反射性不可能训练来增强内在的反射能力。全面的实验评估表明，Reper在图像理解，字幕上的精度和减少幻觉方面的可量化改进。值得注意的是，Reper在模型注意模式和人类视觉焦点之间实现了强烈的一致性，而RPL则优化了细粒度和自由形式的偏好对齐。这些进步在反思中确立了感知，作为对未来多模式的强大范式，尤其是在需要复杂的推理和多步操作的任务中。]]></description>
      <guid>https://arxiv.org/abs/2504.07165</guid>
      <pubDate>Fri, 11 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>面孔：面部表达和通过教学调整的属性理解</title>
      <link>https://arxiv.org/abs/2504.07198</link>
      <description><![CDATA[ARXIV：2504.07198V1公告类型：新 
摘要：人脸在社会交流中起着核心作用，需要将表现的计算机视觉工具用于以人为本的应用。我们提出了面对面的face-lava，这是一种以面部为中心的，内在学习的多模式大语模型，包括面部表达和属性识别。此外，面部闭合能够生成可用于推理的自然语言描述。利用现有的Visual数据库，我们首先开发了FaceinStruct-1M，这是一个以面部为中心的数据库，用于调整MLLM用于面部处理。然后，我们开发了一种新颖的面部特定视觉编码器，该编码器由面部引导的交叉注意力提供动力，将面部几何形状与局部视觉特征相结合。我们评估了跨九个不同数据集和五个不同的面部处理任务的建议方法，包括面部表达识别，动作单位检测，面部属性检测，年龄估计和深膜检测。与现有的开源MLLM相比，面对面的成绩与商业解决方案相比，取得了优越的结果。我们的模型输出还可以在所有任务中的零弹位设置下获得GPT的更高推理评级。我们的数据集和模型都将在https://face-llava.github.io上发布，以支持社会AI和基本视觉研究的未来进步。]]></description>
      <guid>https://arxiv.org/abs/2504.07198</guid>
      <pubDate>Fri, 11 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>农业领域的恐龙的地面恐龙很少改编</title>
      <link>https://arxiv.org/abs/2504.07252</link>
      <description><![CDATA[ARXIV：2504.07252V1公告类型：新 
摘要：深度学习模型正在通过实现自动表型，监测和产量估计来改变农业应用。但是，它们的有效性在很大程度上取决于大量注释的培训数据，这可能是劳动和时间密集的。开放式对象检测的最新进展，尤其是在接地迪诺（Dino）之类的模型中，可以根据文本提示输入来检测兴趣区域的潜在解决方案。最初的零射击实验揭示了制定有效文本提示的挑战，尤其是对于单个叶子和视觉上类似类别的复杂对象。为了解决这些局限性，我们提出了一种有效的几种适应方法，该方法通过删除文本编码器模块（BERT）并引入随机初始化的可训练的文本嵌入来简化接地 - 迪诺的体系结构。这种方法在多个农业数据集中实现了卓越的性能，包括植物叶检测，植物计数，昆虫识别，水果计数和遥感任务。具体而言，它比在农业数据集中的完全微调的Yolo模型高达$ \ sim24 \％$地图高，并且在遥感中，在遥感中，在遥感中，在遥感中以$ \ sim10 \％$优于先前的最先进方法。我们的方法提供了一种有希望的解决方案，用于自动化注释并加速专门的农业AI解决方案的开发。]]></description>
      <guid>https://arxiv.org/abs/2504.07252</guid>
      <pubDate>Fri, 11 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>量化绝对姿势消退的认知不确定性</title>
      <link>https://arxiv.org/abs/2504.07260</link>
      <description><![CDATA[ARXIV：2504.07260V1公告类型：新 
摘要：视觉重新定制是估计相机姿势给定图像的任务。绝对姿势回归通过训练神经网络，直接从图像功能中回归摄像头，从而为该任务提供了解决方案。尽管在记忆和计算效率方面是有吸引力的解决方案，但绝对姿势回归的预测在训练域之外是不准确和不可靠的。在这项工作中，我们提出了一种新的方法，用于通过估计变异框架内观察的可能性来量化绝对姿势回归模型的认知不确定性。除了提供对预测信心的衡量标准外，我们的方法还提供了一个统一的模型，该模型还可以处理观察歧义，并在存在重复结构的情况下概率地将相机定位。我们的方法在捕获不确定性和预测错误之间的关系方面优于现有方法。]]></description>
      <guid>https://arxiv.org/abs/2504.07260</guid>
      <pubDate>Fri, 11 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CEC-MMR：多模式回归的跨凝聚聚类方法</title>
      <link>https://arxiv.org/abs/2504.07301</link>
      <description><![CDATA[ARXIV：2504.07301V1公告类型：新 
摘要：在回归分析的实际应用中，遇到每个属性的多数值并不少见。在这种情况下，通常是高斯的单变量分布是次优的，因为平均值可能位于模式之间，导致预测值与实际数据有显着差异。因此，为了解决这个问题，通常使用神经网络学到的参数（称为混合物密度网络（MDN））的混合分布。但是，这种方法具有重要的固有限制，因为确定具有合理程度准确性的确切组件数量是不可行的。在本文中，我们介绍了CEC-MMR，这是一种基于跨凝聚聚类（CEC）的新方法，该方法允许自动检测回归问题中的组件数量。此外，鉴于属性及其值，我们的方法能够用基础组件唯一地识别它。实验结果表明，与经典的MDN相比，CEC-MMR产生的结果较高。]]></description>
      <guid>https://arxiv.org/abs/2504.07301</guid>
      <pubDate>Fri, 11 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>objaverse ++：带有质量注释的策划的3D对象数据集</title>
      <link>https://arxiv.org/abs/2504.07334</link>
      <description><![CDATA[ARXIV：2504.07334V1公告类型：新 
摘要：本文介绍了Objaverse ++，这是人类专家的详细属性注释的Objaverse的一个精选子集。 3D内容生成的最新进展是由大规模数据集（例如Objaverse）驱动的，objaverse包含从互联网收集的800,000多个3D对象。尽管Objaverse代表了最大的3D资产收集，但其效用受到低质量模型的优势的限制。为了解决此限制，我们手动注释10,000个具有详细属性的3D对象，包括美学质量得分，纹理色彩分类，多对象组合构，透明度特征等。通过实验和对生成结果的用户研究，我们证明了以质量为中心的子集进行预训练的模型比在图像到3D生成任务中较大的OBJAVERSE培训的模型获得了更好的性能。此外，通过比较由我们的标签过滤的多个训练数据的子集，我们的结果表明，数据质量越高，训练损失收敛的速度就越快。这些发现表明，仔细的策展和丰富的注释可以补偿原始数据集的大小，从而有可能提供更有效的途径来开发3D生成模型。我们发布了大约500,000个策划的3D模型的增强数据集，以促进3D计算机视觉中各种下游任务的进一步研究。在不久的将来，我们旨在扩展注释以涵盖整个Objaverse数据集。]]></description>
      <guid>https://arxiv.org/abs/2504.07334</guid>
      <pubDate>Fri, 11 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>dltpose：从准确的致密表面点估计中进行6DOF姿势估计</title>
      <link>https://arxiv.org/abs/2504.07335</link>
      <description><![CDATA[ARXIV：2504.07335V1公告类型：新 
摘要：我们提出了DLTPOSE，这是一种从RGB-D图像中估算6DOF对象的新方法，将稀疏关键方法的精度与密集像素的稳健性结合在一起。 DLTPOSE将每金径向距离预测到一组最小的四个关键点，然后将其馈入我们的新颖的直接线性变换（DLT）公式，以产生准确的3D对象框架表面估计，从而导致更好的6DOF姿势估计。此外，我们介绍了一种新颖的对称性关键点订购方法，旨在处理对象对称性，否则在关键点分配中会导致不一致。以前的基于Kepoint的方法依赖于固定关键点订单，该订单未能说明对称对象所展示的多种有效配置，我们的订单方法利用了这些配置来增强模型学习稳定关键点表示的能力。在基准linemod，遮挡linemod和YCB-VIDEO数据集上进行的广泛实验表明，DLTPOSE的表现优于现有方法，尤其是对于对称和遮挡物体的方法，表明平均平均召回率高于86.5％（LM），79.7％（LM-O）和89.5％（LM-O）和89.5％（89.5％）（YCB-v）。该代码可在https://anonymon.4open.science/r/dltpose_/上找到。]]></description>
      <guid>https://arxiv.org/abs/2504.07335</guid>
      <pubDate>Fri, 11 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>宙斯：多模式医学成像中联合分割的零射门LLM指令</title>
      <link>https://arxiv.org/abs/2504.07336</link>
      <description><![CDATA[ARXIV：2504.07336V1公告类型：新 
摘要：通过持续发展基于UNET和基于变压器的基础骨架，医疗图像细分取得了巨大的成功。但是，现实世界中的临床诊断通常需要整合域知识，尤其是文本信息。进行多模式学习涉及作为解决方案显示的视觉和文本方式，但是收集配对的视觉语言数据集是昂贵且耗时的，因此提出了重大挑战。受到大型语言模型（LLM）众多跨模式任务的卓越能力的启发，我们提出了一个新颖的Vision-Llm联合框架来解决这些问题。具体而言，我们基于相应的医学图像引入了冷冻的LLM，以生成零摄指令生成，并模仿放射学扫描和报告生成过程。 {为了更好地近似现实世界的诊断过程}，我们从多模式放射学图像（例如T1-W或T2-W MRI和CT）中生成更精确的文本指令。基于语义理解和对LLM的丰富知识的令人印象深刻的能力。该过程强调从不同方式中提取特殊特征，并使信息与最终临床诊断的信息团聚。通过生成的文本指令，我们提出的工会细分框架可以处理多模式分割，而无需事先收集的视觉语言数据集。为了评估我们提出的方法，我们对有影响力的基线进行了全面的实验，统计结果和可视化的案例研究证明了我们的新方法的优越性。}}]]></description>
      <guid>https://arxiv.org/abs/2504.07336</guid>
      <pubDate>Fri, 11 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>3D高斯脱落的观点依赖性不确定性估计</title>
      <link>https://arxiv.org/abs/2504.07370</link>
      <description><![CDATA[ARXIV：2504.07370V1公告类型：新 
摘要：3D高斯脱落（3DG）在3D场景重建中越来越流行，其视觉准确性很高。但是，对3DGS场景的不确定性估计仍然没有被忽视，并且对于诸如资产提取和场景完成之类的下游任务至关重要。由于3D高斯人的出现与观点有关，因此从另一个角度和不确定的角度可以确定高斯的颜色。因此，我们建议将3DG中的不确定性建模为可以用球形谐波建模的额外视图依赖性人均特征。这种简单而有效的建模很容易解释，并且可以集成到传统的3DGS管道中。正如我们的实验中所证明的那样，它在保持高精度的同时，它也比集合方法要快得多。]]></description>
      <guid>https://arxiv.org/abs/2504.07370</guid>
      <pubDate>Fri, 11 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多模式3D手轨迹预测的新型扩散模型</title>
      <link>https://arxiv.org/abs/2504.07375</link>
      <description><![CDATA[ARXIV：2504.07375V1公告类型：新 
摘要：预测手运动对于理解人类意图并弥合人类运动和机器人操纵之间的动作空间至关重要。现有的手轨迹预测（HTP）方法预测了以过去以自我为中心的观察为条件的3D空间中的未来手路点。但是，此类模型仅旨在容纳2D以上的视频输入。从2D和3D观测值中，缺乏对多模式环境信息的认识，从而阻碍了3D HTP性能的进一步改善。此外，这些模型忽略了手机运动和耳机摄像头之间的协同作用，要么只预测隔离中的手轨迹，要么仅从过去的框架中编码egomotion。为了解决这些局限性，我们提出了用于多模式3D手轨迹预测的新型扩散模型（MMTWIN）。 MMTWIN旨在吸收多模式信息，作为包含2D RGB图像，3D点云，过去的手指点和文本提示的输入。此外，将两个潜在扩散模型，即eGomotion扩散和HTP扩散为双胞胎，都集成到mmtwin中，以预测相机的egomotion和未来的手轨迹。我们提出了一种新型的混合MAMBA转换器模块，作为HTP扩散到更好的熔融多模式特征的脱氧模型。与最先进的基线相比，我们提出的MMTWIN对三个公开数据集的实验结果以及我们的自我录制数据表明，我们提出的MMTWIN可以预测合理的未来3D手轨迹，并将其概括为未看到的环境。代码和预估计的模型将在https://github.com/irmvlab/mmtwin上发布。]]></description>
      <guid>https://arxiv.org/abs/2504.07375</guid>
      <pubDate>Fri, 11 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Brepformer：基于变压器的B-REP几何特征识别</title>
      <link>https://arxiv.org/abs/2504.07378</link>
      <description><![CDATA[ARXIV：2504.07378V1公告类型：新 
摘要：识别B-REP模型上的几何特征是基于多媒体内容检索的基石技术，并且已广泛应用于智能制造中。但是，先前的研究通常仅专注于加工特征识别（MFR），在有效地捕获复杂几何特征的复杂拓扑和几何特征方面缺乏。在本文中，我们提出了Brepformer，这是一种新型的基于变压器的模型，以识别加工功能和复杂的CAD模型的功能。 Brepformer编码和融合了模型的几何和拓扑特征。之后，Brepformer利用变压器体系结构进行特征传播和识别头来识别几何特征。在变压器的每次迭代中，我们都结合了一个偏差，将边缘特征和拓扑特征结合在一起，以增强每个面部的几何约束。此外，我们还提出了一个名为复杂B-REP功能数据集（CBF）的数据集，其中包括20,000个B-REP模型。通过涵盖更复杂的B-REP模型，它可以更好地与工业应用保持一致。实验结果表明，Brepformer可以在MFINSTSEG，MFTRCAD和我们的CBF数据集上实现最新的准确性。]]></description>
      <guid>https://arxiv.org/abs/2504.07378</guid>
      <pubDate>Fri, 11 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>模型差异学习：基于多重构的合成面检测</title>
      <link>https://arxiv.org/abs/2504.07382</link>
      <description><![CDATA[ARXIV：2504.07382V1公告类型：新 
摘要：图像产生的进步使超现实的合成面部面孔也构成风险，从而使合成面部检测至关重要。先前的研究重点是生成的图像和真实图像之间的一般差异，通常忽略了各种生成技术之间的差异。在本文中，我们探讨了合成图像及其相应发电技术之间的内在关系。我们发现，特定图像在不同的生成方法上表现出显着的重建差异，并且匹配的生成技术提供了更准确的重建。基于此洞察力，我们提出了一个基于多重构的检测器。通过使用多个生成模型逆转和重建图像，我们分析了真实，gan生成和DM生成的图像之间的重建差异，以促进有效的分化。此外，我们介绍了亚洲合成面数据集（ASFD），其中包含由各种gan和dms产生的合成亚洲面孔。该数据集补充了现有的合成面部数据集。实验结果表明，我们的检测器具有强大的概括和鲁棒性，可以实现出色的性能。]]></description>
      <guid>https://arxiv.org/abs/2504.07382</guid>
      <pubDate>Fri, 11 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ID-Booth：具有扩散模型的身份一致的面部生成</title>
      <link>https://arxiv.org/abs/2504.07392</link>
      <description><![CDATA[ARXIV：2504.07392V1公告类型：新 
摘要：生成建模的最新进展使得适用于各种领域（包括面部识别）的高质量合成数据。在这里，最先进的生成模型通常依赖于强大的预审预定扩散模型的调节和微调来促进综合所需身份的现实图像。但是，这些模型通常不会考虑训练期间受试者的身份，从而导致产生和预期身份之间的一致性差。相反，采用基于身份的培训目标的方法倾向于在身份的各个方面过度拟合，进而降低了可以生成的图像的多样性。为了解决这些问题，我们在本文中介绍了一种新颖的基于生成扩散的框架，称为ID-Booth。 ID-Booth由负责数据生成的剥落网络组成，该网络是一个用于绘制较低维的潜在空间和从映射图像的变异自动编码器以及一个允许对生成过程进行迅速控制的文本编码器。该框架利用了一个新颖的三重态身份训练目标​​，并实现了符合身份的图像产生，同时保留了经过预验扩散模型的合成能力。使用最先进的潜在扩散模型和不同提示的实验表明，与竞争方法相比，我们的方法促进了更好的身份一致性和确认性的可分离性，同时实现了更高的图像多样性。反过来，产生的数据允许以隐私性的方式有效地扩大小规模数据集并培训表现更好的识别模型。 ID-Booth框架的源代码可在https://github.com/dariant/id-booth上公开获得。]]></description>
      <guid>https://arxiv.org/abs/2504.07392</guid>
      <pubDate>Fri, 11 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>公平的视角：通过同时进行保形阈值和动态输出修复图像识别的公平保证</title>
      <link>https://arxiv.org/abs/2504.07395</link>
      <description><![CDATA[ARXIV：2504.07395V1公告类型：新 
摘要：我们介绍了Aright Sight，这是一个创新的事后框架，旨在通过将共形预测与动态输出修复机制相结合来确保计算机视觉系统的公平性。我们的方法计算出公平意识的不符合分数，同时评估了预测错误和违反公平性。使用保形预测，我们建立了一个自适应阈值，可提供严格的有限样本，无分配保证。当新图像的不符合分数超过校准阈值时，公平的视线实现了针对性的纠正措施，例如用于分类的logit移动和用于检测的置信度，以减少组和个人公平性差异，而无需重新训练或访问内部模型参数。综合理论分析验证了我们方法的误差控制和收敛属性。同时，基准数据集的广泛经验评估表明，公平的视线大大降低了公平性差异，同时保持了高预测性能。]]></description>
      <guid>https://arxiv.org/abs/2504.07395</guid>
      <pubDate>Fri, 11 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>挠性：定制图像生成的保存和个性的动态控制</title>
      <link>https://arxiv.org/abs/2504.07405</link>
      <description><![CDATA[ARXIV：2504.07405V1公告类型：新 
摘要：随着2D生成模型的快速发展，可以保留主题身份的同时进行多种编辑，这已成为关键的研究重点。现有方法通常面临身份保存和个性化操作之间的固有权衡。我们介绍了Flexip，这是一个新颖的框架，可以通过两个专用组件来解除这些目标：一种用于风格操纵的个性化适配器，以及用于维护身份的保护适配器。通过将两种控制机制显式地注入生成模型，我们的框架可以通过重量适配器的动态调整在推理过程中进行灵活的参数化控制。实验结果表明，我们的方法突破了传统方法的性能局限性，在支持更多样化的个性化生成能力的同时，实现了卓越的身份（项目页面：https：//flexip-tech.gith.github.io/flexip/）。]]></description>
      <guid>https://arxiv.org/abs/2504.07405</guid>
      <pubDate>Fri, 11 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>