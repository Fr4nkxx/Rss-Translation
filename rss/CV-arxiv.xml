<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Tue, 28 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>鹰眼 2：从头开始为前沿视觉语言模型构建训练后数据策略</title>
      <link>https://arxiv.org/abs/2501.14818</link>
      <description><![CDATA[arXiv:2501.14818v1 公告类型：新
摘要：最近，开源视觉语言模型 (VLM) 在使其功能更接近专有前沿模型方面取得了令人鼓舞的进展。然而，大多数开源模型只发布其最终的模型权重，而数据策略和实施的关键细节在很大程度上是不透明的。在这项工作中，我们从以数据为中心的角度解决了 VLM 的后训练问题，展示了数据策略在开发前沿 VLM 中的关键作用。通过从头开始研究和构建我们的后训练数据策略，我们分享了对开发过程的详细见解，旨在使开源社区的竞争模型的开发受益。我们引入的数据策略，加上训练方法和模型设计，产生了一个名为 Eagle2 的高性能 VLM 系列。具体来说，Eagle2-9B 在各种多模态基准上取得了最先进的结果，与某些具有多达 70B 个参数的竞争模型相匹配。]]></description>
      <guid>https://arxiv.org/abs/2501.14818</guid>
      <pubDate>Tue, 28 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于注意力机制的图像字幕集成模型</title>
      <link>https://arxiv.org/abs/2501.14828</link>
      <description><![CDATA[arXiv:2501.14828v1 公告类型：新
摘要：图像字幕通过创建单词和图像实际内容之间的关系，从输入图像中创建信息文本。最近，利用变压器的深度学习模型在自动生成图像字幕方面最为成功。变压器网络的功能已导致与视觉相关的多项活动取得了显着进展。在本文中，我们彻底研究了变压器模型，强调了注意力机制发挥的关键作用。所提出的模型使用变压器编码器-解码器架构来创建文本字幕，并使用深度学习卷积神经网络从图像中提取特征。为了创建字幕，我们提出了一种新颖的集成学习框架，该框架利用基于投票机制的几种深度神经网络架构来提高生成的字幕的丰富性，该投票机制选择具有最高双语评估替补 (BLEU) 分数的字幕。使用公开可用的数据集对所提出的模型进行了评估。使用 Flickr8K 数据集，所提出的模型获得了最高的 BLEU-[1-3] 分数，分别为 0.728、0.495 和 0.323。所提出的模型在 Flickr30k 数据集中的表现优于最新方法，由 BLEU-[1-4] 分数决定，分别为 0.798、0.561、0.387 和 0.269。模型的有效性还通过语义命题图像字幕评估 (SPICE) 指标获得，Flicker8k 数据集的得分率为 0.164，Flicker30k 的得分率为 0.387。最后，集成学习显著推进了图像字幕制作过程，因此可以用于不同领域的各种应用。]]></description>
      <guid>https://arxiv.org/abs/2501.14828</guid>
      <pubDate>Tue, 28 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于皮肤癌诊断的混合可解释深度学习框架：将径向基函数网络与可解释 AI 相结合</title>
      <link>https://arxiv.org/abs/2501.14885</link>
      <description><![CDATA[arXiv:2501.14885v1 公告类型：新
摘要：皮肤癌是全球最普遍且可能危及生命的疾病之一，需要尽早准确诊断以改善患者的治疗效果。传统的诊断方法依赖于临床专业知识和组织病理学分析，通常耗时、主观且容易发生变化。为了解决这些限制，我们提出了一种新颖的混合深度学习框架，该框架将卷积神经网络 (CNN) 与径向基函数 (RBF) 网络相结合，以实现高分类准确率和增强的可解释性。整合 RBF 网络的动机在于它们固有的可解释性和对输入特征的局部响应，这使它们非常适合需要透明度和细粒度决策的任务。与依赖全局特征表示的传统深度学习模型不同，RBF 网络允许将图像片段映射到选定的原型，从而利用单个图像中的显着特征。这使临床医生能够将预测追踪到特定的可解释模式。该框架结合了基于分割的特征提取、用于原型选择的主动学习和 K-Medoids 聚类，以关注这些显著特征。对 ISIC 2016 和 ISIC 2017 数据集的评估证明了该模型的有效性，使用 ResNet50 分别实现了 83.02% 和 72.15% 的分类准确率，并且优于基于 VGG16 的配置。通过为预测生成可解释的解释，该框架与临床工作流程保持一致，弥合了预测性能和可信度之间的差距。这项研究强调了混合模型提供可操作见解的潜力，推动了用于高风险医疗应用的可靠 AI 辅助诊断工具的开发。]]></description>
      <guid>https://arxiv.org/abs/2501.14885</guid>
      <pubDate>Tue, 28 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过概率校准提高不确定性感知凝视估计的可靠性</title>
      <link>https://arxiv.org/abs/2501.14894</link>
      <description><![CDATA[arXiv:2501.14894v1 公告类型：新
摘要：当前基于深度学习的外观不确定性感知凝视估计模型产生不一致且不可靠的不确定性估计，限制了它们在下游应用中的采用。在本研究中，我们提出了一种工作流程，使用一些事后样本的概率校准来提高不确定性估计的准确性。概率校准过程采用简单的二次回归模型来补偿深度学习模型估计的不确定性的不准确性。二次模型的训练与主深度学习模型分离，因此不需要昂贵的权重调整。添加的校准过程轻量级且相对独立于深度学习过程，使其运行速度快且易于实现。我们在四个潜在应用场景下评估了校准过程的有效性，其中两个数据集由于数据收集设置而具有独特的图像特征。当校准和测试数据具有相似的特征时，校准过程最有效。即使在校准和测试数据不同的次优情况下，校准过程仍然可以进行校正，以减少未校准模型的不确定性估计中的预测误差。]]></description>
      <guid>https://arxiv.org/abs/2501.14894</guid>
      <pubDate>Tue, 28 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Glissando-Net：深度单视图类别级别姿势估计和 3D 重建</title>
      <link>https://arxiv.org/abs/2501.14896</link>
      <description><![CDATA[arXiv:2501.14896v1 公告类型：新
摘要：我们提出了一种深度学习模型，称为 Glissando-Net，用于从单个 RGB 图像同时估计姿势并在类别级别重建物体的 3D 形状。以前的工作主要集中在估计姿势（通常在实例级别）或重建形状，但不是两者兼而有之。Glissando-Net 由两个联合训练的自动编码器组成，一个用于 RGB 图像，另一个用于点云。我们在 Glissando-Net 中采用了两个关键的设计选择，以便在给定单个 RGB 图像作为输入的情况下更准确地预测物体的 3D 形状和姿势。首先，我们使用来自图像解码器的变换特征图来增强点云编码器和解码器的特征图，从而实现训练和预测中有效的 2D-3D 交互。其次，我们在解码器阶段预测对象的 3D 形状和姿势。这样，我们可以更好地利用训练阶段才出现的 3D 点云信息来训练网络，以实现更准确的预测。我们联合训练 RGB 和点云数据的两个编码器-解码器，以学习如何在推理过程中将潜在特征传递给点云解码器。在测试中，3D 点云的编码器被丢弃。Glissando-Net 的设计灵感来自 codeSLAM。与针对场景 3D 重建的 codeSLAM 不同，我们专注于物体的姿态估计和形状重建，并直接预测物体姿态和姿态不变的 3D 重建，而无需代码优化步骤。大量实验，包括消融研究和与竞争方法的比较，证明了我们提出的方法的有效性，并且与最先进的方法相比具有优势。]]></description>
      <guid>https://arxiv.org/abs/2501.14896</guid>
      <pubDate>Tue, 28 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>遥感视觉语言数据集生成中的幻觉测量和缓解</title>
      <link>https://arxiv.org/abs/2501.14905</link>
      <description><![CDATA[arXiv:2501.14905v1 公告类型：新
摘要：视觉语言模型在各个领域都取得了令人瞩目的成果。然而，它在遥感领域的应用仍然有限，这主要是由于成对的图像文本数据稀缺。为了弥补这一差距，合成字幕生成引起了人们的兴趣，传统上依赖于使用元数据或边界框的基于规则的方法。虽然这些方法提供了一些描述，但它们往往缺乏捕捉复杂广域场景所需的深度。大型语言模型 (LLM) 为生成更具描述性的字幕提供了一种有前途的替代方案，但它们可以产生通用输出并且容易产生幻觉。在本文中，我们提出了一种新方法，通过将地图集成为外部数据源来增强遥感的视觉语言数据集，从而能够生成详细、上下文丰富的字幕。此外，我们还提出了测量和缓解 LLM 生成文本中幻觉的方法。我们介绍了 fMoW-mm，这是一个包含卫星图像、地图、元数据和文本注释的多模态数据集。我们证明了它在小样本设置下自动目标识别的有效性，与其他视觉语言遥感数据集相比取得了优异的性能。]]></description>
      <guid>https://arxiv.org/abs/2501.14905</guid>
      <pubDate>Tue, 28 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Light3R-SfM：面向前馈运动结构</title>
      <link>https://arxiv.org/abs/2501.14914</link>
      <description><![CDATA[arXiv:2501.14914v1 公告类型：新
摘要：我们提出了 Light3R-SfM，这是一种前馈、端到端可学习框架，用于从不受约束的图像集合中高效地进行大规模运动结构 (SfM)。与依赖昂贵的匹配和全局优化来实现精确的 3D 重建的现有 SfM 解决方案不同，Light3R-SfM 通过一种新颖的潜在全局对齐模块解决了这一限制。该模块用可学习的注意机制取代了传统的全局优化，有效地捕获了图像之间的多视图约束，从而实现了稳健而精确的相机姿势估计。Light3R-SfM 通过检索分数引导的最短路径树构建稀疏场景图，与简单方法相比，可显着减少内存使用量和计算开销。大量实验表明，Light3R-SfM 实现了具有竞争力的准确性，同时显着减少了运行时间，使其成为具有运行时间约束的实际应用中 3D 重建任务的理想选择。这项工作开创了一种数据驱动的前馈 SfM 方法，为可扩展、准确、高效的野外 3D 重建铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2501.14914</guid>
      <pubDate>Tue, 28 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用基于轮廓的可区分渲染进行血管造影的 3D/2D 配准</title>
      <link>https://arxiv.org/abs/2501.14918</link>
      <description><![CDATA[arXiv:2501.14918v1 公告类型：新
摘要：我们提出了一种数字减影血管造影 (DSA) 图像的 3D/2D 配准方法，以提供有关脑血流动力学和血管结构的宝贵见解。我们的方法将配准公式化为姿势估计问题，利用前后和侧面 DSA 视图并采用可微分渲染。在真实和合成数据集上进行的初步实验证明了我们方法的有效性，定性和定量评估都突出了其在临床应用中的潜力。代码可在 https://github.com/taewoonglee17/TwoViewsDSAReg 上找到。]]></description>
      <guid>https://arxiv.org/abs/2501.14918</guid>
      <pubDate>Tue, 28 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过插入时间注意模块对超声心动图分割进行运动增强：一种高效、适应性强且可扩展的方法</title>
      <link>https://arxiv.org/abs/2501.14929</link>
      <description><![CDATA[arXiv:2501.14929v1 公告类型：新
摘要：心脏解剖分割对于临床评估心脏功能和疾病诊断以指导治疗和干预至关重要。在执行分割时，深度学习 (DL) 算法与传统图像处理方法相比显著提高了准确性。最近的研究表明，使用运动信息增强 DL 分割可以进一步改善它。已经提出了一系列注入运动信息的方法，但其中许多方法增加了输入图像的维数（这在计算上很昂贵）或没有使用最佳方法来插入运动信息，例如非 DL 配准、非基于注意的网络或单头注意。在这里，我们提出了一种新颖的、计算效率高的替代方案，其中新颖的可扩展时间注意模块 (TAM) 多次提取时间特征交互，并且 TAM 具有多头 KQV 投影交叉注意架构。该模块可以无缝集成到各种现有的基于 CNN 或 Transformer 的网络中，为未来实施的纳入提供了新的灵活性。对不同心脏数据集、二维超声心动图 (CAMUS) 和三维超声心动图 (MITEA) 的广泛评估表明，当该模型集成到 UNet、FCN8s、UNetR、SwinUNetR 和最近的 I2UNet 等成熟的骨干网络中时，其效果显著。我们进一步发现，与当代替代方案相比，优化的 TAM 增强型 FCN8s 网络表现良好。我们的结果证实了 TAM 在不同数据集和骨干网络中的稳健性、可扩展性和通用性。]]></description>
      <guid>https://arxiv.org/abs/2501.14929</guid>
      <pubDate>Tue, 28 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MATCHA: 走向万物相配</title>
      <link>https://arxiv.org/abs/2501.14945</link>
      <description><![CDATA[arXiv:2501.14945v1 公告类型：新
摘要：在图像之间建立对应关系是计算机视觉领域的一项基本挑战，是运动结构、图像编辑和点跟踪等任务的基础。传统方法通常专门针对特定的对应类型，几何、语义或时间，而人类自然会识别这些领域的对齐。受这种灵活性的启发，我们提出了 MATCHA，这是一种统一的特征模型，旨在“统治一切”，在不同的匹配任务之间建立稳健的对应关系。基于扩散模型特征可以编码多种对应类型的见解，MATCHA 通过基于注意的模块动态融合高级语义和低级几何特征来增强这种能力，从而创建富有表现力、多功能且强大的特征。此外，MATCHA 集成了来自 DINOv2 的对象级特征以进一步提高泛化能力，使单个特征能够匹配任何东西。大量实验证明，MATCHA 在几何、语义和时间匹配任务中始终超越最先进的方法，为计算机视觉中基本对应问题的统一方法奠定了新的基础。据我们所知，MATCHA 是第一种能够使用单一统一特征有效解决各种匹配任务的方法。]]></description>
      <guid>https://arxiv.org/abs/2501.14945</guid>
      <pubDate>Tue, 28 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>VideoPure：基于扩散的视频识别对抗性净化</title>
      <link>https://arxiv.org/abs/2501.14999</link>
      <description><![CDATA[arXiv:2501.14999v1 公告类型：新 
摘要：最近的研究表明，视频识别模型容易受到对抗性示例的攻击，对下游应用程序构成严重的安全风险。然而，目前的研究主要集中在对抗性攻击上，对防御机制的探索工作有限。此外，由于视频的时空复杂性，现有的视频防御方法面临成本高、过度拟合和防御性能有限的问题。最近，基于扩散的对抗净化方法在图像域中实现了强大的防御性能。然而，由于视频中增加了时间维度，将这些基于扩散的对抗净化方法直接应用于视频域会导致性能和效率下降。为了实现一种高效有效的视频对抗防御方法，我们提出了第一个基于扩散的视频净化框架来提高视频识别模型的对抗鲁棒性：VideoPure。给定一个对抗样本，我们首先采用时间 DDIM 反演将输入分布转换为时间一致且轨迹定义的分布，从而覆盖对抗噪声并保留更多视频结构。然后，在 DDIM 去噪期间，我们利用每个去噪步骤的中间结果并进行引导式时空优化，在保持时间一致性的同时消除对抗噪声。最后，我们将优化后的中间结果列表输入视频识别模型进行多步投票以获得预测类。我们在基准数据集和模型上研究了我们的方法对黑盒、灰盒和自适应攻击的防御性能。与其他对抗净化方法相比，我们的方法总体上对不同攻击表现出更好的防御性能。我们的代码可在 https://github.com/deep-kaixun/VideoPure 获得。]]></description>
      <guid>https://arxiv.org/abs/2501.14999</guid>
      <pubDate>Tue, 28 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>HuGDiffusion：通过 3D 高斯扩散实现的可推广单图像人体渲染</title>
      <link>https://arxiv.org/abs/2501.15008</link>
      <description><![CDATA[arXiv:2501.15008v1 公告类型：新
摘要：我们提出了 HuGDiffusion，这是一种可推广的 3D 高斯分层 (3DGS) 学习管道，用于从单视图输入图像中实现人物角色的新型视图合成 (NVS)。现有方法通常需要单目视频或校准的多视图图像作为输入，在具有任意和/或未知相机姿势的现实场景中，其适用性可能会减弱。在本文中，我们旨在通过基于扩散的框架生成一组 3DGS 属性，该框架以从单个图像中提取的人类先验为条件。具体而言，我们从精心整合的以人为中心的特征提取程序开始，以推断出信息丰富的条件信号。根据我们的经验观察，联合学习整个 3DGS 属性难以优化，我们设计了一种多阶段生成策略来获得不同类型的 3DGS 属性。为了促进训练过程，我们研究构建代理地面实况 3D 高斯属性作为高质量属性级监督信号。通过大量实验，我们的 HuGDiffusion 显示出比最先进的方法显著的性能改进。我们的代码将公开发布。]]></description>
      <guid>https://arxiv.org/abs/2501.15008</guid>
      <pubDate>Tue, 28 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于小样本分类的视觉语言模型的互补子空间低秩自适应</title>
      <link>https://arxiv.org/abs/2501.15040</link>
      <description><![CDATA[arXiv:2501.15040v1 公告类型：新
摘要：视觉语言模型 (VLM) 已被设计用于大规模图像文本对齐，作为预训练的基础模型。对于下游的少量样本分类任务，参数高效微调 (PEFT) VLM 在计算机视觉社区中获得了广泛的欢迎。已经研究了诸如快速调整和线性适配器之类的 PEFT 方法用于微调 VLM，而很少考虑将低秩自适应 (LoRA) 算法用于少量样本微调 VLM。使用 LoRA 进行少量样本微调的主要障碍是灾难性遗忘问题。因为视觉语言对齐知识对于少量样本学习的普遍性很重要，而低秩自适应会干扰预训练权重矩阵最具信息量的方向。我们提出了互补子空间低秩自适应 (Comp-LoRA) 方法来规范少量样本 VLM 微调中的灾难性遗忘问题。具体来说，我们优化了互补子空间中的低秩矩阵，从而在学习新的少样本信息时保留了 VLM 的一般视觉语言对齐能力。我们对所提出的 Comp-LoRA 方法和其他 PEFT 方法进行了比较实验，以对 VLM 进行少样本分类的微调。我们还展示了我们提出的方法相对于直接将 LoRA 应用于 VLM 的灾难性遗忘问题的抑制。结果表明，所提出的方法比基线方法的 Top-1 准确率高出约 +1.0\%，并且将 VLM 零样本性能与基线方法相比保持了约 +1.3\% 的 Top-1 准确率。]]></description>
      <guid>https://arxiv.org/abs/2501.15040</guid>
      <pubDate>Tue, 28 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>提示感知可控阴影去除</title>
      <link>https://arxiv.org/abs/2501.15043</link>
      <description><![CDATA[arXiv:2501.15043v1 公告类型：新
摘要：阴影去除旨在恢复阴影区域的图像内容。虽然基于深度学习的方法已经显示出有希望的结果，但它们仍然面临关键挑战：1）不受控制地去除所有阴影，或 2）可控去除但严重依赖于精确的阴影区域蒙版。为了解决这些问题，我们引入了一种新颖的范式：提示感知可控阴影去除。与现有方法不同，我们的范式允许根据用户提示（例如，点、线或主题蒙版）从特定主题中定向去除阴影。这种方法消除了对阴影注释的需要，并提供了灵活的、用户控制的阴影去除。具体来说，我们提出了一个端到端可学习模型，\emph{\textbf{P}}rompt-\emph{\textbf{A}}ware \emph{\textbf{C}}ntrollable \emph{\textbf{S}}hadow \emph{\textbf{R}}emoval \emph{\textbf{Network}}work (PACSRNet)。 PACSRNet 由两个关键模块组成：提示感知模块，根据用户提示为指定主题生成阴影蒙版；阴影去除模块，利用第一个模块中的阴影先验恢复阴影区域的内容。此外，我们通过线性操作合并来自提示感知模块的特征信息来增强阴影去除模块，为阴影去除提供提示引导的支持。认识到现有的阴影去除数据集缺乏多样化的用户提示，我们贡献了一个专门为基于提示的可控阴影去除设计的新数据集。大量实验结果证明了 PACSRNet 的有效性和优越性。]]></description>
      <guid>https://arxiv.org/abs/2501.15043</guid>
      <pubDate>Tue, 28 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>实现自动驾驶中稳健的无监督注意力预测</title>
      <link>https://arxiv.org/abs/2501.15045</link>
      <description><![CDATA[arXiv:2501.15045v1 公告类型：新
摘要：稳健地预测自动驾驶系统的关注区域对于驾驶安全至关重要，但由于获取大规模注意力标签的劳动密集型性质以及自动驾驶场景与自然场景之间的领域差距，带来了重大挑战。复杂的交通环境进一步加剧了这些挑战，包括恶劣天气下的摄像头损坏、噪音干扰和长尾分布的中心偏差。为了解决这些问题，我们提出了一种稳健的无监督注意力预测方法。不确定性挖掘分支通过分析自然场景中多个预训练模型的共性和差异来细化预测，而知识嵌入块通过结合驾驶知识来自适应地增强伪标签，从而弥合领域差距。此外，我们引入了 RoboMixup，这是一种新颖的数据增强方法，它通过软注意和动态增强提高了对损坏的鲁棒性，并通过将随机裁剪集成到 Mixup 作为正则化器来减轻中心偏差。为了系统地评估自动驾驶注意力预测的鲁棒性，我们引入了 DriverAttention-C 基准，包括三个子集的 100k 帧：BDD-A-C、DR(eye)VE-C 和 DADA-2000-C。我们的方法在三个公共数据集和提出的鲁棒性基准上实现了相当于或超过完全监督的最先进的方法的性能，在 KLD 和 CC 指标中分别将相对损坏退化降低了 58.8% 和 52.8%，并将中心偏差鲁棒性提高了 12.4% 和 11.4%。代码和数据可在 https://github.com/zaplm/DriverAttention 获得。]]></description>
      <guid>https://arxiv.org/abs/2501.15045</guid>
      <pubDate>Tue, 28 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>