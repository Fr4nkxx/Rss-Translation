<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>CS.CV更新arxiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.cv更新arxiv.org e-print存档。</description>
    <lastBuildDate>Fri, 21 Mar 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>cam-seg：语义图像的连续值嵌入方法</title>
      <link>https://arxiv.org/abs/2503.15617</link>
      <description><![CDATA[ARXIV：2503.15617V1公告类型：新 
摘要：传统的基于变压器的语义分割依赖于量化的嵌入。但是，我们的分析表明，使用量化的嵌入（例如VQ-VAE）对分割掩模的自动编码器精度比连续值嵌入（例如KL-VAE）低8％。在此激励的情况下，我们提出了一个连续值的语义分割的嵌入框架。通过将语义面膜生成重新定义为一个连续的图像对扩散过程，我们的方法消除了对离散潜在表示的需求，同时保留了细粒度的空间和语义细节。我们的主要贡献包括扩散引导的自动回归变压器，该变压器通过对图像特征中的长距离依赖性进行建模来学习连续的语义嵌入空间。我们的框架包含一个统一的体系结构，该体系结合了用于连续特征提取的VAE编码器，用于条件嵌入生成的扩散引导的变压器以及用于语义掩盖重建的VAE解码器。我们的设置有助于通过嵌入空间的连续性来实现零击域的适应能力。跨不同数据集（例如，城市景观和域移动变体）进行的实验表明了对分配转移的最新鲁棒性，包括不利天气（例如，雾，雪）和视图变化。我们的模型还表现出强大的噪声弹性，在高斯噪声，中等运动模糊和中等亮度/对比度变化下，达到了强大的性能（与基线相比，与基线相比约为95％），同时仅经历中等影响（$ \ $ \ $ \ $ \ $ 90％的AP）（与基线相比约为90％AP），来自50％的盐和胡椒噪声，饱和效果，饱和效果，饱和度，饱和度和饱和率和饱和度和饱和度和饱和率和小变化。可用代码：https：//github.com/mahmed10/camss.git]]></description>
      <guid>https://arxiv.org/abs/2503.15617</guid>
      <pubDate>Fri, 21 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLAVA-MORE：LLM和视觉主机的比较研究，用于增强视觉教学调整</title>
      <link>https://arxiv.org/abs/2503.15621</link>
      <description><![CDATA[ARXIV：2503.15621V1公告类型：新 
摘要：多模式大语言模型（MLLM）的最新进展突出了视觉主链和基础语言模型的关键作用。虽然先前的工作主要集中于将这些组件扩展到数十亿个参数，但模型大小，体系结构和性能之间的权衡仍未得到充实。此外，培训数据和评估协议的不一致性阻碍了直接比较，因此很难得出最佳的设计选择。在本文中，我们介绍了Llava-More，这是一个新的MLLM家族，将最近的语言模型与不同的视觉主链整合在一起。为了确保公平的比较，我们采用统一的培训协议，该协议始终在所有体系结构中应用。我们的分析系统地探讨了中小型LLM（包括PHI-4，Llama-3.1和Gemma-2），以评估随后的多模式推理，生成和指导，同时检查模型大小与性能之间的关系。除了评估LLM对最终结果的影响外，我们还对各种视觉编码器进行了全面的研究，从基于夹子的架构到Dinov2，Siglip和Siglip2等替代方案。其他实验研究了图像分辨率增加和预训练数据集变化的影响。总体而言，我们的结果为更有效的MLLM的设计提供了见解，提供了可再现的评估框架，可促进直接比较并可以指导未来的模型开发。我们的源代码和训练有素的模型可在以下网址公开获取：https：//github.com/aimagelab/llava-more。]]></description>
      <guid>https://arxiv.org/abs/2503.15621</guid>
      <pubDate>Fri, 21 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Earthscape：用于表面地质映射和地面分析的多模式数据集</title>
      <link>https://arxiv.org/abs/2503.15625</link>
      <description><![CDATA[ARXIV：2503.15625V1公告类型：新 
摘要：表面地质映射对于理解地面表面过程，应对气候变化和国家安全等现代挑战以及支持工程和资源管理中的常见应用至关重要。但是，传统的映射方法是劳动密集型的，限制了空间覆盖范围，并引入了潜在的偏见。为了解决这些局限性，我们引入了Earthscape，这是一种新型的AI-Ready多模式数据集，专门设计用于表面地质映射和地面表面分析。 Earthscape整合了高分辨率的空中RGB和近红外（NIR）图像，数字高程模型（DEM），多规模DEM衍生的地形特征以及水文和基础设施矢量数据。该数据集为七个不同的表面地质类别提供了详细的注释，这些地质类别包括各种地质过程。我们使用开源的原始数据提出了全面的数据处理管道，并使用不同的空间模式来建立基线基准，以证明地球的实用性。作为具有扩展愿景的活着数据集，Earthscape弥合了计算机视觉和地球科学之间的差距，为推进多模式学习，地理空间分析和地质映射的研究提供了宝贵的资源。我们的代码可在https://github.com/masseygeo/earthscape上找到。]]></description>
      <guid>https://arxiv.org/abs/2503.15625</guid>
      <pubDate>Fri, 21 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>视觉语音模型：教授语音模型以交谈图像</title>
      <link>https://arxiv.org/abs/2503.15633</link>
      <description><![CDATA[ARXIV：2503.15633V1公告类型：新 
摘要：视觉模型的最新成功提出了一个问题，即如何等效地将经过验证的语音模型与视觉理解，这是建立能够自由地谈论图像的多模式语音模型的重要里程碑。建立这样的对话愿景语音模型带来了其独特的挑战：（i）成对的图像语音数据集比图像培训对应物要稀缺得多，（ii）确保推理的实时延迟至关重要，至关重要，因此可以带来计算和记忆约束，并且（iii）该模型应保留无刺激性的特征（例如，扬声器），从文本中推断出来。在这项工作中，我们介绍了Moshivis，增强了Moshi最近的对话演讲LLM，并通过轻量级适应模块进行视觉输入。另一种动态门控机制使模型可以更轻松地在视觉输入和无关的对话主题之间切换。为了降低培训成本，我们设计了一个简单的单阶段，有效的微调管道，其中我们利用图像文本（即“无语”）和图像语音样本的混合物。我们通过音频和文本提示评估了下游视觉理解任务的模型，并报告与Moshivis相互作用的定性样本。我们的推理代码将被提供，以及用于评估音频评估的图像语音数据。]]></description>
      <guid>https://arxiv.org/abs/2503.15633</guid>
      <pubDate>Fri, 21 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>以上下文驱动的无培训网络，用于轻量级场景文本细分和识别</title>
      <link>https://arxiv.org/abs/2503.15639</link>
      <description><![CDATA[ARXIV：2503.15639V1公告类型：新 
摘要：现代场景文本识别系统通常取决于需要广泛培训的大型端到端体系结构，并且对于实时场景而言非常昂贵。在这种情况下，由于对内存，计算资源和延迟的限制，重型模型的部署变得不切实际。为了应对这些挑战，我们提出了一个新颖的，无训练的插件框架，该框架利用了预训练的文本识别器的优势，同时最大程度地减少了冗余计算。我们的方法使用基于上下文的理解并介绍一个基于注意力的分割阶段，该阶段在像素级别上完善了候选文本区域，从而改善了下游识别。与其执行传统的文本检测，遵循特征映射和源图像之间的块级比较，并使用预告片的字幕器来利用上下文信息，从而使框架可以直接从场景上下文中生成单词预测。符合文本的语言和词汇评估是通过语义和词法评估来获得最终分数的。满足或超过预定义的置信度阈值的预测绕过了端到端文本谱分析的较重过程，从而确保推理更快并削减了不必要的计算。公共基准测试的实验表明，我们的范式在与最先进的系统相当的情况下实现了性能，但需要更少的资源。]]></description>
      <guid>https://arxiv.org/abs/2503.15639</guid>
      <pubDate>Fri, 21 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过运动不变的视频和手术工具姿势信息获得的多模式手势识别</title>
      <link>https://arxiv.org/abs/2503.15647</link>
      <description><![CDATA[ARXIV：2503.15647V1公告类型：新 
摘要：实时识别手术手势是一块垫脚石，可以自动识别，技能评估，术中辅助，并最终是手术自动化。当前的机器人手术系统为我们提供了丰富的多模式数据，例如视频和运动学。虽然多模式神经网络中的一些最新作品学习了视觉和运动学数据之间的关系，但当前方法将运动信息视为独立信号，而工具尖位姿势之间没有潜在的关系。但是，仪器姿势在几何相关上是相关的，而潜在的几何形状可以帮助神经网络学习手势表示。因此，我们建议使用关系图网络将运动不变度度量（曲率和扭转）与视觉和运动学数据组合在一起，以捕获不同数据流之间的基本关系。我们表明，当将不变信号与工具位置相结合时，手势识别会有所改善，在拼图缝合数据集上达到90.3 \％帧的精度。我们的结果表明，与传统位置和季度表示相比，运动不变的信号和位置相比是手势运动的更好表示。我们的结果强调了对运动识别的运动学建模的必要性。]]></description>
      <guid>https://arxiv.org/abs/2503.15647</guid>
      <pubDate>Fri, 21 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过机器学习与运输相关的表面检测：分析马德里和维也纳的时间趋势</title>
      <link>https://arxiv.org/abs/2503.15653</link>
      <description><![CDATA[ARXIV：2503.15653V1公告类型：新 
摘要：本研究探讨了机器学习与城市空中图像分析的整合，重点是识别汽车和行人的基础设施表面并分析历史趋势。它强调了从卷积体系结构到基于变压器的预训练模型的过渡，强调了它们在全球地理空间分析中的潜力。提出了一个工作流程，用于自动生成地理空间数据集，从而从各种来源（包括WMS/WMTS链接，矢量制图和OpenStreetMap（OSM）OpenStreetMap（OSM）Overpass Passpass-Turbo请求创建语义分割数据集。开发的代码允许使用公开可用的数据无需手动标记的数据来进行快速的数据集生成过程，用于训练机器学习模型。使用来自马德里和维也纳各个地理办公室的空中图像和矢量数据，为汽车和行人表面检测生成了两个数据集。为每个城市训练和评估了一个基于变压器的模型，证明了良好的准确性值。历史趋势分析涉及将受过训练的模型应用于早期图像，该图像早于矢量数据10到20年的可用性，成功地确定了各个城市地区的行人和汽车基础设施的时间趋势。该技术适用于市政政府以最低的成本收集有价值的数据。]]></description>
      <guid>https://arxiv.org/abs/2503.15653</guid>
      <pubDate>Fri, 21 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>UI-Vision：以视觉感知和互动为中心的以桌面为中心的GUI基准</title>
      <link>https://arxiv.org/abs/2503.15661</link>
      <description><![CDATA[ARXIV：2503.15661V1公告类型：新 
摘要：导航图形用户界面（GUI）以自动化文档编辑和文件管理等任务的自主量可以极大地增强计算机工作流程。尽管现有的研究重点是在线设置，但由于数据收集挑战和许可问题，桌面环境对许多专业和日常任务至关重要，但仍未得到充实。我们介绍了UI-Vision，这是第一个全面的，许可证的基准测试，用于在现实世界桌面环境中对计算机使用代理的离线，精细评估。 Unlike online benchmarks, UI-Vision provides: (i) dense, high-quality annotations of human demonstrations, including bounding boxes, UI labels, and action trajectories (clicks, drags, and keyboard inputs) across 83 software applications, and (ii) three fine-to-coarse grained tasks-Element Grounding, Layout Grounding, and Action Prediction-with well-defined metrics to rigorously evaluate agents&#39; performance in桌面环境。我们的评估揭示了在UI-TARS-72B等最新模型中的关键局限性，包括了解专业软件，空间推理以及诸如拖放之类的复杂动作的问题。这些发现突出了开发完全自主的计算机使用代理所面临的挑战。通过将UI-Vision作为开源，我们旨在推进更有能力的代理商来实现现实桌面任务。]]></description>
      <guid>https://arxiv.org/abs/2503.15661</guid>
      <pubDate>Fri, 21 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>朝向云的可扩展，灵活的场景流</title>
      <link>https://arxiv.org/abs/2503.15666</link>
      <description><![CDATA[ARXIV：2503.15666V1公告类型：新 
摘要：场景流估计是描述时间连续观察之间3D运动的任务。该论文旨在为具有两个重要属性的建筑场景流估计器建立基础：它们是可扩展的，即它们可以访问更多的数据和计算，并且它们具有灵活性，即它们在各种域，在各种运动模式中均可在各种运动模式下锻炼，而无需进行大量的高参数调谐。
  在本文中，我们为此提出了一些具体的贡献。在第1章中，我们将场景流及其先前的方法进行环境化。在第2章中，我们提出了一个蓝图，用于构建和扩展前馈场流量估计器，而无需通过大规模蒸馏来从强大的无监督测试时间优化方法提供的伪标记中进行大规模蒸馏。在第3章中，我们介绍了一个基准，以更好地衡量各种物体类型的质量估计质量，更好地将我们关心的焦点和对场景流量估计器的期望，并使用此基准进行构成公共挑战，从而产生重大进展。在第4章中，我们提出了最新的无监督场景流估计器，该场景流估计器引入了一个新的，完整的序列问题表述，并在3D点跟踪之类的相邻域中表现出巨大的希望。最后，在第5章中，我对场景流的下一步及其潜在的未来更广泛的影响进行了哲理。]]></description>
      <guid>https://arxiv.org/abs/2503.15666</guid>
      <pubDate>Fri, 21 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Diffportrait360：360视图合成的一致肖像扩散</title>
      <link>https://arxiv.org/abs/2503.15667</link>
      <description><![CDATA[ARXIV：2503.15667V1公告类型：新 
摘要：从单视图像生成人头的高质量360度视图对于实现可访问的沉浸式触发应用程序和可扩展的个性化内容创建至关重要。虽然全部产量的尖端方法仅限于建模现实的人头，但最新的基于扩散的动态 - 友善的头部合成的方法只能产生正面视图，并与视图一致性斗争，从而阻止其转换为真正的3D模型，以从任意角度渲染。我们介绍了一种新颖的方法，可产生完全一致的360度头视图，可容纳人类，风格化和拟人形式，包括眼镜和帽子等配件。我们的方法建立在diffportrait3D框架上，并结合了用于背面细节的自定义控制网和双重外观模块，以确保全局前后后背的一致性。通过在连续的视图序列上进行训练并集成了背部参考图像，我们的方法可以实现稳健的，局部连续的视图合成。我们的模型可用于实时，自由观看点渲染，在对象合成中的最先进方法和360度的头部生成中，用于实时，自由观看点渲染，用于实时，非常具有挑战性的输入肖像。]]></description>
      <guid>https://arxiv.org/abs/2503.15667</guid>
      <pubDate>Fri, 21 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Chrome：带有咬合弹性和多视图的人类重建，来自单个图像</title>
      <link>https://arxiv.org/abs/2503.15671</link>
      <description><![CDATA[ARXIV：2503.15671V1公告类型：新 
摘要：从单个图像中重建衣服的人是具有广泛应用程序的计算机视觉中的一项基本任务。尽管现有的单眼人类重建解决方案已经显示出令人鼓舞的结果，但它们通常依赖于人类受试者处于无咬合环境中的假设。因此，当遇到野外遮挡图像时，这些算法会产生多视图不一致和零散的重建。此外，大多数单眼3D人类重建的算法利用了几何学先验，例如用于培训和推理的SMPL注释，在现实世界应用中获得的挑战极具挑战性。为了解决这些局限性，我们提出了Chrome：从单个图像中，具有遮挡弹性和多视图符合性的人类重建，这是一条新型管道，旨在重建具有单个闭塞图像的多视图一致性，而无需单个闭合图像，而无需接下来的truth truth Truth Truth Presort Presit notem notement notementions。具体而言，Chrome利用多视频扩散模型从闭塞输入中首先合成无咬合的人类图像，与现成的姿势控制兼容，以明确执行合成过程中的跨视图一致性。然后，对3D重建模型进行了训练，以预测一组3D高斯人，这些3D高斯在遮挡的输入和合成视图上都可以进行跨视图的详细信息，以产生凝聚力和准确的3D表示。 Chrome在充满挑战的条件下的新型视图合成（最高3 dB PSNR）和几何重建方面都取得了重大改进。]]></description>
      <guid>https://arxiv.org/abs/2503.15671</guid>
      <pubDate>Fri, 21 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GASP：统一的几何和语义自我监督的预训练用于自主驾驶</title>
      <link>https://arxiv.org/abs/2503.15672</link>
      <description><![CDATA[ARXIV：2503.15672V1公告类型：新 
摘要：基于下一步预测的自我监督的预训练使大型语言模型能够捕获文本的基本结构，并在大规模应用时导致了大量任务的前所未有的性能。同样，自主驾驶会产生大量时空数据，暗示了利用量表学习环境的潜在几何和语义结构及其演变的可能性。在这个方向上，我们提出了一种几何和语义自我监督的预训练方法gasp，该方法通过在时空的任何查询未来点上预测统一表示，（1）一般占用率，捕获了3D场景的不断发展的结构； （2）自我占用，对环境的自我车辆路径进行建模； （3）视觉基础模型的蒸馏高水平特征。通过对几何和语义4D占用场进行建模，而不是原始传感器测量值，该模型可以学习环境的结构化，可推广的表示及其随时间的演变。我们在多个自动驾驶基准上验证了喘气，表明语义占用预测，在线映射和自我轨迹预测的显着改善。我们的结果表明，连续的4D几何和语义占用预测为自主驾驶提供了可扩展有效的预训练范式。有关代码和其他可视化，请参见\ href {https://research.zenseact.com/publications/gasp/。]]></description>
      <guid>https://arxiv.org/abs/2503.15672</guid>
      <pubDate>Fri, 21 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过语义相似性传播的高度时间一致性在半监督的视频语义分段中用于自动飞行</title>
      <link>https://arxiv.org/abs/2503.15676</link>
      <description><![CDATA[ARXIV：2503.15676V1公告类型：新 
摘要：RGB摄像机的语义细分对于对自动飞行车辆的感知至关重要。通过捕获的视频进行预测的稳定性对于它们的可靠性至关重要，并且对代理商的可信度至关重要。在本文中，我们提出了一种轻巧的视频语义分割方法，以实时推理实现船上的载入时间，以通过跨帧的语义相似性传播在空中数据上进行高度的时间一致性。 SSP暂时地传播了具有全局注册对齐方式的有效图像分割模型的预测，以补偿相机运动。它使用从两个帧的特征相似性计算出的权重将当前估计和先前的预测与线性插值结合在一起。由于数据的可用性是该领域的挑战，因此我们为稀疏标记的数据集提供了一致性感知的知识蒸馏培训程序，很少有注释。我们使用大型图像分割模型作为教师来训练有效的SSP，我们利用同一培训视频中标记和未标记的框架之间的牢固相关性，以在所有帧上获得高质量的监督。 KD-SSP分别获得了较高的精度和可比的推理速度，分别在iavid和RuralsCapes上分别获得了基本图像分割模型的显着时间一致性增加，分别为12.5％和6.7％TC。在这些航空数据集上，KD-SSP比针对一般应用提出的其他视频方法提供了优越的细分质量和推理速度折衷，并显示出更高的一致性。该代码将在接受后公开提供。]]></description>
      <guid>https://arxiv.org/abs/2503.15676</guid>
      <pubDate>Fri, 21 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>您要检测到的变化：通过混合数据生成地球观察中的语义变化检测</title>
      <link>https://arxiv.org/abs/2503.15683</link>
      <description><![CDATA[ARXIV：2503.15683V1公告类型：新 
摘要：基于非常高分辨率（VHR）图像的大规模颞上变化检测对于地球监测至关重要。到目前为止，这仍然很差：方法要么需要大量注释的数据（语义情况），要么仅限于限制数据集（二进制设置）。大多数方法没有表现出时间和空间适应所需的多功能性：建筑设计中的简单性和对现实和全面数据集进行预处理。合成数据集是关键解决方案，但仍无法处理复杂而多样化的场景。在本文中，我们提出了HYSCDG的生成管道，用于创建一个大型混合语义变化检测数据集，该数据集包含真实的VHR图像和Interded图像，以及日期和更改图的土地覆盖语义图。 HYSCDG在语义和空间引导下，产生了逼真的图像，从而导致了全面且混合传输数据集FSC-180K。我们在五个变化检测案例（二进制和语义）上评估FSC-180K，从零射门到混合和顺序训练，以及在低数据状态训练下。实验表明，在我们的混合数据集上进行预处理会导致每种配置中的一个完全合成数据集的显着性能提升，优于SyntheWorld。所有代码，模型和数据都可以在此处提供：$ \ href {https://yb23.github.io/projects/cywd/} {https://yb23.github.io/projectsss/cywd/} $。]]></description>
      <guid>https://arxiv.org/abs/2503.15683</guid>
      <pubDate>Fri, 21 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>人图像合成的多焦点条件潜扩散</title>
      <link>https://arxiv.org/abs/2503.15686</link>
      <description><![CDATA[ARXIV：2503.15686V1公告类型：新 
摘要：潜在扩散模型（LDM）在高分辨率图像生成中表现出很强的功能，并且已广泛用于姿势引导的人图像合成（PGPIS），从而产生了令人鼓舞的结果。但是，LDM的压缩过程通常会导致细节的恶化，尤其是在敏感区域，例如面部特征和衣服纹理。在本文中，我们提出了一种多焦点条件潜扩散（MCLD）方法，通过根据这些敏感区域的分离，姿势不变的特征来解决这些局限性来解决这些局限性。我们的方法利用了多焦点条件聚合模块，该模块有效地集成了面部身份和特定于纹理的信息，从而增强了模型生成外观现实和符合身份持续图像的能力。我们的方法在DeepFashion数据集上显示了一致的身份和外观产生，并且由于其产生的一致性而启用了灵活的人图像编辑。该代码可在https://github.com/jqliu09/mcld上找到。]]></description>
      <guid>https://arxiv.org/abs/2503.15686</guid>
      <pubDate>Fri, 21 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>