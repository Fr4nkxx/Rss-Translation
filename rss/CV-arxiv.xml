<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Tue, 19 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>通过上下文敏感提示和细粒度标记检测恶意模因</title>
      <link>https://arxiv.org/abs/2411.10480</link>
      <description><![CDATA[arXiv:2411.10480v1 公告类型：新
摘要：社交媒体上多模态内容的盛行使自动审核策略变得复杂。这要求增强多模态分类，并更深入地理解图像和模因中低调的含义。尽管之前的努力旨在通过微调来提高模型性能，但很少有人探索考虑模态、提示、标记和微调的端到端优化流程。在本研究中，我们提出了一个用于复杂任务中的模型优化的端到端概念框架。实验支持这种传统而新颖的框架的有效性，实现了最高的准确度和 AUROC。消融实验表明，孤立的优化本身并不是无效的。]]></description>
      <guid>https://arxiv.org/abs/2411.10480</guid>
      <pubDate>Tue, 19 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MFP3D：利用 3D 点云进行单目食物分量估计</title>
      <link>https://arxiv.org/abs/2411.10492</link>
      <description><![CDATA[arXiv:2411.10492v1 公告类型：新
摘要：食物份量估计对于监测健康和跟踪饮食摄入量至关重要。基于图像的饮食评估涉及使用计算机视觉技术分析进食场合图像，它正在逐渐取代 24 小时回忆等传统方法。然而，由于投影到 2D 图像平面时会丢失 3D 信息，因此准确估计图像中的营养成分仍然具有挑战性。现有的份量估计方法很难在现实世界场景中部署，因为它们依赖于特定要求，例如物理参考对象、高质量深度信息或多视图图像和视频。在本文中，我们介绍了 MFP3D，这是一种仅使用单个单目图像即可准确估计食物份量的新框架。具体来说，MFP3D 由三个关键模块组成：(1) 3D 重建模块，从 2D 图像生成食物的 3D 点云表示；(2) 特征提取模块，从 3D 点云和 2D RGB 图像中提取和连接特征；(3) 部分回归模块，采用深度回归模型根据提取的特征估计食物的体积和能量含量。我们的 MFP3D 在 MetaFood3D 数据集上进行了评估，结果表明，与现有方法相比，其在准确估计部分方面有显著改进。]]></description>
      <guid>https://arxiv.org/abs/2411.10492</guid>
      <pubDate>Tue, 19 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>边界注意力约束的零样本布局到图像生成</title>
      <link>https://arxiv.org/abs/2411.10495</link>
      <description><![CDATA[arXiv:2411.10495v1 公告类型：新 
摘要：最近的文本到图像扩散模型擅长从文本生成高分辨率图像，但在精确控制空间构图和对象计数方面存在困难。为了应对这些挑战，一些研究开发了布局到图像 (L2I) 方法，将布局指令合并到文本到图像模型中。然而，现有的 L2I 方法通常需要微调预训练参数或为扩散模型训练额外的控制模块。在这项工作中，我们提出了一种新颖的零样本 L2I 方法 BACON（边界注意力约束生成），它消除了对额外模块或微调的需求。具体来说，我们使用文本视觉交叉注意力特征图来量化生成的图像的布局与提供的指令之间的不一致，然后计算损失函数以在扩散逆过程中优化潜在特征。为了增强空间可控性并缓解复杂布局指令中的语义故障，我们利用自注意力特征图中的像素到像素相关性来对齐交叉注意力图，并结合受边界注意力约束的三个损失函数来更新潜在特征。在 L2I 和非 L2I 预训练扩散模型上的综合实验结果表明，在 DrawBench 和 HRS 基准上，我们的方法在图像合成方面在数量和质量上都优于现有的零样本 L2I 技术。]]></description>
      <guid>https://arxiv.org/abs/2411.10495</guid>
      <pubDate>Tue, 19 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于鲁棒定向对象检测的结构张量表示</title>
      <link>https://arxiv.org/abs/2411.10497</link>
      <description><![CDATA[arXiv:2411.10497v1 公告类型：新
摘要：定向物体检测除了预测物体位置和边界框外，还预测方向。由于角周期性，精确预测方向仍然具有挑战性，这会引入边界不连续性问题和对称性模糊性。受边缘和角检测经典著作的启发，本文提出将定向边界框中的方向表示为结构张量。这种表示结合了基于高斯的方法和角度编码器解决方案的优势，提供了一种简单而有效的方法，该方法对角周期性问题具有鲁棒性，而无需额外的超参数。对五个数据集的广泛评估表明，所提出的结构张量表示在完全监督和弱监督任务中均优于以前的方法，以最小的计算开销实现了高精度的角度预测。因此，这项工作将结构张量确立为定向物体检测中编码方向的稳健而模块化的替代方案。我们公开提供我们的代码，允许无缝集成到现有的物体检测器中。]]></description>
      <guid>https://arxiv.org/abs/2411.10497</guid>
      <pubDate>Tue, 19 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>提示引导的环境一致性对抗补丁</title>
      <link>https://arxiv.org/abs/2411.10498</link>
      <description><![CDATA[arXiv:2411.10498v1 公告类型：新
摘要：物理世界中的对抗性攻击对基于视觉的系统（例如面部识别和自动驾驶）的安全构成了重大威胁。现有的对抗性补丁方法主要侧重于提高攻击性能，但它们通常会生成易于被人类检测到的补丁，并且难以实现环境一致性，即将补丁融入环境中。本文介绍了一种生成对抗性补丁的新方法，该方法解决了补丁的视觉自然性和环境一致性。我们提出了提示引导的环境一致性对抗性补丁 (PG-ECAP)，这是一种将补丁与环境对齐以确保无缝集成到环境中的方法。该方法利用扩散模型来生成既具有环境一致性又能有效逃避检测的补丁。为了进一步提高自然性和一致性，我们引入了两种对齐损失：即时对齐损失和潜在空间对齐损失，以确保生成的补丁保持其对抗性，同时自然地融入其环境。在数字和物理领域进行的大量实验表明，PG-ECAP 在攻击成功率和环境一致性方面优于现有方法。]]></description>
      <guid>https://arxiv.org/abs/2411.10498</guid>
      <pubDate>Tue, 19 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FitDiT：提升真实服装细节，实现高保真虚拟试穿</title>
      <link>https://arxiv.org/abs/2411.10499</link>
      <description><![CDATA[arXiv:2411.10499v1 公告类型：新 
摘要：尽管基于图像的虚拟试穿取得了长足的进步，但新兴方法在生成跨不同场景的高保真和稳健的试穿图像方面仍然面临挑战。这些方法通常会遇到诸如纹理感知维护和尺寸感知试穿等问题，这些问题阻碍了它们的整体有效性。为了解决这些限制，我们提出了一种新颖的服装感知增强技术，称为 FitDiT，该技术旨在使用扩散变换器 (DiT) 进行高保真虚拟试穿，为高分辨率特征分配更多参数和注意力。首先，为了进一步改善纹理感知维护，我们引入了一种服装纹理提取器，该提取器结合服装先验进化来微调服装特征，有助于更好地捕捉条纹、图案和文字等丰富细节。此外，我们通过定制频率距离损失来引入频域学习，以增强高频服装细节。为了解决尺寸感知试穿问题，我们采用了扩张-放松掩模策略，以适应正确的服装长度，防止在跨类别试穿期间生成填满整个掩模区域的服装。采用上述设计，FitDiT 在定性和定量评估中均超越了所有基线。它擅长制作具有照片般逼真和复杂细节的合身服装，同时在 DiT 结构瘦身后，单个 1024x768 图像的推理时间也达到了 4.57 秒，优于现有方法。]]></description>
      <guid>https://arxiv.org/abs/2411.10499</guid>
      <pubDate>Tue, 19 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>OnlyFlow：基于光流的视频扩散模型运动调节</title>
      <link>https://arxiv.org/abs/2411.10501</link>
      <description><![CDATA[arXiv:2411.10501v1 公告类型：新
摘要：我们考虑了文本到视频生成任务的问题，这些任务需要精确控制，适用于各种应用，例如相机运动控制和视频到视频编辑。解决此问题的大多数方法都依赖于提供用户定义的控件，例如二进制掩码或相机运动嵌入。在我们的方法中，我们提出了 OnlyFlow，这种方法利用首先从输入视频中提取的光流来调节生成的视频的运动。使用文本提示和输入视频，OnlyFlow 允许用户生成尊重输入视频运动以及文本提示的视频。这是通过应用于输入视频的光流估计模型来实现的，然后将其输入到可训练的光流编码器。然后将输出特征图注入文本到视频的主干模型中。我们进行了定量、定性和用户偏好研究，以表明 OnlyFlow 在各种任务上与最先进的方法相比都具有积极的优势，即使 OnlyFlow 并未针对此类任务进行专门训练。因此，OnlyFlow 是一种多功能、轻量且高效的文本转视频生成运动控制方法。模型和代码将在 GitHub 和 HuggingFace 上提供。]]></description>
      <guid>https://arxiv.org/abs/2411.10501</guid>
      <pubDate>Tue, 19 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一切都是视频：通过下一帧预测统一模态</title>
      <link>https://arxiv.org/abs/2411.10503</link>
      <description><![CDATA[arXiv:2411.10503v1 公告类型：新
摘要：多模态学习涉及整合来自文本、图像、音频和视频等各种模态的信息，对于视觉问答、跨模态检索和字幕生成等众多复杂任务至关重要。传统方法依赖于特定于模态的编码器和后期融合技术，这可能会阻碍适应新任务或模态时的可扩展性和灵活性。为了解决这些限制，我们引入了一个新颖的框架，将任务重构的概念从自然语言处理 (NLP) 扩展到多模态学习。我们建议将不同的多模态任务重新表述为统一的下一帧预测问题，从而允许单个模型处理不同的模态而无需特定于模态的组件。该方法将所有输入和输出视为视频中的连续帧，从而实现模态的无缝集成和跨任务的有效知识传递。我们的方法在一系列任务上进行了评估，包括文本转文本、图像转文本、视频转视频、视频转文本和音频转文本，证明了该模型能够以最小的调整跨模态进行推广。我们表明，任务重构可以显著简化跨各种任务的多模态模型设计，为更通用的多模态基础模型奠定基础。]]></description>
      <guid>https://arxiv.org/abs/2411.10503</guid>
      <pubDate>Tue, 19 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>USP-Gaussian：统一基于尖峰的图像重建、姿势校正和高斯溅射</title>
      <link>https://arxiv.org/abs/2411.10504</link>
      <description><![CDATA[arXiv:2411.10504v1 公告类型：新
摘要：尖峰相机是一种创新的神经形态相机，它以 40 kHz 的频率捕捉 0-1 比特流的场景，越来越多地用于通过神经辐射场 (NeRF) 或 3D 高斯溅射 (3DGS) 进行 3D 重建任务。以前基于尖峰的 3D 重建方法通常采用案例流水线：首先基于已建立的尖峰到图像重建算法从尖峰流中进行高质量图像重建，然后进行相机姿势估计和 3D 重建。然而，这种级联方法存在大量累积误差，初始图像重建的质量限制会对姿势估计产生负面影响，最终降低 3D 重建的保真度。为了解决这些问题，我们提出了一个协同优化框架 \textbf{USP-Gaussian}，将基于尖峰的图像重建、姿势校正和高斯分层统一到一个端到端框架中。利用 3DGS 提供的多视图一致性和尖峰相机的运动捕捉功能，我们的框架实现了联合迭代优化，无缝集成了尖峰到图像网络和 3DGS 之间的信息。在具有准确姿势的合成数据集上进行的实验表明，我们的方法通过有效消除级联错误而超越了以前的方法。此外，我们集成了姿势优化，以在具有不准确初始姿势的真实场景中实现稳健的 3D 重建，通过有效降低噪音和保留精细纹理细节而优于替代方法。我们的代码、数据和训练模型将在 \url{https://github.com/chenkang455/USP-Gaussian} 上提供。]]></description>
      <guid>https://arxiv.org/abs/2411.10504</guid>
      <pubDate>Tue, 19 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DR-BFR：用于盲人脸修复的扩散模型退化表示</title>
      <link>https://arxiv.org/abs/2411.10508</link>
      <description><![CDATA[arXiv:2411.10508v1 公告类型：新 
摘要：盲人脸恢复 (BFR) 从根本上受到影响模型泛化的退化类型和程度范围广泛的挑战。扩散模型的最新进展在这一领域取得了长足的进步。然而，一个关键的限制是它们缺乏对特定退化的认识，导致潜在的问题，例如不自然的细节和不准确的纹理。在本文中，我们通过无监督对比学习和重建损失，使扩散模型能够将各种退化作为退化提示从低质量 (LQ) 人脸图像中分离出来，并证明这种能力显着提高了性能，特别是在恢复图像的自然性方面。我们新颖的恢复方案称为 DR-BFR，通过结合退化表示 (DR) 和 LQ 图像的内容特征来指导潜在扩散模型 (LDM) 的去噪。 DR-BFR 包含两个模块：1）退化表示模块 (DRM)：该模块从 LQ 人脸中提取具有与内容无关特征的退化表示，并通过对比学习和专门设计的 LQ 重建估计退化空间中的合理分布。2）潜在扩散恢复模块 (LDRM)：该模块感知潜在空间中的退化特征和内容特征，从而能够从 LQ 输入中恢复高质量图像。我们的实验表明，所提出的 DR-BFR 在各种数据集上在定量和定性方面都明显优于最先进的方法。DR 有效地区分了盲人脸逆问题中的各种退化，并为 LDM 提供了相当强大的提示。]]></description>
      <guid>https://arxiv.org/abs/2411.10508</guid>
      <pubDate>Tue, 19 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>TESGNN：用于高效、稳健的多视图 3D 场景理解的时间等变场景图神经网络</title>
      <link>https://arxiv.org/abs/2411.10509</link>
      <description><![CDATA[arXiv:2411.10509v1 公告类型：新
摘要：场景图已被证明对各种场景理解任务非常有效，因为它们可以紧凑而明确地表示关系信息。然而，当前的方法往往忽视了从 3D 点云生成场景图时保持对称性的重要性，这会导致准确性和鲁棒性降低，尤其是在处理嘈杂的多视图数据时。据我们所知，这项工作首次实现了等变场景图神经网络 (ESGNN)，用于从 3D 点云生成语义场景图，特别是为了增强场景理解。此外，先前方法的一个重大限制是缺乏时间建模来捕捉场景内动态演变实体之间的时间相关关系。为了解决这一差距，我们引入了一个新颖的时间层，利用 ESGNN 的对称性保持特性，通过近似图匹配算法将跨多个序列的场景图融合为统一的全局表示。我们的组合架构称为时间等变场景图神经网络 (TESGNN)，它不仅在场景估计精度方面超越了现有的最先进方法，而且还实现了更快的收敛。重要的是，TESGNN 计算效率高，使用现有框架实现起来也很简单，非常适合机器人和计算机视觉领域的实时应用。这种方法为解决复杂的多视图场景理解挑战提供了更强大、更可扩展的解决方案。我们的源代码公开发布在：https://github.com/HySonLab/TESGraph]]></description>
      <guid>https://arxiv.org/abs/2411.10509</guid>
      <pubDate>Tue, 19 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Any2Any：具有共形预测的不完全多模态检索</title>
      <link>https://arxiv.org/abs/2411.10513</link>
      <description><![CDATA[arXiv:2411.10513v1 公告类型：新
摘要：自主代理通过集成多模态输入（例如视觉、音频和 LiDAR）来感知和解释周围环境。这些感知模态支持检索任务，例如机器人中的地点识别。然而，当前的多模态检索系统在部分数据由于传感器故障或无法访问而丢失时会遇到困难，例如无声视频或缺少 RGB 信息的 LiDAR 扫描。我们提出了 Any2Any——一种新颖的检索框架，可解决查询和参考实例都具有不完整模态的场景。与以前仅限于两种模态插补的方法不同，Any2Any 无需训练生成模型即可处理任意数量的模态。它使用跨模态编码器计算成对相似性，并采用具有共形预测的两阶段校准过程来对齐相似性。 Any2Any 可实现跨多模态数据集（例如文本激光雷达和文本时间序列）的有效检索。它在 KITTI 数据集上实现了 35% 的 Recall@5，与具有完整模态的基线模型相当。]]></description>
      <guid>https://arxiv.org/abs/2411.10513</guid>
      <pubDate>Tue, 19 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>牛津尖顶数据集：对大规模 LiDAR 视觉定位、重建和辐射场方法进行基准测试</title>
      <link>https://arxiv.org/abs/2411.10546</link>
      <description><![CDATA[arXiv:2411.10546v1 公告类型：新
摘要：本文介绍了使用定制的多传感器感知单元以及来自地面激光雷达扫描仪 (TLS) 的毫米级精确地图在牛津著名地标及其周围捕获的大规模多模态数据集。感知单元包括三个同步全局快门彩色摄像头、一个汽车 3D 激光雷达扫描仪和一个惯性传感器 - 全部经过精确校准。我们还为涉及定位、重建和新视图合成的任务建立了基准，这使得能够评估同步定位和映射 (SLAM) 方法、运动结构 (SfM) 和多视图立体 (MVS) 方法以及辐射场方法，例如神经辐射场 (NeRF) 和 3D 高斯溅射。为了评估 3D 重建，TLS 3D 模型被用作基本事实。通过将移动 LiDAR 扫描与 TLS 3D 模型进行配准来计算定位地面实况。辐射场方法不仅使用从输入轨迹中采样的姿势进行评估，还使用远离训练姿势的轨迹的视点进行评估。我们的评估展示了最先进的辐射场方法的一个关键限制：我们表明它们倾向于过度拟合训练姿势/图像，并且不能很好地推广到非序列姿势。与使用相同视觉输入的 MVS 系统相比，它们在 3D 重建方面的表现也不尽如人意。我们的数据集和基准旨在促进辐射场方法和 SLAM 系统更好地集成。原始和处理后的数据以及用于解析和评估的软件可在 https://dynamic.robots.ox.ac.uk/datasets/oxford-spires/ 上访问。]]></description>
      <guid>https://arxiv.org/abs/2411.10546</guid>
      <pubDate>Tue, 19 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Vision Eagle Attention：推动图像分类发展的新视角</title>
      <link>https://arxiv.org/abs/2411.10564</link>
      <description><![CDATA[arXiv:2411.10564v1 公告类型：新
摘要：在计算机视觉任务中，聚焦图像内相关区域的能力对于提高模型性能至关重要，尤其是当关键特征较小、细微或空间分散时。卷积神经网络 (CNN) 通常平等对待图像的所有区域，这可能导致特征提取效率低下。为了应对这一挑战，我引入了 Vision Eagle Attention，这是一种新颖的注意力机制，它使用卷积空间注意力来增强视觉特征提取。该模型应用卷积来捕获局部空间特征并生成注意力图，该注意力图有选择地强调图像中信息量最大的区域。这种注意力机制使模型能够专注于判别性特征，同时抑制不相关的背景信息。我将 Vision Eagle Attention 集成到轻量级 ResNet-18 架构中，证明了这种组合可以产生高效而强大的模型。我已经在三个广泛使用的基准数据集上评估了所提出的模型的性能：FashionMNIST、Intel Image Classification 和 OracleMNIST，主要关注图像分类。实验结果表明，该方法提高了分类准确率。此外，该方法还有望扩展到其他视觉任务，例如物体检测、分割和视觉跟踪，为各种基于视觉的应用提供计算效率高的解决方案。代码可从以下网址获取：https://github.com/MahmudulHasan11085/Vision-Eagle-Attention.git]]></description>
      <guid>https://arxiv.org/abs/2411.10564</guid>
      <pubDate>Tue, 19 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>动态摄像机的运动扩散引导 3D 全局 HMR</title>
      <link>https://arxiv.org/abs/2411.10582</link>
      <description><![CDATA[arXiv:2411.10582v1 公告类型：新
摘要：运动捕捉技术通过提供捕捉和分析人体运动细节的工具，改变了从电影和游戏行业到体育科学和医疗保健等众多领域。单目全局人体网格和运动重建 (GHMR) 主题中的终极目标是在野外使用动态相机捕捉的任何单目视频上实现与传统多视图捕捉相当的精度。这是一项具有挑战性的任务，因为单目输入具有固有的深度模糊性，而移动相机增加了额外的复杂性，因为渲染的人体运动现在是人类和相机运动的产物。如果不考虑这种混淆，现有的 GHMR 方法通常会输出不切实际的运动，例如，未考虑的人体根部平移会导致脚部滑动。我们提出了 DiffOpt，一种使用扩散优化的新型 3D 全局 HMR 方法。我们的主要见解是，人体运动生成方面的最新进展，例如运动扩散模型 (MDM)，包含强大的连贯人体运动先验。我们方法的核心是使用 MDM 先验优化初始运动重建。此步骤可实现更全局连贯的人体运动。我们的优化联合优化了运动先验损失和重新投影损失，以正确分离人体和相机运动。我们使用来自全球 3D 人体姿势和形状电磁数据库 (EMDB) 和 Egobody 的视频序列验证了 DiffOpt，并展示了优于其他最先进的全局 HMR 方法的卓越全局人体运动恢复能力，最突出的是长视频设置。]]></description>
      <guid>https://arxiv.org/abs/2411.10582</guid>
      <pubDate>Tue, 19 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>