<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Tue, 10 Sep 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>SCARF：可扩展的持续学习框架，用于节省内存的多神经辐射场</title>
      <link>https://arxiv.org/abs/2409.04482</link>
      <description><![CDATA[arXiv:2409.04482v1 公告类型：新 
摘要：本文介绍了一种新颖的持续学习框架，用于合成多个场景的新视图，逐步学习多个 3D 场景，并仅使用即将到来的新场景的训练数据更新网络参数。我们以神经辐射场 (NeRF) 为基础，它使用多层感知器将场景的密度和辐射场建模为隐式函数。虽然 NeRF 及其扩展已显示出在单个 3D 场景中渲染照片般逼真的新颖视图的强大能力，但有效管理这些不断增长的 3D NeRF 资产是一个新的科学问题。很少有作品关注多场景的有效表示或持续学习能力，这对于 NeRF 的实际应用至关重要。为了实现这些目标，我们的关键思想是将多个场景表示为跨场景权重矩阵和从全局参数生成器生成的一组特定于场景的权重矩阵的线性组合。此外，我们提出了一种不确定表面知识蒸馏策略，将先前场景的辐射场知识转移到新模型中。用这样的权重矩阵表示多个 3D 场景可显著减少内存需求。同时，不确定表面蒸馏策略极大地克服了灾难性遗忘问题，并保持了先前场景的照片级逼真的渲染质量。实验表明，所提出的方法在 NeRF-Synthetic、LLFF 和 TanksAndTemples 数据集上实现了持续学习 NeRF 的最先进的渲染质量，同时保持了极低的存储成本。]]></description>
      <guid>https://arxiv.org/abs/2409.04482</guid>
      <pubDate>Tue, 10 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>跳出 BBox 思考：不受约束的生成对象合成</title>
      <link>https://arxiv.org/abs/2409.04559</link>
      <description><![CDATA[arXiv:2409.04559v1 公告类型：新
摘要：将对象合成到图像中涉及多个非平凡子任务，例如对象放置和缩放、颜色/照明协调、视点/几何调整以及阴影/反射生成。最近的生成图像合成方法利用扩散模型同时处理多个子任务。然而，现有模型面临局限性，因为它们依赖于在训练期间遮罩原始对象，这将它们的生成限制在输入掩码中。此外，获取指定新图像中对象位置和比例的准确输入掩码可能非常具有挑战性。为了克服这些限制，我们定义了一个不受约束的生成对象合成的新问题，即生成不受掩码的限制，并在合成的配对数据集上训练基于扩散的模型。我们首创的模型能够生成超出掩码的对象效果，例如阴影和反射，从而增强图像的真实感。此外，如果提供了空蒙版，我们的模型会自动将对象放置在不同的自然位置和比例中，从而加速合成工作流程。我们的模型在各种质量指标和用户研究中都优于现有的对象放置和合成模型。]]></description>
      <guid>https://arxiv.org/abs/2409.04559</guid>
      <pubDate>Tue, 10 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于手部物体抓取生成的多模态扩散</title>
      <link>https://arxiv.org/abs/2409.04560</link>
      <description><![CDATA[arXiv:2409.04560v1 公告类型：新
摘要：在这项工作中，我们专注于生成对物体的手势。与以前针对给定物体生成手势的工作相比，我们的目标是通过单个模型实现对手和物体形状的泛化。我们提出的方法多模态抓握扩散 (MGD) 从异构数据源中学习两种模态的先验和条件后验分布。因此，它通过利用大规模 3D 对象数据集来缓解手-物体抓握数据集的限制。根据定性和定量实验，有条件和无条件的手部抓握生成都实现了良好的视觉合理性和多样性。所提出的方法也能很好地推广到看不见的物体形状。代码和权重将在 \url{https://github.com/noahcao/mgd} 上提供。]]></description>
      <guid>https://arxiv.org/abs/2409.04560</guid>
      <pubDate>Tue, 10 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>双层跨模态对比聚类</title>
      <link>https://arxiv.org/abs/2409.04561</link>
      <description><![CDATA[arXiv:2409.04561v1 公告类型：新
摘要：图像聚类是无监督学习中的关键任务，它将图像分到没有标签的不同聚类中。虽然以前的深度聚类方法取得了显著的效果，但它们只探索了图像本身的内在信息，而忽略了外部监督知识来提高图像的语义理解。最近，大规模数据集上的视觉语言预训练模型已用于各种下游任务并取得了很大的成果。然而，视觉表示学习和文本语义学习之间存在差距，如何正确利用两种不同模态的表示进行聚类仍然是一个巨大的挑战。为了应对这些挑战，我们提出了一种新的图像聚类框架，称为双层跨模态对比聚类（DXMC）。首先，引入外部文本信息来构建语义空间，并采用该语义空间来生成图像-文本对。其次，将图像-文本对分别发送到预先训练的图像和文本编码器以获得图像和文本嵌入，随后将其输入到四个精心设计的网络中。第三，在不同模态和不同层次的判别性表示之间进行双层跨模态对比学习。在五个基准数据集上的大量实验结果证明了我们提出的方法的优越性。]]></description>
      <guid>https://arxiv.org/abs/2409.04561</guid>
      <pubDate>Tue, 10 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>早期至晚期融合对不完美配准多模态 MRI 胰腺分割的影响</title>
      <link>https://arxiv.org/abs/2409.04563</link>
      <description><![CDATA[arXiv:2409.04563v1 公告类型：新
摘要：多模态融合有望实现更好的胰腺分割。然而，在模型中在哪里进行融合仍然是一个悬而未决的问题。目前尚不清楚在分析不完美对齐的图像对时是否存在融合信息的最佳位置。这项胰腺分割研究中的两个主要对齐挑战是 1) 胰腺可变形和 2) 呼吸使腹部变形。即使在图像配准后，相关变形通常也无法纠正。我们研究了早期到晚期融合对胰腺分割的影响。我们使用了 163 名受试者的 353 对 T2 加权 (T2w) 和 T1 加权 (T1w) 腹部 MR 图像，并附有胰腺标签。我们使用图像配准 (deeds) 来对齐图像对。我们训练了一组具有不同融合点的基本 UNet，从早期到晚期，以评估早期到晚期融合对不完美对齐图像的分割性能的影响。我们评估了 nnUNet 上融合点的泛化。使用基本 UNet 模型的单模态 T2w 基线的 Dice 得分为 0.73，而 nnUNet 模型上的相同基线达到 0.80。对于基本 UNet，最佳融合方法发生在编码器的中间（早期/中期融合），与基线相比，Dice 得分在统计上显着提高了 0.0125。对于 nnUNet，最佳融合方法是在模型之前进行简单的图像连接（早期融合），与基线相比，这导致 Dice 得分显著增加 0.0021。特定块中的融合可以提高性能，但融合的最佳块是模型特定的，增益很小。在未完美注册的数据集中，融合是一个微妙的问题，设计艺术对于发掘潜在见解仍然至关重要。未来需要创新来更好地解决腹部图像对不完美对齐情况下的融合问题。]]></description>
      <guid>https://arxiv.org/abs/2409.04563</guid>
      <pubDate>Tue, 10 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用额外刺激行为进行基于视频的自闭症分类的新型数据集</title>
      <link>https://arxiv.org/abs/2409.04598</link>
      <description><![CDATA[arXiv:2409.04598v1 公告类型：新
摘要：自闭症谱系障碍 (ASD) 可以不同程度地影响个人，包括整体健康、沟通和感觉处理方面的挑战，而且这种情况通常始于年幼。因此，对于医疗专业人员来说，准确诊断幼儿的 ASD 至关重要，但这样做很困难。可以负责任地利用深度学习来提高解决此任务的效率。然而，数据的可用性仍然是一个相当大的障碍。因此，在这项工作中，我们引入了视频 ASD 数据集——一个包含视频帧卷积和注意力图特征数据的数据集——以促进 ASD 分类任务的进一步进展。原始视频展示了儿童对化学感官刺激的反应，包括听觉、触觉和视觉。该数据集包含 2,467 个视频的帧特征，总共约 140 万帧。此外，还加入了头部姿势角度以解释头部运动噪音，以及味觉和嗅觉视频的全句文本标签，描述面部表情在与刺激物互动之前、之后和之后很长时间内的变化。除了提供特征外，我们还在这些数据上测试基础模型，以展示运动噪音如何影响性能以及对更多数据和更复杂标签的需求。]]></description>
      <guid>https://arxiv.org/abs/2409.04598</guid>
      <pubDate>Tue, 10 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于 3D 物体检测的点金字塔多尺度特征融合</title>
      <link>https://arxiv.org/abs/2409.04601</link>
      <description><![CDATA[arXiv:2409.04601v1 公告类型：新
摘要：有效的点云处理对于基于 LiDAR 的自动驾驶系统至关重要。智能车辆的物体检测需要能够理解多个尺度的特征，其中道路使用者可能以不同的尺寸出现。最近的方法侧重于特征聚合算子的设计，该算子从编码器主干中收集不同尺度的特征并将它们分配给兴趣点。虽然在聚合模块方面做出了努力，但如何融合这些多尺度特征的重要性却被忽视了。这导致跨尺度的特征通信不足。为了解决这个问题，本文提出了点金字塔 RCNN（POP-RCNN），这是一种基于特征金字塔的点云 3D 物体检测框架。POP-RCNN 由点金字塔特征增强（PPFE）模块组成，用于建立跨空间尺度和语义深度的连接以进行信息交换。PPFE 模块有效地融合了多尺度特征以获得丰富的信息，而不会增加特征聚合的复杂性。为了弥补点密度不一致的影响，部署了点密度置信度模块。这种设计集成允许使用轻量级特征聚合器，并强调浅层和深层语义，从而实现 3D 物体检测的检测框架。该方法具有很强的适应性，可以应用于各种现有框架以增加特征丰富度，尤其是对于长距离检测。通过在基于体素和基于点体素的基线中采用 PPFE，在 KITTI 和 Waymo Open Dataset 上的实验结果表明，即使在有限的计算空间下，所提出的方法也能实现卓越的性能。]]></description>
      <guid>https://arxiv.org/abs/2409.04601</guid>
      <pubDate>Tue, 10 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用可区分局部对齐的视频自监督对比学习</title>
      <link>https://arxiv.org/abs/2409.04607</link>
      <description><![CDATA[arXiv:2409.04607v1 公告类型：新
摘要：稳健的逐帧嵌入对于执行视频分析和理解任务至关重要。我们提出了一种基于对齐时间视频序列的自监督表示学习方法。我们的框架使用基于 Transformer 的编码器来提取帧级特征，并利用它们找到视频序列之间的最佳对齐路径。我们引入了新颖的局部对齐对比 (LAC) 损失，它结合了可微分的局部对齐损失来捕获局部时间依赖性，并结合了对比损失来增强判别学习。之前关于视频对齐的研究主要集中在使用跨序列对的全局时间排序，而我们的损失鼓励识别得分最高的子序列对齐。LAC 使用可微分的 Smith-Waterman (SW) 仿射方法，该方法具有通过训练阶段学习的灵活参数化，使模型能够动态调整时间间隙惩罚长度。评估表明，我们学习到的表示在动作识别任务上优于现有的最先进方法。]]></description>
      <guid>https://arxiv.org/abs/2409.04607</guid>
      <pubDate>Tue, 10 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于医学图像合成的多条件去噪扩散概率模型 (mDDPM)</title>
      <link>https://arxiv.org/abs/2409.04670</link>
      <description><![CDATA[arXiv:2409.04670v1 公告类型：新
摘要：医学成像应用在人体解剖学、病理学和成像领域具有高度专业化。因此，用于训练医学成像深度学习应用的带注释训练数据集不仅需要高度准确，而且还需要足够多样化和大，以涵盖几乎所有符合这些规范的合理示例。我们认为，可以通过带注释的合成图像的受控生成框架来实现这一目标，需要多个条件规范作为输入来提供控制。我们采用去噪扩散概率模型 (DDPM) 来训练肺部 CT 领域的大规模生成模型，并扩展无分类器采样策略以展示一个这样的生成框架。我们表明，我们的方法可以生成带注释的肺部 CT 图像，这些图像可以忠实地表示解剖结构，令人信服地欺骗专家将它们视为真实的。我们的实验表明，当在可比的大型医学数据集上进行训练时，这种性质的受控生成框架在实现生成的医学图像的解剖一致性方面可以超越几乎所有最先进的图像生成模型。]]></description>
      <guid>https://arxiv.org/abs/2409.04670</guid>
      <pubDate>Tue, 10 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于神经增强的全景高动态范围拼接</title>
      <link>https://arxiv.org/abs/2409.04679</link>
      <description><![CDATA[arXiv:2409.04679v1 公告类型：新
摘要：由于输入的低动态范围 (LDR) 图像区域饱和，并且不同曝光导致 LDR 图像之间的强度变化很大，因此通过拼接具有不同曝光和成对重叠视场 (OFOV) 的多个几何同步的 LDR 图像来生成没有视觉伪影的高动态范围 (HDR) 场景的信息丰富的全景 LDR 图像是一项挑战。幸运的是，由于它们的 OFOV，此类图像的拼接本质上是物理驱动方法和数据驱动方法融合的完美场景。基于这一新见解，本文提出了一种基于神经增强的新型全景 HDR 拼接算法。物理驱动方法是使用 OFOV 构建的。首先使用物理驱动方法生成每个视图的不同曝光图像，然后通过数据驱动方法进行细化，最后用于生成具有不同曝光的全景 LDR 图像。所有不同曝光度的全景LDR图像通过多尺度曝光融合算法组合在一起，生成最终的全景LDR图像。实验结果表明，该算法优于现有的全景拼接算法。]]></description>
      <guid>https://arxiv.org/abs/2409.04679</guid>
      <pubDate>Tue, 10 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>C2F-CHART：一种图表分类的课程学习方法</title>
      <link>https://arxiv.org/abs/2409.04683</link>
      <description><![CDATA[arXiv:2409.04683v1 公告类型：新
摘要：在科学研究中，图表通常是直观表示数据的主要方法。但是，图表的可访问性仍然是一个重大问题。为了改进图表理解流程，我们专注于优化图表分类组件。我们利用课程学习，它受到人类学习过程的启发。在本文中，我们介绍了一种利用粗到细课程学习的图表分类新训练方法。我们的方法，我们称之为 C2F-CHART（粗到细），利用类间相似性来创建不同难度级别的学习任务。我们在 ICPR 2022 CHART-Infographics UB UNITEC PMC 数据集上对我们的方法进行了基准测试，其表现优于最先进的结果。]]></description>
      <guid>https://arxiv.org/abs/2409.04683</guid>
      <pubDate>Tue, 10 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于领域泛化的双流特征增强</title>
      <link>https://arxiv.org/abs/2409.04699</link>
      <description><![CDATA[arXiv:2409.04699v1 公告类型：新
摘要：域泛化（DG）任务旨在从源域中学习一个可以处理分布不均（OOD）问题的鲁棒模型。为了提高模型在未知域中的泛化能力，增加训练样本的多样性是一种有效的解决方案。然而，现有的增强方法总是存在一些局限性。一方面，大多数 DG 方法中的增强方式是不够的，因为由于随机性，模型在最坏情况下可能看不到扰动的特征，因此无法充分探索特征的可迁移性。另一方面，这些方法不涉及判别特征中的因果关系，这会因虚假相关性而损害模型的泛化能力。为了解决这些问题，我们提出了一种双流特征增强（DFA）方法，从两个角度构建一些硬特征。首先，为了提高可迁移性，我们以领域相关的增强方式构建了一些有针对性的特征。在不确定性的指导下，生成了一些硬跨域虚拟特征来模拟领域转移。其次，为了考虑因果关系，我们通过对抗掩码解开虚假相关的非因果信息，然后可以通过这些硬因果相关信息提取更具判别性的特征。与以前的固定合成策略不同，这两种增强被集成到一个统一的可学习特征解开模型中。基于这些硬特征，采用对比学习来保持语义一致性并提高模型的鲁棒性。在多个数据集上进行的大量实验表明，我们的方法可以在领域泛化方面实现最先进的性能。我们的代码可在以下位置获得：https://github.com/alusi123/DFA。]]></description>
      <guid>https://arxiv.org/abs/2409.04699</guid>
      <pubDate>Tue, 10 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>释放通用分割模型的力量：红外小目标检测的简单基线</title>
      <link>https://arxiv.org/abs/2409.04714</link>
      <description><![CDATA[arXiv:2409.04714v1 公告类型：新
摘要：深度学习的最新进展极大地推动了红外小物体检测 (IRSTD) 领域的发展。尽管取得了显著的成功，但这些 IRSTD 方法与自然图像域中的通用分割方法之间仍然存在显著的差距。这种差距主要源于显著的模态差异和红外数据的有限可用性。在本研究中，我们旨在通过研究通用分割模型（如 Segment Anything Model (SAM)）对 IRSTD 任务的适应性来弥合这种分歧。我们的研究表明，许多通用分割模型可以实现与最先进的 IRSTD 方法相当的性能。然而，它们在 IRSTD 中的全部潜力仍未得到充分开发。为了解决这个问题，我们提出了一个简单、轻量但有效的基线模型来分割小红外物体。通过适当的提炼策略，我们使较小的学生模型能够胜过最先进的方法，甚至超越经过微调的教师结果。此外，我们通过引入一种由密集和稀疏查询组成的新型查询设计来有效地编码多尺度特征，从而提高了模型的性能。通过在四个流行的 IRSTD 数据集上进行大量实验，我们的模型与现有方法相比，在准确度和吞吐量方面都表现出了显著的提升，在 NUDT 上超过 SAM 和 Semantic-SAM 14 IoU，在 IRSTD1k 上超过 4 IoU。源代码和模型将在 https://github.com/O937-blip/SimIR 上发布。]]></description>
      <guid>https://arxiv.org/abs/2409.04714</guid>
      <pubDate>Tue, 10 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>跨器官领域自适应神经网络用于胰腺内镜超声图像分割</title>
      <link>https://arxiv.org/abs/2409.04718</link>
      <description><![CDATA[arXiv:2409.04718v1 公告类型：新
摘要：准确分割胰腺内镜超声 (EUS) 图像中的病变对于有效诊断和治疗至关重要。然而，收集足够清晰的 EUS 图像以进行有效诊断是一项艰巨的任务。最近，领域自适应 (DA) 已被用于通过利用来自其他领域的相关知识来解决这些挑战。大多数 DA 方法仅关注同一器官的多视图表示，这使得仍然难以用有限的语义信息清晰地描绘肿瘤病变区域。虽然从不同器官转移同质相似性可能会有益于这个问题，但由于它们之间存在巨大的领域差距，因此缺乏相关工作。为了应对这些挑战，我们提出了跨器官肿瘤分割网络 (COTS-Nets)，由通用网络和辅助网络组成。通用网络利用边界损失来学习不同肿瘤的共同边界信息，尽管数据有限且质量低下，但仍能准确描绘 EUS 中的肿瘤。同时，我们在通用网络中加入了一致性损失，以使胰腺 EUS 的预测与其他器官的肿瘤边界保持一致，以缓解领域差距。为了进一步缩小跨器官领域差距，辅助网络整合了来自不同器官的多尺度特征，帮助通用网络获取领域不变的知识。系统实验表明，COTS-Nets 显著提高了胰腺癌诊断的准确性。此外，我们开发了胰腺癌内镜超声 (PCEUS) 数据集，其中包含 501 张经病理证实的胰腺 EUS 图像，以促进模型开发。]]></description>
      <guid>https://arxiv.org/abs/2409.04718</guid>
      <pubDate>Tue, 10 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>VidLPRO：用于机器人和腹腔镜手术的 $\underline{Vid}$eo-$\underline{L}$语言 $\underline{P}$ 再训练框架</title>
      <link>https://arxiv.org/abs/2409.04732</link>
      <description><![CDATA[arXiv:2409.04732v1 公告类型：新
摘要：我们介绍了 VidLPRO，这是一种专为机器人和腹腔镜手术设计的新型视频语言 (VL) 预训练框架。虽然现有的手术 VL 模型主要依赖于对比学习，但我们提出了一种更全面的方法来捕捉复杂的时间动态并将视频与语言对齐。VidLPRO 集成了视频文本对比学习、视频文本匹配和掩码语言建模目标，以学习丰富的 VL 表示。为了支持这个框架，我们提出了 GenSurg+，这是一个精心策划的数据集，源自 GenSurgery，包含 17k 个手术视频剪辑，并与 GPT-4 使用 Whisper 模型提取的转录本生成的字幕配对。该数据集满足了外科领域对大规模、高质量 VL 数据的需求。在基准数据集（包括 Cholec80 和 AutoLaparo）上进行的大量实验证明了我们方法的有效性。 VidLPRO 在零样本手术阶段识别方面实现了最先进的性能，显著优于现有的手术 VL 模型，例如 SurgVLP 和 HecVL。我们的模型在准确率上提高了 21.5%，在 F1 得分上提高了 15.7%，为该领域树立了新的标杆。值得注意的是，VidLPRO 即使在单帧推理中也表现出强大的性能，同时随着时间上下文的增加而有效地扩展。消融研究揭示了帧采样策略对模型性能和计算效率的影响。这些结果强调了 VidLPRO 作为手术视频理解基础模型的潜力。]]></description>
      <guid>https://arxiv.org/abs/2409.04732</guid>
      <pubDate>Tue, 10 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>