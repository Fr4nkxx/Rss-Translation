<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Thu, 28 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>有参考指导的条件文本到图像生成</title>
      <link>https://arxiv.org/abs/2411.16713</link>
      <description><![CDATA[arXiv:2411.16713v1 公告类型：新
摘要：文本到图像的扩散模型在根据文本指令合成视觉效果惊人的图像方面取得了巨大成功。尽管在创建高保真视觉效果方面取得了显著进展，但文本到图像模型仍然难以精确呈现主题，例如文本拼写。为了应对这一挑战，本文探讨了使用图像的附加条件，为扩散模型生成特定主题提供视觉指导。此外，此参考条件使模型能够以文本标记器的词汇无法充分表示的方式进行调节，并进一步将模型的泛化扩展到生成非英语文本拼写等新功能。我们开发了几个小规模的专家插件，有效地赋予稳定扩散模型采用不同参考的能力。每个插件都经过辅助网络和损失函数的训练，这些函数针对英语场景文本生成、多语言场景文本生成和徽标图像生成等应用进行了定制。我们的专家插件在所有任务上都表现出比现有方法更优的结果，每个任务仅包含 28.55M 个可训练参数。]]></description>
      <guid>https://arxiv.org/abs/2411.16713</guid>
      <pubDate>Thu, 28 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>TPIE：使用文本指令进行拓扑保留图像编辑</title>
      <link>https://arxiv.org/abs/2411.16714</link>
      <description><![CDATA[arXiv:2411.16714v1 公告类型：新 
摘要：在实际应用中，保留拓扑结构非常重要，特别是在医疗保健和医学等敏感领域，人体解剖结构的正确性至关重要。然而，大多数现有的图像编辑模型都侧重于操纵强度和纹理特征，往往忽略了图像中的对象几何形状。为了解决这个问题，本文介绍了一种新方法，即带文本指令的拓扑保留图像编辑 (TPIE)，该方法首次通过文本引导的生成扩散模型确保编辑图像中的拓扑和几何形状保持完整。更具体地说，我们的方法将新生成的样本视为给定输入模板的可变形变体，从而允许可控和保留结构的编辑。我们提出的 TPIE 框架由两个关键模块组成：(i) 基于自动编码器的注册网络，从成对训练图像中学习由速度场参数化的对象变换的潜在表示；以及 (ii) 一种新颖的潜在条件几何扩散 (LCDG) 模型，可有效捕获以自定义文本指令为条件的学习变换特征的数据分布。我们在一组不同的 2D 和 3D 图像上验证了 TPIE，并将它们与最先进的图像编辑方法进行了比较。实验结果表明，我们的方法在生成具有良好保留拓扑的更逼真图像方面优于其他基线。我们的代码将在 Github 上公开发布。]]></description>
      <guid>https://arxiv.org/abs/2411.16714</guid>
      <pubDate>Thu, 28 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PaRCE：基于 CNN 的图像分类的概率和重建能力估计</title>
      <link>https://arxiv.org/abs/2411.16715</link>
      <description><![CDATA[arXiv:2411.16715v1 公告类型：新
摘要：卷积神经网络 (CNN) 在图像分类任务中非常流行且有效，但往往对其预测过于自信。各种工作都试图量化与这些模型相关的不确定性，检测分布不均 (OOD) 输入，或识别图像中的异常区域，但有限的工作试图开发一种可以准确估计感知模型对各种不确定性来源的置信度的整体方法。我们开发了一种基于概率和重建的能力估计 (PaRCE) 方法，并将其与现有的不确定性量化和 OOD 检测方法进行比较。我们发现我们的方法可以最好地区分正确分类、错误分类和具有异常区域的 OOD 样本，以及具有视觉图像修改导致高、中、低预测精度的样本。我们描述了如何扩展我们的方法以用于异常定位任务，并展示了我们的方法区分图像中感知模型熟悉的区域和不熟悉的区域的能力。我们发现，我们的方法生成的可解释分数可以最可靠地捕捉感知模型置信度的整体概念。]]></description>
      <guid>https://arxiv.org/abs/2411.16715</guid>
      <pubDate>Thu, 28 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用形式化验证对文本转视频模型进行神经符号评估</title>
      <link>https://arxiv.org/abs/2411.16718</link>
      <description><![CDATA[arXiv:2411.16718v1 公告类型：新
摘要：Sora、Gen-3、MovieGen 和 CogVideoX 等文本到视频模型的最新进展正在突破合成视频生成的界限，并在机器人、自动驾驶和娱乐等领域得到应用。随着这些模型的普及，出现了各种指标和基准来评估生成的视频的质量。然而，这些指标强调视觉质量和流畅度，忽略了时间保真度和文本到视频对齐，而这两者对于安全关键型应用至关重要。为了解决这一差距，我们引入了 NeuS-V，这是一种新颖的合成视频评估指标，它使用神经符号形式验证技术严格评估文本到视频的对齐。我们的方法首先将提示转换为正式定义的时间逻辑 (TL) 规范，并将生成的视频转换为自动机表示。然后，它通过根据 TL 规范正式检查视频自动机来评估文本到视频的对齐。此外，我们提供了一个时间扩展提示的数据集，以根据我们的基准评估最先进的视频生成模型。我们发现，与现有指标相比，NeuS-V 与人工评估的相关性高出 5 倍以上。我们的评估进一步表明，当前的视频生成模型在这些时间复杂的提示上表现不佳，这凸显了未来需要改进文本到视频的生成能力。]]></description>
      <guid>https://arxiv.org/abs/2411.16718</guid>
      <pubDate>Thu, 28 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Learn2Synth：使用超梯度学习最佳数据合成</title>
      <link>https://arxiv.org/abs/2411.16719</link>
      <description><![CDATA[arXiv:2411.16719v1 公告类型：新
摘要：通过合成进行域随机化是一种强大的策略，可以训练对输入图像域无偏见的网络。随机化允许网络在训练期间看到几乎无限范围的强度和伪影，从而最大限度地减少对外观的过度拟合并最大限度地提高对看不见的数据的泛化。虽然功能强大，但这种方法依赖于对控制合成图像概率分布的大量超参数的精确调整。我们引入了 Learn2Synth，而不是手动调整这些参数，这是一种新颖的过程，其中使用一小组真实标记数据来学习合成参数。与施加约束以将合成数据与真实数据对齐的方法（例如，对比或对抗技术）不同，这些方法可能会使图像及其标签图错位，我们调整增强引擎，使得在合成数据上训练的分割网络在应用于真实数据时具有最佳准确性。这种方法使训练过程受益于真实的标记示例，而无需使用这些真实示例来训练分割网络，从而避免网络偏向训练集的属性。具体来说，我们开发了参数和非参数策略来增强合成图像，从而增强了分割网络的性能。在合成数据集和真实数据集上的实验结果证明了这种学习策略的有效性。代码可在以下网址获取：https://github.com/HuXiaoling/Learn2Synth。]]></description>
      <guid>https://arxiv.org/abs/2411.16719</guid>
      <pubDate>Thu, 28 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于重要性的扩散模型令牌合并</title>
      <link>https://arxiv.org/abs/2411.16720</link>
      <description><![CDATA[arXiv:2411.16720v1 公告类型：新
摘要：扩散模型擅长生成高质量的图像和视频。然而，一个主要的缺点是它们的高延迟。一个简单而有效的加速方法是合并相似的标记以加快计算速度，尽管这可能会导致一些质量损失。在本文中，我们证明在合并过程中保留重要的标记可以显著提高样本质量。值得注意的是，每个标记的重要性可以通过无分类器指导幅度可靠地确定，因为这个度量与条件输入密切相关并且对应于输出保真度。由于无分类器指导不会产生额外的计算成本或需要额外的模块，我们的方法可以轻松集成到大多数基于扩散的框架中。实验表明，我们的方法在各种应用中的表现都明显优于基线，包括文本到图像合成、多视图图像生成和视频生成。]]></description>
      <guid>https://arxiv.org/abs/2411.16720</guid>
      <pubDate>Thu, 28 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>避免危害：一种保护视觉语言模型免遭越狱的自适应方法</title>
      <link>https://arxiv.org/abs/2411.16721</link>
      <description><![CDATA[arXiv:2411.16721v1 公告类型：新
摘要：视觉语言模型 (VLM) 在受到对抗性攻击时会产生意外和有害的内容，特别是因为它们的视觉功能会产生新的漏洞。现有的防御措施，例如输入预处理、对抗性训练和基于响应评估的方法，由于成本高昂，通常不适合实际部署。为了应对这一挑战，我们提出了 ASTRA，这是一种高效且有效的防御措施，它通过自适应地引导模型远离对抗性特征方向来抵御 VLM 攻击。我们的关键程序包括找到代表有害响应方向的可转移转向向量，并应用自适应激活转向在推理时消除这些方向。为了创建有效的转向向量，我们随机从对抗性图像中消融视觉标记，并识别与越狱最密切相关的标记。然后使用这些标记构建转向向量。在推理过程中，我们执行自适应转向方法，该方法涉及转向向量和校准激活之间的投影，从而导致良性输入的性能下降很小，同时在对抗性输入下强烈避免有害输出。跨多个模型和基线的大量实验证明了我们在减轻越狱风险方面的最先进性能和高效率。此外，ASTRA 表现出良好的可迁移性，既能防御设计时看不见的攻击（即基于结构化的攻击），也能防御来自不同分布的对抗性图像。]]></description>
      <guid>https://arxiv.org/abs/2411.16721</guid>
      <pubDate>Thu, 28 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用视觉语言模型先验进行主动提示学习</title>
      <link>https://arxiv.org/abs/2411.16722</link>
      <description><![CDATA[arXiv:2411.16722v1 公告类型：新
摘要：视觉语言模型 (VLM) 在各种分类任务中都表现出了出色的零样本性能。尽管如此，它们对每个任务手工制作的文本提示的依赖阻碍了对新任务的有效适应。虽然提示学习提供了一种有希望的解决方案，但大多数研究都侧重于最大限度地利用给定的少量标记数据集，往往忽略了谨慎的数据选择策略的潜力，这些策略可以用更少的标记数据实现更高的准确性。这促使我们研究一种预算高效的主动提示学习框架。具体来说，我们引入了一种类引导聚类，利用 VLM 的预训练图像和文本编码器，从而从主动学习的初始轮次开始实现集群平衡的获取功能。此外，考虑到 VLM 表现出的类别置信度差异很大，我们提出了一种基于自适应类别阈值的节省预算的选择性查询。在九个数据集的主动学习场景中进行的大量实验表明，我们的方法优于现有基线。]]></description>
      <guid>https://arxiv.org/abs/2411.16722</guid>
      <pubDate>Thu, 28 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型视觉语言模型中间层的魔鬼：通过注意力透镜解释、检测和缓解物体幻觉</title>
      <link>https://arxiv.org/abs/2411.16724</link>
      <description><![CDATA[arXiv:2411.16724v1 公告类型：新
摘要：大型视觉语言模型 (LVLM) 中的幻觉严重削弱了其可靠性，促使研究人员探索幻觉的原因。然而，大多数研究主要关注语言方面而不是视觉方面。在本文中，我们讨论了 LVLM 如何处理视觉信息以及该过程是否会导致幻觉。首先，我们使用注意力透镜来识别 LVLM 处理视觉数据的阶段，发现中间层至关重要。此外，我们发现这些层可以进一步分为两个阶段：“视觉信息丰富”和“语义细化”，分别将视觉数据传播到对象标记并通过文本进行解释。通过分析视觉信息丰富阶段的注意力模式，我们发现真实标记始终比幻觉标记获得更高的注意力权重，这是幻觉的有力指标。进一步检查多头注意力图发现，幻觉标记通常是由头部与不一致的物体交互而产生的。基于这些见解，我们提出了一种简单的推理时间方法，通过整合各个头部的信息来调整视觉注意力。大量实验表明，这种方法可以有效缓解主流 LVLM 中的幻觉，而无需额外的训练成本。]]></description>
      <guid>https://arxiv.org/abs/2411.16724</guid>
      <pubDate>Thu, 28 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>$\textit{Revelio}$：解释和利用扩散模型中的语义信息</title>
      <link>https://arxiv.org/abs/2411.16725</link>
      <description><![CDATA[arXiv:2411.16725v1 公告类型：新
摘要：我们研究丰富的视觉语义信息如何在不同扩散架构的各个层和去噪时间步中表示。我们利用 k 稀疏自动编码器 (k-SAE) 发现单义可解释特征。我们通过迁移学习在现成的扩散模型特征上使用轻量级分类器证实我们的机械解释。在 $4$ 个数据集上，我们展示了扩散特征对表示学习的有效性。我们深入分析了不同的扩散架构、预训练数据集和语言模型条件如何影响视觉表示粒度、归纳偏差和迁移学习能力。我们的工作是深化黑盒扩散模型可解释性的关键一步。代码和可视化可在以下网址获得：https://github.com/revelio-diffusion/revelio]]></description>
      <guid>https://arxiv.org/abs/2411.16725</guid>
      <pubDate>Thu, 28 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>EmotiveTalk：通过音频信息解耦和情感视频传播生成富有表现力的说话头部</title>
      <link>https://arxiv.org/abs/2411.16726</link>
      <description><![CDATA[arXiv:2411.16726v1 公告类型：新
摘要：扩散模型彻底改变了说话头部生成领域，但在长时间生成中仍然面临表现力、可控性和稳定性方面的挑战。在本研究中，我们提出了一个 EmotiveTalk 框架来解决这些问题。首先，为了更好地控制唇部运动和面部表情的生成，设计了一种视觉引导音频信息解耦 (V-AID) 方法来生成与唇部运动和表情一致的基于音频的解耦表示。具体而言，为了实现音频和面部表情表示空间之间的对齐，我们在 V-AID 中提出了一个基于扩散的共语音时间扩展 (Di-CTE) 模块，以在多源情绪条件约束下生成与表情相关的表示。然后，我们提出了一个精心设计的情绪头部扩散 (ETHD) 主干，以高效生成富有表现力的头部视频，其中包含一个表情解耦注入 (EDI) 模块，可自动将表情与参考肖像解耦，同时整合目标表情信息，实现更具表现力的生成性能。实验结果表明，EmotiveTalk 可以生成富有表现力的头部视频，确保承诺的情绪可控性和长时间生成过程中的稳定性，与现有方法相比取得了最佳性能。]]></description>
      <guid>https://arxiv.org/abs/2411.16726</guid>
      <pubDate>Thu, 28 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于有损神经图像压缩的信息论正则化器</title>
      <link>https://arxiv.org/abs/2411.16727</link>
      <description><![CDATA[arXiv:2411.16727v1 公告类型：新
摘要：有损图像压缩网络旨在在遵守特定失真约束的同时最小化图像的潜在熵。然而，由于神经网络学习量化潜在表示的性质，优化神经网络可能具有挑战性。在本文中，我们的主要发现是，最小化潜在熵在某种程度上等同于最大化条件源熵，这一见解深深植根于信息论等式。基于这一见解，我们提出了一种新的神经图像压缩任务结构正则化方法，将负条件源熵纳入训练目标，从而提高优化效率和模型的泛化能力。所提出的信息论正则化器是可解释的、即插即用的，并且不会产生任何推理开销。大量实验证明了其在规范模型和进一步从各种压缩结构和看不见的域中的潜在表示中挤压比特方面的优势。]]></description>
      <guid>https://arxiv.org/abs/2411.16727</guid>
      <pubDate>Thu, 28 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>卫星图像道路图提取：全球规模数据集和新方法</title>
      <link>https://arxiv.org/abs/2411.16733</link>
      <description><![CDATA[arXiv:2411.16733v1 公告类型：新
摘要：最近，道路图提取因其在自动驾驶、导航等领域的关键作用而受到越来越多的关注。然而，准确高效地提取道路图仍然是一个持续的挑战，主要是由于标记数据严重稀缺。为了解决这一限制，我们收集了一个全球规模的卫星道路图提取数据集，即全球规模数据集。具体来说，全球规模数据集比现有最大的公共道路提取数据集大 $\sim20 \times$，覆盖全球 13,800 $km^2$。此外，我们开发了一种新颖的道路图提取模型，即 SAM-Road++，它采用节点引导的重采样方法来缓解 SAM-Road（一种开创性的最先进道路图提取模型）中训练和推理之间的不匹配问题。此外，我们在 SAM-Road++ 中提出了一种简单而有效的“延长线”策略来缓解道路上的遮挡问题。大量实验证明了收集的全球规模数据集和提出的 SAM-Road++ 方法的有效性，尤其突出了其在看不见的区域的卓越预测能力。数据集和代码可在 \url{https://github.com/earth-insights/samroadplus} 上找到。]]></description>
      <guid>https://arxiv.org/abs/2411.16733</guid>
      <pubDate>Thu, 28 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>吸引力盆地内的无分类器引导可能会导致记忆</title>
      <link>https://arxiv.org/abs/2411.16738</link>
      <description><![CDATA[arXiv:2411.16738v1 公告类型：新
摘要：扩散模型倾向于精确地从训练数据中重现图像。这种对训练数据的精确复制令人担忧，因为它可能导致侵犯版权和/或泄露隐私敏感信息。在本文中，我们提出了一种理解记忆现象的新方法，并提出了一种简单而有效的方法来缓解它。我们认为记忆的发生是因为去噪过程中的吸引盆地将扩散轨迹引向记忆图像。然而，这可以通过引导扩散轨迹远离吸引盆地来缓解，即在出现理想的过渡点时不应用无分类器引导，从该过渡点开始应用无分类器引导。这导致生成图像质量高且与调节机制一致非记忆图像。为了进一步改进这一点，我们提出了一种新的引导技术，\emph{相反引导}，它可以在去噪过程中更快地逃离吸引盆地。我们证明了在发生记忆的各种场景中吸引盆地的存在，并且我们表明我们提出的方法成功地减轻了记忆。]]></description>
      <guid>https://arxiv.org/abs/2411.16738</guid>
      <pubDate>Thu, 28 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>恶劣天气下多场景图像恢复的梯度引导参数掩模</title>
      <link>https://arxiv.org/abs/2411.16739</link>
      <description><![CDATA[arXiv:2411.16739v1 公告类型：新
摘要：从图像中去除雨、雨滴和雪等恶劣天气条件对于各种实际应用至关重要，包括自动驾驶、监控和遥感。然而，现有的多任务方法通常依赖于使用附加参数增强模型来处理多种场景。虽然这使模型能够处理不同的任务，但额外参数的引入大大复杂化了其实际部署。在本文中，我们提出了一种用于恶劣天气下多场景图像恢复的新型梯度引导参数掩码，旨在有效处理不同天气条件下的图像退化而无需额外参数。我们的方法通过评估每种特定天气条件下训练期间的梯度变化强度将模型参数划分为公共和特定成分。这使模型能够精确且自适应地学习每种天气情景的相关特征，在不影响性能的情况下提高效率和效果。该方法根据梯度波动构建特定掩码以隔离受其他任务影响的参数，确保模型在所有场景中实现强大的性能而无需添加额外的参数。我们通过在多个基准数据集上进行大量实验，展示了我们框架的领先性能。具体来说，我们的方法在 Raindrop 数据集上实现了 29.22 的 PSNR 分数，在 Rain 数据集上实现了 30.76 的 PSNR 分数，在 Snow100K 数据集上实现了 29.56 的 PSNR 分数。代码可在以下位置获取：\href{https://github.com/AierLab/MultiTask}{https://github.com/AierLab/MultiTask}。]]></description>
      <guid>https://arxiv.org/abs/2411.16739</guid>
      <pubDate>Thu, 28 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>