<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Tue, 27 Aug 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>从放射科医师报告到图像标签：评估潜在狄利克雷分配在骨科射线照片分类神经网络训练中的作用</title>
      <link>https://arxiv.org/abs/2408.13284</link>
      <description><![CDATA[arXiv:2408.13284v1 公告类型：新
摘要：背景：放射线照相术（X 射线）是骨科的主要方式，改善对射线照片的解释具有临床意义。机器学习 (ML) 彻底改变了数据分析，并以自然语言处理 (NLP) 和人工神经网络 (ANN) 的形式应用于医学，并取得了一些成功。潜在狄利克雷分配 (LDA) 是一种 NLP 方法，可自动将文档分类为主题。成功地将 ML 应用于骨科放射线照相术可以创建用于临床的计算机辅助决策系统。我们研究了自动化 ML 管道如何对放射科医生报告中的骨科创伤射线照片进行分类。方法：2002 年至 2015 年期间在瑞典丹德吕德医院拍摄的腕部和踝部射线照片，附有放射科医生报告。LDA 用于为放射科医生报告中的射线照片创建图像标签。使用射线照片和标签来训练图像识别 ANN。人工审查 ANN 结果以准确估计该方法的实用性和准确性。结果：通过 LDA 生成的图像标签可以成功训练 ANN。与黄金标准相比，ANN 的准确率达到 91% 到 60% 之间，具体取决于标签。结论：我们发现 LDA 不适合以高精度标记报告中的骨科射线照片。然而，尽管如此，ANN 可以学会以高精度检测射线照片中的某些特征。该研究还说明了 ML 和 ANN 如何应用于医学研究。]]></description>
      <guid>https://arxiv.org/abs/2408.13284</guid>
      <pubDate>Tue, 27 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SIn-NeRF2NeRF：通过分割和修复使用指令编辑 3D 场景</title>
      <link>https://arxiv.org/abs/2408.13285</link>
      <description><![CDATA[arXiv:2408.13285v1 公告类型：新
摘要：TL;DR 通过将 3D 对象从背景场景中分离出来，有选择地执行 3D 对象编辑。Instruct-NeRF2NeRF (in2n) 是一种很有前途的方法，它可以使用文本提示编辑由神经辐射场 (NeRF) 组成的 3D 场景。然而，同时对背景和对象执行几何修改（例如缩小、缩放或移动）是一项挑战。在这个项目中，我们通过在将对象从场景中分离后有选择地编辑对象来实现 3D 场景中对象的几何变化。我们分别执行对象分割和背景修复，并演示在三维空间内自由调整大小或移动解开的对象的各种示例。]]></description>
      <guid>https://arxiv.org/abs/2408.13285</guid>
      <pubDate>Tue, 27 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 ControlNet 进行抽象艺术解读</title>
      <link>https://arxiv.org/abs/2408.13287</link>
      <description><![CDATA[arXiv:2408.13287v1 公告类型：新
摘要：我们的研究深入探讨了抽象艺术诠释与文本到图像合成的融合，解决了仅通过文本提示实现对图像构图的精确空间控制的挑战。利用 ControlNet 的功能，我们使用户能够更精细地控制合成过程，从而增强对合成图像的操作。受抽象艺术作品中极简主义形式的启发，我们引入了一种由三角形等几何图元制作的新条件。]]></description>
      <guid>https://arxiv.org/abs/2408.13287</guid>
      <pubDate>Tue, 27 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>考虑神经元之间相似性的深度神经网络增长</title>
      <link>https://arxiv.org/abs/2408.13291</link>
      <description><![CDATA[arXiv:2408.13291v1 公告类型：新
摘要：深度学习通过受人脑启发的神经网络在图像识别任务中表现出色。然而，大型模型提高预测精度的必要性带来了巨大的计算需求和延长的训练时间。微调、知识提炼和修剪等传统方法存在潜在准确性下降等局限性。从人类神经发生中汲取灵感，神经元的形成一直持续到成年期，我们探索了一种在训练阶段逐步增加紧凑模型中神经元数量的新方法，从而有效地管理计算成本。我们提出了一种通过引入基于神经元相似性分布的约束来减少特征提取偏差和神经元冗余的方法。这种方法不仅促进了新神经元的有效学习，而且还增强了给定任务的特征提取相关性。CIFAR-10 和 CIFAR-100 数据集的结果证明了准确性的提高，与通过 Grad-CAM 可视化的传统方法相比，我们的方法更加关注要分类的整个对象。这些结果表明我们的方法对决策过程具有潜力。]]></description>
      <guid>https://arxiv.org/abs/2408.13291</guid>
      <pubDate>Tue, 27 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 CLIP 进行在线零样本分类</title>
      <link>https://arxiv.org/abs/2408.13320</link>
      <description><![CDATA[arXiv:2408.13320v1 公告类型：新
摘要：诸如 CLIP 之类的视觉语言预训练可实现零样本迁移，可根据候选类名对图像进行分类。虽然 CLIP 在各种下游任务上都表现出令人印象深刻的零样本性能，但目标数据的分布尚未得到充分利用。在这项工作中，我们研究了一种新颖的在线零样本迁移场景，其中每幅图像以随机顺序到达进行分类，并且只访问一次即可立即获得预测而不存储其表示。与普通的零样本分类相比，所提出的框架保留了其在线服务的灵活性，同时将到达图像的统计数据视为捕获目标数据分布的辅助信息，这有助于提高实际应用程序的性能。为了应对有效的在线优化挑战，我们首先开发在线标签学习来对目标数据分布进行建模。然后，使用所提出的在线代理学习方法进一步优化视觉空间中每个类的代理，以减轻图像和文本之间的模态差距。理论上可以保证两种在线策略的收敛。通过结合在线标签学习和代理学习的预测标签，我们的在线零样本迁移方法（OnZeta）在 ImageNet 上实现了 $78.94\%$ 的准确率，而无需访问整个数据集。此外，使用不同视觉编码器对其他 13 个下游任务进行的大量实验表明平均改进超过 $3\%$，这证明了我们提案的有效性。代码可在 \url{https://github.com/idstcv/OnZeta} 获得。]]></description>
      <guid>https://arxiv.org/abs/2408.13320</guid>
      <pubDate>Tue, 27 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>扩散变换器中的潜在空间解缠可实现零样本细粒度语义编辑</title>
      <link>https://arxiv.org/abs/2408.13335</link>
      <description><![CDATA[arXiv:2408.13335v1 公告类型：新 
摘要：扩散变换器 (DiT) 在多样化和高质量的文本到图像 (T2I) 生成中取得了显著的成功。然而，文本和图像潜在如何单独和共同地对生成图像的语义做出贡献，在很大程度上仍未被探索。通过对 DiT 潜在空间的研究，我们发现了释放零样本细粒度语义编辑潜力的关键发现：(1) DiT 中的文本和图像空间本质上都是可分解的。(2) 这些空间共同形成一个解开的语义表示空间，实现精确和细粒度的语义控制。(3) 有效的图像编辑需要结合使用文本和图像潜在空间。利用这些见解，我们提出了一个简单有效的提取-操作-样本 (EMS) 框架，用于零样本细粒度图像编辑。我们的方法首先利用多模态大型语言模型将输入图像和编辑目标转换为文本描述。然后，我们根据所需的编辑程度线性操纵文本嵌入，并采用受约束的分数蒸馏采样来操纵图像嵌入。我们通过提出一种新指标来量化扩散模型潜在空间的解缠结程度。为了评估细粒度编辑性能，我们引入了一个综合基准，结合了人工注释、手动评估和自动指标。我们进行了广泛的实验结果和深入的分析，以彻底揭示扩散变压器的语义解缠结特性以及我们提出的方法的有效性。我们的带注释的基准数据集在 https://anonymous.com/anonymous/EMS-Benchmark 上公开提供，促进了该领域的可重复研究。]]></description>
      <guid>https://arxiv.org/abs/2408.13335</guid>
      <pubDate>Tue, 27 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SeA：无监督表征学习中最后一层特征的语义对抗增强</title>
      <link>https://arxiv.org/abs/2408.13351</link>
      <description><![CDATA[arXiv:2408.13351v1 公告类型：新
摘要：从预训练深度模型的某些层中提取的深度特征比传统的手工制作特征表现出更好的性能。与可以在原始输入空间中探索各种增强（例如随机裁剪/翻转）的微调或线性探测相比，使用固定深度特征进行学习的适当增强更具挑战性，并且研究较少，这会降低性能。为了释放固定深度特征的潜力，我们提出了一种新的特征空间语义对抗增强（SeA）进行优化。具体而言，梯度隐含的对抗方向将被投影到由其他示例跨越的子空间以保留语义信息。然后，深度特征将受到语义方向的扰动，并应用增强特征来学习分类器。使用 4 个流行的预训练模型对 11 个基准下游分类任务进行了实验。我们的方法比没有 SeA 的深度特征平均好 $2\%$。此外，与预计能带来良好性能的昂贵微调相比，SeA 在 $11$ 个任务中的 $6$ 个任务上表现出了相当的性能，证明了我们的提案除了效率之外，还具有有效性。代码可在 \url{https://github.com/idstcv/SeA} 上找到。]]></description>
      <guid>https://arxiv.org/abs/2408.13351</guid>
      <pubDate>Tue, 27 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于自动饮食评估的保形食品图像生成</title>
      <link>https://arxiv.org/abs/2408.13358</link>
      <description><![CDATA[arXiv:2408.13358v1 公告类型：新
摘要：传统的饮食评估方法严重依赖自我报告，这既耗时又容易产生偏见。人工智能 (AI) 的最新进展揭示了饮食评估的新可能性，特别是通过分析食物图像。识别食物并从图像中估计食物体积是自动饮食评估的关键程序。然而，这两个程序都需要大量标有食物名称和体积的训练图像，而这些图像目前无法获得。或者，最近的研究表明，可以使用生成对抗网络 (GAN) 人工生成训练图像。尽管如此，使用现有技术方便地生成大量已知体积的食物图像仍然是一个挑战。在这项工作中，我们提出了一种简单的基于 GAN 的神经网络架构，用于条件食物图像生成。生成的图像中食物和容器的形状与参考输入图像中的形状非常相似。我们的实验证明了所生成图像的真实性和所提框架的形状保持能力。]]></description>
      <guid>https://arxiv.org/abs/2408.13358</guid>
      <pubDate>Tue, 27 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>BiGS：用于可重新照明的 3D 高斯溅射的双向高斯基元</title>
      <link>https://arxiv.org/abs/2408.13370</link>
      <description><![CDATA[arXiv:2408.13370v1 公告类型：新
摘要：我们提出了双向高斯基元，这是一种基于图像的新型视图合成技术，旨在表示和渲染动态照明下具有表面和体积材料的 3D 对象。我们的方法将光本征分解集成到高斯溅射框架中，从而实现 3D 对象的实时重新照明。为了在一个有凝聚力的外观模型中统一表面和体积材料，我们通过双向球谐函数采用了依赖于光和视图的散射表示。我们的模型不使用特定的表面法线相关反射函数，使其与法线未定义的高斯溅射等体积表示更兼容。我们通过重建和渲染具有复杂材料的物体来演示我们的方法。使用一次一光 (OLAT) 数据作为输入，我们可以在新颖的照明条件下实时重现逼真的外观。]]></description>
      <guid>https://arxiv.org/abs/2408.13370</guid>
      <pubDate>Tue, 27 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从未知中学习未知：用于小样本开放集识别的多样化负面原型生成器</title>
      <link>https://arxiv.org/abs/2408.13373</link>
      <description><![CDATA[arXiv:2408.13373v1 公告类型：新
摘要：小样本开放集识别 (FSOR) 是一项具有挑战性的任务，它要求模型能够识别已知类，并使用有限的标记数据识别未知类。现有方法，尤其是基于负原型的方法，仅基于已知类数据生成负原型。然而，由于未知空间是无限的，而已知空间是有限的，这些方法的表示能力有限。为了解决这一限制，我们提出了一种新方法，称为 \textbf{D}diversified \textbf{N}egative \textbf{P}rototypes \textbf{G}enerator (DNPG)，它采用“从未知中学习未知”的原理。我们的方法利用从基类中学习到的未知空间信息来为新类生成更具代表性的负原型。在预训练阶段，我们学习基类的未知空间表示。然后，在元学习过程中利用此表示以及类间关系来构建新类的负原型。为了防止原型崩溃并确保适应不同的数据组合，我们引入了交换对齐 (SA) 模块。我们的 DNPG 模型通过从未知空间中学习，生成覆盖更广泛未知空间的负原型，从而在三个标准 FSOR 数据集上实现最佳性能。]]></description>
      <guid>https://arxiv.org/abs/2408.13373</guid>
      <pubDate>Tue, 27 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>N-DriverMotion：使用基于事件的摄像头和直接训练的脉冲神经网络进行驾驶员运动学习和预测</title>
      <link>https://arxiv.org/abs/2408.13379</link>
      <description><![CDATA[arXiv:2408.13379v1 公告类型：新
摘要：驾驶员动作识别是确保驾驶系统安全的主要因素。本文介绍了一种用于学习和预测驾驶员动作的新型系统，以及一个基于事件的高分辨率（1280x720）数据集 N-DriverMotion，该数据集是新收集的，用于在神经形态视觉系统上进行训练。该系统包括一个基于事件的摄像头，可生成第一个代表脉冲输入的高分辨率驾驶员动作数据集，以及有效的脉冲神经网络 (SNN)，可有效训练和预测驾驶员的手势。事件数据集由 13 个驾驶员动作类别组成，按方向（正面、侧面）、照明（明亮、中等、黑暗）和参与者分类。我们提出的一种新型简化四层卷积脉冲神经网络 (CSNN) 直接使用高分辨率数据集进行训练，无需任何耗时的预处理。这使得能够有效地适应设备上的 SNN，以便在高分辨率基于事件的流上进行实时推理。与最近采用神经网络进行视觉处理的手势识别系统相比，所提出的神经形态视觉系统在使用 CSNN 架构识别驾驶员动作时实现了相当的准确率，即 94.04%。我们提出的 CSNN 和数据集可用于为自动驾驶汽车或需要高效神经网络架构的边缘设备开发更安全、更高效的驾驶员监控系统。]]></description>
      <guid>https://arxiv.org/abs/2408.13379</guid>
      <pubDate>Tue, 27 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MICM：重新思考无监督预训练以增强小样本学习</title>
      <link>https://arxiv.org/abs/2408.13385</link>
      <description><![CDATA[arXiv:2408.13385v1 公告类型：新 
摘要：人类表现出从有限数量的标记样本中快速学习的非凡能力，这种能力与当前的机器学习系统形成鲜明对比。无监督小样本学习 (U-FSL) 试图通过减少初始训练阶段对带注释数据集的依赖来弥合这一鸿沟。在这项工作中，我们首先定量评估了蒙版图像建模 (MIM) 和对比学习 (CL) 对小样本学习任务的影响。我们的研究结果强调了 MIM 和 CL 在判别能力和泛化能力方面的各自局限性，这导致它们在 U-FSL 环境中表现不佳。为了解决无监督预训练中泛化和判别能力之间的权衡，我们引入了一种名为蒙版图像对比建模 (MICM) 的新范式。 MICM 创造性地将 CL 的目标对象学习能力与 MIM 的广义视觉特征学习能力相结合，显著提高了其在下游小样本学习推理中的有效性。大量实验分析证实了 MICM 的优势，表明小样本学习的泛化和判别能力都有显著提高。我们全面的定量评估进一步证实了 MICM 的优越性，表明我们基于 MICM 的两阶段 U-FSL 框架明显优于现有的领先基线。]]></description>
      <guid>https://arxiv.org/abs/2408.13385</guid>
      <pubDate>Tue, 27 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>面向任务的高保真文本编辑的扩散反演</title>
      <link>https://arxiv.org/abs/2408.13395</link>
      <description><![CDATA[arXiv:2408.13395v1 公告类型：新
摘要：文本引导扩散模型的最新进展释放了强大的图像处理功能，但平衡真实图像的重建保真度和可编辑性仍然是一项重大挑战。在这项工作中，我们引入了 \textbf{T}ask-\textbf{O} 导向 \textbf{D}iffusion \textbf{I}nversion (\textbf{TODInv})，这是一个新颖的框架，它通过优化扩展 \(\mathcal{P}^*\) 空间内的提示嵌入来反转和编辑针对特定编辑任务量身定制的真实图像。通过利用不同 U-Net 层和时间步骤中的不同嵌入，TODInv 通过相互优化无缝集成反演和编辑，确保高保真度和精确的可编辑性。这种分层编辑机制将任务分为结构、外观和全局编辑，仅优化不受当前编辑任务影响的嵌入。在基准数据集上进行的大量实验表明，TODInv 的性能优于现有方法，在定量和定性方面均有增强，同时展示了其通过少步扩散模型的多功能性。]]></description>
      <guid>https://arxiv.org/abs/2408.13395</guid>
      <pubDate>Tue, 27 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>特征联盟的扰动：迈向可解释的深度神经网络</title>
      <link>https://arxiv.org/abs/2408.13397</link>
      <description><![CDATA[arXiv:2408.13397v1 公告类型：新
摘要：深度神经网络（DNN）固有的“黑箱”性质损害了其透明度和可靠性。最近，可解释人工智能（XAI）引起了研究人员越来越多的关注。出现了几种基于扰动的解释。然而，这些方法往往没有充分考虑特征依赖性。为了解决这个问题，我们引入了一种由特征联盟引导的基于扰动的解释，它利用网络的深度信息来提取相关特征。然后，我们提出了一个精心设计的一致性损失来指导网络解释。进行了定量和定性实验以验证我们提出的方法的有效性。代码可在 github.com/Teriri1999/Perturebation-on-Feature-Coalition 上找到。]]></description>
      <guid>https://arxiv.org/abs/2408.13397</guid>
      <pubDate>Tue, 27 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>TVG：一种无需训练、利用扩散模型的过渡视频生成方法</title>
      <link>https://arxiv.org/abs/2408.13413</link>
      <description><![CDATA[arXiv:2408.13413v1 公告类型：新
摘要：过渡视频在媒体制作中起着至关重要的作用，可增强视觉叙事的流动性和连贯性。变形等传统方法通常缺乏艺术吸引力，并且需要专业技能，从而限制了其有效性。基于扩散模型的视频生成的最新进展为创建过渡提供了新的可能性，但面临着诸如帧间关系建模不佳和内容突然变化等挑战。我们提出了一种新颖的无需训练的过渡视频生成 (TVG) 方法，该方法使用视频级扩散模型来解决这些限制而无需额外的训练。我们的方法利用高斯过程回归 ($\mathcal{GPR}$) 来建模潜在表示，确保帧之间的平滑和动态过渡。此外，我们引入了基于插值的条件控制和频率感知双向融合 (FBiF) 架构来增强时间控制和过渡可靠性。基准数据集和自定义图像对的评估证明了我们的方法在生成高质量平滑过渡视频方面的有效性。代码提供于https://sobeymil.github.io/tvg.com。]]></description>
      <guid>https://arxiv.org/abs/2408.13413</guid>
      <pubDate>Tue, 27 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>