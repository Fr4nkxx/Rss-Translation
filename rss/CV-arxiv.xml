<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Tue, 26 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>梯度加权特征反投影：3D 高斯溅射中特征蒸馏的快速替代方法</title>
      <link>https://arxiv.org/abs/2411.15193</link>
      <description><![CDATA[arXiv:2411.15193v1 公告类型：新
摘要：我们介绍了一种无需训练的高斯分布特征场渲染方法。我们的方法将 2D 特征反向投影到预先训练的 3D 高斯中，使用基于每个高斯对最终渲染的影响的加权和。虽然大多数基于训练的特征场渲染方法在 2D 分割方面表现出色，但在没有后处理的 3D 分割方面表现不佳，但我们的方法在 2D 和 3D 分割中均实现了高质量的结果。实验结果表明，我们的方法快速、可扩展，并且提供与基于训练的方法相当的性能。]]></description>
      <guid>https://arxiv.org/abs/2411.15193</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于高效条件图像生成的自适应可控扩散模型</title>
      <link>https://arxiv.org/abs/2411.15199</link>
      <description><![CDATA[arXiv:2411.15199v1 公告类型：新
摘要：随着人工智能的发展，生成模型越来越受到关注，它代表了创造力，这是智能的一个非常重要的方面。近年来，扩散模型得到了研究，并被证明比以前的方法更加合理和有效。然而，常见的扩散框架存在可控性问题。虽然一些工作考虑了额外的条件来指导特定目标生成的扩散过程，但它只控制生成结果，而不控制其过程。在本文中，我们提出了一种新的自适应框架，$\textit{自适应可控扩散（AC-Diff）模型}$，以自动和完全控制生成过程，不仅包括生成结果的类型，还包括生成过程的长度和参数。输入和条件都将首先输入到$\textit{条件时间步长（CTS）模块}$中，以确定生成所需的步数。然后根据过程的长度，通过我们的 $\textit{自适应混合噪声调度 (AHNS) 模块}$ 估计扩散速率参数。我们进一步使用相应的自适应采样机制训练网络，以学习如何根据条件调整自身，从而提高整体性能。为了实现实际应用，AC-Diff 有望大幅减少平均生成步骤数和执行时间，同时保持与文献扩散模型相同的性能。]]></description>
      <guid>https://arxiv.org/abs/2411.15199</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于深度学习的儿童多动症运动障碍分类</title>
      <link>https://arxiv.org/abs/2411.15200</link>
      <description><![CDATA[arXiv:2411.15200v1 公告类型：新
摘要：儿童多动性运动障碍 (HMD)，包括肌张力障碍 (异常扭曲) 和舞蹈病 (不规则、随机运动)，由于临床特征重叠，在诊断方面面临重大挑战。肌张力障碍的患病率为每百万 2 至 50 人，舞蹈病的患病率为每十万人 5 至 10 人。这些疾病的诊断通常会延迟平均 4.75 至 7.83 年。传统的诊断方法依赖于临床病史和专家体检，但由于这些疾病的病理生理学复杂，专门的测试是无效的。这项研究开发了一个神经网络模型，以从执行运动任务的儿科患者视频记录中区分肌张力障碍和舞蹈病。该模型集成了图卷积网络 (GCN) 来捕获空间关系和长短期记忆 (LSTM) 网络来解释时间动态。注意力机制被纳入其中，以提高模型的可解释性。该模型在盖伊和圣托马斯 NHS 基金会监管部门批准下收集的 50 个视频数据集（31 个舞蹈症为主，19 个肌张力障碍为主）上进行了训练和验证。该模型在每秒 15 帧的速度下实现了 85% 的准确率、81% 的灵敏度和 88% 的特异性。注意力图突出了该模型正确识别非自愿运动模式的能力，错误分类通常是由于身体部位被遮挡或细微的运动变化造成的。这项工作展示了深度学习在提高 HMD 诊断准确性和效率方面的潜力，并可能有助于开发更可靠、更可解释的临床工具。]]></description>
      <guid>https://arxiv.org/abs/2411.15200</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>超越视觉理解：推出用于视觉语言模型基准测试的 PARROT-360V</title>
      <link>https://arxiv.org/abs/2411.15201</link>
      <description><![CDATA[arXiv:2411.15201v1 公告类型：新
摘要：当前用于评估视觉语言模型 (VLM) 的基准通常无法全面评估模型理解和处理复杂视觉和文本内容的能力。它们通常专注于不需要深度推理或集成多种数据模态来解决原始问题的简单任务。为了解决这一差距，我们推出了 PARROT-360V 基准，这是一个新颖而全面的基准，具有 2487 个具有挑战性的视觉谜题，旨在测试 VLM 在复杂视觉推理任务上的表现。我们评估了领先的模型：GPT-4o、Claude-3.5-Sonnet 和 Gemini-1.5-Pro，使用 PARROT-360V 来评估它们将视觉线索与语言技能相结合以类似于人类解决问题的方式解决任务的能力。我们的研究结果揭示了显著的性能差距：最先进的模型在我们的基准测试中得分为 28% 到 56%，明显低于它们在流行基准测试中的表现。这凸显了当前 VLM 在处理复杂、多步骤推理任务方面的局限性，并强调需要更强大的评估框架来推动该领域的发展。]]></description>
      <guid>https://arxiv.org/abs/2411.15201</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DAGSM：利用 GS​​ 增强网格进行解缠头像生成</title>
      <link>https://arxiv.org/abs/2411.15205</link>
      <description><![CDATA[arXiv:2411.15205v1 公告类型：新
摘要：文本驱动的虚拟形象生成因其便利性而备受关注。然而，现有的方法通常将人体连同所有服装建模为单个 3D 模型，限制了其可用性（例如更换服装），并降低了用户对生成过程的控制。为了克服上述限制，我们提出了 DAGSM，这是一种新颖的流程，可根据给定的文本提示生成解开的人体和服装。具体来说，我们将穿衣人的每个部分（例如身体、上衣/下衣）建模为一个 GS 增强网格（GSM），这是一种附加了 2D 高斯的传统网格，可以更好地处理复杂的纹理（例如羊毛、半透明衣服）并制作逼真的布料动画。在生成过程中，我们首先创建未穿衣的身体，然后根据身体进行一系列单独的布料生成，其中我们引入了一种基于语义的算法来实现更好的人衣和服装分离。为了提高纹理质量，我们提出了一个视图一致的纹理细化模块，包括用于纹理样式一致性的跨视图注意机制和用于更新外观的入射角加权去噪 (IAW-DE) 策略。大量实验表明，DAGSM 可以生成高质量的解缠头像，支持服装更换和逼真的动画，并且在视觉质量方面优于基线。]]></description>
      <guid>https://arxiv.org/abs/2411.15205</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Uni-Mlip：用于医学视觉语言预训练的统一自监督</title>
      <link>https://arxiv.org/abs/2411.15207</link>
      <description><![CDATA[arXiv:2411.15207v1 公告类型：新
摘要：通过对比学习进行视觉语言预训练的最新进展显著提高了计算机视觉任务的性能。然而，在医学领域，由于隐私、敏感性和注释复杂性，获取多模态数据通常成本高昂且具有挑战性。为了缓解数据稀缺性并提高模型性能，我们引入了 \textbf{Uni-Mlip}，这是一个统一的自监督框架，专门用于增强医学视觉语言预训练。Uni-Mlip 在数据级和特征级无缝集成了跨模态、单模态和融合模态自监督技术。此外，Uni-Mlip 还定制了单模态图像自监督以适应医学图像的独特特征。我们对不同规模的数据集进行的实验表明，Uni-Mlip 在三个关键下游任务中显著超越了当前最先进的方法：图像文本检索、图像分类和视觉问答 (VQA)。]]></description>
      <guid>https://arxiv.org/abs/2411.15207</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用稳健限制 CDF 匹配进行图像协调</title>
      <link>https://arxiv.org/abs/2411.15213</link>
      <description><![CDATA[arXiv:2411.15213v1 公告类型：新
摘要：将机器学习算法部署到现实世界中仍然是一项艰巨的任务。挑战之一在于输入数据的不可预测的可变性，这可能在个人用户、机构、扫描仪等之间有很大差异。通过使用适当的数据预处理和强大的数据协调，可以降低输入数据的可变性。在本文中，我们提出了一种基于曲线拟合的累积分布函数 (CDF) 匹配的图像协调方法。这种方法不会破坏局部变异性和个体重要特征。与其他已知的直方图匹配算法相比，图像强度的变换是非线性的，但仍然“平滑且有弹性”。非线性变换可以很好地匹配模板。同时，弹性约束有助于保留各个输入之间的局部变异性，这可能会为后续的机器学习处理编码重要特征。与其他方法（尤其是基于 ML 的方法）相比，预定义模板 CDF 为输入数据转换提供了更好、更直观的控制。尽管我们针对 MRI 图像展示了我们的方法，但该方法足够通用，可以应用于其他类型的成像数据。]]></description>
      <guid>https://arxiv.org/abs/2411.15213</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>BiomedCoOp：学习提示生物医学视觉语言模型</title>
      <link>https://arxiv.org/abs/2411.15232</link>
      <description><![CDATA[arXiv:2411.15232v1 公告类型：新
摘要：视觉语言模型 (VLM)（例如 CLIP）的最新进展已证明在视觉任务的自监督表示学习中取得了巨大成功。然而，有效地将 VLM 适应下游应用仍然具有挑战性，因为它们的准确性通常取决于耗时且需要专业知识的即时工程，而完整的模型微调成本高昂。对于生物医学图像尤其如此，与自然图像不同，生物医学图像通常受到有限的注释数据集、不直观的图像对比度和细微的视觉特征的影响。最近的即时学习技术，例如上下文优化 (CoOp) 旨在解决这些问题，但在通用性方面仍然不足。同时，在生物医学图像分析的即时学习方面的探索仍然非常有限。在这项工作中，我们提出了 BiomedCoOp，这是一种新颖的即时学习框架，可以有效地适应 BiomedCLIP，以实现准确且高度可推广的少量生物医学图像分类。我们的方法利用大型语言模型 (LLM) 的平均提示集合的语义一致性以及基于统计的提示选择策略的知识提炼，实现了有效的提示上下文学习。我们对 9 种模式和 10 个器官的 11 个医疗数据集上提出的框架进行了全面验证，并与现有的最先进方法进行了对比，结果显示准确性和通用性都有显著提高。代码将在 https://github.com/HealthX-Lab/BiomedCoOp 上公开提供。]]></description>
      <guid>https://arxiv.org/abs/2411.15232</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>文本嵌入并非您所需要的全部：使用文本自注意力图进行文本到图像语义对齐的注意力控制</title>
      <link>https://arxiv.org/abs/2411.15236</link>
      <description><![CDATA[arXiv:2411.15236v1 公告类型：新
摘要：在文本到图像扩散模型中，每个文本标记的交叉注意图指示所关注的特定图像区域。比较这些句法相关标记的图可以深入了解生成的图像如何很好地反映文本提示。例如，在提示“一辆黑色汽车和一个白色时钟”中，“黑色”和“汽车”的交叉注意图应关注重叠区域以描绘黑色汽车，而“汽车”和“时钟”则不应如此。图中的不正确重叠通常会导致生成缺陷，例如缺少对象和不正确的属性绑定。我们的研究在现有的文本到图像模型中调查此问题时做出了关键观察：（1）不同标记之间的文本嵌入相似性（用作条件输入）可能导致它们的交叉注意图聚焦于相同的图像区域；（2）文本嵌入通常无法忠实地捕捉文本注意图中已有的句法关系。因此，这种句法关系可能会在交叉注意模块中被忽略，从而导致图像生成不准确。为了解决这个问题，我们提出了一种方法，通过测试时间优化将句法关系从文本注意图直接转移到交叉注意模块。我们的方法利用文本注意图中这种固有但未开发的信息来增强跨不同提示的图像-文本语义对齐，而无需依赖外部指导。]]></description>
      <guid>https://arxiv.org/abs/2411.15236</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>组织学图像中组织分类的染色不变表示</title>
      <link>https://arxiv.org/abs/2411.15237</link>
      <description><![CDATA[arXiv:2411.15237v1 公告类型：新
摘要：数字化组织学幻灯片的过程涉及多种因素，这些因素可能会影响整个幻灯片图像 (WSI) 的最终外观，包括染色方案、扫描仪和组织类型。这种可变性构成了域转移，并在多队列设置中训练和测试深度学习 (DL) 算法时导致重大问题。因此，在计算病理学 (CPath) 中开发稳健且可推广的 DL 模型仍然是一个悬而未决的挑战。在这方面，我们提出了一个框架，该框架使用染色矩阵扰动生成训练图像的染色增强版本。此后，我们采用了染色正则化损失来强制源图像和增强图像的特征表示之间的一致性。这样做鼓励模型学习染色不变性，从而学习域不变的特征表示。我们评估了所提出的模型在结直肠癌图像的跨领域多类组织类型分类方面的性能，与其他最先进的方法相比取得了更好的性能。]]></description>
      <guid>https://arxiv.org/abs/2411.15237</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>忠实的无标签知识提炼</title>
      <link>https://arxiv.org/abs/2411.15239</link>
      <description><![CDATA[arXiv:2411.15239v1 公告类型：新
摘要：知识蒸馏方法是一种模型压缩技术，目的是通过使用更大或包含不同归纳偏差的教师网络来训练高性能的学生模型。这些方法在应用于大型计算机视觉基础模型时特别有用，这些模型可以压缩成较小的变体，同时保留所需的特性，例如改进的鲁棒性。本文提出了一种称为中间教师 (TinTeM) 的无标签知识蒸馏方法，该方法通过学习从教师的潜在空间到学生网络的近似正交映射来改进以前的方法。这会产生一个更忠实的学生，它可以更好地复制教师网络在一系列基准测试模型鲁棒性、通用性和分布外检测中的行为。进一步表明，在特定于任务的数据集上使用 TinTeM 进行知识蒸馏可以产生更准确的模型，具有更高的通用性和 OOD 检测性能，并且该技术为在小数据集上训练高性能轻量级模型提供了一条有竞争力的途径。]]></description>
      <guid>https://arxiv.org/abs/2411.15239</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>EfficientViM：基于隐藏状态混合器的状态空间对偶的 Efficient Vision Mamba</title>
      <link>https://arxiv.org/abs/2411.15241</link>
      <description><![CDATA[arXiv:2411.15241v1 公告类型：新
摘要：对于在资源受限的环境中部署神经网络，先前的研究已经构建了轻量级架构，分别使用卷积和注意力来捕获局部和全局依赖关系。最近，状态空间模型已成为一种有效的全局令牌交互，其计算成本在令牌数量上具有良好的线性关系。然而，使用 SSM 构建的高效视觉主干较少被探索。在本文中，我们介绍了 Efficient Vision Mamba (EfficientViM)，这是一种基于隐藏状态混合器的状态空间对偶 (HSM-SSD) 构建的新型架构，可有效捕获全局依赖关系并进一步降低计算成本。在 HSM-SSD 层中，我们重新设计了之前的 SSD 层以在隐藏状态下启用通道混合操作。此外，我们提出了多阶段隐藏状态融合，以进一步增强隐藏状态的表示能力，并提供缓解内存限制操作造成的瓶颈的设计。因此，EfficientViM 系列在 ImageNet-1k 上实现了速度与准确度之间的最佳平衡，与速度更快的第二佳模型 SHViT 相比，性能提升高达 0.7%。此外，与之前的研究相比，我们在缩放图像或采用蒸馏训练时观察到吞吐量和准确度的显著提高。代码可在 https://github.com/mlvlab/EfficientViM 上找到。]]></description>
      <guid>https://arxiv.org/abs/2411.15241</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>视觉语言模型的对抗性提示提炼</title>
      <link>https://arxiv.org/abs/2411.15244</link>
      <description><![CDATA[arXiv:2411.15244v1 公告类型：新 
摘要：大型预训练视觉语言模型 (VLM)（例如对比语言图像预训练 (CLIP)）已被证明容易受到对抗性攻击，这引发了人们对其在自动驾驶和医疗诊断等安全关键场景中的部署的担忧。一种有希望提高预训练 VLM 稳健性的方法是对抗性提示调整 (APT)，它将对抗性训练与提示调整相结合。然而，现有的 APT 方法大多是单模态方法，仅为视觉或文本模态设计提示，限制了它们在稳健性或干净准确性方面的有效性。在这项工作中，我们提出了一种称为对抗性提示蒸馏 (APD) 的新方法，它将 APT 与知识蒸馏相结合以增强 CLIP 的对抗性稳健性。具体来说，APD 是一种双峰方法，它为视觉和文本模态添加了提示，同时利用干净的预训练教师 CLIP 模型来提炼和提升学生 CLIP 模型在下游任务上的性能。在多个基准数据集上进行的大量实验证明了我们的 APD 在自然和对抗性能方面优于当前最先进的 APT 方法。我们的 APD 方法的有效性验证了使用非稳健教师来提高 VLM 的泛化和稳健性的可能性。]]></description>
      <guid>https://arxiv.org/abs/2411.15244</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AnyText2：具有可自定义属性的可视化文本生成和编辑</title>
      <link>https://arxiv.org/abs/2411.15245</link>
      <description><![CDATA[arXiv:2411.15245v1 公告类型：新
摘要：随着文本到图像 (T2I) 领域的发展，生成与视觉内容无缝集成的文本已引起广泛关注。然而，即使生成准确的文本，无法控制字体和颜色也会极大地限制某些应用程序，而且这个问题仍未得到充分解决。本文介绍了一种新方法 AnyText2，它可以在自然场景图像生成和编辑中精确控制多语言文本属性。我们的方法由两个主要部分组成。首先，我们提出了一种 WriteNet+AttnX 架构，将文​​本渲染功能注入预先训练的 T2I 模型中。与其前身 AnyText 相比，我们的新方法不仅增强了图像的真实感，而且推理速度提高了 19.8%。其次，我们探索从场景图像中提取字体和颜色的技术，并开发一个文本嵌入模块，将这些文本属性分别编码为条件。作为 AnyText 的扩展，该方法允许为每行文本自定义属性，从而分别将中文和英文的文本准确率提高了 3.3% 和 9.3%。通过全面的实验，我们展示了该方法的领先性能。代码和模型将在 https://github.com/tyxsspa/AnyText2 开源。]]></description>
      <guid>https://arxiv.org/abs/2411.15245</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LocRef-Diffusion：无需调整的布局和外观引导的生成</title>
      <link>https://arxiv.org/abs/2411.15252</link>
      <description><![CDATA[arXiv:2411.15252v1 公告类型：新
摘要：最近，基于扩散的文本到图像模型在生成高质量图像方面取得了显著成功。然而，在这些图像中个性化、可控地生成实例的挑战仍然是一个需要进一步发展的领域。在本文中，我们提出了 LocRef-Diffusion，这是一种新颖的、无需调整的模型，能够个性化定制图像中多个实例的外观和位置。为了提高实例放置的精度，我们引入了一个布局网络，它通过利用显式实例布局信息和实例区域交叉注意模块来控制实例生成位置。为了提高对参考图像的外观保真度，我们采用了一个外观网络，它提取实例外观特征并通过交叉注意机制将它们集成到扩散模型中。我们在 COCO 和 OpenImages 数据集上进行了广泛的实验，结果表明，我们提出的方法在布局和外观引导生成方面取得了最先进的性能。]]></description>
      <guid>https://arxiv.org/abs/2411.15252</guid>
      <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>