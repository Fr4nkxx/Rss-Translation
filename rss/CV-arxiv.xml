<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Mon, 09 Dec 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>MageBench：将大型多模式模型与代理连接起来</title>
      <link>https://arxiv.org/abs/2412.04531</link>
      <description><![CDATA[arXiv:2412.04531v1 公告类型：新
摘要：LMM 表现出了令人印象深刻的视觉理解能力，并有可能应用于需要强大推理和规划能力的代理。然而，现有的基准测试大多在语言部分评估它们的推理能力，其中思维链完全由文本组成。我们考虑在决策过程中不断更新和需要视觉信号的场景。这种视觉链推理范式更符合多模态代理的需求，但很少被评估。在本文中，我们介绍了 MageBench，这是一个面向推理能力的多模态代理基准测试，虽然具有轻量级环境，但带来了重大的推理挑战并具有巨大的实用价值。该基准测试目前包括三种类型的环境：WebUI、Sokoban 和 Football，共包含 483 种不同的场景。它彻底验证了代理的知识和工程能力、视觉智能和交互技能。结果表明，只有少数产品级模型优于随机行动，所有模型都远不如人类水平。更具体地说，我们发现当前的模型严重缺乏根据视觉反馈修改规划的能力，以及视觉想象、交错图像文本长上下文处理等能力。我们希望我们的工作能从代理的角度为 LMM 提供优化方向。我们在 https://github.com/microsoft/MageBench 上发布了我们的代码和数据。]]></description>
      <guid>https://arxiv.org/abs/2412.04531</guid>
      <pubDate>Mon, 09 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>面具适配器：开放词汇分割的面具里藏着魔鬼</title>
      <link>https://arxiv.org/abs/2412.04533</link>
      <description><![CDATA[arXiv:2412.04533v1 公告类型：新
摘要：最近的开放词汇分割方法采用掩码生成器来预测分割掩码，并利用预先训练的视觉语言模型（例如 CLIP）通过掩码池对这些掩码进行分类。尽管这些方法显示出有希望的结果，但违反直觉的是，通过在掩码区域内池化 CLIP 图像嵌入，准确的掩码通常无法产生准确的分类结果。在本文中，我们揭示了掩码池的性能限制，并介绍了 Mask-Adapter，这是一种简单而有效的方法来解决开放词汇分割中的这些挑战。与直接使用提议掩码相比，我们提出的 Mask-Adapter 从提议掩码中提取语义激活图，提供更丰富的上下文信息并确保掩码和 CLIP 之间的对齐。此外，我们提出了一种掩码一致性损失，鼓励具有相似 IoU 的提议掩码获得相似的 CLIP 嵌入，以增强模型对不同预测掩码的鲁棒性。 Mask-Adapter 以即插即用的方式无缝集成到基于掩码池的开放词汇分割方法中，提供更准确的分类结果。在多个零样本基准测试中开展的大量实验表明，所提出的 Mask-Adapter 在几种成熟的方法上具有显著的性能提升。值得注意的是，Mask-Adapter 还可以有效扩展到 SAM，并在多个开放词汇分割数据集上取得了令人印象深刻的结果。代码和模型可在 \url{https://github.com/hustvl/MaskAdapter} 上找到。]]></description>
      <guid>https://arxiv.org/abs/2412.04533</guid>
      <pubDate>Mon, 09 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>由人类指令引导的基于动作的图像编辑</title>
      <link>https://arxiv.org/abs/2412.04558</link>
      <description><![CDATA[arXiv:2412.04558v1 公告类型：新
摘要：基于文本的图像编辑通常被视为静态任务，涉及根据人工指令插入、删除或修改输入图像元素等操作。鉴于此任务的静态性质，在本文中，我们旨在通过合并操作使此任务动态化。通过这样做，我们打算修改图像中对象的位置或姿势以描绘不同的动作，同时保持对象的视觉属性。为了完成这项具有挑战性的任务，我们提出了一个新模型，该模型通过学习识别对比动作差异来对动作文本指令敏感。模型训练是在新数据集上进行的，该数据集是通过从显示动作前后视觉场景的视频中提取帧来定义的。我们展示了使用基于动作的文本指令和高推理能力在图像编辑方面的显着改进，这使我们的模型能够使用输入图像作为动作的起始场景，同时生成显示动作最终场景的新图像。]]></description>
      <guid>https://arxiv.org/abs/2412.04558</guid>
      <pubDate>Mon, 09 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ARTeFACT：对不同模拟媒体损坏的分割模型进行基准测试</title>
      <link>https://arxiv.org/abs/2412.04580</link>
      <description><![CDATA[arXiv:2412.04580v1 公告类型：新
摘要：准确检测和分类绘画、照片、纺织品、马赛克和壁画等模拟媒体中的损坏对于文化遗产保护至关重要。虽然机器学习模型在损伤算子已知的情况下在纠正退化方面表现出色，但我们表明，即使在监督训练之后，它们也无法稳健地预测损伤的位置；因此，可靠的损伤检测仍然是一个挑战。受此启发，我们引入了 ARTeFACT，这是一个用于检测各种模拟媒体中损伤的数据集，其中包含超过 11,000 条注释，涵盖了不同主题、媒体和历史出处的 15 种损伤。此外，我们还提供了经过人工验证的文本提示来描述图像的语义内容，并得出了注释损伤的额外文本描述。我们在零样本、监督、无监督和文本引导设置中评估了 CNN、Transformer、基于扩散的分割模型和基础视觉模型，揭示了它们在跨媒体类型推广方面的局限性。我们的数据集可在 $\href{https://daniela997.github.io/ARTeFACT/}$ 上找到，这是模拟媒体损坏检测和修复的首个同类基准。]]></description>
      <guid>https://arxiv.org/abs/2412.04580</guid>
      <pubDate>Mon, 09 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>EgoPoints：为以自我为中心的视频提供先进的点跟踪技术</title>
      <link>https://arxiv.org/abs/2412.04592</link>
      <description><![CDATA[arXiv:2412.04592v1 公告类型：新
摘要：我们引入了 EgoPoints，这是以自我为中心的视频中的点跟踪基准。我们在以自我为中心的序列中注释了 4.7K 条具有挑战性的轨迹。与流行的 TAP-Vid-DAVIS 评估基准相比，我们包含了 9 倍以上的视野外点和 59 倍以上的返回视图后需要重新识别 (ReID) 的点。为了衡量模型在这些具有挑战性的点上的性能，我们引入了评估指标，专门监控视野内点、视野外点和需要重新识别的点的跟踪性能。然后，我们提出了一个管道来创建具有自动地面实况的半真实序列。我们通过将动态 Kubric 对象与 EPIC 场中的场景点相结合来生成 11K 个这样的序列。在这些序列上微调点跟踪方法并评估我们注释的 EgoPoints 序列时，我们在所有指标上都提高了 CoTracker，包括跟踪准确度 $\delta^\star_{\text{avg}}$ 提高了 2.7 个百分点，ReID 序列准确度 (ReID$\delta_{\text{avg}}$) 提高了 2.4 个百分点。我们还分别将 PIPs++ 的 $\delta^\star_{\text{avg}}$ 和 ReID$\delta_{\text{avg}}$ 提高了 0.3 和 2.8。]]></description>
      <guid>https://arxiv.org/abs/2412.04592</guid>
      <pubDate>Mon, 09 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>评估和学习单峰视觉和语言模型的一致性</title>
      <link>https://arxiv.org/abs/2412.04616</link>
      <description><![CDATA[arXiv:2412.04616v1 公告类型：新
摘要：单峰视觉和语言模型的对齐程度如何？尽管先前的研究已经开始回答这个问题，但它们的评估方法并不能直接转化为这些模型在实际视觉语言任务中的使用方式。在本文中，我们提出了一种受线性探测启发的直接评估方法来评估视觉语言对齐。我们发现 SSL 视觉模型的对齐程度取决于它们的 SSL 训练目标，并且我们发现 SSL 表示的聚类质量对对齐性能的影响比它们的线性可分性更大。接下来，我们介绍了图像和语言的快速对齐 (SAIL)，这是一种高效的迁移学习框架，可将预训练的单峰视觉和语言模型对齐以用于下游视觉语言任务。由于 SAIL 利用了预训练的单峰模型的优势，因此与从头开始训练的 CLIP 等模型相比，它需要的用于多峰对齐的配对图像文本数据要少得多（6%）。 SAIL 训练仅需要单个 A100 GPU、5 小时的训练时间，并且可以容纳高达 32,768 的批处理大小。SAIL 在 ImageNet 上的零样本准确率达到 73.4%（而 CLIP 为 72.7%），并且在零样本检索、复杂推理和语义分割方面表现出色。此外，SAIL 还提高了视觉编码器的语言兼容性，从而提高了多模态大型语言模型的性能。整个代码库和模型权重都是开源的：https://lezhang7.github.io/sail.github.io/]]></description>
      <guid>https://arxiv.org/abs/2412.04616</guid>
      <pubDate>Mon, 09 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用扩散先验进行视频无模式分割</title>
      <link>https://arxiv.org/abs/2412.04623</link>
      <description><![CDATA[arXiv:2412.04623v1 公告类型：新
摘要：人类的物体永久性是理解物体持久性的基本线索，即使它们在场景中完全被遮挡。当今的物体分割方法没有考虑到世界的这种非模态性质，只适用于可见或模态物体的分割。存在的非模态方法很少；单图像分割方法无法处理高水平的遮挡，而使用时间信息可以更好地推断遮挡，而多帧方法仅专注于分割刚性物体。为此，我们建议通过将视频非模态分割制定为条件生成任务来解决它，利用视频生成模型中的基础知识。我们的方法很简单；我们重新利用这些模型来对物体的模态掩码帧序列以及上下文伪深度图进行条件处理，以了解哪些物体边界可能被遮挡，从而扩展到幻觉物体的完整范围。接下来是内容完成阶段，该阶段能够修复物体的遮挡区域。我们在四个数据集上对我们的方法以及各种最先进的方法进行了基准测试，结果显示，在物体遮挡区域中的非模态分割方面，其显著提高了 13%。]]></description>
      <guid>https://arxiv.org/abs/2412.04623</guid>
      <pubDate>Mon, 09 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>交叉自 KV 缓存修剪，实现高效的视觉语言推理</title>
      <link>https://arxiv.org/abs/2412.04652</link>
      <description><![CDATA[arXiv:2412.04652v1 公告类型：新
摘要：KV 缓存修剪已成为一种有前途的技术，可用于减少长上下文自回归生成中的内存和计算成本。现有的视觉语言模型 (VLM) 方法通常依赖于大型语言模型 (LLM) 的自注意力分数来识别和修剪不相关的标记。然而，这些方法忽略了模态之间固有的分布差异，通常导致不准确的标记重要性估计和关键视觉标记的过度修剪。为了解决这个问题，我们建议将注意力分数分解为模态内注意力（在同一模态内）和模态间注意力（跨模态），通过独立管理这些不同的注意力类型实现更精确的 KV 缓存修剪。此外，我们引入了一个 n-softmax 函数来抵消修剪引起的分布变化，保持注意力分数的原始平滑度并确保稳定的性能。我们最终的免训练方法 \textbf{C}ross-\textbf{S}elf \textbf{P}runing (CSP) 与具有完整 KV 缓存的模型相比实现了具有竞争力的性能，同时显著优于以前的修剪方法。在涵盖 29 个多模态数据集的基准 MileBench 上进行的广泛评估证明了 CSP 的有效性，在对话式具体对话等具有挑战性的任务上实现了高达 41\% 的性能提升，同时将 KV 缓存预算减少了 13.6\%。代码可在 https://github.com/TerryPei/CSP 获得]]></description>
      <guid>https://arxiv.org/abs/2412.04652</guid>
      <pubDate>Mon, 09 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>隐藏在噪声中：两阶段鲁棒图像水印</title>
      <link>https://arxiv.org/abs/2412.04653</link>
      <description><![CDATA[arXiv:2412.04653v1 公告类型：新
摘要：随着图像生成器质量的不断提高，深度伪造成为社会争论的话题。图像水印允许负责任的模型所有者检测和标记他们的人工智能生成的内容，从而可以减轻危害。然而，目前最先进的图像水印方法仍然容易受到伪造和删除攻击。这种脆弱性的部分原因是水印扭曲了生成图像的分布，无意中泄露了有关水印技术的信息。
在这项工作中，我们首先演示了一种基于扩散模型初始噪声的无失真图像水印方法。然而，检测水印需要将为图像重建的初始噪声与所有以前使用的初始噪声进行比较。为了缓解这些问题，我们提出了一个两阶段水印框架来有效检测。在生成过程中，我们用生成的傅里叶模式增强初始噪声，以嵌入有关我们使用的初始噪声组的信息。为了进行检测，我们 (i) 检索相关的噪声组，(ii) 在给定组中搜索可能与我们的图像匹配的初始噪声。这种水印方法实现了最先进的防伪和防删除能力，可抵御大量攻击。]]></description>
      <guid>https://arxiv.org/abs/2412.04653</guid>
      <pubDate>Mon, 09 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>将光学和 SAR 卫星图像、地面运动和土壤数据与变压器相结合进行多类地震后建筑物评估</title>
      <link>https://arxiv.org/abs/2412.04664</link>
      <description><![CDATA[arXiv:2412.04664v1 公告类型：新 
摘要：及时准确地评估建筑物损坏对于地震后的有效响应和恢复至关重要。传统的初步损坏评估 (PDA) 通常依赖于手动上门检查，这不仅耗时，而且还带来重大的安全风险。为了安全地加快 PDA 流程，研究人员研究了使用启发式和机器学习方法处理的卫星图像的适用性。这些方法以街区或单个建筑物的规模输出二进制或最近的多类损坏状态。然而，这种方法的当前性能限制了实际适用性。为了解决这个限制，我们引入了一个元数据丰富的、基于转换器的框架，该框架将高分辨率震后卫星图像与与结构抗震性能相关的建筑物特定元数据相结合。我们的模型在 2023 年 2 月 6 日土耳其-叙利亚地震中建筑物的多类震后损坏识别方面取得了最先进的性能。具体而言，我们证明，结合元数据（例如地震强度指标、土壤特性和 SAR 损坏代理图）不仅可以提高模型的准确性和区分损坏类别的能力，还可以提高其在不同地区的通用性。此外，我们对特征重要性进行了详细的类别分析，以了解模型在不同级别的建筑物损坏中的决策。该分析揭示了各个元数据特征如何独特地有助于预测每个损坏类别。通过利用卫星图像和元数据，我们提出的框架可以更快、更准确地进行损坏评估，从而实现精确的多类别建筑物级评估，从而改善灾难响应并加快受影响社区的恢复工作。]]></description>
      <guid>https://arxiv.org/abs/2412.04664</guid>
      <pubDate>Mon, 09 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ProPLIKS：概率三维人体姿态估计</title>
      <link>https://arxiv.org/abs/2412.04665</link>
      <description><![CDATA[arXiv:2412.04665v1 公告类型：新
摘要：我们提出了一种通过采用概率建模进行 3D 人体姿势估计的新方法。该方法利用非欧几里得几何中规范化流的优势来解决不确定的姿势。具体而言，我们的方法采用针对 SO(3) 旋转群定制的正则化流，并结合基于 M\&quot;obius 变换的耦合机制。这使框架能够准确地表示 SO(3) 上的任何分布，从而有效地解决与不连续性相关的问题。此外，我们将从 2D 像素对齐输入重建 3D 人体图形的挑战重新解释为将这些输入映射到一系列可能的姿势的任务。这种观点承认了任务的内在模糊性，并促进了多视图场景的直接集成方法。这些策略的组合展示了概率模型在复杂场景中对人体姿势估计技术的有效性。我们的方法明显超越了姿势估计领域的现有方法。我们还从 RGB 图像以及医学 X 射线数据集验证了我们对人体姿势估计的方法。]]></description>
      <guid>https://arxiv.org/abs/2412.04665</guid>
      <pubDate>Mon, 09 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LAA-Net：基于物理先验知识的稳健夜间深度估计网络</title>
      <link>https://arxiv.org/abs/2412.04666</link>
      <description><![CDATA[arXiv:2412.04666v1 公告类型：新 
摘要：现有的自监督单目深度估计 (MDE) 模型试图通过使用 GAN 将夜间图像转换为白天版本来提高夜间性能。然而，由于现实世界白天照明变化的复杂性，这可能会导致不一致性，最终导致不准确的估计结果。为了解决这个问题，我们利用关于夜间光波长和光衰减的物理先验知识。具体来说，我们的模型光衰减感知网络 (LAA-Net) 结合了瑞利散射理论的物理见解，以实现稳健的夜间深度估计：LAA-Net 是基于红色通道值进行训练的，因为红光由于其较长的波长在夜间场景下保留了更多信息。此外，基于比尔-朗伯定律，我们引入了红色通道衰减 (RCA) 损失来指导 LAA-Net 的训练。在 RobotCar-Night、nuScenes-Night、RobotCar-Day 和 KITTI 数据集上进行的实验表明，我们的模型优于 SOTA 模型。]]></description>
      <guid>https://arxiv.org/abs/2412.04666</guid>
      <pubDate>Mon, 09 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>扩散增强核心集扩展，实现可扩展数据集提炼</title>
      <link>https://arxiv.org/abs/2412.04668</link>
      <description><![CDATA[arXiv:2412.04668v1 公告类型：新
摘要：随着神经网络的快速扩展，数据存储和通信需求也日益增加。数据集精炼已成为一种有前途的解决方案，通过解决双层优化问题，将大量数据集中的信息浓缩为一组紧凑的合成样本。然而，当前的方法面临着计算效率方面的挑战，特别是在高分辨率数据和复杂架构方面。最近，基于知识精炼的数据集浓缩方法使这一过程在计算上更加可行。然而，随着生成基础模型的最新发展，现在有机会实现更大的压缩，提高精炼数据的质量，并在数据表示中引入有价值的多样性。在这项工作中，我们提出了一个两阶段解决方案。首先，我们通过仅选择最具信息量的补丁来压缩数据集以形成核心集。接下来，我们利用生成基础模型实时动态扩展这个压缩集，提高这些补丁的分辨率并为核心集引入受控的可变性。我们进行了大量的实验，证明了我们的方法在一系列数据集提炼基准上的稳健性和效率。与最先进的方法相比，我们在几个大规模数据集提炼基准上取得了超过 10% 的显著改进。代码将很快发布。]]></description>
      <guid>https://arxiv.org/abs/2412.04668</guid>
      <pubDate>Mon, 09 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于社会信息的行人轨迹预测重建</title>
      <link>https://arxiv.org/abs/2412.04673</link>
      <description><![CDATA[arXiv:2412.04673v1 公告类型：新
摘要：行人轨迹预测仍然是自主系统面临的挑战，特别是由于社交互动的复杂动态。准确的预测不仅需要全面了解每个行人的先前轨迹，还需要全面了解他们与周围环境的互动，其中重要部分是场景中其他行人的动态移动。为了学习有效的社交信息表示，我们提出了一个模型，该模型使用重构器和基于条件变分自动编码器的轨迹预测模块。该模块生成伪轨迹，我们在整个训练过程中将其用作增强。为了进一步引导模型实现社交意识，我们提出了一种新颖的社交损失，有助于预测更稳定的轨迹。我们通过大量实验验证了我们的方法，与 ETH/UCY 和 SDD 基准上最先进的方法相比，我们的方法表现出色。]]></description>
      <guid>https://arxiv.org/abs/2412.04673</guid>
      <pubDate>Mon, 09 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过扩散、行走和切割进行无监督分割</title>
      <link>https://arxiv.org/abs/2412.04678</link>
      <description><![CDATA[arXiv:2412.04678v1 公告类型：新
摘要：我们提出了一种无监督图像分割方法，该方法使用来自预训练的文本到图像扩散模型的特征。受经典谱聚类方法的启发，我们从图像块之间的自注意层构建邻接矩阵，并使用正则化切割进行递归分区。一个关键的见解是，自注意概率分布（它捕获了块之间的语义关系）可以解释为图像中随机游走的转换矩阵。我们利用这一点，首先直接在这些自注意激活上使用随机游走正则化切割来分割图像，最小化簇之间的转换概率，同时最大化簇内的一致性。递归应用，这会产生一个分层分割，反映预训练注意层中的丰富语义，而无需任何额外的训练。接下来，我们将探索从特征构建 NCuts 邻接矩阵的其他方法，以及如何使用随机游走解释自注意力来捕捉长距离关系。最后，我们提出了一种自动确定 NCut 成本标准的方法，避免了手动调整的需要。我们定量分析了在构建 NCuts 邻接矩阵时结合不同特征、恒定与动态 NCut 阈值以及结合多节点路径的效果。我们表明，我们的方法超越了所有现有的零样本无监督分割方法，在 COCO-Stuff-27 和 Cityscapes 上取得了最先进的结果。]]></description>
      <guid>https://arxiv.org/abs/2412.04678</guid>
      <pubDate>Mon, 09 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>