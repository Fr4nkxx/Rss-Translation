<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Wed, 18 Dec 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>公平图像分类的多模态方法：伦理视角</title>
      <link>https://arxiv.org/abs/2412.12165</link>
      <description><![CDATA[arXiv:2412.12165v1 公告类型：新
摘要：在快速发展的人工智能领域，机器感知对于提高性能至关重要。图像分类系统越来越成为各种应用不可或缺的一部分，从医疗诊断到图像生成；然而，这些系统往往表现出有害的偏见，可能导致不公平和歧视性的结果。如果数据没有经过仔细的平衡和过滤，依赖于单一数据模态（即只有图像或只有文本）的机器学习系统可能会夸大训练数据中存在的隐藏偏见。即便如此，这些模型在不适当的环境中使用时仍然会伤害代表性不足的人群，例如当政府机构使用预测性警务强化种族偏见时。本论文探讨了公平图像分类模型开发中技术与道德的交集。具体来说，我专注于提高公平性和使用多种模态来对抗有害人口偏见的方法。集成多模态方法（将视觉数据与文本和元数据等其他模态相结合）可提高图像分类系统的公平性和准确性。该研究批判性地审查了图像数据集和分类算法中现有的偏见，提出了减轻这些偏见的创新方法，并评估了在现实场景中部署此类系统的伦理影响。通过全面的实验和分析，该论文展示了多模态技术如何有助于实现更公平、更合乎道德的人工智能解决方案，最终倡导以公平为优先的负责任的人工智能实践。]]></description>
      <guid>https://arxiv.org/abs/2412.12165</guid>
      <pubDate>Wed, 18 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于指纹的室内定位中的多代理教师辅助表征对齐</title>
      <link>https://arxiv.org/abs/2412.12189</link>
      <description><![CDATA[arXiv:2412.12189v1 公告类型：新
摘要：尽管在视觉和文本领域的知识转移方面取得了显著进展，但将这些成就扩展到室内定位，特别是在学习接收信号强度 (RSS) 指纹数据集之间的可转移表示方面，仍然是一个挑战。这是由于这些 RSS 数据集之间存在固有差异，主要包括建筑结构、WiFi 锚点的输入数量和配置的变化。因此，专门的网络缺乏辨别可转移表示的能力，很容易将环境敏感的线索纳入学习过程，从而限制了它们在应用于特定 RSS 数据集时的潜力。在这项工作中，我们提出了一个即插即用 (PnP) 知识转移框架，通过两个主要阶段促进直接在目标 RSS 数据集上利用专门网络的可转移表示。首先，我们设计了一个专家训练阶段，该阶段具有多个代理生成教师，它们都充当全局适配器，使独立源 RSS 数据集之间的输入差异同质化，同时保留其独特特征。在随后的专家提炼阶段，我们继续引入三重底层约束，这需要通过改进其在目标数据集上的表征学习来最小化专业网络和代理教师之间的基本知识差异。此过程隐式地促进了表征对齐，使其对特定环境动态的敏感度降低。在三个基准 WiFi RSS 指纹数据集上进行的大量实验强调了该框架的有效性，该框架充分发挥了专业网络在定位方面的潜力。]]></description>
      <guid>https://arxiv.org/abs/2412.12189</guid>
      <pubDate>Wed, 18 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 YOLOv11 和 Ensemble OCR 进行收费车辆检测和分类</title>
      <link>https://arxiv.org/abs/2412.12191</link>
      <description><![CDATA[arXiv:2412.12191v1 公告类型：新
摘要：传统的自动收费系统依赖于复杂的硬件配置，需要在安装和维护方面投入大量资金。本研究论文提出了一种创新方法，通过每个广场使用一台摄像头，结合 YOLOv11 计算机视觉架构和集成 OCR 技术，彻底改变自动收费。我们的系统在各种条件下实现了 0.895 的平均精度 (mAP)，车牌识别准确率为 98.5%，车轴检测准确率为 94.2%，OCR 置信度得分为 99.7%。该架构结合了跨 IOU 区域的智能车辆跟踪、通过空间车轮检测模式自动计轴以及通过扩展仪表板界面进行实时监控。在各种环境条件下使用 2,500 张图像进行大量训练，我们的解决方案与传统系统相比，在大幅减少硬件资源的同时，性能也有所提高。这项研究通过引入可扩展、以精确为中心的解决方案为智能交通系统做出了贡献，该解决方案可提高现代收费系统的运营效率和用户体验。]]></description>
      <guid>https://arxiv.org/abs/2412.12191</guid>
      <pubDate>Wed, 18 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>人工智能驱动的立体视频流创新：回顾</title>
      <link>https://arxiv.org/abs/2412.12208</link>
      <description><![CDATA[arXiv:2412.12208v1 公告类型：新
摘要：最近为增强沉浸式和交互式用户体验所做的努力推动了体积视频的发展，体积视频是一种支持 6 DoF 的 3D 内容形式。与传统的 2D 内容不同，体积内容可以以多种方式表示，例如点云、网格或神经表示。然而，由于其结构复杂且数据量大，部署这种新形式的 3D 数据在传输和渲染方面面临重大挑战。这些挑战阻碍了体积视频在日常应用中的广泛采用。近年来，研究人员提出了各种人工智能驱动的技术来应对这些挑战，并提高体积内容流的效率和质量。本文全面概述了人工智能驱动方法在促进体积内容流方面的最新进展。通过这篇评论，我们旨在深入了解当前的最新技术，并为推进体积视频流在实际应用中的部署提出潜在的未来方向。]]></description>
      <guid>https://arxiv.org/abs/2412.12208</guid>
      <pubDate>Wed, 18 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SitPose：使用深度传感器集成学习实时检测坐姿和久坐行为</title>
      <link>https://arxiv.org/abs/2412.12216</link>
      <description><![CDATA[arXiv:2412.12216v1 公告类型：新
摘要：不良的坐姿会导致各种与工作相关的肌肉骨骼疾病（WMSD）。办公室职员大约有81.8%的工作时间是坐着的，久坐行为会导致颈椎病和心血管疾病等慢性疾病。为了解决这些健康问题，我们利用最新的Kinect深度摄像头提出了一种坐姿和久坐检测系统SitPose。该系统实时跟踪骨关节点的3D坐标并计算相关关节的角度值。我们通过招募36名参与者建立了一个包含六种不同坐姿和一种站姿的数据集，共计33,409个数据点。我们将几种最先进的机器学习算法应用于数据集，并比较了它们在坐姿识别方面的表现。结果表明，基于软投票机制的集成学习模型获得了最高的F1分数98.1％。最后，我们基于该集成模型部署了 SitPose 系统，以鼓励更好的坐姿并减少久坐习惯。]]></description>
      <guid>https://arxiv.org/abs/2412.12216</guid>
      <pubDate>Wed, 18 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过邻居推断消除无监督可见红外人员重新识别的通用标签噪声</title>
      <link>https://arxiv.org/abs/2412.12220</link>
      <description><![CDATA[arXiv:2412.12220v1 公告类型：新
摘要：无监督可见红外行人重新识别（USL-VI-ReID）具有重要的研究和实际意义，但由于缺乏注释，仍然具有挑战性。现有方法旨在在无监督环境中学习模态不变的表示。然而，由于聚类结果不理想和模态差异较大，这些方法经常在模态内和模态间遇到标签噪声，从而阻碍有效的训练。为了应对这些挑战，我们提出了一种简单而有效的 USL-VI-ReID 解决方案，即使用邻居信息来减轻通用标签噪声。具体来说，我们引入了邻居引导的通用标签校准 (N-ULC) 模块，它用来自邻近样本的软标签替换同质和异质空间中的显式硬伪标签，以减少标签噪声。此外，我们提出了邻域引导动态加权 (N-DW) 模块，通过最小化不可靠样本的影响来提高训练稳定性。在 RegDB 和 SYSU-MM01 数据集上进行的大量实验表明，尽管我们的方法很简单，但它的表现优于现有的 USL-VI-ReID 方法。源代码可在以下位置获得：https://github.com/tengxiao14/Neighbor-guided-USL-VI-ReID。]]></description>
      <guid>https://arxiv.org/abs/2412.12220</guid>
      <pubDate>Wed, 18 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>濒危警报：一种经过现场验证的自我训练方案，用于检测和保护道路和路边受威胁的野生动物</title>
      <link>https://arxiv.org/abs/2412.12222</link>
      <description><![CDATA[arXiv:2412.12222v1 公告类型：新
摘要：交通事故是一个全球性安全问题，每年造成大量人员死亡。这些死亡中有相当一部分是由动物与车辆相撞 (AVC) 造成的，这不仅危及人类生命，而且对动物种群也构成严重风险。本文提出了一种创新的自我训练方法，旨在检测稀有动物，例如澳大利亚的食火鸡，它们的生存受到道路事故的威胁。所提出的方法解决了现实世界中的关键挑战，包括在资源有限的环境中获取和标记稀有动物物种的传感器数据。它通过利用云和边缘计算以及自动数据标记来实现这一点，以迭代方式提高现场部署模型的检测性能。我们的方法引入了标签增强非最大抑制 (LA-NMS)，它结合了视觉语言模型 (VLM) 来实现自动数据标记。在为期五个月的部署过程中，我们确认了该方法的稳健性和有效性，从而提高了物体检测的准确性并增加了预测置信度。源代码可从以下网址获取：https://github.com/acfr/CassDetect]]></description>
      <guid>https://arxiv.org/abs/2412.12222</guid>
      <pubDate>Wed, 18 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>视频生成能否取代摄影师？生成视频的电影语言研究</title>
      <link>https://arxiv.org/abs/2412.12223</link>
      <description><![CDATA[arXiv:2412.12223v1 公告类型：新
摘要：文本到视频 (T2V) 生成的最新进展利用了扩散模型来增强从文本描述生成的视频的视觉连贯性。然而，大多数研究主要集中在物体运动上，对视频中的电影语言关注有限，而这对于电影摄影师传达情感和叙事节奏至关重要。为了解决这一限制，我们提出了一种三重方法来增强 T2V 模型生成可控电影语言的能力。具体来说，我们引入了一个电影语言数据集，它涵盖了镜头取景、角度和摄像机运动，使模型能够学习不同的电影风格。在此基础上，为了促进稳健的电影对齐评估，我们提出了 CameraCLIP，这是一个在所提出的数据集上进行了微调的模型，它擅长理解生成的视频中复杂的电影语言，并可以在多镜头合成过程中进一步提供有价值的指导。最后，我们提出了 CLIPLoRA，这是一种成本导向的动态 LoRA 合成方法，通过在单个视频中动态融合多个预先训练的电影 LoRA，实现电影语言的平滑过渡和逼真融合。我们的实验表明，CameraCLIP 在评估电影语言和视频之间的一致性方面优于现有模型，R@1 得分达到 0.81。此外，CLIPLoRA 提高了多镜头合成的能力，有可能缩小自动生成的视频与专业电影摄影师拍摄的视频之间的差距。]]></description>
      <guid>https://arxiv.org/abs/2412.12223</guid>
      <pubDate>Wed, 18 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>只需提交一张图片即可找到最合适的生成模型</title>
      <link>https://arxiv.org/abs/2412.12232</link>
      <description><![CDATA[arXiv:2412.12232v1 公告类型：新
摘要：深度生成模型在图像生成中取得了令人鼓舞的成果，并且已经开发了各种生成模型中心，例如 Hugging Face 和 Civitai，使模型开发人员能够上传模型，用户能够下载模型。然而，这些模型中心缺乏先进的模型管理和识别机制，导致用户只能通过文本匹配、下载排序等方式搜索模型，难以有效地找到最符合用户要求的模型。在本文中，我们提出了一种称为生成模型识别 (GMI) 的新设置，旨在使用户能够从大量候选模型中有效地识别出最适合用户需求的生成模型。据我们所知，它还没有被研究过。在本文中，我们介绍了一个由三个关键模块组成的综合解决方案：用于捕获生成的图像分布以及图像与提示之间关系的加权简化核均值嵌入 (RKME) 框架、旨在解决维度挑战的预训练视觉语言模型以及旨在解决跨模态问题的图像询问器。大量实证结果表明，该提案既高效又有效。例如，用户只需提交一个示例图像来描述他们的需求，模型平台就可以实现平均 top-4 识别准确率超过 80%。]]></description>
      <guid>https://arxiv.org/abs/2412.12232</guid>
      <pubDate>Wed, 18 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>OmniPrism：学习解开视觉概念以生成图像</title>
      <link>https://arxiv.org/abs/2412.12242</link>
      <description><![CDATA[arXiv:2412.12242v1 公告类型：新
摘要：创意视觉概念生成通常从参考图像中的特定概念中汲取灵感，以产生相关结果。然而，现有方法通常局限于单方面概念生成，或者在多方面概念场景中很容易被不相关的概念打乱，导致概念混淆并阻碍创意生成。为了解决这个问题，我们提出了 OmniPrism，一种用于创意图像生成的视觉概念解缠方法。我们的方法在自然语言的指导下学习解缠的概念表征，并训练一个扩散模型来整合这些概念。我们利用多模态提取器的丰富语义空间来实现从给定图像和概念指导中解缠的概念。为了解缠具有不同语义的概念，我们构建了一个配对概念解缠数据集 (PCD-200K)，其中每对共享相同的概念，例如内容、风格和构图。我们通过对比正交解缠 (COD) 训练管道学习解缠概念表征，然后将其注入额外的扩散交叉注意层进行生成。设计了一组块嵌入来适应扩散模型中每个块的概念域。大量实验表明，我们的方法可以生成高质量、概念解缠的结果，并且对文本提示和所需概念具有高保真度。]]></description>
      <guid>https://arxiv.org/abs/2412.12242</guid>
      <pubDate>Wed, 18 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>迈向通用合成视频检测器：从面部或背景处理到完全由 AI 生成的内容</title>
      <link>https://arxiv.org/abs/2412.12278</link>
      <description><![CDATA[arXiv:2412.12278v1 公告类型：新
摘要：现有的 DeepFake 检测技术主要关注面部操作，例如换脸或口型同步。然而，文本到视频 (T2V) 和图像到视频 (I2V) 生成模型的进步现在允许完全由 AI 生成的合成内容和无缝背景更改，这对以面部为中心的检测方法提出了挑战，并要求采用更多功能的方法。
为了解决这个问题，我们引入了用于 \underline{I} 识别 \underline{T}ampered 和 synth\underline{E}tic 视频的 \underline{U}universal \underline{N}etwork (\texttt{UNITE}) 模型，与传统检测器不同，该模型可以捕获全帧操作。 \texttt{UNITE} 将检测功能扩展到没有面部、非人类主体和复杂背景修改的场景。它利用基于转换器的架构，通过 SigLIP-So400M 基础模型处理从视频中提取的领域无关特征。鉴于包含面部/背景更改和 T2V/I2V 内容的数据集有限，我们在训练中将任务无关数据与标准 DeepFake 数据集集成在一起。我们通过结合注意力多样性 (AD) 损失来进一步减轻模型过度关注面部的倾向，这促进了视频帧之间的多样化空间注意力。将 AD 损失与交叉熵相结合可提高不同环境下的检测性能。比较评估表明，\texttt{UNITE} 在具有面部/背景操作和完全合成的 T2V/I2V 视频的数据集（跨数据设置）上的表现优于最先进的检测器，展示了其适应性和可推广的检测能力。]]></description>
      <guid>https://arxiv.org/abs/2412.12278</guid>
      <pubDate>Wed, 18 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用预训练几何先验进行高效的以对象为中心的表征学习</title>
      <link>https://arxiv.org/abs/2412.12331</link>
      <description><![CDATA[arXiv:2412.12331v1 公告类型：新
摘要：本文解决了以对象为中心的视频表示学习中的关键挑战。虽然现有方法难以应对复杂场景，但我们提出了一种新颖的弱监督框架，该框架强调几何理解并利用预训练的视觉模型来增强对象发现。我们的方法引入了一种专为以对象为中心的学习而设计的高效槽解码器，无需明确的深度信息即可有效表示多对象场景。在对象及其运动、对象遮挡和相机运动方面复杂性不断增加的合成视频基准测试中的结果表明，我们的方法在保持计算效率的同时实现了与监督方法相当的性能。这使该领域朝着复杂的现实场景中更实际的应用方向发展。]]></description>
      <guid>https://arxiv.org/abs/2412.12331</guid>
      <pubDate>Wed, 18 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>自动驾驶领域的泛化：使用 ROAD-Almaty 数据集评估 YOLOv8、RT-DETR 和 YOLO-NAS</title>
      <link>https://arxiv.org/abs/2412.12349</link>
      <description><![CDATA[arXiv:2412.12349v1 公告类型：新
摘要：本研究调查了三种最先进的物体检测模型（YOLOv8s、RT-DETR 和 YOLO-NAS）在哈萨克斯坦独特的驾驶环境中的领域泛化能力。利用新构建的 ROAD-Almaty 数据集（该数据集涵盖了各种天气、照明和交通条件），我们评估了模型的性能，而无需任何再训练。定量分析表明，RT-DETR 在 IoU=0.5 时实现了 0.672 的平均 F1 分数，比 YOLOv8s（0.458）和 YOLO-NAS（0.526）分别高出约 46% 和 27%。此外，所有模型在 IoU 阈值较高时（例如，将 IoU 从 0.5 增加到 0.75 时，性能下降约 20%）以及在恶劣的环境条件下（例如大雪和低光照场景）都表现出显著的性能下降。这些发现强调了地理上多样化的训练数据集和实施专门的领域适应技术的必要性，以提高全球自动驾驶汽车检测系统的可靠性。这项研究有助于理解自动驾驶领域泛化挑战，特别是在代表性不足的地区。]]></description>
      <guid>https://arxiv.org/abs/2412.12349</guid>
      <pubDate>Wed, 18 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过模态线性表示指导以少 500 倍的参数进行视觉指令调整</title>
      <link>https://arxiv.org/abs/2412.12359</link>
      <description><![CDATA[arXiv:2412.12359v1 公告类型：新
摘要：多模态大型语言模型 (MLLM) 通过将视觉表示集成到大型语言模型 (LLM) 中，显著提高了视觉任务的效率。从 LLM 继承的文本模态为 MLLM 提供了指令跟踪和上下文学习等能力。相比之下，视觉模态通过利用丰富的语义内容、空间信息和基础能力来提高下游任务的性能。这些内在模态在各种视觉任务中协同工作。我们的研究最初揭示了这些模态之间存在持续的不平衡，在视觉指令调整期间，文本通常主导输出生成。这种不平衡发生在使用完全微调和参数高效微调 (PEFT) 方法时。然后我们发现重新平衡这些模态可以显着减少所需的可训练参数数量，从而为进一步优化视觉指令调整提供了方向。我们引入了模态线性表示转向 (MoReS) 来实现这一目标。 MoReS 有效地重新平衡了整个模型的内在模态，其关键思想是通过每个模型层上的视觉子空间中的线性变换来引导视觉表示。为了验证我们的解决方案，我们编写了 LLaVA Steering，这是一套与所提出的 MoReS 方法集成的模型。评估结果表明，组合的 LLaVA Steering 模型所需的可训练参数平均比 LoRA 少 500 倍，同时仍在三个视觉基准和八个视觉问答任务中实现可比性能。最后，我们介绍了 LLaVA Steering Factory，这是一个内部开发的平台，使研究人员能够快速定制具有基于组件的架构的各种 MLLM，以无缝集成最先进的模型，并评估其内在模态不平衡。]]></description>
      <guid>https://arxiv.org/abs/2412.12359</guid>
      <pubDate>Wed, 18 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于文本到图像生成的扩散变压器的有效扩展</title>
      <link>https://arxiv.org/abs/2412.12391</link>
      <description><![CDATA[arXiv:2412.12391v1 公告类型：新 
摘要：我们通过执行广泛而严格的消融，实证研究了用于文本到图像生成的各种扩散变换器 (DiT) 的缩放特性，包括在高达 6 亿张图像的数据集上训练从 0.3B 到 8B 参数的缩放 DiT。我们发现，与基于交叉注意的 DiT 变体相比，纯基于自注意的 DiT 模型 U-ViT 提供了更简单的设计并且扩展更有效，这允许直接扩展额外条件和其他模态。我们发现 2.3B U-ViT 模型在受控设置下可以获得比 SDXL UNet 和其他 DiT 变体更好的性能。在数据扩展方面，我们研究了如何增加数据集大小和增强长标题来提高文本图像对齐性能和学习效率。]]></description>
      <guid>https://arxiv.org/abs/2412.12391</guid>
      <pubDate>Wed, 18 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>