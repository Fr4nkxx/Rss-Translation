<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>CS.CV更新arxiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.cv更新arxiv.org e-print存档。</description>
    <lastBuildDate>Fri, 14 Mar 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>核心^2：收集，反思和完善以更好，更快地产生</title>
      <link>https://arxiv.org/abs/2503.09662</link>
      <description><![CDATA[ARXIV：2503.09662V1公告类型：新 
摘要：使文本对图像（T2I）生成模型快速和井代表一个有希望的研究方向。先前的研究通常集中在提高合成图像的视觉质量上，而以抽样效率为代价，或者在不改善基本模型的生成能力的情况下大幅加速采样。此外，几乎所有推理方法都无法在扩散模型（DMS）和视觉自回归模型（ARM）上同时确保稳定的性能。在本文中，我们引入了一种新颖的插件推理范式，Core^2，其中包括三个子过程：收集，反射和完善。 Core^2首先收集无分类器的指导（CFG）轨迹，然后使用收集的数据来训练一个弱模型，该模型反映了易于学习的内容，同时将推断期间的功能评估数减少了一半。随后，Core^2采用弱到紧张的指导来完善条件输出，从而提高模型产生高频和现实内容的能力，这对于基本模型而言很难捕获。据我们所知，Core^2是第一个在包括SDXL，SD3.5和Flux以​​及Lamagen等手臂的各种DMS中均表现出效率和有效性。它在HPD V2，PIC-PIC，Drawbench，Geneval和T2i-Compbench上表现出显着的性能提高。此外，可以将Core^2与最新的Z-Smpling无缝集成，在PickScore和AES上的表现优于0.3和0.16，同时使用SD3.5.Code在https://github.com/xie-lab.com/xie-lab-mlab-ml/core/core/tree/tree/tree/tree/main/code中释放了5.64s的时间。]]></description>
      <guid>https://arxiv.org/abs/2503.09662</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>无声品牌攻击：无触发数据中毒对文本到图像扩散模型的攻击</title>
      <link>https://arxiv.org/abs/2503.09669</link>
      <description><![CDATA[ARXIV：2503.09669V1公告类型：新 
摘要：文本到图像扩散模型在从文本提示中生成高质量内容方面取得了显着的成功。但是，它们依赖公开可用的数据以及对微调的数据共享的增长趋势使这些模型特别容易受到数据中毒攻击的影响。在这项工作中，我们引入了无声的品牌攻击，这是一种新型的数据中毒方法，该方法操纵文本对图像扩散模型，以生成包含特定品牌徽标或符号的图像，而无需任何文本触发器。我们发现，当训练数据中某些视觉模式反复反复时，即使没有提及，该模型即使没有提及，模型就会在其输出中自然复制它们。利用这一点，我们开发了一种自动数据中毒算法，该算法并不明显地将徽标注入原始图像，以确保它们自然融合并保持未被发现。在此中毒数据集上训练的模型生成了包含徽标的图像，而不会降低图像质量或文本对齐。我们在大规模高质量的图像数据集和样式的个性化数据集上实验验证了我们在两个现实设置中进行静默的品牌攻击，即使没有特定的文本触发器，也可以达到高成功率。包括徽标检测在内的人类评估和定量指标表明，我们的方法可以隐秘地嵌入徽标。]]></description>
      <guid>https://arxiv.org/abs/2503.09669</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过利用局部过渡相干性加速扩散采样</title>
      <link>https://arxiv.org/abs/2503.09675</link>
      <description><![CDATA[ARXIV：2503.09675V1公告类型：新 
摘要：基于文本的扩散模型在产生文本描述中产生高质量的图像和视频方面取得了重大突破。但是，在实用应用中，脱索过程的冗长抽样时间仍然是一个重要的瓶颈。先前的方法要么忽略相邻步骤之间的统计关系，要么依赖于注意力或在它们之间的特征相似性，这通常仅与特定的网络结构一起使用。为了解决这个问题，我们在相邻步骤之间的过渡操作员中发现了新的统计关系，重点是网络的输出关系。这种关系对网络结构没有任何要求。基于此观察结果，我们提出了一种名为LTC-ACCEL的新型无训练加速方法，该方法使用确定的关系来估算基于相邻步骤的当前过渡操作员。由于没有关于网络结构的具体假设，LTC-ACCEL几乎适用于几乎所有基于扩散的方法，几乎​​适用于几乎所有现有的加速技术，因此很容易与它们结合。实验结果表明，LTC-ACCEL可以显​​着加快文本对图和文本对视频合成的采样，同时保持竞争性样本质量。具体而言，LTC-ACCEL在稳定扩散V2中达到了1.67倍的速度，视频生成模型的加速度为1.55倍。当与蒸馏模型结合使用时，LTC-Accel在视频生成中实现了10倍的速度，可实时生成超过16fps。]]></description>
      <guid>https://arxiv.org/abs/2503.09675</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>I2V3D：可控制的图像到视频生成3D指南</title>
      <link>https://arxiv.org/abs/2503.09733</link>
      <description><![CDATA[ARXIV：2503.09733V1公告类型：新 
摘要：我们提出了I2V3D，这是一个新颖的框架，将静态图像动画为具有精确3D控制的动态视频，利用3D几何指导和先进的生成模型的优势。我们的方法结合了计算机图形管道的精确度，可以准确控制诸如摄像机运动，对象旋转和角色动画的元素，以及生成AI的视觉保真度，从而从粗糙的输入中产生高质量的视频。 To support animations with any initial start point and extended sequences, we adopt a two-stage generation process guided by 3D geometry: 1) 3D-Guided Keyframe Generation, where a customized image diffusion model refines rendered keyframes to ensure consistency and quality, and 2) 3D-Guided Video Interpolation, a training-free approach that generates smooth, high-quality video frames between keyframes using bidirectional guidance.实验结果突出了我们的框架在通过将3D几何形状与生成模型相一致的单个输入图像中产生可控的高质量动画中的有效性。我们的框架代码将公开发布。]]></description>
      <guid>https://arxiv.org/abs/2503.09733</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>暹罗网络检测两个虹膜图像是否是单卵</title>
      <link>https://arxiv.org/abs/2503.09749</link>
      <description><![CDATA[ARXIV：2503.09749V1公告类型：新 
摘要：在Daugman风格的Iris识别中，传统上认为同一人的左右虹膜纹理与两个无关的人的虹膜不同。但是，先前的研究表明，人类可以检测到两个虹膜图像来自同一人的不同眼睛或单卵双胞胎的眼睛，精度约为80％。在这项工作中，我们采用了暹罗网络架构和对比度学习，将一对虹膜图像分类为来自单卵或非共生虹膜的虹膜图像。例如，可以将其作为快速，无创的​​测试，以确定双胞胎是单卵还是非单同生的。我们构建了一个包括合成单卵对的数据集（同一个体的不同虹膜的图像）和天然的单子斑对（来自相同双胞胎的人的不同图像的图像），除了来自无关的个体的非单卵双胞胎对，以确保对模型能力的全面评估。为了更深入地了解学习的表示形式，我们使用（1）原始输入图像，（2）仅虹膜图像和（3）仅非IRIS仅图像来训练和分析模型的三种变体。该比较揭示了虹膜特定的纹理细节和上下文眼线提示在识别单卵虹膜模式中的至关重要性。结果表明，利用全眼区域信息的模型优于仅针对仅IRIS数据的训练的模型，强调了虹膜和眼部特征之间的细微互动。我们的方法使用完整的虹膜图像达到了精度水平，该图像超过了先前报告的单卵虹膜对分类的图像。这项研究介绍了旨在确定一对虹膜图像是否起源于单卵单个个体的第一个分类器。]]></description>
      <guid>https://arxiv.org/abs/2503.09749</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SASNET：空间自适应正弦神经网络</title>
      <link>https://arxiv.org/abs/2503.09750</link>
      <description><![CDATA[ARXIV：2503.09750V1公告类型：新 
摘要：正弦神经网络（SNN）已成为计算机视觉和图形中低维信号的强大隐式神经表示（INR）。它们可以实现高频信号重建和光滑的歧管建模；但是，他们经常会遭受频谱偏见，训练不稳定和过度拟合的困扰。为了应对这些挑战，我们提出了SASNET，具有空间自适应的SNN，可稳固地提高紧凑型INR的能力，以适应详细信号。 SASNET集成了频率嵌入层以控制频率成分并减轻频谱偏置，以及共同优化的空间自适应掩模，该面罩定位神经元影响，降低网络冗余并改善收敛稳定性。 SASNET忠实地重建高频信号而不拟合低频区域，Sasnet忠实地重建了高频信号。我们的实验表明，SASNET优于最先进的INR，实现了强大的拟合精度，超分辨率能力和抑制噪声，而无需牺牲模型紧凑。]]></description>
      <guid>https://arxiv.org/abs/2503.09750</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于合成事件摄像机数据生成和算法开发的启用Pytorch的工具</title>
      <link>https://arxiv.org/abs/2503.09754</link>
      <description><![CDATA[ARXIV：2503.09754V1公告类型：新 
摘要：事件或神经形态摄像机通过异步报告亮度（称为事件）的显着变化，与常规摄像机相比，具有改善的动态范围，时间分辨率和较低的数据带宽，从而为自然场景提供了新的编码。但是，它们在特定领域的研究任务中的采用部分受到了有限的商业可用性，缺乏现有数据集的影响以及与预测其非线性光学编码，独特噪声模型和基于张量的数据处理要求相关的挑战。为了应对这些挑战，我们在Python中引入了用于神经处理和集成的合成事件，该事件是基于Pytorch的库，用于模拟和处理事件摄像机数据。 SENPI包括一个可区分的数字双胞胎，将基于强度的数据转换为事件表示形式，可以评估事件摄像机性能，同时处理远期模型的非平滑和非线性性质，图书馆还支持基于事件的I/O，操作，过滤和可视化，为合成和实际事件基于事件的数据提供高效且可扩展的工作表。我们证明了Senpi通过将合成输出与真实事件摄像机数据进行比较，并使用这些结果来得出有关基于事件的感知的属性和实用性的结论，从而证明了Senpi产生基于事件的现实数据的能力。此外，我们还展示了Senpi在不同的噪声条件下探索事件相机行为的用途，并优化了事件对比度阈值，以改善目标条件下的编码。最终，Senpi的目标是通过为事件数据生成和算法开发提供可访问的工具来降低研究人员进入障碍，从而使其成为推进神经形态视觉系统研究的宝贵资源。]]></description>
      <guid>https://arxiv.org/abs/2503.09754</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>偏见：研究文本对图像模型中的偏见相互作用</title>
      <link>https://arxiv.org/abs/2503.09763</link>
      <description><![CDATA[ARXIV：2503.09763V1公告类型：新 
摘要：文本对图像（TTI）模型所表现出的偏见通常被视为独立的，但实际上，它们可能与之有着密切相关的。解决一个维度的偏见，例如种族或年龄，可以无意中影响另一个维度，例如性别，减轻或加剧现有差异。了解这些相互依存关系对于设计更公平的生成模型至关重要，但是定量测量这种效果仍然是一个挑战。在本文中，我们旨在通过引入BiasConnect来解决这些问题，BiasConnect是一种新颖的工具，旨在分析和量化TTI模型中的偏差相互作用。我们的方法利用基于反事实的框架生成成对因果图，该图表揭示了给定文本提示的偏见相互作用的基本结构。此外，我们的方法还提供了经验估计，以指示当给定偏置修改时，其他偏差维度如何向理想分布转移或远离理想分布。我们的估计值与偏置后的相互依赖观察结果具有很强的相关性（+0.69）。我们演示了biasConnect选择最佳偏置缓解轴的实用性，比较了他们所学习的依赖项的不同TTI模型，并了解TTI模型中的截面社会偏见的扩增。]]></description>
      <guid>https://arxiv.org/abs/2503.09763</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SEQSAM：使用SAM进行医学图像分割的自回旋多重假设预测</title>
      <link>https://arxiv.org/abs/2503.09797</link>
      <description><![CDATA[ARXIV：2503.09797V1公告类型：新 
摘要：预训练的分割模型是用于分割图像的强大而灵活的工具。最近，这种趋势已扩展到医学成像。然而，由于不清楚的对象边界和注释工具引起的错误，这些方法通常仅对给定图像产生一个单一的预测，从而忽略了医学图像中固有的不确定性。多项选择学习是一种通过多个学习的预测头来生成多个口罩的技术。 However, this cannot readily be extended to producing more outputs than its initial pre-training hyperparameters, as the sparse, winner-takes-all loss function makes it easy for one prediction head to become overly dominant, thus not guaranteeing the clinical relevancy of each mask produced.我们介绍了SeqSAM，这是一种由RNN启发的顺序生成多个掩码的方法，该方法使用二分匹配损失来确保每个掩码的临床相关性，并可以产生任意数量的掩码。我们显示出在两个公开可用数据集中产生的每个面具的质量的显着提高。我们的代码可在https://github.com/benjamintowle/seqsam上找到。]]></description>
      <guid>https://arxiv.org/abs/2503.09797</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>评估合成数据对自动驾驶中对象检测任务的影响</title>
      <link>https://arxiv.org/abs/2503.09803</link>
      <description><![CDATA[ARXIV：2503.09803V1公告类型：新 
摘要：自动驾驶系统的应用不断增加，需要大规模的高质量数据集，以确保在各种情况下的稳健性能。由于其成本效益，精确的地面真相标签的可用性以及对特定边缘案例建模的能力，合成数据已成为增加现实世界数据集的可行解决方案。但是，合成数据可能会引入分布差异和偏见，从而影响现实世界中的模型性能。为了评估合成数据的效用和局限性，我们使用BIT Technology Solutions GmbH生成的多个现实世界数据集和合成数据集进行了对照实验。我们的研究涵盖了两种传感器模式，相机和激光镜头，并研究了2D和3D对象检测任务。我们比较了经过实际，合成和混合数据集训练的模型，从而分析了它们的稳健性和泛化功能。我们的发现表明，使用真实数据和合成数据的结合可以提高对象检测模型的鲁棒性和概括，从而强调合成数据在推进自动驾驶技术方面的潜力。]]></description>
      <guid>https://arxiv.org/abs/2503.09803</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用视频数据进行自动化道路安全分析的深度学习方法如何？一项实验研究</title>
      <link>https://arxiv.org/abs/2503.09807</link>
      <description><![CDATA[ARXIV：2503.09807V1公告类型：新 
摘要：基于图像的多对象检测（MOD）和多对象跟踪（MOT）正在快速前进。已经开发了各种2D和3D MOD以及MOT方法，用于单眼和立体声摄像机。道路安全分析可以从这些进步中受益。由于崩溃是罕见事件，因此已经开发了安全分析的替代措施（SMO）。 （半）自动化安全分析方法提取道路用户轨迹以计算安全指标，例如，碰撞时间（TTC）和后侵点时间（PET）。受到MOD和MOT的深度学习成功的启发，我们使用带注释的Kitti流量视频数据集研究了三种MOT方法，其中一种基于立体相机。开发了两个后处理步骤，即IDSPLIT和SS，以改善跟踪结果并研究影响TTC的因素。实验结果表明，尽管在相互作用的数量或与TTC分布的相似性方面具有一些优势，但所有测试的方法系统地过度估计了相互作用的数量和TTC的低估：他们报告了更多的相互作用和更严重的相互作用，使道路用户的交互作用似乎不那么安全。进一步的努力将用于测试更多方法和更多数据，特别是从路边传感器来验证结果并改善性能。]]></description>
      <guid>https://arxiv.org/abs/2503.09807</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>微调视觉语言模型具有基于图的知识，用于解释的医学图像分析</title>
      <link>https://arxiv.org/abs/2503.09808</link>
      <description><![CDATA[ARXIV：2503.09808V1公告类型：新 
摘要：糖尿病性视网膜病（DR）的准确分期对于指导及时干预和防止视力丧失至关重要。但是，当前的分期模型几乎是无法解释的，并且大多数公共数据集都没有图像级标签以外的临床推理或解释。在本文中，我们提出了一种新颖的方法，该方法将图表学习与视觉语言模型（VLM）相结合，以提供可解释的DR诊断。我们的方法通过构建编码关键的视网膜血管特征（例如血管形态和空间连接性）来利用光学相干层析成像造影术（OCTA）图像。然后，图形神经网络（GNN）执行DR登台，而集成梯度则突出显示关键节点和边缘及其各自的特征，以驱动分类决策。我们收集基于图的知识，该知识将模型的预测归因于生理结构及其特征。然后，我们将其转换为VLMS的文本描述。我们对这些文本描述和相应的图像进行指导调节，以培训学生VLM。该最终代理可以仅基于单个图像输入来对疾病进行分类，并以人类解释的方式解释其决定。对专有和公共数据集的实验评估表明，我们的方法不仅提高了分类准确性，而且还提供了更多可解释的结果。一项专家研究进一步表明，我们的方法提供了更准确的诊断解释，并为八颗图像中病理学的精确定位铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2503.09808</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>孤立的频道视觉变压器：从预处理到多通道登录</title>
      <link>https://arxiv.org/abs/2503.09826</link>
      <description><![CDATA[ARXIV：2503.09826V1公告类型：新 
摘要：Vision Transformers（VIT）在标准RGB图像处理任务中取得了巨大的成功。但是，将VIT应用于多通道成像（MCI）数据，例如，对于医学和遥感应用程序，仍然是一个挑战。特别是，MCI数据通常由从不同方式中获得的层组成。直接培训此类数据可能会掩盖互补信息并损害性能。在本文中，我们为大规模MCI数据集引入了一个简单而有效的预审前框架。我们的方法称为孤立的通道VIT（IC-VIT），对图像通道进行了单独修补，从而可以预处理多模式多渠道任务。我们表明，这种频道修补程序是MCI处理的关键技术。更重要的是，一个人可以在单个通道上为IC-VIT预处理，并在下游的多频道数据集上进行列表。该预处理的框架捕获了贴片和通道之间的依赖性，并产生了强大的特征表示。对各种任务和基准的实验，包括用于细胞显微镜成像的跳跃CP和CHAMMI，以及用于卫星成像的SO2SAT-LCZ42，表明所提出的IC-VIT在现有的通道自动适应方法上提供了4-14个百分点的性能改善点。此外，其有效的培训使其成为在异质数据上大规模预测基础模型的合适候选者。]]></description>
      <guid>https://arxiv.org/abs/2503.09826</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>分辨率不变自动编码器</title>
      <link>https://arxiv.org/abs/2503.09828</link>
      <description><![CDATA[ARXIV：2503.09828V1公告类型：新 
摘要：深度学习具有明显的高级医学成像分析，但是图像分辨率的变化仍然是一个被忽视的挑战。大多数方法通过重新采样图像来解决此问题，从而导致信息丢失或计算效率低下。尽管存在针对特定任务的解决方案，但未提出统一的方法。我们介绍了一个分辨率不变的自动编码器，该自动编码器通过学习的可变调整过程在网络中的每个层调整空间调整大小，以2的传统因素取代固定的空间向下/UPSMPLING，这确保了一致的潜在空间分辨率，无论输入或输出分辨率如何，都可以确保一个一致的潜在空间分辨率。我们的模型使能够在图像潜在的图像上执行各种下游任务，同时维持不同分辨率的性能，克服传统方法的短缺。我们证明了它在不确定性感知的超分辨率，分类和生成建模任务中的有效性，并展示了我们的方法如何优于传统基线，而整个分辨率之间的性能损失最少。]]></description>
      <guid>https://arxiv.org/abs/2503.09828</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>探索在扩散U-NET中编码的位置，以进行无训练的高分辨率图像生成</title>
      <link>https://arxiv.org/abs/2503.09830</link>
      <description><![CDATA[ARXIV：2503.09830V1公告类型：新 
摘要：通过预先训练的U-NET剥夺高分辨率潜在的潜在，会导致重复和无序的图像模式。尽管最近的研究通过使原始和更高分辨率的deno流程对齐过程来提高生成质量，但次优产生的根本原因仍缺乏探索。通过对U-NET中编码的位置的全面分析，我们将其归因于编码不一致的位置，这是由于位置信息不足的传播信息从零填充到卷积层中潜在特征的传播不足，随着分辨率的增加。为了解决这个问题，我们提出了一种新颖的无培训方法，引入了渐进边界补体（PBC）方法。此方法在特征图内创建动态虚拟图像边界，以增强位置信息传播，从而使高质量和丰富的高分辨率图像合成。广泛的实验证明了我们方法的优越性。]]></description>
      <guid>https://arxiv.org/abs/2503.09830</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>