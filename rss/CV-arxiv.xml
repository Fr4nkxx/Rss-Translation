<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Tue, 04 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>使用动态增强磁共振图像早期预测乳腺癌新辅助化疗病理完全缓解的两阶段双任务学习策略</title>
      <link>https://arxiv.org/abs/2502.00051</link>
      <description><![CDATA[arXiv:2502.00051v1 公告类型：新 
摘要：理由和目标：病理完全缓解 (pCR) 的早期预测可以促进乳腺癌患者的个性化治疗。为了提高新辅助化疗早期时间点的预测准确性，我们提出了一种两阶段双任务学习策略，使用早期治疗磁共振图像训练深度神经网络以早期预测 pCR。方法：我们使用来自全国多机构 I-SPY2 临床试验的数据集开发并验证了两阶段双任务学习策略，该数据集包括在三个时间点获取的动态对比增强磁共振图像：治疗前 (T0)、3 周后 (T1) 和治疗 12 周后 (T2)。首先，我们训练了一个卷积长短期记忆网络来预测 pCR 并提取 T2 时的潜在空间图像特征。在第二阶段，我们训练了一个双任务网络，使用来自 T0 和 T1 的图像同时预测 pCR 和 T2 时的图像特征。这使我们能够在不使用T2图像的情况下更早地预测pCR。结果：传统的单阶段单任务策略使用时间点T0和T1的所有数据预测pCR的受试者工作特征曲线下面积（AUROC）为0.799。通过使用提出的两阶段双任务学习策略，AUROC提高到0.820。结论：提出的两阶段双任务学习策略可以显著提高模型在新辅助化疗早期（第3周）预测pCR的性能（p=0.0025）。早期预测模型可以潜在地帮助医生在化疗早期进行早期干预并制定个性化计划。]]></description>
      <guid>https://arxiv.org/abs/2502.00051</guid>
      <pubDate>Tue, 04 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SpikingRTNH：用于 4D 雷达物体检测的脉冲神经网络</title>
      <link>https://arxiv.org/abs/2502.00074</link>
      <description><![CDATA[arXiv:2502.00074v1 公告类型：新
摘要：最近，4D 雷达已成为自动驾驶汽车中 3D 物体检测的关键传感器，既可以在恶劣天气下提供稳定的感知，也可以提供用于物体形状识别的高密度点云。然而，处理这种高密度数据需要大量的计算资源和能源消耗。我们提出了 SpikingRTNH，这是第一个使用 4D 雷达数据进行 3D 物体检测的脉冲神经网络 (SNN)。通过用泄漏积分和发射 (LIF) 脉冲神经元取代传统的 ReLU 激活函数，SpikingRTNH 实现了显着的能源效率提升。此外，受人类认知过程的启发，我们引入了生物自上而下的推理 (BTI)，它按从高到低密度的顺序处理点云。这种方法有效地利用了噪声较低、重要性较高的点进行检测。在 K-Radar 数据集上进行的实验表明，带有 BTI 的 SpikingRTNH 可显著降低 78% 的能耗，同时实现与 ANN 同类产品相当的检测性能（51.1% AP 3D，57.0% AP BEV）。这些结果证实了 SNN 在自动驾驶系统中实现节能的基于 4D 雷达的物体检测的可行性。所有代码均可在 https://github.com/kaist-avelab/k-radar 上找到。]]></description>
      <guid>https://arxiv.org/abs/2502.00074</guid>
      <pubDate>Tue, 04 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>色彩校正对胶囊内镜病理检测的影响</title>
      <link>https://arxiv.org/abs/2502.00076</link>
      <description><![CDATA[arXiv:2502.00076v1 公告类型：新
摘要：最近，人们开始探索使用深度学习进行无线胶囊内窥镜 (WCE) 中的病理检测。然而，深度学习模型可能会受到用于训练它们的数据集的颜色质量的影响，从而影响检测、分割和分类任务。在这项工作中，我们使用两个突出的物体检测模型 Retinanet 和 YOLOv5 评估颜色校正对病理检测的影响。我们首先使用两种不同的颜色校正函数生成流行的 WCE 数据集（即 SEE-AI 数据集）的两个颜色校正版本。然后，我们评估 Retinanet 和 YOLOv5 在数据集的原始版本和颜色校正版本上的性能。结果表明，颜色校正使模型生成更大的边界框和与地面实况注释的更大交叉区域。此外，颜色校正会导致某些病理的假阳性数量增加。然而，这些效果并没有转化为 F1 分数、IoU 和 AP50 等性能指标的持续改善。代码可在 https://github.com/agossouema2011/WCE2024 上找到。关键词：无线胶囊内窥镜、色彩校正、Retinanet、YOLOv5、检测]]></description>
      <guid>https://arxiv.org/abs/2502.00076</guid>
      <pubDate>Tue, 04 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CerraData-4MM：塞拉多 (Cerrado) 上用于土地利用和土地覆盖分类的多模式基准数据集</title>
      <link>https://arxiv.org/abs/2502.00083</link>
      <description><![CDATA[arXiv:2502.00083v1 公告类型：新
摘要：塞拉多面临着越来越大的环境压力，尽管存在类别不平衡和视觉相似类别等挑战，但仍然需要准确的土地利用和土地覆盖 (LULC) 制图。为了解决这个问题，我们提出了 CerraData-4MM，这是一个多模态数据集，结合了 Sentinel-1 合成孔径雷达 (SAR) 和 Sentinel-2 多光谱图像 (MSI)，具有 10m 空间分辨率。该数据集包括两个分层分类级别，分别有 7 个和 14 个类，重点关注多样化的 Bico do Papagaio 生态区。我们通过评估标准 U-Net 和更复杂的 Vision Transformer (ViT) 模型来强调 CerraData-4MM 对高级语义分割技术进行基准测试的能力。 ViT 在多模态场景中实现了出色的性能，第一层级的最高宏观 F1 得分为 57.60%，平均交并比 (mIoU) 为 49.05%。两种模型在处理少数类时都存在困难，尤其是在第二层级，U-Net 的性能下降到 F1 得分 18.16%。类平衡可以提高代表性不足的类的代表性，但会降低整体准确性，这凸显了加权训练的权衡。CerraData-4MM 为推进深度学习模型处理类不平衡和多模态数据融合提供了一个具有挑战性的基准。代码、训练模型和数据可在 https://github.com/ai4luc/CerraData-4MM 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2502.00083</guid>
      <pubDate>Tue, 04 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AIN：阿拉伯语包容性大型多式联运模型</title>
      <link>https://arxiv.org/abs/2502.00094</link>
      <description><![CDATA[arXiv:2502.00094v1 公告类型：新
摘要：随着大型语言模型 (LLM) 的快速发展及其向大型多模态模型 (LMM) 的演变，英语和中文等高资源语言取得了重大进展。虽然阿拉伯语 LLM 取得了显着进展，但阿拉伯语 LMM 仍然基本上未被探索，通常只关注语言和视觉理解的几个特定方面。为了弥补这一差距，我们引入了 AIN - 阿拉伯语包容性多模态模型 - 旨在在不同领域脱颖而出。AIN 是一种英语-阿拉伯语双语 LMM，旨在在英语和阿拉伯语中表现出色，利用精心构建的 360 万个高质量阿拉伯语-英语多模态数据样本。AIN 展示了最先进的阿拉伯语性能，同时还拥有强大的英语视觉能力。在最近的 CAMEL-Bench 基准测试中，该基准测试包含 38 个子域，包括多图像理解、复杂视觉感知、手写文档理解、视频理解、医学成像、植物疾病和基于遥感的土地使用理解，我们的 AIN 表现出色，7B 模型的表现优于 GPT-4o，在 8 个域和 38 个子域中平均绝对增益为 3.4%。AIN 的卓越功能使其成为向阿拉伯语使用者提供适用于各种应用的高级多模式生成 AI 工具的重要一步。]]></description>
      <guid>https://arxiv.org/abs/2502.00094</guid>
      <pubDate>Tue, 04 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ProtoSnap：楔形文字符号的原型对齐</title>
      <link>https://arxiv.org/abs/2502.00129</link>
      <description><![CDATA[arXiv:2502.00129v1 公告类型：新
摘要：楔形文字系统在三千多年的时间里一直是古代近东地区传播知识的媒介。楔形文字符号具有复杂的内部结构，这是专家古文字学分析的主题，因为符号形状的变化见证了历史发展以及文字和文化的传播。然而，以前的自动化技术大多将符号类型视为分类，并没有明确地模拟它们高度多样化的内部配置。在这项工作中，我们提出了一种无监督方法，通过利用强大的生成模型和原型字体图像的外观和结构作为先验来恢复楔形文字符号的细粒度内部配置。我们的方法 ProtoSnap 强制对使用深度图像特征找到的匹配项进行结构一致性处理，以估计楔形文字字符的不同配置，将基于骨架的模板捕捉到拍摄的楔形文字符号上。我们提供了专家注释的新基准并评估了我们在此任务上的方法。我们的评估表明，我们的方法成功地将原型骨架与各种楔形文字符号对齐。此外，我们表明，以我们的方法生成的结构为条件可以生成具有正确结构配置的合成数据，从而大大提高了楔形文字符号识别的性能，超越了现有技术，尤其是对罕见符号的识别。我们的代码、数据和训练模型可在项目页面上找到：https://tau-vailab.github.io/ProtoSnap/]]></description>
      <guid>https://arxiv.org/abs/2502.00129</guid>
      <pubDate>Tue, 04 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 YOLOv8 探索结肠镜检查图像中深度学习息肉检测的迁移学习</title>
      <link>https://arxiv.org/abs/2502.00133</link>
      <description><![CDATA[arXiv:2502.00133v1 公告类型：新
摘要：深度学习方法在反对任务中表现出色；然而，它们在有限的训练数据下学习特定领域应用程序的能力仍然是一个重大挑战。迁移学习技术通过利用相关数据集预训练的知识来解决此问题，从而更快、更有效地学习新任务。找到合适的预训练数据集对于确定迁移学习的成功和整体模型性能至关重要。在本文中，我们研究了在七个不同的数据集上预训练 YOLOv8n 模型的影响，评估了它们在转移到息肉检测任务时的有效性。我们比较了具有多样化对象的大型通用数据集是否优于具有与息肉相似特征的小众数据集。此外，我们评估了数据集大小对迁移学习效果的影响。在息肉数据集上进行的实验表明，在相关数据集上进行预训练的模型始终优于从头开始训练的模型，凸显了在具有共享领域特定特征的数据集上进行预训练的好处。]]></description>
      <guid>https://arxiv.org/abs/2502.00133</guid>
      <pubDate>Tue, 04 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ALBAR：对抗性学习方法可减轻动作识别中的偏见</title>
      <link>https://arxiv.org/abs/2502.00156</link>
      <description><![CDATA[arXiv:2502.00156v1 公告类型：新
摘要：机器学习模型中的偏差可能导致不公平的决策，虽然它在图像和文本领域已经得到充分研究，但在动作识别中仍未得到充分探索。动作识别模型通常受到背景偏差（即根据背景线索推断动作）和前景偏差（即依赖于主体外观）的影响，这可能对自动驾驶汽车或辅助生活监控等现实生活中的应用造成不利影响。虽然之前的方法主要侧重于使用专门的增强来减轻背景偏差，但我们彻底研究了这两种偏差。我们提出了 ALBAR，这是一种新颖的对抗性训练方法，它可以减轻前景和背景偏差，而无需专门了解偏差属性。我们的框架将对抗性交叉熵损失应用于采样的静态剪辑（其中所有帧都相同），并旨在使用建议的熵最大化损失使其类概率均匀。此外，我们引入了梯度惩罚损失来对去偏过程进行正则化。我们在已建立的背景和前景偏差协议上评估了我们的方法，创造了新的最先进水平，并在 HMDB51 上将综合去偏性能大大提高了 12% 以上。此外，我们发现现有 UCF101 偏差评估协议中存在背景泄漏问题，这提供了一种预测动作的捷径，但不能准确衡量模型的去偏能力。我们通过为参与者提出更细粒度的分割边界来解决这个问题，我们的方法也优于现有方法。项目页面：https://joefioresi718.github.io/ALBAR_​​webpage/]]></description>
      <guid>https://arxiv.org/abs/2502.00156</guid>
      <pubDate>Tue, 04 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>高斯提升：一种简单、快速、灵活的 3D 实例分割方法</title>
      <link>https://arxiv.org/abs/2502.00173</link>
      <description><![CDATA[arXiv:2502.00173v1 公告类型：新 
摘要：我们引入了高斯提升 (LBG)，这是一种用于 3D 高斯溅射辐射场 (3DGS) 的开放世界实例分割的新方法。最近，3DGS 场已经成为一种高效且明确的替代神经场方法的高质量新视图合成方法。我们的 3D 实例分割方法直接从 SAM（或者 FastSAM 等）中提取 2D 分割蒙版，以及来自 CLIP 和 DINOv2 的特征，将它们直接融合到 3DGS（或类似的高斯辐射场，如 2DGS）上。与以前的方法不同，LBG 不需要每个场景进行训练，允许它在任何现有的 3DGS 重建上无缝运行。我们的方法不仅比现有方法快一个数量级，而且更简单；它还具有高度模块化，无需对 3D 高斯函数进行特定参数化，即可对现有的 3DGS 场进行 3D 语义分割。此外，我们的技术在保持灵活性和效率的同时，实现了 2D 语义新视图合成和 3D 资产提取结果的卓越语义分割。我们进一步介绍了一种新方法来评估 3D 辐射场分割方法中单独分割的 3D 资产。]]></description>
      <guid>https://arxiv.org/abs/2502.00173</guid>
      <pubDate>Tue, 04 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DermaSynth：利用开放获取皮肤病学数据集生成丰富的合成图像-文本对</title>
      <link>https://arxiv.org/abs/2502.00196</link>
      <description><![CDATA[arXiv:2502.00196v1 公告类型：新
摘要：开发皮肤病学视觉大型语言模型 (LLM) 的主要障碍是缺乏大型图像-文本对数据集。我们引入了 DermaSynth，这是一个由 45,205 张图像（13,568 张临床图像和 35,561 张皮肤镜图像）中整理出的 92,020 个合成图像-文本对组成的数据集，用于皮肤病学相关的临床任务。利用最先进的 LLM，使用 Gemini 2.0，我们使用临床相关提示和自我指导方法来生成多样化和丰富的合成文本。通过定位以减少潜在的幻觉，将数据集的元数据合并到输入提示中。最终的数据集基于开放获取的皮肤病学图像存储库（DERM12345、BCN20000、PAD-UFES-20、SCIN 和 HIBA），这些存储库拥有宽松的 CC-BY-4.0 许可证。我们还对 5,000 个样本的初步 Llama-3.2-11B-Vision-Instruct 模型 DermatoLlama 1.0 进行了微调。我们预计该数据集将支持和加速皮肤病学的 AI 研究。这项工作的基础数据和代码可在 https://github.com/abdurrahimyilmaz/DermaSynth 上找到。]]></description>
      <guid>https://arxiv.org/abs/2502.00196</guid>
      <pubDate>Tue, 04 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>EcoWeedNet：一种轻量级自动化杂草检测方法，适用于可持续的下一代农业消费电子产品</title>
      <link>https://arxiv.org/abs/2502.00205</link>
      <description><![CDATA[arXiv:2502.00205v1 公告类型：新
摘要：可持续农业在确保世界消费者粮食安全方面发挥着至关重要的作用。可持续精准农业面临的一个关键挑战是杂草生长，因为杂草与农作物共享基本资源，如水、土壤养分和阳光，这些资源会显著影响农作物产量。传统的除草方法包括使用化学除草剂和人工除草方法。然而，这些方法可能会破坏环境并造成健康危害。在精准农业中采用自动化计算机视觉技术和地面农业消费电子车辆提供了可持续的低碳解决方案。然而，先前的研究存在准确度和精度低、计算成本高等问题。这项研究提出了 EcoWeedNet，这是一种新型模型，具有增强的杂草检测性能，而不会增加显着的计算复杂性，符合低碳农业实践的目标。此外，我们的模型重量轻，最适合部署在地面消费电子农业车辆和机器人上。通过在反映真实场景的 CottonWeedDet12 基准数据集上进行全面实验，证明了所提模型的有效性。EcoWeedNet 的性能接近大型模型，但参数却少得多。（约占 YOLOv4 的 4.21% 的参数和 6.59% 的 GFLOP）。这项工作为开发具有更低能耗和更低碳足迹的下一代农业消费电子产品的自动杂草检测方法做出了有效贡献。这项工作为可持续农业消费技术铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2502.00205</guid>
      <pubDate>Tue, 04 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>混合随机森林和 CNN 框架用于高光谱图像中的分块油水分类</title>
      <link>https://arxiv.org/abs/2502.00232</link>
      <description><![CDATA[arXiv:2502.00232v1 公告类型：新 
摘要：提出了一种用于高光谱图像（HSI）中油水分类的新型混合随机森林和卷积神经网络（CNN）框架。为了解决保留空间上下文的挑战，将图像分成更小的、不重叠的图块，作为训练、验证和测试的基础。随机森林在像素分类中表现出色，优于 XGBoost、基于注意力的 U-Net 和 HybridSN 等模型。然而，随机森林失去了空间上下文，限制了它充分利用高光谱数据中空间关系的能力。为了提高性能，在随机森林生成的概率图上训练了 CNN，利用了 CNN 整合空间上下文的能力。与基线相比，混合方法的召回率提高了 7.6%（至 0.85），F1 得分提高了 2.4%（至 0.84），AUC 提高了 0.54%（至 0.99）。这些结果凸显了将概率输出与空间特征学习相结合对高光谱图像进行情境感知分析的有效性。]]></description>
      <guid>https://arxiv.org/abs/2502.00232</guid>
      <pubDate>Tue, 04 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于 Transformer 的不同字体格式的矢量字体分类：TrueType 与 PostScript</title>
      <link>https://arxiv.org/abs/2502.00250</link>
      <description><![CDATA[arXiv:2502.00250v1 公告类型：新
摘要：现代字体采用基于矢量的格式，可确保可扩展性而不会损失质量。虽然许多关于字体的深度学习研究都集中在位图格式上，但矢量字体的深度学习仍未得到充分探索。在涉及矢量字体深度学习的研究中，字体表示的选择通常是常规的。然而，字体表示格式是影响字体相关任务中机器学习模型计算性能的因素之一。在这里，我们表明，在基于 Transformer 的矢量字体分类中，基于 PostScript 轮廓的字体表示优于基于 TrueType 轮廓的字体表示。TrueType 轮廓将字符形状表示为点序列及其相关标志，而 PostScript 轮廓将它们表示为命令序列。在之前的研究中，当字体被视为矢量图形的一部分时，主要使用 PostScript 轮廓，而当仅关注字体时，主要使用 TrueType 轮廓。在以往的研究中，使用 PostScript 还是 TrueType 轮廓主要取决于文件格式规范和先例设置，而不是性能考虑。到目前为止，很少有研究比较哪种轮廓格式提供了更好的嵌入表示。我们的研究结果表明，信息聚合对于基于 Transformer 的矢量图形深度学习至关重要，就像语言模型中的标记化和基于位图的图像识别模型中的块划分一样。这一见解为未来矢量图形研究中选择轮廓格式提供了宝贵的指导。]]></description>
      <guid>https://arxiv.org/abs/2502.00250</guid>
      <pubDate>Tue, 04 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>洞察：通过基于情境感知危险检测和边缘案例评估的视觉语言模型提高自动驾驶安全性</title>
      <link>https://arxiv.org/abs/2502.00262</link>
      <description><![CDATA[arXiv:2502.00262v2 公告类型：新
摘要：自动驾驶系统在处理不可预测的边缘情况时面临重大挑战，例如对抗性行人运动、危险的车辆操纵和突然的环境变化。由于传统检测和预测方法的局限性，当前的端到端驾驶模型难以推广到这些罕见事件。为了解决这个问题，我们提出了 INSIGHT（用于广义危险跟踪的语义和视觉输入集成），这是一个分层视觉语言模型 (VLM) 框架，旨在增强危险检测和边缘情况评估。通过使用多模态数据融合，我们的方法整合了语义和视觉表示，从而能够精确解释驾驶场景并准确预测潜在危险。通过对 VLM 进行监督微调，我们使用基于注意力的机制和坐标回归技术优化空间危险定位。 BDD100K 数据集上的实验结果表明，与现有模型相比，危险预测的直观性和准确性有显著提高，泛化性能显著提升。这一进步增强了自动驾驶系统的稳健性和安全性，确保在复杂的现实场景中提高态势感知和潜在决策能力。]]></description>
      <guid>https://arxiv.org/abs/2502.00262</guid>
      <pubDate>Tue, 04 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MCM：用于从蒙版图像中进行有效概念学习的多层概念图</title>
      <link>https://arxiv.org/abs/2502.00266</link>
      <description><![CDATA[arXiv:2502.00266v1 公告类型：新
摘要：自然语言处理中常用的掩蔽策略在概念学习等视觉任务中仍未得到充分探索，传统方法通常依赖于完整图像。然而，使用掩蔽图像可以使感知输入多样化，可能为使用大规​​模 Transformer 模型的概念学习提供显着优势。为此，我们提出了多层概念图 (MCM)，这是第一项基于掩蔽图像设计有效概念学习方法的工作。具体而言，我们通过在不同的编码器和解码器层之间建立相关性来引入非对称概念学习架构，使用来自重建任务的后向梯度更新概念标记。在不同粒度级别上学习到的概念标记有助于通过填补空白来重建掩蔽图像块，或者将重建结果引导到反映特定概念的方向上。此外，我们在广泛的指标中展示了定量和定性结果，表明 MCM 通过对不到 75% 的总图像块进行训练显着降低了计算成本，同时提高了概念预测性能。此外，在潜在空间中编辑特定概念标记可以从掩码图像生成有针对性的图像，从而对齐可见的上下文块和提供的概念。通过进一步调整测试时间掩码比率，我们可以生成一系列将可见块与提供的概念融合在一起的重建，与所选比率成比例。]]></description>
      <guid>https://arxiv.org/abs/2502.00266</guid>
      <pubDate>Tue, 04 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>