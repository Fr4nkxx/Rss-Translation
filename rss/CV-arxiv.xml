<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Mon, 03 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>STAMP：可扩展任务和与模型无关的协作感知</title>
      <link>https://arxiv.org/abs/2501.18616</link>
      <description><![CDATA[arXiv:2501.18616v1 公告类型：新
摘要：感知对于自动驾驶至关重要，但单智能体感知通常受到传感器物理限制的限制，导致在严重遮挡、恶劣天气条件以及检测远距离物体时性能下降。多智能体协作感知提供了一种解决方案，但在集成具有不同模型架构的异构智能体时会出现挑战。为了应对这些挑战，我们提出了 STAMP，这是一种可扩展的任务和模型无关的异构智能体的协作感知管道。STAMP 利用轻量级适配器-转换器对在特定于智能体和共享协议域之间转换鸟瞰图 (BEV) 特征，从而实现高效的特征共享和融合。这种方法最大限度地减少了计算开销，增强了可扩展性并保持了模型安全性。在模拟和真实世界数据集上的实验表明，STAMP 的精度与最先进的模型相当或更高，同时计算成本显着降低。作为首创的任务和模型无关框架，STAMP 旨在推动可扩展且安全的移动系统研究，以实现 5 级自动驾驶。我们的项目页面位于 https://xiangbogaobarry.github.io/STAMP，代码位于 https://github.com/taco-group/STAMP。]]></description>
      <guid>https://arxiv.org/abs/2501.18616</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>车载通信的视觉辅助信道预测：使用 RGB 图像进行接收功率预测的案例研究</title>
      <link>https://arxiv.org/abs/2501.18618</link>
      <description><![CDATA[arXiv:2501.18618v1 公告类型：新 
摘要：6G 的通信场景和信道特性将更加复杂，难以表征。传统的信道预测方法在实现准确性、实用性和通用性之间的最佳平衡方面面临挑战。此外，它们往往无法有效利用环境特征。在融合通信和人工智能作为 6G 关键发展愿景的框架内，实现信道特性的智能预测势在必行。视觉辅助方法已用于除信道预测之外的各种无线通信任务，并已显示出更高的效率和性能。在本文中，我们提出了一种用于毫米波车载通信场景中信道预测的视觉辅助两阶段模型，仅利用 RGB 图像即可实现准确的接收功率预测。首先，我们通过 RGB 摄像机获取传播环境的原始图像。其次，在第一阶段，我们采用三种典型的计算机视觉方法（包括目标检测、实例分割和二值掩码）从原始图像中提取环境信息，在第二阶段，我们根据处理后的图像实现接收功率的预测。在第一阶段和第二阶段，我们分别使用预先训练好的 YOLOv8 和 ResNets，并在数据集上进行微调。最后，我们进行了五次实验来评估所提模型的性能，证明了其可行性、准确性和泛化能力。本文提出的模型为实现车载通信中的智能信道预测提供了新颖的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2501.18618</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FAAGC：基于形状空间理论的自适应测地曲线特征增强</title>
      <link>https://arxiv.org/abs/2501.18619</link>
      <description><![CDATA[arXiv:2501.18619v1 公告类型：新
摘要：深度学习模型已广泛应用于各个领域和行业。然而，由于数据有限且不足，许多领域仍然面临挑战。本文提出了一种在预形状空间中自适应测地曲线 (FAAGC) 的特征增强方法来增加数据。在预形状空间中，具有相同形状的对象位于一个大圆上。因此，我们将深度模型表示投影到预形状空间中并为每个类构建一条测地曲线，即大圆的圆弧。然后通过沿这些测地线路径采样来执行特征增强。大量实验表明，FAAGC 提高了数据稀缺条件下的分类准确率，并且在各种特征类型中具有良好的泛化性。]]></description>
      <guid>https://arxiv.org/abs/2501.18619</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>图像中出现的统计语言学三定律</title>
      <link>https://arxiv.org/abs/2501.18620</link>
      <description><![CDATA[arXiv:2501.18620v1 Announce Type: new 
摘要：图像作为伴随文明演进的产物，与自然语言一样，随着文明的进步而发展。图像不仅在日常生活中随处可见，而且在形态上也受到技术的影响，随着时间的演进而呈现出各种特征。语言是一串代表思想的符号。虽然书面语言通常与文字和声音的紧密结合有关，但作为视觉符号和感知的结合，图像的沟通能力也同样重要。这一点尤其值得注意，因为我们的中枢神经系统接收到的 60% 的感觉输入都来自视觉。鉴于图像固有的符号系统，我们很好奇图像是否也能表现出统计语言学的规律。为了探索这一点，我们从人类思维与视觉感知的关系开始，解读图像是如何通过后者机制形成的。基于前人研究发现预训练的深度卷积神经网络与人类视觉系统高度相关，我们使用 VGG-19 通过每个核定义单词，并计算灰度值大于 90% 的像素数。通过 (a) 对单词频率进行排序，(b) 随机化核出现的顺序并进行相同的单词计数累加，以及 (c) 逐层累加单词计数，我们惊讶地发现，Zipf、Heaps 和 Benford 统计语言学定律也存在于代表不同图像的文本的单词中。]]></description>
      <guid>https://arxiv.org/abs/2501.18620</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>VLMaterial：使用大型视觉语言模型进行程序化材质生成</title>
      <link>https://arxiv.org/abs/2501.18623</link>
      <description><![CDATA[arXiv:2501.18623v1 公告类型：新
摘要：程序材料以功能节点图的形式表示，在计算机图形学中普遍用于逼真的材料外观设计。它们允许用户执行直观和精确的编辑以实现所需的视觉外观。但是，给定输入图像创建程序材料需要专业知识和大量工作。在这项工作中，我们利用将程序材料转换为标准 Python 程序的能力，并微调大型预训练视觉语言模型 (VLM) 以从输入图像生成此类程序。为了实现有效的微调，我们还贡献了一个开源程序材料数据集，并建议通过提示另一个预训练的大型语言模型 (LLM) 来执行程序级增强。通过广泛的评估，我们表明我们的方法在合成和现实世界的示例上都优于以前的方法。]]></description>
      <guid>https://arxiv.org/abs/2501.18623</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>可变形 Beta Splatting</title>
      <link>https://arxiv.org/abs/2501.18630</link>
      <description><![CDATA[arXiv:2501.18630v1 公告类型：新
摘要：3D 高斯溅射 (3DGS) 通过实现实时渲染，实现了先进的辐射场重建。然而，它依赖于高斯核进行几何处理和低阶球面谐波 (SH) 进行颜色编码，这限制了它捕获复杂几何形状和多种颜色的能力。我们引入了可变形 Beta 溅射 (DBS)，这是一种可变形且紧凑的方法，可增强几何和颜色表示。DBS 用可变形 Beta 核取代高斯核，后者提供有界支持和自适应频率控制，以更高的保真度捕获精细的几何细节，同时实现更好的内存效率。此外，我们将 Beta 核扩展到颜色编码，这有助于改进漫反射和镜面反射分量的表示，与基于 SH 的方法相比，可获得更优的结果。此外，与之前依赖高斯特性的致密化技术不同，我们通过数学证明，仅调整正则化不透明度即可确保分布保持的马尔可夫链蒙特卡罗 (MCMC)，与溅射核类型无关。实验结果表明，DBS 实现了最先进的视觉质量，同时仅利用了 45% 的参数，渲染速度比基于 3DGS 的方法快 1.5 倍。值得注意的是，基于溅射的方法首次超越了最先进的神经辐射场，凸显了 DBS 在实时辐射场渲染方面的卓越性能和效率。]]></description>
      <guid>https://arxiv.org/abs/2501.18630</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>了解注视点渲染中的深度感知</title>
      <link>https://arxiv.org/abs/2501.18635</link>
      <description><![CDATA[arXiv:2501.18635v1 公告类型：新
摘要：实时虚拟和增强现实的真正愿景是在沉浸式显示器上完整地再现我们的视觉现实。为此，注视点渲染利用人类周边视觉的空间敏锐度的局限性，将计算资源分配给中央凹，同时降低周边的质量。这些方法通常源自对人类视觉系统的空间分辨率及其感知周边模糊的能力的研究，从而有可能实时实现高空间质量。然而，模糊对依赖于亮度对比度的其他视觉线索（例如深度）的影响在很大程度上仍未得到探索。理解这种相互作用至关重要，因为准确的深度表示是视觉真实感的一个基本方面。在本文中，我们首次进行了评估，探索了注视点渲染对立体深度感知的影响。我们设计了一个心理视觉实验来定量研究周边模糊对深度感知的影响。我们的分析表明，立体视觉敏锐度不会受到高水平周边模糊的影响（甚至会有所改善）。根据我们的研究，我们推导出一个简单的感知模型，该模型可以确定不影响立体视觉敏锐度的注视点数量。此外，我们还根据文献中报道的常见注视点实践对该模型进行了分析。研究结果表明，注视点渲染不会影响立体深度感知，即使注视点强度比常用的注视点强度高出 2 倍，立体视觉敏锐度也不会受到影响。最后，我们进行了一项验证实验，并表明我们的研究结果适用于复杂的自然刺激。]]></description>
      <guid>https://arxiv.org/abs/2501.18635</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用基础视觉转换器的稳健特征进行材料微观结构-属性关系的机器学习</title>
      <link>https://arxiv.org/abs/2501.18637</link>
      <description><![CDATA[arXiv:2501.18637v1 公告类型：新
摘要：从数据中机器学习微观结构-属性关系是计算材料科学中的一种新兴方法。大多数现有的机器学习工作都集中在为每种微观结构-属性关系开发特定于任务的模型上。我们建议利用预先训练的基础视觉转换器来提取与任务无关的微观结构特征，然后对微观结构相关的属性进行轻量级机器学习。我们在两个机器学习案例研究中使用预先训练的最先进的视觉转换器（CLIP、DINOV2、SAM）展示了我们的方法：（i）基于模拟数据的两相微观结构的弹性模量；（ii）基于文献中发表的实验数据的镍基和钴基高温合金的维氏硬度。我们的结果表明，基础视觉变换器具有实现稳健的微结构表示和微结构-属性关系高效机器学习的潜力，而无需昂贵的特定任务训练或定制深度学习模型的微调。]]></description>
      <guid>https://arxiv.org/abs/2501.18637</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用神经网络对流体进行直接位移场估计的图像测速技术</title>
      <link>https://arxiv.org/abs/2501.18641</link>
      <description><![CDATA[arXiv:2501.18641v1 公告类型：新
摘要：粒子图像测速 (PIV) 是实验流体力学研究的重要工具。已经提出了几种稳健的方法来从图像中估计速度场，但是，仍然需要替代方法来提高结果的空间分辨率。这项工作提出了一种使用神经网络和光流方程来估计流体流场的新方法，以预测连续图像之间的位移矢量。结果是位移的连续表示，可以在图像的全空间分辨率上进行评估。该方法已在合成和实验图像上得到验证。在瞬时速度场的估计以及确定的时间平均湍流量和功率谱密度方面获得了准确的结果。所提出的方法与以前使用机器学习完成此任务的尝试不同：它不需要任何先前的训练，可以直接用于任何一对图像。]]></description>
      <guid>https://arxiv.org/abs/2501.18641</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DebiasPI：通过文本到图像生成模型的快速迭代实现推理时去偏</title>
      <link>https://arxiv.org/abs/2501.18642</link>
      <description><![CDATA[arXiv:2501.18642v1 公告类型：新
摘要：道德干预提示已成为一种对抗文本到图像生成 AI 模型的人口统计学偏见的工具。现有的解决方案要么需要重新训练模型，要么难以生成反映性别和种族所需分布的图像。我们提出了一种称为 DebiasPI 的推理时间流程，用于通过提示迭代进行去偏倚，该流程通过使用户能够控制图像生成中个人人口统计属性的分布来提供及时干预。DebiasPI 通过探测模型的内部状态或使用外部属性分类器来跟踪已生成的属性。它的控制循环引导文本到图像模型选择尚未充分表示的属性，借助 DebiasPI，我们能够创建具有同等种族和性别表示的图像，以可视化新闻标题的挑战性概念。我们还对年龄、体型、职业和肤色等属性进行了实验，并测量了当我们的干预提示针对不相关属性类型的分布时属性如何变化。例如，我们发现，如果要求文本到图像模型平衡种族代表性，性别代表性会有所改善，但肤色会变得不那么多样化。尝试使用各种干预提示覆盖各种肤色表明，该模型很难生成最苍白的肤色。我们进行了各种消融研究，在这些研究中，我们删除了 DebiasPI 的属性控制，结果揭示了该模型倾向于生成年轻的男性角色。它有时通过生成双面板图像来可视化职业成功，其中成功前的黑皮肤人成功后皮肤变白，或者将性别从成功前的女性转变为成功后的男性，从而进一步激发使用 DebiasPI 进行道德干预提示。]]></description>
      <guid>https://arxiv.org/abs/2501.18642</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于增强现实的鞋子 3D 重建</title>
      <link>https://arxiv.org/abs/2501.18643</link>
      <description><![CDATA[arXiv:2501.18643v1 公告类型：新
摘要：本文介绍了一种基于移动的解决方案，通过 3D 建模和增强现实 (AR) 增强在线鞋类购物，充分利用 3D 高斯分布的效率。该框架解决了静态 2D 图像的局限性，从 2D 图像生成逼真的 3D 鞋类模型，平均峰值信噪比 (PSNR) 为 0.32，并支持通过智能手机进行沉浸式 AR 交互。创建了一个包含 3120 张图像的自定义鞋类分割数据集，其中性能最佳的分割模型实现了 0.95 的交并比 (IoU) 得分。本文展示了 3D 建模和 AR 通过提供逼真的虚拟交互来彻底改变在线购物的潜力，并适用于更广泛的时尚类别。]]></description>
      <guid>https://arxiv.org/abs/2501.18643</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用多模态 LLM 进行深度学习的图像、文本和语音数据增强：一项调查</title>
      <link>https://arxiv.org/abs/2501.18648</link>
      <description><![CDATA[arXiv:2501.18648v1 公告类型：新
摘要：在过去五年中，研究已经从传统的机器学习 (ML) 和深度学习 (DL) 方法转向利用大型语言模型 (LLM)（包括多模态）进行数据增强，以增强泛化能力，并对抗训练深度卷积神经网络时的过度拟合。然而，虽然现有的调查主要集中于 ML 和 DL 技术或有限的模态（文本或图像），但在解决基于 LLM 的方法的最新进展和多模态应用方面仍然存在差距。本调查通过探索利用多模态 LLM 增强图像、文本和音频数据的最新文献来填补这一空白，提供对这些过程的全面了解。我们概述了基于 LLM 的图像、文本和语音增强中采用的各种方法，并讨论了当前方法中发现的局限性。此外，我们从文献中确定了这些限制的潜在解决方案，以提高使用多模态 LLM 进行数据增强实践的有效性。这项调查为未来的研究奠定了基础，旨在改进和扩展多模态 LLM 的使用，以提高深度学习应用的数据集质量和多样性。（调查论文 GitHub Repo：https://github.com/WSUAgRobotics/data-aug-multi-modal-llm。关键词：LLM 数据增强、LLM 文本数据增强、LLM 图像数据增强、LLM 语音数据增强、音频增强、语音增强、chatGPT 数据增强、DeepSeek R1 文本数据增强、DeepSeek R1 图像增强、使用 LLM 进行图像增强、使用 LLM 进行文本增强、用于深度学习应用的 LLM 数据增强）]]></description>
      <guid>https://arxiv.org/abs/2501.18648</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用多模态 LLaMA 3.2 进行参数高效的 LoRA 微调，实现高精度心电图图像解释</title>
      <link>https://arxiv.org/abs/2501.18670</link>
      <description><![CDATA[arXiv:2501.18670v1 公告类型：新
摘要：心电图 (ECG) 解释是心脏诊断的基石。本文探讨了一种使用多模态 LLaMA 3.2 模型增强 ECG 图像解释的实用方法。我们使用了一种参数高效的微调策略，即低秩自适应 (LoRA)，专门用于提高模型理解 ECG 图像的能力，并在各种心脏状况下获得更好的结果。我们的方法是针对 ECG 分析量身定制的，并利用 ECGInstruct，这是一个拥有 100 万个样本的大规模指令数据集。该数据集是一个丰富的合成 ECG 图像集合，由来自受信任的开源存储库（如 MIMIC-IV ECG 和 PTB-XL）的原始 ECG 数据生成。ECGInstruct 中的每个 ECG 图像都附带专家编写的问题和详细答案，涵盖各种 ECG 解释场景，包括心肌梗死和传导紊乱等复杂心脏病。我们的微调方法通过集成低秩自适应技术有效地适应了 LLaMA 3.2 模型（基于 LLaMA 3），通过仅更新一小部分参数（特别是忽略“lm_head”和“embed_tokens”层）来关注效率。本文详细介绍了模型设置、我们高效的微调方法和实施细节。我们通过大量实验提供了全面的评估，证明了我们的方法在各种 ECG 解释任务中的有效性。结果令人信服地表明，我们的参数高效的 LoRA 微调在 ECG 图像解释中取得了出色的性能，明显优于基线模型，并且在识别各种心脏异常（包括来自 PTB-XL 数据集的 70 多种情况）方面达到了与传统基于 CNN 的方法相当或超过其准确度。]]></description>
      <guid>https://arxiv.org/abs/2501.18670</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于建模探测器响应的点云非配对平移</title>
      <link>https://arxiv.org/abs/2501.18674</link>
      <description><![CDATA[arXiv:2501.18674v1 公告类型：新
摘要：对探测器响应进行建模是时间投影室面临的一个关键挑战。我们将这个问题视为一个不成对的点云转换任务，介于从模拟和实验运行中收集的数据之间。有效的转换可以帮助消除噪声和构建高保真模拟器。基于最近在扩散概率模型方面的工作，我们提出了一个执行这种映射的新框架。我们证明了我们的方法在合成领域和来自主动目标时间投影室的数据中的成功。]]></description>
      <guid>https://arxiv.org/abs/2501.18674</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>人类 Re-ID 遇见 LVLM：我们能期待什么？</title>
      <link>https://arxiv.org/abs/2501.18698</link>
      <description><![CDATA[arXiv:2501.18698v1 公告类型：新
摘要：大型视觉语言模型 (LVLM) 被视为从内容生成到虚拟助手以及多模式搜索或检索等各种任务的突破性进展。然而，对于许多此类应用，这些方法的性能受到了广泛批评，尤其是与每个特定领域中最先进的方法和技术相比时。在这项工作中，我们比较了领先的大型视觉语言模型在人类重新识别任务中的表现，以专门为该问题设计的最先进的 AI 模型所获得的性能为基准。我们使用著名的 Market1501 数据集，将 ChatGPT-4o、Gemini-2.0-Flash、Claude 3.5 Sonnet 和 Qwen-VL-Max 的结果与基线 ReID PersonViT 模型进行比较。我们的评估流程包括数据集管理、快速工程和指标选择，以评估模型的性能。结果从许多不同的角度进行分析：相似度得分、分类准确度和分类指标，包括精度、召回率、F1 得分和曲线下面积 (AUC)。我们的结果证实了 LVLM 的优势，但也证实了它们的严重局限性，这些局限性往往会导致灾难性的答案，应该成为进一步研究的范围。最后，我们推测一些进一步的研究应该融合传统和 LVLM，以结合两种技术的优势并实现性能的稳步提升。]]></description>
      <guid>https://arxiv.org/abs/2501.18698</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>