<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Thu, 12 Dec 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>Mogo：用于高质量 3D 人体运动生成的 RQ 分层因果变换器</title>
      <link>https://arxiv.org/abs/2412.07797</link>
      <description><![CDATA[arXiv:2412.07797v1 公告类型：新 
摘要：在文本到运动生成领域，与 GPT 型自回归模型 (T2M-GPT) 相比，Bert 型掩蔽模型 (MoMask, MMM) 目前可产生更高质量的输出。然而，这些 Bert 型模型通常缺乏视频游戏和多媒体环境中应用所需的流式输出能力，这是 GPT 型模型固有的特性。此外，它们在分布外生成方面表现出较弱的性能。为了在利用 GPT 型结构的同时超越 BERT 型模型的质量，而无需添加使缩放数据复杂化的额外细化模型，我们提出了一种新颖的架构 Mogo（Motion Only Generate Once），它通过训练单个 Transformer 模型来生成高质量逼真的 3D 人体运动。Mogo 仅包含两个主要组件：1）RVQ-VAE，一种分层残差矢量量化变分自动编码器，可高精度地离散化连续运动序列； 2）分层因果变换器，负责以自回归方式生成基础运动序列，同时推断不同层之间的残差。实验结果表明，Mogo 可以生成长达 260 帧（13 秒）的连续循环运动序列，超过了 HumanML3D 等现有数据集 196 帧（10 秒）的长度限制。在 HumanML3D 测试集上，Mogo 的 FID 得分为 0.079，优于 GPT 型模型 T2M-GPT（FID = 0.116）、AttT2M（FID = 0.112）和 BERT 型模型 MMM（FID = 0.080）。此外，我们的模型在分布外生成方面取得了最佳的定量性能。]]></description>
      <guid>https://arxiv.org/abs/2412.07797</guid>
      <pubDate>Thu, 12 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>学习纠正：视觉常识推理干扰项的可解释反馈生成</title>
      <link>https://arxiv.org/abs/2412.07801</link>
      <description><![CDATA[arXiv:2412.07801v1 公告类型：新 
摘要：大型多模态模型 (LMM) 在视觉常识推理 (VCR) 任务中表现出色，该任务旨在根据图像中的视觉常识回答多项选择题。然而，LMM 在干扰项中出现潜在视觉常识错误时纠正它们的能力尚未得到充分探索。从人类教师如何设计具有挑战性的干扰项来测试学生对概念或技能的理解并帮助他们识别和纠正对答案的错误中汲取灵感，我们是 LMM 模拟这一错误纠正过程的先驱研究。为此，我们使用 GPT-4 作为“老师”来收集可解释反馈数据集 VCR-DF 进行错误纠正，作为评估 LMM 识别误解和阐明 VCR 干扰项中错误原因的能力的基准，以得出最终答案。此外，我们提出了基于 LMM 的教学专家指导反馈生成 (PEIFG) 模型，以结合可学习的专家提示和多模式教学作为反馈生成的指导。实验结果表明，我们的 PEIFG 明显优于现有的 LMM。我们相信我们的基准为评估 LMM 的能力提供了一个新的方向。]]></description>
      <guid>https://arxiv.org/abs/2412.07801</guid>
      <pubDate>Thu, 12 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>语言模型作为视觉解释器</title>
      <link>https://arxiv.org/abs/2412.07802</link>
      <description><![CDATA[arXiv:2412.07802v1 公告类型：新
摘要：在本文中，我们提出了语言模型作为视觉解释器 LVX，这是一种使用树状结构语言解释来解释视觉模型内部工作原理的系统方法，无需进行模型训练。我们战略的核心是视觉模型和 LLM 之间的协作来制定解释。一方面，LLM 被用来描绘分层的视觉属性，同时，文本到图像 API 检索与这些文本概念最一致的图像。通过将收集到的文本和图像映射到视觉模型的嵌入空间，我们构建了一个层次结构的视觉嵌入树。通过使用语言模板查询 LLM，可以动态地修剪和生长这棵树，从而根据模型定制解释。这样的方案使我们能够无缝地合并新属性，同时根据模型的表示消除不需要的概念。当应用于测试样本时，我们的方法以属性树的形式提供人类可理解的解释。除了解释之外，我们还通过在生成的概念层次结构上校准视觉模型来重新训练视觉模型，从而使模型能够融入精炼的视觉属性知识。为了评估我们方法的有效性，我们引入了新的基准并进行了严格的评估，证明了其合理性、可靠性和稳定性。]]></description>
      <guid>https://arxiv.org/abs/2412.07802</guid>
      <pubDate>Thu, 12 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>3DSRBench：全面的 3D 空间推理基准</title>
      <link>https://arxiv.org/abs/2412.07825</link>
      <description><![CDATA[arXiv:2412.07825v1 公告类型：新 
摘要：3D 空间推理是分析和解释 3D 空间内物体的位置、方向和空间关系的能力。这使模型能够全面了解 3D 场景，从而使其适用于更广泛的领域，例如自动导航、机器人技术和 AR/VR。虽然大型多模态模型 (LMM) 在广泛的图像和视频理解任务中取得了显着进展，但它们对各种自然图像执行 3D 空间推理的能力研究较少。在这项工作中，我们提出了第一个全面的 3D 空间推理基准 3DSRBench，其中包含 12 种问题类型的 2,772 个手动注释的视觉问答对。我们通过平衡数据分布和采用新颖的 FlipEval 策略对 3D 空间推理能力进行了稳健而彻底的评估。为了进一步研究 3D 空间推理相对于 12 种问题的稳健性。相机 3D 视点，我们的 3DSRBench 包括两个子集，其中包含对具有常见和不常见视点的成对图像的 3D 空间推理问题。我们对各种开源和专有 LMM 进行了基准测试，揭示了它们在 3D 感知的各个方面（例如高度、方向、位置和多对象推理）的局限性，以及它们在具有不常见相机视点的图像上的性能下降。我们的 3DSRBench 提供了有关具有强大 3D 推理能力的 LMM 未来发展的宝贵发现和见解。我们的项目页面和数据集可在 https://3dsrbench.github.io 上找到。]]></description>
      <guid>https://arxiv.org/abs/2412.07825</guid>
      <pubDate>Thu, 12 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于事件的视觉低延迟可扩展流媒体</title>
      <link>https://arxiv.org/abs/2412.07889</link>
      <description><![CDATA[arXiv:2412.07889v1 公告类型：新
摘要：最近，我们见证了用于高速、低功耗视频捕获的新型“基于事件”相机传感器的兴起。这些传感器不是记录离散图像帧，而是仅当给定像素的亮度变化超过某个阈值时，以微秒精度输出异步“事件”元组。尽管这些传感器已经实现了引人注目的新型计算机视觉应用，但这些应用通常需要昂贵、耗电的 GPU 系统，这使得它们无法部署在针对事件相机进行了优化的低功耗设备上。虽然接收器驱动的速率自适应是现代视频流解决方案的关键特性，但在基于事件的视觉系统领域，这一主题尚未得到充分探索。在现实世界的事件相机数据集上，我们首先证明最先进的物体检测应用程序能够抵御剧烈的数据丢失，并且这种丢失可能会在每个时间窗口的末尾加权。然后，我们提出了一种基于 Media Over QUIC 的可扩展事件数据流方法，优先考虑对象检测性能和低延迟。应用服务器可以同时接收多个流中的互补事件数据，并根据需要丢弃流以保持一定的延迟。在小型网络上端到端传输的延迟目标为 5 毫秒的情况下，我们观察到检测 mAP 的平均减少量低至 0.36。在更宽松的延迟目标 50 毫秒的情况下，我们观察到平均 mAP 减少量低至 0.19。]]></description>
      <guid>https://arxiv.org/abs/2412.07889</guid>
      <pubDate>Thu, 12 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Pix2Poly：一种从遥感影像中提取端到端多边形建筑物足迹的序列预测方法</title>
      <link>https://arxiv.org/abs/2412.07899</link>
      <description><![CDATA[arXiv:2412.07899v1 公告类型：新
摘要：从遥感数据中提取建筑物足迹多边形对于重建、导航和制图等多项城市理解任务至关重要。尽管该领域取得了重大进展，但提取准确的多边形建筑物足迹仍然是一个悬而未决的问题。在本文中，我们介绍了 Pix2Poly，这是一种基于注意力的端到端可训练和可区分的深度神经网络，能够直接生成环形图格式的显式高质量建筑物足迹。Pix2Poly 采用生成编码器-解码器转换器来生成一系列图顶点标记，其连接信息由最佳匹配网络学习。与以前的图学习方法相比，我们的方法是一种真正的端到端可训练方法，可以提取高质量的建筑物足迹和道路网络，而无需复杂、计算密集的栅格损失函数和复杂的训练管道。在对多个复杂且具有挑战性的数据集进行 Pix2Poly 评估后，我们报告称，Pix2Poly 在多个矢量形状质量指标方面优于最先进的方法，同时它是一种完全明确的方法。我们的代码可在 https://github.com/yeshwanth95/Pix2Poly 上找到。]]></description>
      <guid>https://arxiv.org/abs/2412.07899</guid>
      <pubDate>Thu, 12 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>适用于动态和噪声网络的具有掩蔽变压器的鲁棒多描述神经视频编解码器</title>
      <link>https://arxiv.org/abs/2412.07922</link>
      <description><![CDATA[arXiv:2412.07922v1 公告类型：新 
摘要：多描述编码 (MDC) 是一种有前途的抗错误源编码方法，特别适用于具有多条（但嘈杂且不可靠）路径的动态网络。然而，传统的 MDC 视频编解码器存在架构繁琐、可扩展性差、抗损失能力有限和压缩效率较低等问题。因此，MDC 从未被广泛采用。受神经视频编解码器潜力的启发，本文重新思考了 MDC 设计。我们提出了一种新颖的 MDC 视频编解码器 NeuralMDC，展示了经过掩蔽标记预测训练的双向变换器如何大大简化 MDC 视频编解码器的设计。为了压缩视频，NeuralMDC 首先将每帧标记为其潜在表示，然后拆分潜在标记以创建包含相关信息的多个描述。 NeuralMDC 不使用运动预测和扭曲操作，而是训练双向掩蔽变换器来模拟潜在表示的时空依赖性，并根据过去预测当前表示的分布。预测的分布用于独立地对每个描述进行熵编码并推断任何可能丢失的标记。大量实验表明，NeuralMDC 实现了最先进的损失恢复能力，同时将压缩效率的牺牲降到最低，远远优于现有的最佳基于残差编码的容错神经视频编解码器。]]></description>
      <guid>https://arxiv.org/abs/2412.07922</guid>
      <pubDate>Thu, 12 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PGRID：利用高分辨率航空影像进行非正规开发中的电网重建</title>
      <link>https://arxiv.org/abs/2412.07944</link>
      <description><![CDATA[arXiv:2412.07944v1 公告类型：新
摘要：截至 2023 年，全球流离失所者人数达到创纪录的 1.17 亿，是十年前的两倍多 [22]。其中，3200 万人是联合国难民署授权的难民，870 万人居住在难民营。这些人群面临的一个关键问题是缺乏电力，全球 870 万难民和流离失所者中，80% 的人依靠传统生物质做饭，缺乏可靠的电力来完成做饭和给手机充电等基本任务。通常，收集柴火的负担落在妇女和儿童身上，他们经常前往 20 公里外的危险地区，这增加了他们的脆弱性。[7]
电力接入可以大大缓解这些挑战，但一个主要障碍是缺乏准确的电网基础设施地图，特别是在难民营等资源受限的环境中，而这是能源接入规划所必需的。现有的电网地图通常已过时、不完整，或依赖于昂贵而复杂的技术，从而限制了其实用性。为了解决这一问题，PGRID 是一种基于应用的新型方法，它利用高分辨率航空影像来检测电线杆并分割电线，从而创建精确的电网地图。PGRID 在肯尼亚的图尔卡纳地区进行了测试，具体是卡库马和卡洛贝耶营地，占地 84 平方公里，居住着 20 多万居民。
我们的研究结果表明，PGRID 可提供高保真度的电网地图，尤其是在非规划定居点，电线杆检测和线路分割的 F1 得分分别为 0.71 和 0.82。这项研究重点介绍了一种利用开放数据和有限标签来改善非规划定居点电网地图的实际应用，在这些定居点，越来越多的流离失所者迫切需要可持续的能源基础设施解决方案。]]></description>
      <guid>https://arxiv.org/abs/2412.07944</guid>
      <pubDate>Thu, 12 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>平衡共享和特定任务的表示：深度感知视频全景分割的混合方法</title>
      <link>https://arxiv.org/abs/2412.07966</link>
      <description><![CDATA[arXiv:2412.07966v1 公告类型：新
摘要：在这项工作中，我们提出了 Multiformer，一种基于掩模变换器范式的深度感知视频全景分割 (DVPS) 的新方法。我们的方法学习在分割、单目深度估计和对象跟踪子任务之间共享的对象表示。与最近逐步细化通用对象表示的统一方法相比，我们提出了一种混合方法，在每个解码器块中使用特定于任务的分支，最终将它们融合到块接口处的共享表示中。在 Cityscapes-DVPS 和 SemKITTI-DVPS 数据集上进行的大量实验表明，Multiformer 在所有 DVPS 指标中都实现了最先进的性能，远远优于以前的方法。借助 ResNet-50 主干，Multiformer 超越了之前的最佳结果 3.0 DVPQ 点，同时还提高了深度估计精度。使用 Swin-B 主干，Multiformer 进一步将性能提高了 4.0 DVPQ 点。Multiformer 还为多任务解码器架构的设计提供了宝贵的见解。]]></description>
      <guid>https://arxiv.org/abs/2412.07966</guid>
      <pubDate>Thu, 12 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>TTVD：基于 Voronoi 图的测试时间自适应几何框架</title>
      <link>https://arxiv.org/abs/2412.07980</link>
      <description><![CDATA[arXiv:2412.07980v1 公告类型：新
摘要：由于训练数据存在常见的分布偏移，深度学习模型在部署到真实数据上时经常难以泛化。测试时自适应 (TTA) 是一种在推理时用于解决此问题的新兴方案。在 TTA 中，模型在对测试数据进行预测时同时进行在线调整。基于邻居的方法最近引起了关注，其中原型嵌入提供位置信息以缓解训练和测试数据之间的特征偏移。然而，由于它们固有的简单性限制，它们通常难以学习有用的模式并遇到性能下降。为了应对这一挑战，我们从几何角度研究了 TTA 问题。我们首先揭示了基于邻居的方法的底层结构与 Voronoi 图一致，Voronoi 图是一种用于空间划分的经典计算几何模型。基于这一观察，我们提出了通过 Voronoi 图指导 (TTVD) 进行测试时调整，这是一个利用这种几何特性优势的新框架。具体来说，我们探索了两个关键结构：1） 聚类诱导 Voronoi 图 (CIVD)：它整合了自监督和基于熵的方法的联合贡献，以提供更丰富的信息。2） 功率图 (PD)：Voronoi 图的广义版本，通过为每个 Voronoi 单元分配权重来细化分区。我们在 CIFAR-10-C、CIFAR-100-C、ImageNet-C 和 ImageNet-R 上在严格的同行评审设置下进行的实验表明，与最先进的方法相比，TTVD 取得了显着的改进。此外，大量的实验结果还探讨了批次大小和类别不平衡的影响，这是实际应用中常见的两种情况。这些分析进一步验证了我们提出的框架的稳健性和适应性。]]></description>
      <guid>https://arxiv.org/abs/2412.07980</guid>
      <pubDate>Thu, 12 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于扩散的注意力扭曲，实现一致的 3D 场景编辑</title>
      <link>https://arxiv.org/abs/2412.07984</link>
      <description><![CDATA[arXiv:2412.07984v1 公告类型：新
摘要：我们提出了一种使用扩散模型进行 3D 场景编辑的新方法，旨在确保各个视角的视图一致性和真实感。我们的方法利用从单个参考图像中提取的注意特征来定义预期的编辑。通过将这些特征与从高斯溅射深度估计中得出的场景几何图形对齐，这些特征在多个视图之间被扭曲。将这些扭曲的特征注入其他视点可以实现编辑的连贯传播，从而实现 3D 空间中的高保真度和空间对齐。广泛的评估证明了我们的方法在生成 3D 场景的多功能编辑方面的有效性，与现有方法相比，显著提高了场景操作的能力。项目页面：\url{https://attention-warp.github.io}]]></description>
      <guid>https://arxiv.org/abs/2412.07984</guid>
      <pubDate>Thu, 12 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用平铺和缩放增强对人脸检测器的远程对抗补丁攻击</title>
      <link>https://arxiv.org/abs/2412.07996</link>
      <description><![CDATA[arXiv:2412.07996v1 Announce Type: new 
摘要：本文讨论了针对人脸检测器的远程对抗补丁（RAP）的攻击可行性。针对人脸检测器的RAP与针对一般物体检测器的RAP类似，但前者在攻击过程中存在多个问题，而后者没有。（1）可以检测各种尺度的物体。特别是，CNN在特征提取过程中卷积的小物体的面积很小，因此影响推理结果的面积也很小。（2）它是一个二分类，因此类之间的特征差距很大。这使得通过将推理结果引导到不同的类来攻击推理结果变得困难。在本文中，我们针对每个问题提出了一种新的补丁放置方法和损失函数。与针对一般物体检测器的补丁相比，针对所提出的人脸检测器的补丁表现出更好的检测阻碍效果。]]></description>
      <guid>https://arxiv.org/abs/2412.07996</guid>
      <pubDate>Thu, 12 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MAGIC：通过协作 LLM 代理掌握情境中的物理对抗生成</title>
      <link>https://arxiv.org/abs/2412.08014</link>
      <description><![CDATA[arXiv:2412.08014v1 公告类型：新
摘要：驾驶场景中的物理对抗攻击可能会暴露视觉感知模型中的关键漏洞。然而，由于现实世界背景的多样性以及保持视觉自然性的要求，开发此类攻击仍然具有挑战性。基于这一挑战，我们将物理对抗攻击重新表述为一次性补丁生成问题。我们的方法通过考虑特定场景上下文的深度生成模型生成对抗补丁，从而能够在匹配环境中直接物理部署。主要挑战在于同时实现两个目标：生成有效误导物体检测系统的对抗补丁，同时确定场景中适合上下文的位置。我们提出了 MAGIC（掌握上下文中的物理对抗生成），这是一个由多模态 LLM 代理驱动的新型框架，以应对这些挑战。MAGIC 通过语言和视觉功能的协同作用自动理解场景上下文并协调对抗补丁生成。 MAGIC 协调了三个专门的 LLM 代理：adv-patch 生成代理 (GAgent) 通过针对文本到图像模型的战略提示工程掌握欺骗性补丁的创建。adv-patch 部署代理 (DAgent) 通过根据场景理解确定最佳放置策略来确保上下文连贯性。自我检查代理 (EAgent) 通过对这两个过程进行关键监督和迭代改进来完成这三部曲。我们在数字和物理层面验证了我们的方法，即 nuImage 和手动捕获的真实场景，其中统计和视觉结果都证明我们的 MAGIC 功能强大且有效地攻击广泛使用的对象检测系统。]]></description>
      <guid>https://arxiv.org/abs/2412.08014</guid>
      <pubDate>Thu, 12 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>NeRF-NQA：针对 NeRF 和神经视图合成方法生成的场景的无参考质量评估</title>
      <link>https://arxiv.org/abs/2412.08029</link>
      <description><![CDATA[arXiv:2412.08029v1 公告类型：新
摘要：神经视图合成 (NVS) 已证明能够使用具有稀疏视图的图像集生成高保真密集视点视频。然而，现有的质量评估方法（如 PSNR、SSIM 和 LPIPS）并不适用于 NVS 和 NeRF 变体合成的密集视点场景，因此，它们通常无法捕捉感知质量，包括 NVS 合成场景的空间和角度方面。此外，缺乏密集的地面真实视图使得对 NVS 合成场景进行完整的参考质量评估具有挑战性。例如，LLFF 等数据集仅提供稀疏图像，不足以进行完整的全参考评估。为了解决上述问题，我们提出了 NeRF-NQA，这是第一个针对从 NVS 和 NeRF 变体合成的密集观察场景的无参考质量评估方法。 NeRF-NQA 采用联合质量评估策略，结合视图和点方法，评估 NVS 生成场景的质量。视图方法评估每个单独合成视图的空间质量和整体视图间一致性，而点方法则侧重于场景表面点的角度质量及其复合点间质量。进行了广泛的评估，将 NeRF-NQA 与 23 种主流视觉质量评估方法（来自图像、视频和光场评估领域）进行比较。结果表明，NeRF-NQA 明显优于现有的评估方法，并且在评估没有参考的 NVS 合成场景方面表现出明显的优势。本文的实现可在 https://github.com/VincentQQu/NeRF-NQA 获得。]]></description>
      <guid>https://arxiv.org/abs/2412.08029</guid>
      <pubDate>Thu, 12 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>视频语义分割中的静态-动态类级感知一致性</title>
      <link>https://arxiv.org/abs/2412.08034</link>
      <description><![CDATA[arXiv:2412.08034v1 公告类型：新
摘要：视频语义分割（VSS）已广泛应用于同步定位和地图绘制、自动驾驶和监控等许多领域。其核心挑战是如何利用时间信息实现更好的分割。以前的努力主要集中在像素级静态-动态上下文匹配，利用光流和注意机制等技术。相反，本文重新考虑了类级别的静态-动态上下文，并提出了一种新颖的静态-动态类级感知一致性（SD-CPC）框架。在该框架中，我们提出了具有对比学习的多变量类原型和静态-动态语义对齐模块。前者为模型提供类级约束，获得个性化的类间特征和多样化的类内特征。后者首先建立帧内空间多尺度和多层次相关性以实现静态语义对齐。然后，基于跨帧静态感知差异，执行两阶段跨帧选择性聚合以实现动态语义对齐。同时，我们提出了一种基于窗口的注意力图计算方法，利用跨帧聚合过程中注意力点的稀疏性来降低计算成本。在 VSPW 和 Cityscapes 数据集上进行的大量实验表明，所提出的方法优于最先进的方法。我们的实现将在 GitHub 上开源。]]></description>
      <guid>https://arxiv.org/abs/2412.08034</guid>
      <pubDate>Thu, 12 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>