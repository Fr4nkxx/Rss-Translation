<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>CS.CV更新arxiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.cv更新arxiv.org e-print存档。</description>
    <lastBuildDate>Wed, 19 Mar 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>情境感知的多模式AI揭示了五个世纪的艺术进化中的隐藏途径</title>
      <link>https://arxiv.org/abs/2503.13531</link>
      <description><![CDATA[ARXIV：2503.13531V1公告类型：新 
摘要：多模式生成AI的兴起正在改变技术和艺术的交集，从而更深入地了解大型艺术品。尽管它的创造力已广泛探索，但它在潜在空间中代表艺术品的潜力仍然没有散发出来。我们使用尖端生成的AI，特别是稳定的扩散，通过提取两种类型的潜在信息，以模型：形式方面（例如颜色）和上下文方面（例如主题）来分析500年的西方绘画。我们的发现表明，上下文信息比正式元素更成功地区分艺术时期，风格和个人艺术家。此外，使用从绘画中提取的上下文关键字，我们展示了艺术表达如何随社会变化而演变。我们的生成实验将前瞻性环境注入历史艺术品中，成功地重现了艺术品的进化轨迹，突出了社会与艺术之间相互互动的重要性。这项研究表明，多模式AI如何通过整合时间，文化和历史背景来扩展传统的形式分析。]]></description>
      <guid>https://arxiv.org/abs/2503.13531</guid>
      <pubDate>Wed, 19 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>对文本到图像扩散模型中视觉概念挖掘的全面调查</title>
      <link>https://arxiv.org/abs/2503.13576</link>
      <description><![CDATA[ARXIV：2503.13576V1公告类型：新 
摘要：文本对图像扩散模型在产生文本提示中产生高质量的不同图像方面取得了重大进步。但是，文本信号的固有局限性通常阻止这些模型完全捕获特定的概念，从而降低了它们的可控性。为了解决这个问题，几种方法已结合了个性化技术，并利用参考图像来挖掘互补文本输入并增强文本到图像扩散模型的可控性的视觉概念表示。尽管有这些进展，但对视觉概念挖掘的全面，系统的探索仍然有限。在本文中，我们将现有研究分为四个关键领域：概念学习，概念擦除，概念分解和概念组合。该分类为视觉概念挖掘（VCM）技术的基本原理提供了宝贵的见解。此外，我们确定了关键挑战，并提出了未来的研究方向，以推动这一重要而有趣的领域前进。]]></description>
      <guid>https://arxiv.org/abs/2503.13576</guid>
      <pubDate>Wed, 19 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>看到未来，感知未来：统一的驱动世界模式，用于未来的一代和感知</title>
      <link>https://arxiv.org/abs/2503.13587</link>
      <description><![CDATA[ARXIV：2503.13587V1公告类型：新 
摘要：我们提出了Unifure，这是一种简单而有效的驾驶世界模型，无缝地将未来的场景产生和感知整合到一个框架中。与仅关注像素级的未来预测或几何推理的现有模型不同，我们的方法共同对未来的外观（即RGB图像）和几何形状（即深度）进行建模，从而确保相干预测。具体来说，在培训期间，我们首先引入了双层共享方案，该方案在共享潜在空间中转移图像和深度序列，从而使这两种方式都可以从共享的特征学习中受益。此外，我们提出了一种多尺度的潜在相互作用机制，该机制有助于在多个空间尺度下图像和深度特征之间的双向细化，从而有效地增强了几何学的一致性和感知对齐方式。在测试过程中，我们的统一可以通过仅将当前图像作为输入来轻松预测未来的图像深度对。 Nuscenes数据集的广泛实验表明，Unifure在未来的生成和感知任务上的表现优于专业模型，强调了统一的，结构上意识到的世界模型的优势。项目页面位于https://github.com/dk-liang/unifuture。]]></description>
      <guid>https://arxiv.org/abs/2503.13587</guid>
      <pubDate>Wed, 19 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>让合成数据发光：域重新组装和软融合单个域概括</title>
      <link>https://arxiv.org/abs/2503.13617</link>
      <description><![CDATA[ARXIV：2503.13617V1公告类型：新 
摘要：单个域概括（SDG）旨在使用来自单个源的数据在各种情况下具有一致性的模型。当使用潜在扩散模型（LDMS）显示出在增强有限源数据方面的希望时，我们证明，由于合成和真实目标域之间的显着特征分布差异，直接使用合成数据可能有害，从而导致性能降级。为了解决这个问题，我们提出了歧视域的重新组装和软融合（DRSF），这是一个利用合成数据来改善模型概括的训练框架。我们采用LDM来生产各种伪目标域样品，并引入两个关键模块来处理分配偏差。首先，判别特征解耦和重新组装（DFDR）模块使用熵引导的注意来重新校准通道级别的特征，抑制合成噪声，同时保持语义一致性。其次，多峰值域软融合（MDSF）模块使用潜在空间特征插值的对抗训练，从而在域之间创建连续的特征跃迁。关于对象检测和语义分割任务的广泛的可持续发展目标实验表明，DRSF仅通过边缘计算开销实现了可观的性能增长。值得注意的是，DRSF的即插即用体系结构可以通过无监督的域适应范式进行无缝集成，从而强调了其在应对各种和现实世界中的挑战方面的广泛适用性。]]></description>
      <guid>https://arxiv.org/abs/2503.13617</guid>
      <pubDate>Wed, 19 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Omnia de egotempo：基准对以中心视频中多模式LLM的时间理解</title>
      <link>https://arxiv.org/abs/2503.13646</link>
      <description><![CDATA[ARXIV：2503.13646V1公告类型：新 
摘要：理解细粒的时间动力学在以自我为中心的视频中至关重要，其中连续流捕获频繁的特写互动与对象。在这项工作中，我们可以发现，当前的以自我为中心的视频提问数据集通常包括只能使用少量框架或常识性推理来回答的问题，而无需在实际视频中扎根。我们的分析表明，这些基准上的最新多模式大型语言模型（MLLM）仅使用文本或单个框架作为输入来实现出色的高性能。为了解决这些限制，我们介绍了Egotempo，这是一个专门设计用于评估以egentric域中时间理解的数据集。 Egotempo强调需要在整个视频中集成信息的任务，以确保模型需要依靠时间模式而不是静态提示或预先存在的知识。关于Egotempo的广泛实验表明，当前的MLLM在以自我为中心视频的时间推理中仍然缺乏，因此我们希望Egotempo能够催化该领域的新研究并激发更好的模型，以更好地捕捉时间动力学的复杂性。数据集和代码可在https://github.com/google-research-datasets/egotempo.git上找到。]]></description>
      <guid>https://arxiv.org/abs/2503.13646</guid>
      <pubDate>Wed, 19 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Web伪影攻击破坏视觉语言模型</title>
      <link>https://arxiv.org/abs/2503.13652</link>
      <description><![CDATA[ARXIV：2503.13652V1公告类型：新 
摘要：视觉语言模型（VLMS）（例如，剪辑，LLAVA）在大规模，策划的Web数据集上进行了训练，导致他们学习语义概念和不相关的视觉信号之间的意外相关性。这些关联通过导致预测依靠偶然模式而不是真正的视觉理解来降低模型的准确性。先前的工作将这些相关性武器化为攻击向量，以操纵模型预测，例如将欺骗性的类文本插入印刷攻击中的图像。这些攻击因VLMS的文本较重的偏见而取得了成功，其标题与可见的单词相呼应而不是描述内容。但是，此攻击仅集中在与目标类完全匹配的文本上，忽略了更广泛的相关性，包括不匹配的文本和图形符号，这些符号是由网络规模数据中的丰富品牌内容引起的。为了解决这一差距，我们介绍了基于人工制品的攻击：一种新颖的操纵类，使用非匹配文本和图形元素误导模型。与印刷攻击不同，这些文物不是预定义的，这使得它们更难防御，但更具挑战性。我们通过将人工制品攻击作为搜索问题来解决这一问题，并在五个数据集中证明了它们的有效性，其中一些工件相互加强以达到100％的攻击成功率。这些攻击跨模型的效果多达90％，使得攻击看不见的模型成为可能。为了防止这些攻击，我们将先前工作的伪像意识提示扩展到图形设置。相对于标准提示，我们看到，成功率最高15％，这表明可以增强模型鲁棒性的方向有前途的方向。]]></description>
      <guid>https://arxiv.org/abs/2503.13652</guid>
      <pubDate>Wed, 19 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>五：用于评估新兴扩散和整流流模型的细粒视频编辑基准测试</title>
      <link>https://arxiv.org/abs/2503.13684</link>
      <description><![CDATA[ARXIV：2503.13684V1公告类型：新 
摘要：最近出现了许多文本对视频（T2V）编辑方法，但是缺乏公平评估的标准化基准导致索赔不一致，并且无法评估模型对超参数的敏感性。细粒度的视频编辑对于实现精确的对象级修改至关重要，同时保持上下文和时间一致性。为了解决这个问题，我们介绍了五个，这是一种精细的视频编辑基准，用于评估新出现的扩散和整流的流程模型。我们的基准包括74个现实世界的视频和26个生成的视频，其中包含6种精细的编辑类型，420个对象级编辑提示对及其相应的面具。此外，我们通过引入FlowedIt来调整最新的整流流（RF）T2V生成模型，金字塔 - 流和WAN2.1，从而导致无训练和无倒置的视频编辑模型金字塔和WAN-EDIT。我们使用15个指标在我们的五个基准测试上评估了五种基于扩散的和两种基于RF的编辑方法，涵盖了背景保护，文本视频相似性，时间一致性，视频质量和运行时。为了进一步增强对象级别的评估，我们介绍了五-ACC，这是一种新型的度量杠杆语言模型（VLM），以评估细粒视频编辑的成功。实验结果表明，基于RF的编辑显着优于基于扩散的方法，Wan-Edit实现了最佳的整体性能，并且对超参数的敏感性最低。匿名网站上的更多视频演示可用：https：//sites.google.com/view/five-benchmark]]></description>
      <guid>https://arxiv.org/abs/2503.13684</guid>
      <pubDate>Wed, 19 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>适应未知：无训练的视听事件感知的动态阈值</title>
      <link>https://arxiv.org/abs/2503.13693</link>
      <description><![CDATA[ARXIV：2503.13693V1公告类型：新 
摘要：在视听事件感知的领域中，该领域的重点是对不同模式（音频和视觉）事件的时间定位和分类，现有方法受培训数据中可用的词汇限制。这种限制极大地阻碍了他们概括新颖，看不见的事件类别的能力。此外，此任务的注释过程是劳动密集型的，需要跨模态和时间段的大量手动标记，从而限制了当前方法的可扩展性。当前的最新模型忽略了事件分布的变化，随着时间的推移，它们会降低其适应不断变化的视频动态的能力。此外，以前的方法依靠晚融合来结合音频和视觉信息。这种方法虽然直接，但会导致多模式相互作用的显着丧失。为了应对这些挑战，我们提出了视听自适应视频分析（$ \ text {av}^2 \ text {a} $），这是一种模型 -  agnostic方法，不需要进一步的培训并整合得分级别的融合技术来保留富裕的多态相互作用。 $ \ text {av}^2 \ text {a} $还包括视频内偏移算法，利用输入视频数据以及从先前框架进行预测以动态调整后续帧的事件分布。此外，我们为视听事件感知提出了第一个无训练的开放式摄影基线，表明$ \ text {av}^2 \ text {a} $对幼稚的无训练底线进行了实质性改进。我们演示了零射门和弱监督的最先进方法，$ \ text {av}^2 \ text {a} $的有效性，从现有方法方面取得了显着改善的性能指标。]]></description>
      <guid>https://arxiv.org/abs/2503.13693</guid>
      <pubDate>Wed, 19 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>长vmnet：通过固定记忆加速长期视频理解</title>
      <link>https://arxiv.org/abs/2503.13707</link>
      <description><![CDATA[ARXIV：2503.13707V1公告类型：新 
摘要：长期视频理解对于各种应用程序（例如视频检索，汇总和问题答案）至关重要。然而，传统方法需要实质性的计算能力，并且通常被GPU记忆所瓶颈。为了解决此问题，我们提出了长期VIDEO内存网络，LongeVmnet，这是一种新颖的视频理解方法，该方法采用固定尺寸的内存表示来存储从输入视频中采样的歧视性补丁。通过利用识别歧视令牌的神经采样器，长量为长vmnet可提高效率。此外，LongyVmnet只需要通过视频进行扫描，从而极大地提高了效率。我们在其余的ADL数据集上的结果表明，长期视频检索和回答问题的推理时间为18倍-75倍，具有竞争性的预测性能。]]></description>
      <guid>https://arxiv.org/abs/2503.13707</guid>
      <pubDate>Wed, 19 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在室内场景中改善360度神经辐射场的几何一致性</title>
      <link>https://arxiv.org/abs/2503.13710</link>
      <description><![CDATA[ARXIV：2503.13710V1公告类型：新 
摘要：照片真实的渲染和新颖的视图合成在从游戏到路径计划的人类计算机互动任务中起着至关重要的作用。神经辐射场（NERFS）模型场景是连续体积功能，并具有出色的渲染质量。但是，nerf通常在大型低纹理的地区挣扎，生产被称为“浮点”的多云文物，从而减少了现实的现实主义，尤其是在室内环境中，墙壁，天花板和地板等无特色的建筑表面。为了克服这一限制，先前的工作已将几何约束集成到NERF管道中，通常利用从运动或多视图立体声来源的结构中得出的深度信息。然而，常规的RGB功能对应方法在准确估计无纹理区域的深度方面面临挑战，从而导致不可靠的约束。在360度“ Inside-Out”视图中，这一挑战更加复杂，在相邻图像之间稀疏的视觉重叠进一步阻碍了深度估计。为了解决这些问题，我们提出了一种用于计算密集深度先验的高效且可靠的方法，该方法专门针对室内环境中的大型低纹理建筑表面量身定制。我们引入了一种新颖的深度损失功能，以提高这些挑战性低的区域中的渲染质量，而互补的深度点正则化进一步完善了其他领域的深度一致性。在两个合成360度室内场景上使用瞬时NGP进行实验表明，与标准的光度损失和平均平方误差深度监督相比，我们的方法的视觉保真度提高了。]]></description>
      <guid>https://arxiv.org/abs/2503.13710</guid>
      <pubDate>Wed, 19 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://arxiv.org/abs/2503.13721</link>
      <description><![CDATA[ARXIV：2503.13721V1公告类型：新 
摘要：最近，由于重建无纹理区域的可变形且可扩展的斑块，补丁信息的方法在多视图立体声中表现出显着的有效性。但是，这种方法主要强调扩大无纹理区域中的接受场，同时忽略了易于忽视的边缘衬托引起的变形不稳定，这可能导致匹配的扭曲。为了解决这个问题，我们提出了SED-MV，该SED-MV采用了泛型分割和多个针对性扩散策略，用于分割驱动和边缘对准的斑块变形。具体而言，为了防止意外的边缘滑动，我们首先采用SAM2作为深度边缘指导，以指导斑块变形，然后采用多门象扩散策略，以确保斑块与深度边缘全面对齐。此外，为了避免潜在的随机初始化不准确，我们将loftr和单眼深度图的稀疏点结合在一起，从深度透射率V2恢复可靠且逼真的深度图，以进行初始化和监督指导。最后，我们将分割图像与单眼深度图集成在一起，以利用实体​​遮挡关系，然后进一步将它们视为遮挡图，以实现两个不同的边缘约束，从而促进了咬合遮挡意识到的斑块变形。 ETH3D，TANK和TAMPES，BLEDENDMVS和Strecha数据集的广泛结果验证了我们提出的方法的最先进性能和鲁棒的概括能力。]]></description>
      <guid>https://arxiv.org/abs/2503.13721</guid>
      <pubDate>Wed, 19 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>朝着压缩视频的可扩展建模以进行有效的动作识别</title>
      <link>https://arxiv.org/abs/2503.13724</link>
      <description><![CDATA[ARXIV：2503.13724V1公告类型：新 
摘要：训练强大的深层视频表示已被证明是在计算上具有挑战性的，这是由于大量解码开销，巨大的原始视频流尺寸及其固有的高时间冗余性。与现有方案不同，专门在压缩视频域中运行，并利用所有免费可用的模式，即i-frames和p-frames（运动向量和残差）提供了一个计算效率的替代方案。现有方法将此任务视为一个天真的多模式问题，忽略了跨P框架的时间相关性和隐式稀疏性，以模拟与同一动作的视频更强大的共享表示形式，从而使培训和概括更加容易。通过重新审视优势视频理解主链的高级设计，我们将推理速度提高了56美元，同时保持相似的性能。为此，我们提出了一个混合端到端框架，该框架将跨三个关键概念的学习分配，以将推理成本降低$ 330 \ times $ $ vestres ART：首先，一种专门设计的双编码方案，具有有效的峰值时间调节器，以最大程度地减少延迟，同时保留交叉数字特征聚合。其次，统一的变压器模型，使用全局自我注意来捕获模式间依赖性，以增强i框架-P框架上下文相互作用。第三，一个多模式混合器块，用于模拟关节时空令牌嵌入的丰富表示形式。实验表明，我们的方法可实现轻巧的体系结构，以实现UCF-101，HMDB-51，K-400，K-600，K-600和SS-V2数据集的最新视频识别性能，其成本有利（$ 0.73 $ j/v）和快速推论（$ 16 $ v/s）。我们的观察结果为有效的下一代时空学习者带来了实用设计选择的新见解。代码可用。]]></description>
      <guid>https://arxiv.org/abs/2503.13724</guid>
      <pubDate>Wed, 19 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>TextInvision：文本和及时复杂性驱动视觉文本生成基准测试</title>
      <link>https://arxiv.org/abs/2503.13730</link>
      <description><![CDATA[ARXIV：2503.13730V1公告类型：新 
摘要：用嵌入式文本生成图像对于自动生产视觉和多模式文档（例如教育材料和广告）至关重要。但是，现有的基于扩散的文本对图像模型通常难以将文本准确地嵌入图像中，面临拼写准确性，上下文相关性和视觉连贯性的挑战。由于缺乏全面的基准，评估此类模型将文本嵌入文本嵌入文本的能力变得复杂。在这项工作中，我们介绍了TextInvision，这是一个大规模，文本和迅速复杂性驱动的基准测试，旨在评估扩散模型有效地将视觉文本整合到图像中的能力。我们精心设计了一套各种提示和文本，这些提示和文本考虑了各种属性和文本特征。此外，我们准备了一个图像数据集，以测试不同角色表示的变异自动编码器（VAE）模型，这强调了VAE体系结构还可以在扩散框架内的文本生成中构成挑战。通过对多个模型的广泛分析，我们确定了常见错误，并突出了诸如拼写不准确和上下文不匹配等问题。通过查明不同提示和文本的故障点，我们的研究为AI生成的多模式内容的未来进步奠定了基础。]]></description>
      <guid>https://arxiv.org/abs/2503.13730</guid>
      <pubDate>Wed, 19 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从同步中学习：在具有挑战性的场景</title>
      <link>https://arxiv.org/abs/2503.13739</link>
      <description><![CDATA[ARXIV：2503.13739V1公告类型：新 
摘要：多视角人协会是迈向人工活动多视图分析的基本步骤。尽管人们的重新识别功能已被证明有效，但它们在具有类似出场的人的挑战场景中变得不可靠。因此，更强大的关联需要跨视图几何约束。但是，大多数现有的方法要么使用地面身份标签完全监督，要么需要难以获得的校准相机参数。在这项工作中，我们研究了从同步中学习的潜力，并提出了一种自制的无校准的多视图人协会方法，即自我MVA，而无需使用任何注释。具体而言，我们提出了一个自我监督的学习框架，该框架由编码器模型和一个自我监督的借口任务，跨视图图像同步，旨在区分同一时间是否捕获来自不同视图的两个图像。该模型编码每个人的统一几何和外观特征，我们在应用匈牙利匹配以弥合实例和图像距离之间的差距后，利用同步标签进行训练。为了进一步减少解决方案空间，我们提出了两种类型的自我监督线性约束：多视图重新投影和成对边缘关联。对三个具有挑战性的公共基准数据集（Wildtrack，MVOR和士兵）进行的广泛实验表明，我们的方法取得了最新的结果，超过了现有的无监督和完全监督的方法。代码可在https://github.com/camma-public/self-mva上找到。]]></description>
      <guid>https://arxiv.org/abs/2503.13739</guid>
      <pubDate>Wed, 19 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>C2D-ISR：从连续到离散量表优化基于注意力的图像超分辨率</title>
      <link>https://arxiv.org/abs/2503.13740</link>
      <description><![CDATA[ARXIV：2503.13740V1公告类型：新 
摘要：近年来，注意力机制已在单像超分辨率（SISR）中被利用，从而获得了令人印象深刻的重建结果。但是，这些进步仍然受到对简单培训策略和网络体系结构的依赖限制，这些策略和网络体系结构旨在离散采样量表，这阻碍了该模型有效访问多个量表的信息的能力。为了解决这些局限性，我们提出了一个新颖的框架\ textbf {c2d-isr}，用于从性能和复杂性角度优化基于注意力的图像超分辨率模型。我们的方法基于两阶段的训练方法和分层编码机制。新的培训方法涉及对离散量表模型的连续规模培训，从而可以学习尺度间相关性和多尺度特征表示。此外，我们通过现有的基于注意力的网络结构概括了层次编码机制，这些机制可以改善空间特征融合，跨尺度信息聚合，更重要的是，推断更快。我们已经根据三个有效的基于注意力的主机（Swinir-L，Srformer-l和Mambairv2-L）评估了C2D-ISR框架，并在超级分辨率性能（最高0.2DB）和计算复杂性降低（最高11％）方面，对其他现有优化框架进行了显着改善。源代码将在www.github.com上公开提供。]]></description>
      <guid>https://arxiv.org/abs/2503.13740</guid>
      <pubDate>Wed, 19 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>