<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>CS.CV更新arxiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.cv更新arxiv.org e-print存档。</description>
    <lastBuildDate>Fri, 04 Apr 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>LSC-ADL：通过半自动群集生成的日常生活（ADL）的活性数据集</title>
      <link>https://arxiv.org/abs/2504.02060</link>
      <description><![CDATA[ARXIV：2504.02060V1公告类型：新 
摘要：LifeLogging涉及通过可穿戴摄像机不断捕获个人数据，从而提供以自我为中心的日常活动的视图。 LifeLog检索旨在从这些数据中搜索和检索相关的时刻，但现有方法在很大程度上忽略了活动级注释，从而捕获时间关系并丰富语义理解。在这项工作中，我们介绍了LSC-ADL，这是一种源自LSC数据集的ADL注销的LIFELOG数据集，将日常生活（ADL）的活动纳入结构化语义层。使用半自动方法，该方法具有用于类内部聚类和人类在循环验证的HDBSCAN算法，我们生成准确的ADL注释以增强检索性解释性。通过将行动识别纳入LIFELOG检索中，LSC-ADL弥合了现有研究中的一个危险差距，从而提供了对日常生活的更加感知的代表。我们认为，该数据集将在LifeLog检索，活动识别和以自我为中心的愿景方面进行研究，最终提高检索内容的准确性和解释性。可以在https://bit.ly/lsc-adl-annotations下载ADL注释。]]></description>
      <guid>https://arxiv.org/abs/2504.02060</guid>
      <pubDate>Fri, 04 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>更好地对齐，更好地听音频大语模型</title>
      <link>https://arxiv.org/abs/2504.02061</link>
      <description><![CDATA[ARXIV：2504.02061V1公告类型：新 
摘要：音频对于多模式视频理解至关重要。一方面，视频本质上包含音频，该音频将互补的信息提供给视觉。此外，视频大型语言模型（视频llms）可能会遇到许多以音频为中心的设置。但是，现有的视频插件和视听大语模型（AV-LLMS）在利用音频信息时表现出缺陷，从而导致理解和幻觉疲软。为了解决问题，我们深入研究了模型体系结构和数据集。 （1）从建筑的角度来看，我们提出了一个细粒的Av-llm，即海豚。在时间和空间维度上的音频和视觉方式的同时对齐可确保对视频的全面和准确的了解。具体而言，我们设计了一个视听多尺度适配器，用于多尺度信息聚合，该适配器可以实现空间对齐。对于时间对齐，我们提出了视听的交织合并。 （2）从数据集的角度来看，我们策划了一个称为AVU的视听字幕和指令数据集。它包括520万种不同的开放式数据元素（视频，音频，问题，答案），并介绍了一种新颖的数据分配策略。广泛的实验表明，我们的模型不仅在视听理解中取得了出色的表现，还可以减轻潜在的幻觉。]]></description>
      <guid>https://arxiv.org/abs/2504.02061</guid>
      <pubDate>Fri, 04 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>弗雷斯卡（Fresca）：在扩散模型中揭示缩放空间</title>
      <link>https://arxiv.org/abs/2504.02154</link>
      <description><![CDATA[ARXIV：2504.02154V1公告类型：新 
摘要：扩散模型为图像任务提供了令人印象深刻的可控性，主要是通过编码特定于任务信息和无分类器指导的噪声预测，可调节缩放。这种缩放机制隐含地定义了``缩放空间&#39;&#39;，其细粒语义操纵的潜力仍然没有被忽略。我们研究了这个空间，从基于反演的编辑开始，其中条件/无条件噪声预测之间的差异带有关键的语义信息。我们的核心贡献源于对噪声预测的傅立叶分析，表明其低频和高频成分在整个扩散过程中的发展都不同。基于此洞察力，我们引入了弗雷斯卡（Fresca），这是一种直接的方法，该方法将指导范围独立于傅立叶域中的不同频段。弗雷斯卡（Fresca）明显地增强了现有的图像编辑方法，而无需重新培训。令人兴奋的是，其有效性扩展到图像理解任务，例如深度估计，从而在多个数据集中产生定量增长。]]></description>
      <guid>https://arxiv.org/abs/2504.02154</guid>
      <pubDate>Fri, 04 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>UAVTWIN：使用高斯分裂的无人机神经数字双胞胎</title>
      <link>https://arxiv.org/abs/2504.02158</link>
      <description><![CDATA[ARXIV：2504.02158V1公告类型：新 
摘要：我们提出了UAVTWIN，这是一种从现实世界环境中创建数字双胞胎的方法，并促进了嵌入无人驾驶飞机（无人机）的下游模型训练下游模型的数据增强。具体而言，我们的方法着重于综合前景组件，例如从无人机的角度来看，在复杂场景背景中运动中的各种人类实例。这是通过整合3D高斯脱落（3DG）来重建背景以及可控的合成人类模型来实现的，这些人类模型以多种姿势显示出不同的外观和动作。据我们所知，Uavtwin是基于无人机的感知的第一种方法，它能够基于3DG生成高保真的数字双胞胎。提出的工作通过对具有多个动态对象和显着外观变化的现实世界环境的数据增强来显着增强下游模型，其中通常会在基于3DGS的建模中引入伪影。为了应对这些挑战，我们提出了一种新颖的外观建模策略和一个掩盖改进模块，以增强3D高斯脱落的训练。与最近的方法相比，我们通过在PSNR方面提高了1.23 dB的改善，证明了神经渲染的高质量。此外，我们通过显示人类检测任务的MAP提高2.5％至13.7％来验证数据增强的有效性。]]></description>
      <guid>https://arxiv.org/abs/2504.02158</guid>
      <pubDate>Fri, 04 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>较少的概括：通过内在生成解锁更多的可控性</title>
      <link>https://arxiv.org/abs/2504.02160</link>
      <description><![CDATA[ARXIV：2504.02160V1公告类型：新 
摘要：尽管由于其广泛的应用，因此在图像生成中广泛探索了受试者驱动的生成，但它仍然在数据可扩展性和扩展性方面面临挑战。对于第一个挑战，从策划单人物数据集转移到多个受试者的数据集并缩放它们特别困难。在第二个单个主体生成上的第二个方法中，最新方法使得在处理多​​主体方案时很难应用。在这项研究中，我们提出了一条高度一致的数据合成管道，以应对这一挑战。该管道利用扩散变压器的固有内在生成能力，并生成高谐波多主体配对数据。此外，我们介绍了UNO，其中包括进行性跨模式比对和通用旋转位置嵌入。这是一个从文本到图像模型进行迭代训练的多图像对象模型。广泛的实验表明，我们的方法可以达到高稠度，同时确保单个受试者和多主体驱动的生成中的可控性。]]></description>
      <guid>https://arxiv.org/abs/2504.02160</guid>
      <pubDate>Fri, 04 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MDP：具有潜伏期约束的多维视觉模型</title>
      <link>https://arxiv.org/abs/2504.02168</link>
      <description><![CDATA[ARXIV：2504.02168V1公告类型：新 
摘要：当前的结构修剪方法面临两个重要的局限性：（i）它们通常将修剪限制为诸如频道（例如频道），使积极的参数减少挑战，并且（ii）它们重点放在参数和翻牌上，而现有的延迟感知方法经常依赖于简单的，次级的线性模型，这些模型依赖于次级线性的模型，从而使多个互动效果互动，从而使多个互动效果。在本文中，我们通过引入多维修剪（MDP）来解决这两种局限性，这是一种新型范式，在包括各种修剪的粒度通道，查询，钥匙，头部，嵌入式和块中共同优化，该范式在各种修剪的粒度通道中进行了优化。 MDP采用高级延迟建模技术来准确捕获所有可闭合维度的延迟变化，从而在延迟和准确性之间取得了最佳平衡。通过将修剪作为混合成员非线性程序（MINLP）进行重新修复，MDP有效地识别了所有可闭合维度的最佳修剪结构，同时尊重延迟约束。这个多功能框架支持CNN和变压器。广泛的实验表明，MDP明显优于先前的方法，尤其是在高修剪比下。在ImageNet上，MDP的速度增加了28％，而+1.4 TOP-1的准确性提高了先前的工作，例如HALP进行Resnet50修剪。在最新的变压器修剪方法（同构）的情况下，MDP提供了37％的加速度，并提高了+0.7 TOP-1精度。]]></description>
      <guid>https://arxiv.org/abs/2504.02168</guid>
      <pubDate>Fri, 04 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>前景重点：增强伪装图像生成中的连贯性和忠诚度</title>
      <link>https://arxiv.org/abs/2504.02180</link>
      <description><![CDATA[ARXIV：2504.02180V1公告类型：新 
摘要：伪装的图像生成正在作为解决伪装视觉感知中数据稀缺的解决方案，为数据收集和标签提供了具有成本效益的替代方案。最近，最先进的方法仅使用前景对象成功地生成了伪装的图像。但是，它面临两个关键弱点：1）背景知识并未与前景特征有效整合，从而导致缺乏前景背景连贯性（例如，颜色差异）； 2）生成过程没有优先考虑前景对象的保真度，这会导致失真，尤其是对于小物体。为了解决这些问题，我们提出了一种前景感知的伪装图像生成（FOCIG）模型。具体来说，我们引入了一个前景感知的特征集成模块（FAFIM），以增强前景特征和背景知识之间的集成。此外，前景感知的剥夺损失旨在增强前景重建监督。各种数据集上的实验表明，我们的方法在整体伪装的图像质量和前景保真度上都优于先前的方法。]]></description>
      <guid>https://arxiv.org/abs/2504.02180</guid>
      <pubDate>Fri, 04 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ESC：删除知识删除的空间概念</title>
      <link>https://arxiv.org/abs/2504.02199</link>
      <description><![CDATA[ARXIV：2504.02199V1公告类型：新 
摘要：随着对深度学习中隐私的担忧继续增长，个人对在受过训练的模型中对个人知识的潜在剥削感到越来越担心。尽管为解决这个问题进行了几项研究，但他们通常未能考虑用户对完全知识擦除的现实需求。此外，我们的调查表明，现有方法有通过嵌入功能泄漏个人知识的风险。为了解决这些问题，我们介绍了一个新颖的知识删除概念（KD），这是一项高级任务，考虑了这两个问题，并提供了适当的指标知识保留得分（KR），用于评估特征空间中的知识保留。为了实现这一目标，我们提出了一种名为“擦除空间概念”（ESC）的新颖的无训练擦除方法，该方法通过消除功能中的相关激活来限制遗忘知识的重要子空间。此外，我们建议使用培训（ESC-T）ESC，它使用可学习的面具来更好地平衡遗忘和保留KD知识之间的权衡。我们在各种数据集和模型上进行的广泛实验表明，我们提出的方法达到了最快，最先进的性能。值得注意的是，我们的方法适用于各种遗忘的场景，例如面部域设置，证明了我们方法的普遍性。该代码可在http://github.com/ku-vgi/esc上找到。]]></description>
      <guid>https://arxiv.org/abs/2504.02199</guid>
      <pubDate>Fri, 04 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于卫星的洪水范围映射的地理空间人工智能：概念，进步和未来的观点</title>
      <link>https://arxiv.org/abs/2504.02214</link>
      <description><![CDATA[ARXIV：2504.02214V1公告类型：新 
摘要：基于卫星的洪水范围绘制的地理空间人工智能（GEOAI）系统地将人工智能技术与卫星数据集成在一起，以识别洪水事件并评估其影响，以进行灾难管理和空间决策。主要输出通常包括洪水范围图，这些图描绘了受影响区域，以及其他分析输出（例如不确定性估计和变化检测）。]]></description>
      <guid>https://arxiv.org/abs/2504.02214</guid>
      <pubDate>Fri, 04 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AC-Lora：自动组件Lora用于个性化艺术风格图像生成</title>
      <link>https://arxiv.org/abs/2504.02231</link>
      <description><![CDATA[ARXIV：2504.02231V1公告类型：新 
摘要：个性化的图像生成使用户可以保留提供的少量图像的样式或主题，以进行进一步的图像生成。随着大型文本到图像模型的进步，已经开发了许多技术来有效地微调这些模型的个性化模型，例如低级适应（Lora）。但是，基于洛拉的方法通常面临调整等级参数以获得令人满意的结果的挑战。为了应对这一挑战，提出了AutoComponent-Lora（AC-Lora），该挑战能够自动将Lora矩阵的信号组件和噪声组件分开，以快速有效地个性化的艺术风格图像生成。该方法基于奇异值分解（SVD）和动态启发式方法，以更新训练期间的超参数。在克服模型中的现有方法中，表现出色的性能不足以适应或过度拟合问题。使用FID，剪辑，恐龙和成像智能验证了结果，平均提高了9％。]]></description>
      <guid>https://arxiv.org/abs/2504.02231</guid>
      <pubDate>Fri, 04 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>社会植物：深入研究多人的手势理解</title>
      <link>https://arxiv.org/abs/2504.02244</link>
      <description><![CDATA[ARXIV：2504.02244V1公告类型：新 
摘要：人类手势识别的先前研究在很大程度上忽略了多人互动，这对于理解自然发生的手势的社会背景至关重要。现有数据集中的这种局限性在将人类手势与语言和语音等其他方式保持一致时提出了重大挑战。为了解决这个问题，我们介绍了社交史密斯，这是第一个专门为多人手势分析而设计的大型数据集。社会植物具有多种自然情景，并支持多种手势分析任务，包括基于视频的识别和时间定位，为在复杂的社交互动过程中提供了促进手势研究的宝贵资源。此外，我们提出了一个新颖的视觉问题回答（VQA）任务，以基准在社交手势理解的方面基于视觉语言模型（VLMS）的表现。我们的发现突出了当前的手势识别模型的几个局限性，为未来的方向提供了有关改进该领域的方向的见解。可以从huggingface.co/datasets/irohxu/socialgesture获得社交史密斯。]]></description>
      <guid>https://arxiv.org/abs/2504.02244</guid>
      <pubDate>Fri, 04 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>重新思考暂时搜索以了解长期视频理解</title>
      <link>https://arxiv.org/abs/2504.02259</link>
      <description><![CDATA[ARXIV：2504.02259V1公告类型：新 
摘要：有效理解长形视频仍然是计算机视觉中的重大挑战。在这项工作中，我们重新审视时间搜索范例，以了解长期视频理解，研究与所有最先进的（SOTA）长篇小说视觉语言模型（VLMS）有关的基本问题。特别是，我们的贡献是两个方面：首先，我们将时间搜索作为一个长期的视频干草问题问题，即，在经过特定查询的情况下，在数以万计的实际视频中找到了数以万计的相关帧（通常为一到五）。为了验证我们的配方，我们创建了LV-Haystack，这是第一个基准，其中包含3,874个人类注销实例，其中包含具有细粒度评估指标，用于评估关键帧搜索质量和计算效率。 LV-Haystack的实验结果突出了时间搜索功能的显着研究差距，SOTA钥匙帧选择方法仅在LVBENCH子集中获得2.1％的时间F1分数。
  接下来，受图像中的视觉搜索的启发，我们重新考虑了时间搜索并提出了一个轻巧的钥匙帧搜索框架T*，该框架将昂贵的时间搜索作为空间搜索问题。 T*利用图像中通常使用的卓越视觉定位功能，并引入了一种自适应缩放机制，该机制在时间和空间尺寸上都可以运行。我们广泛的实验表明，当与现有方法集成时，T*显着改善了SOTA长期视频理解性能。具体而言，在推理预算为32帧的情况下，T*将GPT-4O的性能从50.5％提高到53.1％，而Llava-onevision-72b的性能从LongVideObench XL XL子集的56.5％提高到62.4％。我们的Pytorch代码，基准数据集和模型包含在补充材料中。]]></description>
      <guid>https://arxiv.org/abs/2504.02259</guid>
      <pubDate>Fri, 04 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Wonderturbo：在0.72秒内生成互动3D世界</title>
      <link>https://arxiv.org/abs/2504.02261</link>
      <description><![CDATA[ARXIV：2504.02261V1公告类型：新 
摘要：交互式3D代表正在获得动力，并引起广泛的关注，以创造身临其境的虚拟体验的潜力。但是，当前3D一代技术的关键挑战在于实现实时互动。为了解决这个问题，我们介绍了Wonderturbo，这是第一个实时交互式3D场景生成框架，能够在0.72秒内生成3D场景的新观点。具体而言，Wonderturbo在3D场景生成中加速了几何和外观建模。在几何形状方面，我们提出了一种创新方法，该方法通过动态更新构建有效的3D几何表示，每种方法仅需0.26秒。此外，我们设计了QuickDepth，这是一个轻巧的深度完成模块，该模块为Stepplat提供一致的深度输入，从而进一步提高了几何精度。对于外观建模，我们开发了FastPaint，这是一种针对即时涂料的2步扩散模型，该模型的重点是保持空间外观一致性。实验结果表明，与基线方法相比，Wonderturbo达到了显着的15倍加速度，同时保留了出色的空间一致性并提供了高质量的输出。]]></description>
      <guid>https://arxiv.org/abs/2504.02261</guid>
      <pubDate>Fri, 04 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>mmtl-uniAD：辅助驾驶感知中多模式和多任务学习的统一框架</title>
      <link>https://arxiv.org/abs/2504.02264</link>
      <description><![CDATA[ARXIV：2504.02264V1公告类型：新 
摘要：先进的驾驶员援助系统需要对驾驶员的心理/身体状态和交通环境有全面的了解，但是现有的作品通常会忽略这些任务之间联合学习的潜在好处。本文提出了MMTL-UNIAD，这是一个统一的多模式多任务学习框架，同时识别驾驶员行为（例如，环顾四周，谈话），驾驶员情绪（例如，焦虑，幸福），车辆行为（例如，停车，停车，转弯）和交通环境（例如，交通量，交通量，交通量，交通流量）。一个关键的挑战是避免任务之间的负面转移，这可能会损害学习绩效。为了解决这个问题，我们将两个关键组件介绍到框架中：一个是多轴区域注意力网络，用于提取全局上下文敏感的特征，另一个是双支链接多模式嵌入，从任务共享和特定于任务特定的特征学习多模式嵌入。前者使用多发机制来提取与任务相关的特征，从而减轻由任务无关的特征引起的负转移。后者采用双分支结构来自适应调整任务共享和特定于任务的参数，从而增强交叉任务知识转移，同时减少任务冲突。我们使用一系列消融研究评估了助手数据集上的MMTL-UNIAD，并表明它在所有四个任务上都优于最先进的方法。该代码可在https://github.com/wenzhuo-liu/mmtl-uniad上找到。]]></description>
      <guid>https://arxiv.org/abs/2504.02264</guid>
      <pubDate>Fri, 04 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>minkocc：迈向实时标签高效的语义占用预测</title>
      <link>https://arxiv.org/abs/2504.02270</link>
      <description><![CDATA[ARXIV：2504.02270V1公告类型：新 
摘要：开发3D语义占用预测模型通常依赖于对监督学习的密集3D注释，这一过程既是劳动力和资源密集型的过程，强调了对标签有效效率甚至不含标签的方法的需求。为了解决这个问题，我们介绍了MinkoCC，这是针对相机和LIDARS的多模式3D语义占用预测框架，该预测框架提出了两步半监督训练程序。在这里，一个明确的3D注释的小数据集温暖了培训过程；然后，通过更简单的累积的LiDAR扫描和图像来继续监督 - 通过视觉基础模型标记为语义。 MinkoCC有效地利用了这些传感器丰富的监督线索，并将对手动标签的依赖减少了90 \％，同时保持了竞争精度。此外，提出的模型通过早期融合和利用稀疏卷积网络进行实时预测，结合了LIDAR和相机数据的信息。凭借其在监督和计算方面的效率，我们旨在将MinkOCC扩展到策划的数据集之外，从而使3D语义占用预测在自主驾驶中的更广泛的现实部署。]]></description>
      <guid>https://arxiv.org/abs/2504.02270</guid>
      <pubDate>Fri, 04 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>