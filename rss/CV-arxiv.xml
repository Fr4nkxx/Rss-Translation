<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Mon, 16 Sep 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>激活函数优化方法：可学习系列线性单元（LSLU）</title>
      <link>https://arxiv.org/abs/2409.08283</link>
      <description><![CDATA[arXiv:2409.08283v1 Announce Type: new 
摘要：有效的激活函数引入了非线性变换，为神经网络提供了更强的拟合能力，帮助其更好地适应真实的数据分布。华为诺亚实验室认为动态激活函数比静态激活函数更适合增强神经网络的非线性能力，清华大学的相关研究也建议使用动态调整的激活函数。我们借鉴清华大学和华为诺亚实验室使用微调激活函数的思路，提出了一种基于级数的可学习激活函数LSLU（Learnable Series Linear Units），该方法在提高准确率的同时简化了深度学习网络。该方法引入可学习的参数{\theta}和{\omega}来控制激活函数，使其适应当前层的训练阶段，提高模型的泛化能力，其原理是在每一个激活层增加非线性，从而提高网络整体的非线性。我们评估了 LSLU 在 CIFAR10、CIFAR100 和特定任务数据集（例如 Silkworm）上的性能，以验证其有效性。分析了可学习参数 {\theta} 和 {\omega} 的收敛行为及其对泛化的影响。我们的实证结果表明，LSLU 在加快训练速度的同时，增强了原始模型在各种任务中的泛化能力。在 VanillaNet 训练中，参数 {\theta} 最初减小，然后增加，然后稳定下来，而 {\omega} 则呈现相反的趋势。最终，LSLU 在 CIFAR100 上为 VanillaNet 实现了 3.17% 的准确率提升（表 3）。代码可在 https://github.com/vontran2021/Learnable-series-linear-units-LSLU 获得。]]></description>
      <guid>https://arxiv.org/abs/2409.08283</guid>
      <pubDate>Mon, 16 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SIG：用于生成人脸识别评估数据集的合成身份生成管道</title>
      <link>https://arxiv.org/abs/2409.08345</link>
      <description><![CDATA[arXiv:2409.08345v1 公告类型：新
摘要：随着人工智能应用的扩展，模型的评估面临着越来越严格的审查。确保公众准备就绪需要评估数据集，这些数据集与训练数据不同，因为它们是独立的并且符合隐私法规的道德来源。人脸识别系统的性能和公平性在很大程度上取决于这些评估数据集的质量和代表性。这些数据有时会在未经用户同意的情况下从互联网上抓取，从而引起道德问题，可能会禁止在没有适当发布的情况下使用这些数据。在极少数情况下，数据是在征得用户同意的情况下在受控环境中收集的，但是，这个过程耗时、昂贵，而且在后勤上难以执行。这为那些无法调动收集合乎道德的评估数据集所需的大量资源的人设置了障碍。为了应对这些挑战，我们引入了合成身份生成管道 (SIG)，它可以有针对性地创建合乎道德、平衡的人脸识别评估数据集。我们提出并演示的流程可生成具有可控姿势、面部特征和人口统计属性（例如种族、性别和年龄）的高质量合成身份图像。我们还发布了一个名为 ControlFace10k 的开源评估数据集，其中包含 3,336 个独特合成身份的 10,008 张人脸图像，这些合成身份在种族、性别和年龄上保持平衡，使用提出的 SIG 流程生成。我们使用最先进的人脸识别算法分析 ControlFace10k 和非合成 BUPT 数据集，以证明其作为评估工具的有效性。该分析突出了数据集的特征及其在评估不同人口群体的算法偏差方面的实用性。]]></description>
      <guid>https://arxiv.org/abs/2409.08345</guid>
      <pubDate>Mon, 16 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>重新思考带有部分注释的多标签识别的提示策略</title>
      <link>https://arxiv.org/abs/2409.08381</link>
      <description><![CDATA[arXiv:2409.08381v1 公告类型：新
摘要：像 CLIP 这样的视觉语言模型 (VLM) 已经通过利用提示学习适应了具有部分注释的多标签识别 (MLR)，其中为每个类学习正面和负面提示以将其嵌入与共享视觉文本特征空间中的类存在或不存在相关联。虽然这种方法通过依赖 VLM 先验来提高 MLR 性能，但我们假设学习负面提示可能不是最优的，因为用于训练 VLM 的数据集缺少明确关注类缺失的图像标题对。为了分析正面和负面提示学习对 MLR 的影响，我们引入了 PositiveCoOp 和 NegativeCoOp，其中只有一个提示是在 VLM 指导下学习的，而另一个则被直接在共享特征空间中学习的嵌入向量所取代，而不依赖于文本编码器。通过实证分析，我们观察到负面提示会降低 MLR 性能，而仅学习正面提示，结合学习到的负面嵌入 (PositiveCoOp)，其性能优于双重提示学习方法。此外，我们量化了提示学习相对于简单的仅视觉特征基线提供的性能优势，观察到当缺失标签比例较低时，基线表现出与双重提示学习方法 (DualCoOp) 相当的强大性能，同时需要一半的训练计算和 16 倍的参数]]></description>
      <guid>https://arxiv.org/abs/2409.08381</guid>
      <pubDate>Mon, 16 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>3D 点云中的持续学习：采用光谱技术进行样本选择</title>
      <link>https://arxiv.org/abs/2409.08388</link>
      <description><![CDATA[arXiv:2409.08388v1 公告类型：新
摘要：我们引入了一种用于 3D 对象分类 (CL3D) 的持续学习的新框架。我们的方法基于使用谱聚类从每个类中选择原型。对于非欧几里得数据（例如点云），只要可以定义样本对之间的距离度量，就可以使用谱聚类。选择合适的距离度量使我们能够利用 3D 几何特征来识别每个类的代表性原型。我们探索在输入空间（3D 点）、局部特征空间（1024 维点）和全局特征空间中聚类的有效性。我们在 ModelNet40、ShapeNet 和 ScanNet 数据集上进行实验，仅通过使用输入空间特征就实现了最先进的精度。通过利用组合的输入、局部和全局特征，我们改进了 ModelNet 和 ShapeNet 上的最新技术，利用了竞争方法使用的近一半的内存。对于具有挑战性的 ScanNet 数据集，我们的方法将准确率提高了 4.1%，同时仅消耗竞争对手所用内存的 28%，证明了我们方法的可扩展性。]]></description>
      <guid>https://arxiv.org/abs/2409.08388</guid>
      <pubDate>Mon, 16 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>360PanT：无需训练的文本驱动的 360 度全景图到全景图的翻译</title>
      <link>https://arxiv.org/abs/2409.08397</link>
      <description><![CDATA[arXiv:2409.08397v1 公告类型：新
摘要：在 360 度全景图的翻译中保持边界连续性仍然是现有文本驱动的图像到图像翻译方法面临的重大挑战。这些方法通常会在翻译后的全景图边界处产生视觉上不协调的不连续性，从而破坏沉浸式体验。为了解决这个问题，我们提出了 360PanT，这是一种无需训练的基于文本的 360 度全景图到全景图翻译方法，具有边界连续性。我们的 360PanT 通过两个关键组件实现无缝翻译：边界连续性编码和具有空间控制的无缝平铺翻译。首先，边界连续性编码通过构建扩展的输入图像将输入 360 度全景图的关键边界连续性信息嵌入到嘈杂的潜在表示中。其次，利用这种嵌入的噪声潜在表示并在目标提示的指导下，具有空间控制的无缝平铺翻译能够生成左右两半相同的翻译图像，同时遵循扩展输入的结构和语义布局。此过程可确保最终翻译的 360 度全景图具有无缝边界连续性。在现实世界和合成数据集上的实验结果证明了我们的 360PanT 在翻译 360 度全景图方面的有效性。代码可在 \href{https://github.com/littlewhitesea/360PanT}{https://github.com/littlewhitesea/360PanT} 获得。]]></description>
      <guid>https://arxiv.org/abs/2409.08397</guid>
      <pubDate>Mon, 16 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CF-PRNet：用于点云补全和重建的由粗到精的原型细化网络</title>
      <link>https://arxiv.org/abs/2409.08443</link>
      <description><![CDATA[arXiv:2409.08443v1 公告类型：新
摘要：在现代农业中，对植物和水果的精确监测对于高通量表型分析和自动收获等任务至关重要。本文解决了从部分视图重建水果精确 3D 形状的挑战，这在农业环境中很常见。我们引入了 CF-PRNet，这是一个由粗到细的原型细化网络，它在训练阶段利用高分辨率 3D 数据，但只需要单个 RGB-D 图像即可进行实时推理。我们的方法首先使用一系列卷积块提取从水果部分视图构建的不完整点云数据。提取的特征用于生成缩放向量，这些缩放向量细化两个连续构建的 3D 网格原型 - 一个粗粒度，一个细粒度。这种渐进式细化有助于完成最终点云的详细工作，从而实现详细而准确的重建。 CF-PRNet 表现出了优异的性能指标，倒角距离为 3.78、F1 分数为 66.76%、准确率为 56.56%、召回率为 85.31%，并在甜椒形状完成与重建挑战赛中获得第一名。]]></description>
      <guid>https://arxiv.org/abs/2409.08443</guid>
      <pubDate>Mon, 16 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过大型语言模型实现统一的面部动作单元识别框架</title>
      <link>https://arxiv.org/abs/2409.08444</link>
      <description><![CDATA[arXiv:2409.08444v1 公告类型：新
摘要：面部动作单元（AU）在情感计算领域具有重要意义。在本文中，我们提出了 AU-LLaVA，这是第一个基于大型语言模型（LLM）的统一 AU 识别框架。AU-LLaVA 由视觉编码器、线性投影层和预训练的 LLM 组成。我们精心制作文本描述并在各种 AU 数据集上对模型进行微调，使其能够为同一输入图像生成不同格式的 AU 识别结果。在 BP4D 和 DISFA 数据集上，AU-LLaVA 对近一半的 AU 提供了最准确的识别结果。与之前的基准测试结果相比，我们的模型在特定 AU 识别中实现了高达 11.4% 的 F1 分数改进。在 FEAFA 数据集上，与之前的基准测试结果相比，我们的方法在所有 24 个 AU 上都实现了显着改进。 AU-LLaVA 在 AU 识别方面表现出卓越的性能和多功能性。]]></description>
      <guid>https://arxiv.org/abs/2409.08444</guid>
      <pubDate>Mon, 16 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>VistaFormer：用于卫星图像时间序列分割的可扩展视觉变换器</title>
      <link>https://arxiv.org/abs/2409.08461</link>
      <description><![CDATA[arXiv:2409.08461v1 公告类型：新
摘要：我们介绍了 VistaFormer，这是一种基于 Transformer 的轻量级模型架构，用于遥感图像的语义分割。该模型使用基于 Transformer 的多尺度编码器和轻量级解码器，该解码器聚合了编码器块中捕获的全局和局部注意力。VistaFormer 使用无位置自注意力层，简化了模型架构，无需插入时间和空间代码，当训练和测试图像分辨率不同时，这会降低模型性能。我们研究了用于过滤云等噪声输入信号的简单技术，并证明可以通过用邻域注意力 (NA) 替代多头自注意力 (MHSA) 来提高模型的可扩展性。在 PASTIS 和 MTLCC 裁剪类型分割基准上的实验表明，VistaFormer 比同类模型实现了更好的性能，并且使用 MHSA 只需要 8% 的浮点运算，使用 NA 只需要 11%，同时还使用更少的可训练参数。采用 MHSA 的 VistaFormer 在 PASTIS 基准上将最先进的 mIoU 分数提高了 0.1%，在 MTLCC 基准上将最先进的 mIoU 分数提高了 3%，而采用 NA 的 VistaFormer 在 MTLCC 基准上将最先进的 mIoU 分数提高了 3.7%。]]></description>
      <guid>https://arxiv.org/abs/2409.08461</guid>
      <pubDate>Mon, 16 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>VLTP：面向任务的分割的视觉语言引导标记修剪</title>
      <link>https://arxiv.org/abs/2409.08464</link>
      <description><![CDATA[arXiv:2409.08464v1 公告类型：新 
摘要：视觉变换器 (ViT) 已成为许多分割模型的骨干，始终如一地实现最先进的 (SOTA) 性能。然而，它们的成功是以巨大的计算成本为代价的。图像标记修剪是解决这种复杂性的最有效策略之一。然而，以前的方法在应用于更复杂的面向任务的分割 (TOS) 时就显得不足了，其中每个图像块的类别不是预定义的，而是依赖于特定的输入任务。这项工作引入了视觉语言引导标记修剪 (VLTP)，这是一种新颖的标记修剪机制，可以加速基于 ViT 的分割模型，特别是对于由多模态大语言模型 (MLLM) 引导的 TOS。我们认为 ViT 不需要通过其所有层处理每个图像标记，只需要与推理任务相关的标记。我们设计了一个新的修剪解码器，将图像标记和视觉语言指导作为输入，以预测每个图像标记与任务的相关性。只有相关性高的图像标记才会传递到 ViT 的更深层。实验表明，VLTP 框架将 ViT 的计算成本降低了约 25%，而性能却没有下降，将计算成本降低了约 40%，而性能仅下降了 1%。]]></description>
      <guid>https://arxiv.org/abs/2409.08464</guid>
      <pubDate>Mon, 16 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于开放词汇分割的泛化增强适配器</title>
      <link>https://arxiv.org/abs/2409.08468</link>
      <description><![CDATA[arXiv:2409.08468v1 公告类型：新 
摘要：视觉语言模型 (VLM) 已展示出卓越的开放词汇对象识别能力，这促使它们适应分割等密集预测任务。然而，由于 VLM 缺乏像素级粒度且可用于微调的数据有限，导致过度拟合和泛化能力差，因此直接将 VLM 应用于此类任务仍然具有挑战性。为了解决这些限制，我们提出了泛化增强适配器 (GBA)，这是一种新颖的适配器策略，可增强 VLM 对开放词汇分割的泛化和鲁棒性。GBA 包含两个核心组件：(1) 风格多样化适配器 (SDA)，将特征分解为振幅和相位分量，仅对振幅进行操作以丰富特征空间表示，同时保持语义一致性； (2) 相关约束适配器 (CCA) 利用交叉注意在文本类别和目标区域之间建立更紧密的语义关联，抑制不相关的低频“噪声”信息并避免错误关联。通过浅层 SDA 和深度 CCA 的协同作用，GBA 有效地缓解了过度拟合问题并增强了特征表示的语义相关性。作为一种简单、高效、即插即用的组件，GBA 可以灵活地集成到各种基于 CLIP 的方法中，表现出广泛的适用性并在多个开放词汇分割基准上取得了最佳性能。]]></description>
      <guid>https://arxiv.org/abs/2409.08468</guid>
      <pubDate>Mon, 16 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>RT-DETRv3：具有分层密集正向监督的实时端到端对象检测</title>
      <link>https://arxiv.org/abs/2409.08475</link>
      <description><![CDATA[arXiv:2409.08475v1 公告类型：新
摘要：RT-DETR 是第一个基于 Transformer 的实时端到端物体检测器。它的效率来自于框架设计和匈牙利匹配。然而，与 YOLO 系列等密集监督检测器相比，匈牙利匹配提供的监督要稀疏得多，导致模型训练不足，难以达到最优效果。针对这些问题，我们提出了一种基于 RT-DETR 的分层密集正监督方法，称为 RT-DETRv3。首先，我们引入了一个基于 CNN 的辅助分支，提供密集监督，与原始解码器协作以增强编码器特征表示。其次，为了解决解码器训练不足的问题，我们提出了一种涉及自注意力扰动的新型学习策略。该策略在多个查询组中多样化了正样本的标签分配，从而丰富了正监督。此外，我们引入了一个共享权重的解码器分支进行密集正监督，以确保更多高质量查询与每个基本事实相匹配。值得注意的是，上述所有模块都仅用于训练。我们进行了大量实验来证明我们的方法在 COCO val2017 上的有效性。RT-DETRv3 的表现明显优于现有的实时检测器，包括 RT-DETR 系列和 YOLO 系列。例如，与 RT-DETR-R18/RT-DETRv2-R18 相比，RT-DETRv3-R18 在保持相同延迟的情况下实现了 48.1% 的 AP（+1.6%/+1.4%）。同时，它只需要一半的 epoch 就能达到相当的性能。此外，RT-DETRv3-R101 可以实现令人印象深刻的 54.6% 的 AP，优于 YOLOv10-X。代码即将发布。]]></description>
      <guid>https://arxiv.org/abs/2409.08475</guid>
      <pubDate>Mon, 16 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PSTNet：通过多尺度对齐和频域整合增强息肉分割</title>
      <link>https://arxiv.org/abs/2409.08501</link>
      <description><![CDATA[arXiv:2409.08501v1 公告类型：新
摘要：准确分割结肠镜检查图像中的结肠息肉对于有效诊断和管理结肠直肠癌 (CRC) 至关重要。然而，当前基于深度学习的方法主要依赖于融合跨多个尺度的 RGB 信息，由于 RGB 域信息受限以及多尺度聚合过程中特征错位的挑战，导致准确识别息肉受到限制。为了解决这些限制，我们提出了带分流变压器的息肉分割网络 (PSTNet)，这是一种集成图像中存在的 RGB 和频域线索的新方法。 PSTNet 包含三个关键模块：频率特征注意模块 (FCAM)，用于提取频率线索和捕捉息肉特征；特征补充对齐模块 (FSAM)，用于对齐语义信息和减少错位噪声；交叉感知定位模块 (CPM)，用于将频率线索与高级语义协同起来，实现高效的息肉分割。在具有挑战性的数据集上进行的大量实验表明，PSTNet 在各种指标上显著提高了息肉分割准确率，始终优于最先进的方法。频域线索的整合和 PSTNet 新颖的架构设计有助于推进计算机辅助息肉分割，促进更准确的 CRC 诊断和管理。]]></description>
      <guid>https://arxiv.org/abs/2409.08501</guid>
      <pubDate>Mon, 16 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用热传感器阵列 (TSA) 识别人类室内日常生活行为</title>
      <link>https://arxiv.org/abs/2409.08508</link>
      <description><![CDATA[arXiv:2409.08508v1 公告类型：新
摘要：家庭中使用的日常活动监测系统为健康状况提供了重要信息，尤其是对于老年人。已经引入了多种方法来实现这些目标，通常是侵入式和非侵入式。侵入式方法包括可穿戴设备，非侵入式方法包括运动检测系统，包括运动传感器和热传感器阵列 (TSA)。TSA 系统在保护个人隐私和选择其精确的空间位置时具有优势。在本研究中，昼夜监测人类的日常生活活动，构建相应的活动时间序列和空间概率分布并采用 TSA 系统。监测的活动分为两类：睡眠和日常活动。结果表明，无论白天还是晚上，都可以区分类别。将获得的睡眠活动持续时间与使用相同原始数据的先前研究进行了比较。结果表明，睡眠活动持续时间平均为 9 小时/天，日常生活活动为 7 小时/天。使用受监测位置的双变量分布来确定人的空间概率分布。总而言之，结果表明睡眠活动占主导地位。我们的研究表明，TSA 是监测人类活动的最佳选择。我们提出的方法解决了以前的人类活动监测系统遇到的局限性，例如在知道其精确空间位置的同时保护人类隐私。]]></description>
      <guid>https://arxiv.org/abs/2409.08508</guid>
      <pubDate>Mon, 16 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用监督毒药漏洞来加强自监督防御</title>
      <link>https://arxiv.org/abs/2409.08509</link>
      <description><![CDATA[arXiv:2409.08509v1 公告类型：新
摘要：可用性毒药通过在图像中引入与类相关的快捷特征来利用监督学习 (SL) 算法，使得在中毒数据上训练的模型对于现实世界的数据集毫无用处。自监督学习 (SSL) 利用增强来学习实例区分，被认为是针对中毒数据的强大防御手段。然而，通过在 CIFAR-10 和 ImageNet-100 数据集上对多种毒药扩展 SSL 的研究，我们证明它通常表现不佳，远低于在干净数据上进行训练的表现。利用 SL 对毒药攻击的脆弱性，我们在 SL 上引入了对抗性训练 (AT) 来混淆毒药特征并指导 SSL 的稳健特征学习。我们提出的防御措施，称为 VESPR（针对稳健 SSL 的监督毒药漏洞利用），超过了之前六种防御措施在七种流行可用性毒药中的表现。 VESPR 表现出优于所有先前防御措施的性能，将中毒模型的最低和平均 ImageNet-100 测试准确率分别提高了 16% 和 9%。通过分析和消融研究，我们阐明了 VESPR 学习稳健类特征的机制。]]></description>
      <guid>https://arxiv.org/abs/2409.08509</guid>
      <pubDate>Mon, 16 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CasDyF-Net：通过级联动态滤波器进行图像去雾</title>
      <link>https://arxiv.org/abs/2409.08510</link>
      <description><![CDATA[arXiv:2409.08510v1 公告类型：新
摘要：图像去雾旨在通过减少大气散射和吸收效应来恢复图像清晰度和视觉质量。虽然深度学习在这一领域取得了重大进展，但越来越多的方法受到网络深度的限制。因此，许多方法采用了并行分支策略。然而，它们通常优先考虑分辨率、感受野或频域分割等方面，而不会根据输入特征的分布动态划分分支。受动态过滤的启发，我们提出使用级联动态滤波器通过基于特征图分布动态生成滤波器核来创建多分支网络。为了更好地处理分支特征，我们提出了一个残差多尺度块（RMB），结合不同的感受野。此外，我们还引入了一种基于动态卷积的局部融合方法来合并来自相邻分支的特征。在 RESIDE、Haze4K 和 O-Haze 数据集上进行的实验验证了我们方法的有效性，我们的模型在 RESIDE-Indoor 数据集上实现了 43.21dB 的 PSNR。代码可在 https://github.com/dauing/CasDyF-Net 上找到。]]></description>
      <guid>https://arxiv.org/abs/2409.08510</guid>
      <pubDate>Mon, 16 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>