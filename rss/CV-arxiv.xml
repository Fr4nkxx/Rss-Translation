<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Tue, 11 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>VistaFlow：通过 Q-Learning 进行动态分辨率管理的逼真体积重建</title>
      <link>https://arxiv.org/abs/2502.05222</link>
      <description><![CDATA[arXiv:2502.05222v1 公告类型：新
摘要：我们介绍了 VistaFlow，这是一种可扩展的三维成像技术，能够从一组 2D 照片中重建完全交互式的 3D 体积图像。我们的模型通过可微分渲染系统合成新视点，该系统能够对照片级逼真的 3D 场景进行动态分辨率管理。我们通过引入 QuiQ 来实现这一点，QuiQ 是一种新型中间视频控制器，通过 Q 学习进行训练，通过以毫秒精度调整渲染分辨率来保持始终如一的高帧率。值得注意的是，VistaFlow 在集成 CPU 显卡上本地运行，使其适用于移动和入门级设备，同时仍提供高性能渲染。VistaFlow 绕过神经辐射场 (NeRF)，使用 PlenOctree 数据结构以最低的硬件要求渲染复杂的光交互，例如反射和地下散射。我们的模型能够在消费级硬件上以每秒超过 100 帧的速度，以 1080p 的分辨率实现新颖的视图合成，超越最先进的方法。通过根据每台设备的功能定制渲染质量，VistaFlow 有可能提高从高端工作站到廉价微控制器等各种硬件上照片级逼真 3D 场景渲染的效率和可访问性。]]></description>
      <guid>https://arxiv.org/abs/2502.05222</guid>
      <pubDate>Tue, 11 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>L2GNet：用于广义医学图像分割的解剖结构最佳局部到全局表示</title>
      <link>https://arxiv.org/abs/2502.05229</link>
      <description><![CDATA[arXiv:2502.05229v1 公告类型：新
摘要：连续潜在空间 (CLS) 和离散潜在空间 (DLS) 模型（如 AttnUNet 和 VQUNet）在医学图像分割方面表现出色。相比之下，协同连续和离散潜在空间 (CDLS) 模型在处理细粒度和粗粒度信息方面表现出色。然而，它们在建模长距离依赖关系方面遇到了困难。基于 CLS 或 CDLS 的模型（例如 TransUNet 或 SynergyNet）擅长捕获长距离依赖关系。由于它们严重依赖于使用自注意力的特征池化或聚合，因此它们可能会捕获冗余区域之间的依赖关系。这妨碍了对解剖结构内容的理解，对类内和类间依赖关系的建模提出了挑战，增加了假阴性并损害了泛化。为了解决这些问题，我们提出了 L2GNet，它通过使用最佳传输关联从 DLS 获得的离散代码并在可训练参考上对齐代码来学习全局依赖关系。 L2GNet 实现了判别式即时表示学习，而无需在自注意力模型中使用额外的权重矩阵，因此在医疗应用中具有计算效率。在多器官分割和心脏数据集上进行的大量实验表明，L2GNet 优于最先进的方法，包括 CDLS 方法 SynergyNet，为增强深度学习模型在医学图像分析中的性能提供了一种新方法。]]></description>
      <guid>https://arxiv.org/abs/2502.05229</guid>
      <pubDate>Tue, 11 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AI 生成媒体检测调查：从非 MLLM 到 MLLM</title>
      <link>https://arxiv.org/abs/2502.05240</link>
      <description><![CDATA[arXiv:2502.05240v1 公告类型：新
摘要：人工智能生成媒体的激增对信息真实性和社会信任构成了重大挑战，因此对可靠的检测方法的需求很高。检测人工智能生成媒体的方法发展迅速，与多模态大型语言模型 (MLLM) 的进步同步。当前的检测方法可分为两大类：基于非 MLLM 和基于 MLLM 的方法。前者采用由深度学习技术驱动的高精度、领域特定检测器，而后者采用基于 MLLM 的通用检测器，集成了真实性验证、可解释性和定位功能。尽管该领域取得了重大进展，但在全面调查从领域特定到通用检测方法的转变方面，文献中仍然存在差距。本文通过对这两种方法进行系统回顾来解决这一差距，从单模态和多模态的角度对它们进行分析。我们对这些类别进行了详细的比较分析，研究了它们方法上的相似之处和不同之处。通过此分析，我们探索了潜在的混合方法并确定了伪造检测中的关键挑战，为未来的研究提供了方向。此外，随着 MLLM 在检测任务中越来越普遍，道德和安全考虑已成为全球关注的关键问题。我们研究了不同司法管辖区内围绕生成式人工智能 (GenAI) 的监管环境，为该领域的研究人员和从业者提供了宝贵的见解。]]></description>
      <guid>https://arxiv.org/abs/2502.05240</guid>
      <pubDate>Tue, 11 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用人类水平的概念进行可解释的故障检测</title>
      <link>https://arxiv.org/abs/2502.05275</link>
      <description><![CDATA[arXiv:2502.05275v1 公告类型：新
摘要：可靠的故障检测在安全关键应用中至关重要。然而，众所周知，神经网络会对错误分类的样本产生过度自信的预测。因此，这仍然是一个问题，因为现有的置信度得分函数依赖于类别级信号 logits 来检测故障。这项研究引入了一种创新策略，利用人类层面的概念来实现双重目的：可靠地检测模型何时失败并透明地解释原因。通过为每个类别整合一组细微的信号，我们的方法可以对模型的置信度进行更细粒度的评估。我们提出了一种简单但非常有效的方法，该方法基于对输入图像的概念激活的序数排序。我们的方法没有任何花哨的东西，显著降低了各种现实世界图像分类基准中的假阳性率，具体来说，在 ImageNet 上降低了 3.7%，在 EuroSAT 上降低了 9%。]]></description>
      <guid>https://arxiv.org/abs/2502.05275</guid>
      <pubDate>Tue, 11 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Invizo：阿拉伯语手写文档光学字符识别解决方案</title>
      <link>https://arxiv.org/abs/2502.05277</link>
      <description><![CDATA[arXiv:2502.05277v1 公告类型：新
摘要：将阿拉伯语文本图像转换为纯文本是学术界和工业界广泛研究的课题。然而，由于阿拉伯文字变体的复杂性，阿拉伯语手写和印刷文本的识别面临着艰巨的挑战。这项工作提出了一种端到端解决方案，用于识别阿拉伯语手写、印刷和阿拉伯数字，并以结构化的方式呈现数据。我们在文本检测任务上达到了 81.66% 的精度、78.82% 的召回率和 79.07% 的 F 度量，为所提出的解决方案提供了支持。所提出的识别模型结合了最先进的基于 CNN 的特征提取和基于 Transformer 的序列建模，以适应笔迹、笔画粗细、对齐和噪声条件的变化。对该模型的评估表明，它在印刷文本和手写文本上都表现出色，印刷文本的 CER 为 0.59%，WER 为 1.72%，手写文本的 CER 为 7.91%，WER 为 31.41%。事实证明，所提出的总体解决方案在实际 OCR 任务中是可靠的。配备了检测和识别模型以及其他特征提取和匹配辅助算法。通过通用实现，使该解决方案适用于任何给定的阿拉伯手写或印刷文档或收据。因此，它对于任何给定上下文都是实用且有用的。]]></description>
      <guid>https://arxiv.org/abs/2502.05277</guid>
      <pubDate>Tue, 11 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>医学图像密集对比表示学习中假阳性和假阴性问题的同态先验</title>
      <link>https://arxiv.org/abs/2502.05282</link>
      <description><![CDATA[arXiv:2502.05282v1 公告类型：新
摘要：密集对比表示学习 (DCRL) 极大地提高了图像密集预测任务的学习效率，显示出其在降低医学图像收集和密集注释的大量成本方面的巨大潜力。然而，医学图像的特性使得对应关系发现不可靠，从而带来了 DCRL 中大规模假阳性和假阴性 (FP&amp;N) 对的未解决的问题。在本文中，我们提出了 GEoMetric 视觉密集相似性 (GEMINI) 学习，它将同态嵌入 DCRL 之前，并能够可靠地发现对应关系以实现有效的密集对比。我们提出了一种可变形同态学习 (DHL)，它对医学图像的同态进行建模，并学习估计可变形映射以在拓扑保持的情况下预测像素的对应关系。它有效地减少了配对的搜索空间，并通过梯度驱动负对的隐式和软学习。我们还提出了一种几何语义相似度（GSS），它提取特征中的语义信息来衡量对应学习的对齐程度。它将提高学习效率和变形性能，可靠地构建正对。我们在实验中对两个典型的表示学习任务实现了两个实用变体。我们在七个数据集上取得了令人鼓舞的结果，这些结果优于现有方法，表明我们具有巨大的优势。我们将在配套链接上发布我们的代码：https://github.com/YutingHe-list/GEMINI。]]></description>
      <guid>https://arxiv.org/abs/2502.05282</guid>
      <pubDate>Tue, 11 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 YOLO 和基于规则的方法进行无人机检测和跟踪</title>
      <link>https://arxiv.org/abs/2502.05292</link>
      <description><![CDATA[arXiv:2502.05292v1 公告类型：新
摘要：无人机或无人驾驶飞行器传统上用于军事任务、战争和间谍活动。然而，由于涉及安全和检查、运输、研究目的和休闲无人机飞行的多种工业应用，无人机的使用量显着增加。公共场所无人机活动量的增加需要采取监管行动，以保护隐私和安全。因此，检测非法无人机活动（例如边界侵占）成为必要。此类检测任务通常由深度学习模型自动执行，这些模型在带注释的图像数据集上进行训练。本文以以前的工作为基础，并扩展了已经发布的开源数据集。提供了整个数据集的描述和分析。该数据集用于训练 YOLOv7 深度学习模型及其一些小变体，并提供结果。由于检测模型基于单个图像输入，因此使用简单的基于互相关的跟踪器来减少检测丢失并提高视频中的跟踪性能。最后对整个无人机检测系统进行总结。]]></description>
      <guid>https://arxiv.org/abs/2502.05292</guid>
      <pubDate>Tue, 11 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>实现细粒度肾血管分割：使用 FH-Seg 进行全面分层学习</title>
      <link>https://arxiv.org/abs/2502.05320</link>
      <description><![CDATA[arXiv:2502.05320v1 公告类型：新
摘要：精确的肾血管细粒度分割对于肾脏病分析至关重要，但由于图像多样且注释不足，它面临挑战。现有方法难以准确分割肾血管的复杂区域，例如内壁和外壁、动脉和病变。在本文中，我们介绍了 FH-Seg，这是一种全尺度分层学习框架，旨在全面分割肾血管。具体而言，FH-Seg 采用全尺度跳跃连接，将详细的解剖信息与跨尺度的上下文语义融合在一起，有效地弥合了结构和病理背景之间的差距。此外，我们实现了一个可学习的分层软注意门，以自适应地减少非核心信息的干扰，增强对关键血管特征的关注。为了推进肾脏病理分割研究，我们还开发了一个大型肾血管 (LRV) 数据集，其中包含 5,600 条肾动脉的 16,212 张细粒度注释图像。在 LRV 数据集上进行的大量实验表明，FH-Seg 的准确率较高（71.23% Dice、73.06% F1），比 Omni-Seg 分别高出 2.67 和 2.13 个百分点。代码可在以下网址获取：https://github.com/hrlblab/FH-seg。]]></description>
      <guid>https://arxiv.org/abs/2502.05320</guid>
      <pubDate>Tue, 11 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>NextBestPath：高效绘制未知环境的 3D 地图</title>
      <link>https://arxiv.org/abs/2502.05378</link>
      <description><![CDATA[arXiv:2502.05378v1 公告类型：新
摘要：这项工作解决了主动 3D 地图绘制的问题，其中代理必须找到一条有效的轨迹来详尽地重建新场景。以前的方法主要预测代理位置附近的下一个最佳视图，这很容易卡在局部区域。此外，由于几何复杂性有限和地面真实网格不准确，现有的室内数据集不足。为了克服这些限制，我们引入了一个新数据集 AiMDoom，其中包含 Doom 视频游戏的地图生成器，能够在不同的室内环境中更好地对主动 3D 地图进行基准测试。此外，我们提出了一种称为下一个最佳路径 (NBP) 的新方法，它可以预测长期目标，而不是仅仅关注短视视图。该模型联合预测长期目标和障碍物地图的累积表面覆盖增益，使其能够使用统一模型有效地规划最佳路径。通过利用在线数据收集、数据增强和课程学习，NBP 在现有 MP3D 数据集和我们的 AiMDoom 数据集上的表现显著优于最先进的方法，在不同复杂程度的室内环境中实现了更高效的地图绘制。]]></description>
      <guid>https://arxiv.org/abs/2502.05378</guid>
      <pubDate>Tue, 11 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从粗到细的结构感知艺术风格迁移</title>
      <link>https://arxiv.org/abs/2502.05387</link>
      <description><![CDATA[arXiv:2502.05387v1 公告类型：新
摘要：艺术风格迁移旨在使用风格图像和内容图像合成保留与风格图像相同艺术表现力的目标图像，同时保留内容图像的基本内容。许多最近提出的风格迁移方法都有一个共同的问题；即它们只是将风格图像的纹理和颜色迁移到内容图像的全局结构中。结果，内容图像具有与风格图像的局部结构不相似的局部结构。在本文中，我们提出了一种有效的方法，可用于迁移风格模式，同时将局部风格结构融合到局部内容结构中。在我们的方法中，首先使用粗略网络在低分辨率下重建不同级别的粗略风格化特征，其中粗略地迁移风格颜色分布，并将内容结构与风格结构相结合。然后，采用重建的特征和内容特征，使用具有三个结构选择性融合 (SSF) 模块的精细网络合成具有高分辨率的高质量结构感知风格化图像。通过生成有吸引力的高质量风格化结果并与一些最先进的风格转换方法进行比较，证明了我们方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2502.05387</guid>
      <pubDate>Tue, 11 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>超越并摆脱扩散：可逆引导一致性训练</title>
      <link>https://arxiv.org/abs/2502.05391</link>
      <description><![CDATA[arXiv:2502.05391v1 公告类型：新
摘要：图像生成中的指导引导模型朝着更高质量或更有针对性的输出发展，通常通过无分类器指导 (CFG) 在扩散模型 (DM) 中实现。然而，最近的一致性模型 (CM) 提供较少的功能评估，依赖于从预训练的 DM 中提取 CFG 知识来实现​​指导，这使得它们成本高昂且不灵活。在这项工作中，我们提出了可逆引导一致性训练 (iGCT)，这是一种完全由数据驱动的引导 CM 的新型训练框架。iGCT 作为一项开创性的工作，有助于快速和引导图像生成和编辑，而无需训练和提炼 DM，大大降低了整体计算要求。iGCT 解决了高指导尺度下 CFG 中出现的饱和伪影。我们在 CIFAR-10 和 ImageNet64 上进行的大量实验表明，与 CFG 相比，iGCT 显著提高了 FID 和精度。在指导值为 13 的情况下，iGCT 将精度提高到 0.8，而 DM 的精度则下降到 0.47。我们的工作迈出了第一步，即在不依赖 DM 的情况下实现 CM 的指导和反演。]]></description>
      <guid>https://arxiv.org/abs/2502.05391</guid>
      <pubDate>Tue, 11 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于图像压缩的卷积深度着色：基于颜色网格的方法</title>
      <link>https://arxiv.org/abs/2502.05402</link>
      <description><![CDATA[arXiv:2502.05402v1 公告类型：新
摘要：图像压缩优化技术的研究是学术界内外持续关注的话题。一种有望在未来改进该领域的方法是图像着色，因为图像着色算法可以减少图像需要存储的颜色数据量。我们的工作重点是优化基于颜色网格的方法，以实现全自动图像颜色信息保留，并结合卷积着色网络架构进行图像压缩。更一般地说，使用卷积神经网络进行图像重新着色，我们希望最大限度地减少存储的颜色信息量，同时仍然能够忠实地重新着色图像。我们的结果产生了一个有希望的图像压缩比，同时仍然允许成功的图像重新着色达到高 CSIM 值。]]></description>
      <guid>https://arxiv.org/abs/2502.05402</guid>
      <pubDate>Tue, 11 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>海洋环境下无人机深度单目位姿估计的视觉在环仿真</title>
      <link>https://arxiv.org/abs/2502.05409</link>
      <description><![CDATA[arXiv:2502.05409v1 公告类型：新
摘要：本文提出了一种视觉在环仿真环境，用于在海洋环境中运行的无人机的深度单目姿态估计。最近，一个具有变压器架构的深度神经网络已经成功训练，可以估计无人机相对于研究船驾驶舱的姿态，克服了基于 GPS 的方法的几个局限性。然而，由于研究船的有限可用性和相关的运营成本，在实际海洋环境中验证深度姿态估计方案带来了重大挑战。为了解决这些问题，我们利用高斯溅射的最新进展提出了一个照片般逼真的 3D 虚拟环境，高斯溅射是一种新技术，它通过将图像像素建模为 3D 空间中的高斯分布来表示 3D 场景，从多个视点创建轻量级和高质量的视觉模型。这种方法可以创建一个集成现场收集的多个真实世界图像的虚拟环境。由此产生的模拟结果支持在室内测试飞行动作，同时验证飞行软件、硬件和深度单目姿态估计方案的各个方面。这种方法为测试和验证船载无人机的自主飞行提供了一种经济高效的解决方案，特别侧重于基于视觉的控制和估计算法。]]></description>
      <guid>https://arxiv.org/abs/2502.05409</guid>
      <pubDate>Tue, 11 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Show-o Turbo：加速统一多模式理解和生成</title>
      <link>https://arxiv.org/abs/2502.05415</link>
      <description><![CDATA[arXiv:2502.05415v1 公告类型：新 
摘要：人们对构建统一的多模态理解和生成模型的研究兴趣日益浓厚，其中 Show-o 是一个显著的代表，在文本到图像和图像到文本生成方面都表现出巨大的潜力。Show-o 的推理涉及逐步去噪图像标记和自回归解码文本标记，因此不幸的是，两方面都存在效率低下的问题。本文引入了 Show-o Turbo 来弥补这一差距。我们首先基于文本标记的并行解码，为 Show-o 中的图像和文本生成确定一个统一的去噪视角。然后，我们建议将一致性蒸馏 (CD)（一种缩短扩散模型去噪过程的合格方法）扩展到 Show-o 的多模态去噪轨迹。我们引入了一种轨迹分割策略和课程学习程序来提高训练收敛性。从经验上看，在文本到图像生成中，Show-o Turbo 在不使用无分类器指导 (CFG) 的情况下，在 4 个采样步骤中显示 GenEval 得分为 0.625，优于原始 Show-o (8 个步骤和 CFG)；在图像到文本生成中，Show-o Turbo 的速度提高了 1.5 倍，而性能没有明显下降。代码可在 https://github.com/zhijie-group/Show-o-Turbo 上找到。]]></description>
      <guid>https://arxiv.org/abs/2502.05415</guid>
      <pubDate>Tue, 11 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LRA-GNN：用于面部年龄估计的具有初始和动态残差的潜在关系感知图神经网络</title>
      <link>https://arxiv.org/abs/2502.05423</link>
      <description><![CDATA[arXiv:2502.05423v1 公告类型：新
摘要：人脸信息主要集中在人脸关键点上，前沿研究开始利用图神经网络将人脸分割成块作为节点来建模复杂的人脸表示。然而，这些方法基于相似度阈值构建节点到节点的关系，因此存在一些潜在关系缺失的问题。这些潜在关系对于人脸衰老的深度语义表示至关重要。在本文中，我们提出了一种新的具有初始和动态残差的潜在关系感知图神经网络（LRA-GNN），以实现稳健而全面的人脸表示。具体来说，我们首先利用面部关键点作为先验知识构建初始图，然后对初始图采用随机游走策略以获得全局结构，两者共同指导后续的有效探索和全面表示。然后，LRA-GNN 利用多注意机制来捕获潜在关系，并基于上述指导生成一组包含丰富面部信息和完整结构的全连接图。为了避免全连通图上深度特征提取的过度平滑问题，我们精心设计了深度残差图卷积网络，融合了自适应初始残差和动态发展残差，以确保信息的一致性和多样性。最后，为了提高估计精度和泛化能力，我们提出了渐进式强化学习来优化集成分类回归器。我们提出的框架在几个年龄估计基准上超越了最先进的基线，证明了其实力和有效性。]]></description>
      <guid>https://arxiv.org/abs/2502.05423</guid>
      <pubDate>Tue, 11 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>