<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Mon, 02 Dec 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>场景副驾驶：在人机交互下将文本生成为视频</title>
      <link>https://arxiv.org/abs/2411.18644</link>
      <description><![CDATA[arXiv:2411.18644v1 公告类型：新
摘要：视频生成已达到令人印象深刻的质量，但仍然存在诸如时间不一致和违反物理定律等伪影。利用 3D 场景可以通过对场景实体进行精确控制从根本上解决这些问题。为了便于轻松生成各种逼真的场景，我们提出了 Scene Copilot，这是一个将大型语言模型 (LLM) 与程序 3D 场景生成器相结合的框架。具体来说，Scene Copilot 由 Scene Codex、BlenderGPT 和 Human 组成。Scene Codex 旨在将文本用户输入转换为 3D 场景生成器可以理解的命令。BlenderGPT 为用户提供了一种直观直接的方式来精确控制生成的 3D 场景和最终输出视频。此外，用户可以利用 Blender UI 来接收即时视觉反馈。此外，我们还策划了一个代码格式的对象程序数据集，以进一步增强我们系统的功能。各个组件无缝协作，帮助用户生成所需的 3D 场景。大量实验证明了我们的框架在定制 3D 场景和视频生成方面的能力。]]></description>
      <guid>https://arxiv.org/abs/2411.18644</guid>
      <pubDate>Mon, 02 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Bi-ICE：通过概念和输入嵌入之间的双向交互进行图像分类的内部可解释框架</title>
      <link>https://arxiv.org/abs/2411.18645</link>
      <description><![CDATA[arXiv:2411.18645v1 公告类型：新
摘要：内部可解释性是一个有前途的领域，专注于揭示人工智能系统的内部机制，并开发可扩展的自动化方法，以在机械层面理解这些系统。虽然大量研究已经探索了从高级问题或算法假设开始的自上而下的方法，以及从低级或电路级描述构建更高级别抽象的自下而上的方法，但大多数努力都集中在分析大型语言模型上。此外，将内部可解释性应用于大规模图像任务的关注有限，主要关注架构和功能级别以可视化学习到的概念。在本文中，我们首先提出了一个概念框架，支持大规模图像分类任务的内部可解释性和多级分析。我们引入了概念和输入嵌入之间的双向交互 (Bi-ICE) 模块，它促进了计算、算法和实现级别的可解释性。该模块通过基于人类可理解的概念生成预测、量化其贡献并在输入中定位它们来增强透明度。最后，我们展示了图像分类中增强的透明度，测量概念贡献并精确定位它们在输入中的位置。我们的方法通过展示概念学习及其收敛的过程来强调算法的可解释性。]]></description>
      <guid>https://arxiv.org/abs/2411.18645</guid>
      <pubDate>Mon, 02 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>RoMo：强大的运动分割功能可改善运动结构</title>
      <link>https://arxiv.org/abs/2411.18650</link>
      <description><![CDATA[arXiv:2411.18650v1 公告类型：新
摘要：在从单目随意拍摄的视频重建和生成 4D 场景方面取得了广泛进展。虽然这些任务严重依赖于已知的相机姿势，但使用运动结构 (SfM) 查找此类姿势的问题通常取决于稳健地将视频的静态部分与动态部分分离。缺乏对这个问题的稳健解决方案限制了 SfM 相机校准管道的性能。我们提出了一种基于视频的运动分割的新方法，以识别相对于固定世界框架移动的场景组件。我们简单但有效的迭代方法 RoMo 将光流和极线线索与预先训练的视频分割模型相结合。它优于无监督的运动分割基线以及从合成数据训练的监督基线。更重要的是，现成的 SfM 管道与我们的分割掩码相结合，为具有动态内容的场景建立了新的最先进的相机校准方法，其性能远远优于现有方法。]]></description>
      <guid>https://arxiv.org/abs/2411.18650</guid>
      <pubDate>Mon, 02 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>可解释的小样本泛化的语言表征学习</title>
      <link>https://arxiv.org/abs/2411.18651</link>
      <description><![CDATA[arXiv:2411.18651v1 公告类型：新
摘要：人类只需观察几个例子就能识别物体，这是人类对现实世界环境固有语言理解的非凡能力。开发语言化和可解释的表示可以显著提高低数据环境下的模型泛化能力。在这项工作中，我们提出了语言化表示学习 (VRL)，这是一种使用少量数据自动提取人类可解释特征进行物体识别的新方法。我们的方法以自然语言的形式独特地捕捉类间差异和类内共性，通过使用视觉语言模型 (VLM) 来识别不同类别之间的关键判别特征和同一类别内的共同特征。然后，这些语言化特征通过 VLM 映射到数字向量。得到的特征向量可以进一步用于训练和推断下游分类器。实验结果表明，在相同的模型规模下，VRL 比之前最先进的方法实现了 24% 的绝对改进，同时使用的数据量减少了 95% 且模型更小。此外，与人工标记的属性相比，VRL 学习到的特征在用于下游分类任务时可实现 20% 的绝对增益。代码位于：https://github.com/joeyy5588/VRL/tree/main。]]></description>
      <guid>https://arxiv.org/abs/2411.18651</guid>
      <pubDate>Mon, 02 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Surf-NeRF：表面规则化神经辐射场</title>
      <link>https://arxiv.org/abs/2411.18652</link>
      <description><![CDATA[arXiv:2411.18652v1 公告类型：新
摘要：神经辐射场 (NeRF) 提供高保真、连续的场景表示，可以真实地表示光的复杂行为。尽管最近的研究（如 Ref-NeRF）通过物理启发模型改进了几何形状，但 NeRF 克服形状辐射模糊性并收敛到与真实几何形状一致的表示的能力仍然有限。我们展示了表面光场模型的课程学习如何帮助 NeRF 收敛到更几何准确的场景表示。我们引入了四个额外的正则化项来施加几何平滑度、法线一致性以及场景中几何形状的朗伯和镜面外观的分离，以符合物理模型。与当前基于反射的 NeRF 变体相比，我们的方法使位置编码的 NeRF 的法线提高了 14.4%，基于网格的模型提高了 9.2%。这包括分离的视图相关外观，使 NeRF 具有与捕获的场景一致的几何表示。我们证明了我们的方法与现有 NeRF 变体的兼容性，这是为几何关键应用实现基于辐射度的表示的关键步骤。]]></description>
      <guid>https://arxiv.org/abs/2411.18652</guid>
      <pubDate>Mon, 02 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AToM：利用 GPT-4Vision 奖励在事件级别对齐文本到运动模型</title>
      <link>https://arxiv.org/abs/2411.18654</link>
      <description><![CDATA[arXiv:2411.18654v1 公告类型：新
摘要：最近，文本到动作模型为以更高的效率和灵活性创建逼真的人体运动开辟了新的可能性。然而，由于文本提示和期望的运动结果之间的复杂关系，将动作生成与事件级文本描述对齐带来了独特的挑战。为了解决这个问题，我们引入了 AToM，这是一个通过利用 GPT-4Vision 的奖励来增强生成的动作和文本提示之间的一致性的框架。AToM 包括三个主要阶段：首先，我们构建一个数据集 MotionPrefer，将三种类型的事件级文本提示与生成的动作配对，涵盖运动的完整性、时间关系和频率。其次，我们设计了一个利用 GPT-4Vision 进行详细运动注释的范式，包括每个子任务的视觉数据格式、任务特定指令和评分规则。最后，我们使用以该范式为指导的强化学习对现有的文本到动作模型进行微调。实验结果表明，AToM 显著提高了文本到运动生成的事件级对齐质量。]]></description>
      <guid>https://arxiv.org/abs/2411.18654</guid>
      <pubDate>Mon, 02 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>HDI-Former：使用帧和事件进行对象检测的混合动态交互 ANN-SNN Transformer</title>
      <link>https://arxiv.org/abs/2411.18658</link>
      <description><![CDATA[arXiv:2411.18658v1 公告类型：新
摘要：结合帧和事件的互补优势已广泛应用于具有挑战性的场景中的物体检测。然而，大多数物体检测方法使用两个独立的人工神经网络 (ANN) 分支，限制了两个视觉流之间的跨模态信息交互，并且在从低功耗事件流中提取时间线索时遇到挑战。为了应对这些挑战，我们提出了 HDI-Former，一种混合​​动态交互 ANN-SNN Transformer，标志着首次尝试设计直接训练的混合 ANN-SNN 架构，用于使用帧和事件进行高精度和节能的物体检测。从技术上讲，我们首先提出一种新颖的语义增强自注意力机制，该机制加强了 ANN Transformer 分支内图像编码标记之间的相关性，以获得更好的性能。然后，我们设计了一个 Spiking Swin Transformer 分支来对低功耗事件流中的时间线索进行建模。最后，我们提出了一种受生物启发的 ANN 和 SNN 子网络之间的动态交互机制，用于跨模态信息交互。结果表明，我们的 HDI-Former 大大优于 11 种最先进的方法和我们的 4 个基线。我们的 SNN 分支也表现出与具有相同架构的 ANN 相当的性能，同时在 DSEC-Detection 数据集上消耗的能量减少了 10.57$\times$。我们的开源代码可在补充材料中找到。]]></description>
      <guid>https://arxiv.org/abs/2411.18658</guid>
      <pubDate>Mon, 02 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DHCP：通过大型视觉语言模型中的跨模态注意力模式检测幻觉</title>
      <link>https://arxiv.org/abs/2411.18659</link>
      <description><![CDATA[arXiv:2411.18659v1 公告类型：新
摘要：大型视觉语言模型 (LVLM) 在复杂的多模态任务上表现出色。然而，它们仍然受到严重的幻觉问题的困扰，包括对象、属性和关系幻觉。为了准确检测这些幻觉，我们研究了幻觉和非幻觉状态之间跨模态注意力模式的变化。利用这些区别，我们开发了一种能够识别幻觉的轻量级检测器。我们提出的方法，通过跨模态注意力模式检测幻觉 (DHCP)，很简单，不需要额外的 LVLM 训练或额外的 LVLM 推理步骤。实验结果表明，DHCP 在幻觉检测方面取得了显著的表现。通过提供对 LVLM 中幻觉的识别和分析的新见解，DHCP 有助于提高这些模型的可靠性和可信度。]]></description>
      <guid>https://arxiv.org/abs/2411.18659</guid>
      <pubDate>Mon, 02 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>OOD-HOI：超越训练领域的文本驱动的 3D 全身人机交互生成</title>
      <link>https://arxiv.org/abs/2411.18660</link>
      <description><![CDATA[arXiv:2411.18660v1 公告类型：新
摘要：从文本描述生成逼真的 3D 人机交互 (HOI) 是一个活跃的研究课题，在虚拟和增强现实、机器人和动画领域具有潜在应用。然而，由于缺乏大规模交互数据和难以确保物理合理性，尤其是在域外 (OOD) 场景中，创建高质量的 3D HOI 仍然具有挑战性。当前的方法往往侧重于身体或手部，这限制了它们产生连贯而逼真的交互的能力。在本文中，我们提出了 OOD-HOI，这是一个文本驱动的框架，用于生成全身人机交互，可以很好地推广到新物体和动作。我们的方法集成了双分支互惠扩散模型来合成初始交互姿势、接触引导交互细化器以根据预测的接触面积提高物理准确性，以及动态自适应机制，其中包括语义调整和几何变形以提高鲁棒性。实验结果表明，与现有方法相比，我们的 OOD-HOI 可以在 OOD 场景中生成更逼真、物理上更合理的 3D 交互姿势。]]></description>
      <guid>https://arxiv.org/abs/2411.18660</guid>
      <pubDate>Mon, 02 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>HoliSDiP：通过整体语义和扩散先验实现图像超分辨率</title>
      <link>https://arxiv.org/abs/2411.18662</link>
      <description><![CDATA[arXiv:2411.18662v1 公告类型：新
摘要：文本到图像扩散模型已成为现实世界图像超分辨率 (Real-ISR) 的强大先验。然而，现有方法可能会由于嘈杂的文本提示和缺乏空间信息而产生意想不到的结果。在本文中，我们提出了 HoliSDiP，这是一个利用语义分割为基于扩散的 Real-ISR 提供​​精确文本和空间指导的框架。我们的方法使用语义标签作为简洁的文本提示，同时通过分割掩码和我们提出的 Segmentation-CLIP Map 引入密集的语义指导。大量实验表明，HoliSDiP 通过降低提示噪声和增强空间控制，在各种 Real-ISR 场景中实现了图像质量的显着改善。]]></description>
      <guid>https://arxiv.org/abs/2411.18662</guid>
      <pubDate>Mon, 02 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于增强视频扩散采样的时空跳跃引导</title>
      <link>https://arxiv.org/abs/2411.18664</link>
      <description><![CDATA[arXiv:2411.18664v1 公告类型：新
摘要：扩散模型已成为生成高质量图像、视频和 3D 内容的强大工具。虽然 CFG 等采样引导技术可以提高质量，但它们会降低多样性和运动。自动引导可以缓解这些问题，但需要额外的弱模型训练，从而限制了其在大型模型中的实用性。在这项工作中，我们引入了时空跳过引导 (STG)，这是一种简单的无训练采样引导方法，用于增强基于 Transformer 的视频扩散模型。STG 通过自扰采用隐式弱模型，避免了对外部模型或额外训练的需求。通过有选择地跳过时空层，STG 生成原始模型的对齐、降级版本，以提高样本质量，而不会影响多样性或动态程度。我们的贡献包括：(1) 引入 STG 作为视频传播模型的高效、高性能引导技术；(2) 通过层跳过模拟弱模型，消除了对辅助模型的需求；(3) 确保质量增强的引导，而不会损害样本多样性或动态（与 CFG 不同）。如需更多结果，请访问 https://junhahyung.github.io/STGuidance。]]></description>
      <guid>https://arxiv.org/abs/2411.18664</guid>
      <pubDate>Mon, 02 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SpotLight：通过漫射实现阴影引导物体重新照明</title>
      <link>https://arxiv.org/abs/2411.18665</link>
      <description><![CDATA[arXiv:2411.18665v1 公告类型：新
摘要：最近的研究表明，扩散模型可以用作强大的神经渲染引擎，可用于将虚拟对象插入图像中。然而，与典型的基于物理的渲染器不同，神经渲染引擎受到缺乏对照明设置的手动控制的限制，而这通常对于改善或个性化所需的图像结果至关重要。在本文中，我们展示了只需指定对象所需的阴影即可实现对象重新照明的精确照明控制。更令人惊讶的是，我们表明，仅将对象的阴影注入预先训练的基于扩散的神经渲染器，使其能够根据所需的光位置准确地遮蔽对象，同时在目标背景图像中正确协调对象（及其阴影）。我们的方法 SpotLight 利用现有的神经渲染方法，无需额外训练即可实现可控的重新照明结果。具体来说，我们展示了它在最近文献中的两个神经渲染器中的使用。我们表明，SpotLight 在数量和感知上都实现了卓越的对象合成结果，这已由用户研究证实，优于专为重新照明而设计的现有基于扩散的模型。]]></description>
      <guid>https://arxiv.org/abs/2411.18665</guid>
      <pubDate>Mon, 02 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>3D 场景图引导视觉语言预训练</title>
      <link>https://arxiv.org/abs/2411.18666</link>
      <description><![CDATA[arXiv:2411.18666v1 公告类型：新
摘要：3D 视觉语言 (VL) 推理因其能够将 3D 物理世界与自然语言描述联系起来而备受关注。现有方法通常遵循特定于任务的高度专业化的范例。因此，这些方法专注于有限范围的推理子任务，并且严重依赖手工制作的模块和辅助损失。这凸显了对更简单、统一和通用模型的需求。在本文中，我们利用 3D 场景图和自然语言之间的内在联系，提出了一个 3D 场景图引导的视觉语言预训练 (VLP) 框架。我们的方法利用模态编码器、图卷积层和交叉注意层来学习适应各种 3D VL 推理任务的通用表示，从而无需针对特定任务进行设计。预训练目标包括：1）场景图引导的对比学习，利用 3D 场景图和自然语言之间的强相关性，将 3D 对象与各种细粒度级别的文本特征对齐；2）掩蔽模态学习，使用跨模态信息重建掩蔽单词和 3D 对象。我们不是直接重建掩蔽对象的 3D 点云，而是使用位置线索来预测它们的语义类别。大量实验表明，我们的预训练模型在对几个下游任务进行微调后，在 3D 视觉基础、3D 密集字幕和 3D 问答等任务中实现了与现有方法相当或更好的性能。]]></description>
      <guid>https://arxiv.org/abs/2411.18666</guid>
      <pubDate>Mon, 02 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过 3D 高斯溅射进行点云无监督预训练</title>
      <link>https://arxiv.org/abs/2411.18667</link>
      <description><![CDATA[arXiv:2411.18667v1 公告类型：新
摘要：在大规模未标记数据集上进行预训练有助于模型在 3D 视觉任务上实现强大的性能，尤其是在注释有限的情况下。然而，由于体积渲染的固有特性，现有的基于渲染的自监督框架在预训练期间计算要求高且内存密集。在本文中，我们提出了一个名为 GS$^3$ 的高效框架来学习点云表示，它将快速 3D 高斯 Splatting 无缝集成到基于渲染的框架中。我们框架背后的核心思想是通过将渲染的 RGB 图像与真实 RGB 图像进行比较来预训练点云编码器，因为只有富含学习到的丰富几何和外观信息的高斯点才能产生高质量的渲染。具体而言，我们将输入的 RGB-D 图像反向投影到 3D 空间并使用点云编码器提取逐点特征。然后，我们从学习到的点云特征中预测场景的 3D 高斯点，并使用基于图块的光栅化器进行图像渲染。最后，预训练的点云编码器可以进行微调以适应各种下游 3D 任务，包括高级感知任务（如 3D 分割和检测）以及低级任务（如 3D 场景重建）。对下游任务的大量实验证明了预训练点云编码器的强大可迁移性和我们自监督学习框架的有效性。此外，我们的 GS$^3$ 框架效率很高，与之前基于渲染的框架 Ponder 相比，实现了大约 9$\times$ 的预训练加速和不到 0.25$\times$ 的内存成本。]]></description>
      <guid>https://arxiv.org/abs/2411.18667</guid>
      <pubDate>Mon, 02 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>面向长视频的分块生成</title>
      <link>https://arxiv.org/abs/2411.18668</link>
      <description><![CDATA[arXiv:2411.18668v1 公告类型：新 
摘要：由于时空域的固有复杂性以及计算巨大张量所需的大量 GPU 内存需求，生成长时长视频一直是一项重大挑战。虽然基于扩散的生成模型在视频生成任务中实现了最先进的性能，但它们通常是使用预定义的视频分辨率和长度进行训练的。在推理过程中，应首先指定具有特定分辨率和长度的噪声张量，然后模型将同时对整个视频张量（所有帧一起）进行去噪。当指定的分辨率和/或长度超过一定限制时，这种方法很容易引发内存不足 (OOM) 问题。解决该问题的方法之一是自回归地生成许多具有强块间时空关系的短视频块，然后将它们连接在一起形成长视频。在该方法中，长视频生成任务被划分为多个短视频生成子任务，并将每个子任务的成本降低到可行的水平。在本文中，我们对使用自回归逐块策略的长视频生成进行了详细的调查。我们解决了将短图像到视频模型应用于长视频任务时引起的常见问题，并设计了一种高效的 $k$ 步搜索解决方案来缓解这些问题。]]></description>
      <guid>https://arxiv.org/abs/2411.18668</guid>
      <pubDate>Mon, 02 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>