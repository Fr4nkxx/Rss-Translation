<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>CS.CV更新arxiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.cv更新arxiv.org e-print存档。</description>
    <lastBuildDate>Wed, 26 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>海报：科学海报摘要的多模式基准</title>
      <link>https://arxiv.org/abs/2502.17540</link>
      <description><![CDATA[ARXIV：2502.17540V1公告类型：新 
摘要：从多模式文档中生成准确而简洁的文本摘要很具有挑战性，尤其是在处理诸如科学海报之类的视觉上复杂内容时。我们介绍了海报，这是一种新颖的基准测试，旨在推动视力模型的发展，这些模型可以理解并将科学海报总结到研究论文摘要中。我们的数据集包含16,305次会议海报，并将其相应的摘要与摘要配对。每个海报以图像格式提供，并提出各种视觉理解挑战，例如复杂的布局，密集的文本区域，表格和数字。我们在海报上基准了最先进的多模式大型语言模型（MLLM），并证明它们难以准确解释和总结科学海报。我们提出了段并总结，这是一种层次结构方法，在自动指标上的当前MLLM胜过，在Rouge-L中获得了3.14％的增长。这将是关于海报摘要的未来研究的起点。]]></description>
      <guid>https://arxiv.org/abs/2502.17540</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CNN的先验推广性估计值</title>
      <link>https://arxiv.org/abs/2502.17622</link>
      <description><![CDATA[arxiv：2502.17622v1公告类型：新 
摘要：我们制定了整个卷积神经网络的截短的奇异价值分解。我们证明，计算出的左右单数矢量对于识别卷积神经网络的哪些图像的性能很可能会出现较差。为了创建此诊断工具，我们定义了两个指标：右图投影率和左图投影率。右（左）投影率评估了图像（标签）投影到右（左）单数向量上的保真度。我们观察到，这两个比率都能够确定图像分类问题的类不平衡的存在。此外，在应用于图像分割时，发现仅需要未标记数据的正确投影比与模型的性能相关。这表明正确的投影率可能是一个有用的指标，可以估计模型在样品上的表现良好的可能性。]]></description>
      <guid>https://arxiv.org/abs/2502.17622</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CALIBREFINE：基于深度学习的在线自动无目标激光摄像机校准，并具有迭代和注意力驱动后的后。</title>
      <link>https://arxiv.org/abs/2502.17648</link>
      <description><![CDATA[ARXIV：2502.17648V1公告类型：新 
摘要：准确的多传感器校准对于在自动驾驶，机器人技术和智能运输等应用中部署强大的感知系统至关重要。现有的激光摄像机校准方法通常依赖于手动放置的目标，初步参数估计或密集的数据预处理，从而限制了其在现实世界中的可扩展性和适应性。在这项工作中，我们提出了一个全自动，无目标和在线校准框架的Calibrefine，该框架直接处理了Raw LiDar Point云和相机图像。我们的方法分为四个阶段：（1）一种常见的特征歧视器，它在自动检测到的对象上训练 - 使用相对位置，外观嵌入和语义类别 - 生成可靠的激光镜相机，（2）粗同质同型 - 基于校准，（3）随着其他数据框架的可用，迭代的改进，以逐步改善对齐方式，（4）基于注意力的改进，可解决非平面通过利用视觉变压器和交叉注意机制来扭曲。通过在两个城市交通数据集上进行的大量实验，我们表明，Calibrefine提供了高精度校准结果，人类的参与度最少，表现优于最先进的无目标方法，并保持与手动调谐碱的竞争或超越竞争。我们的发现突出了如何与迭代和自我监督的基于注意力的调整以及在复杂的，真实世界的条件下保持一致的传感器融合，而无需地面真相校准矩阵或详细填写数据预处理。]]></description>
      <guid>https://arxiv.org/abs/2502.17648</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>金属：通过测试时间缩放的图表生成的多代理框架</title>
      <link>https://arxiv.org/abs/2502.17651</link>
      <description><![CDATA[ARXIV：2502.17651V1公告类型：新 
摘要：图表生成旨在生成代码以产生满足所需的视觉属性的图表，例如文本，布局，颜色和类型。它具有巨大的潜力，可以在财务分析，研究表现，教育和医疗保健中赋予自动专业报告的能力。在这项工作中，我们建立了一个基于视觉语言模型（VLM）的多代理框架，以生成有效的自动图表。生成高质量的图表需要强大的视觉设计技能和精确的编码功能，将所需的视觉属性嵌入代码中。对于直接提示VLM，很难进行如此复杂的多模式推理过程。为了解决这些挑战，我们提出了金属，这是一个多代理框架，将图表生成的任务分解为专业代理之间的迭代协作。金属在图表生成任务中的当前最佳结果的准确性提高了5.2％。金属框架表现出测试时间缩放的现象：随着对数计算预算从512个令牌增长到8192代币，其性能会单调增加。此外，我们发现在金属的批评过程中分离不同的方式可以增强VLM在多模式背景下的自我纠正能力。]]></description>
      <guid>https://arxiv.org/abs/2502.17651</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>蔬菜场中的半监督杂草检测：内域和跨域实验</title>
      <link>https://arxiv.org/abs/2502.17673</link>
      <description><![CDATA[arxiv：2502.17673v1公告类型：新 
摘要：强大的杂草检测仍然是精确除草的一项具有挑战性的任务，不仅需要有效的杂草检测模型，还需要大规模的标记数据。但是，由于需要专业的专业知识来识别植物物种的耗时，劳动密集型过程，因此很难获得足够的模型培训数据。这项研究介绍了半监督对象检测（SSOD）方法，用于利用未标记的数据来增强杂草检测，并提出了一种新的基于Yolov8的SSOD方法，即Weedteacher。对四种SSOD方法进行了实验比较，包括三种现有的框架（即，培训者，有效的教师和小教人员）和除草剂，以及完全监督的基线，以在域内和交叉域的情况下进行杂草检测。创建了一个新的，多样化的杂草数据集，作为测试台，包括来自两个不同域的19,931个现场图像，包括由2021年至2023年到2023年和11,496的手持设备获得的标记为8,435个（基本 - 域）图像。 2024年被基于地面的移动平台收购。该域内实验使用了使用10％的标签训练的型号基本域图像并在其余90％的数据上进行了测试，表明基于Yolov8的WoodTeacher在所有四种SSOD方法中都达到了最高的精度，并提高了2.6％的MAP@50和3.1％MAP@50：95。监督基线（即Yolov8l）。在合并未标记的新域数据的跨域实验中，所有四种SSOD方法都导致对其受监督对应物的改进或有限的改进。需要进行研究来解决跨域数据利用率在稳健的杂草检测方面的困难。]]></description>
      <guid>https://arxiv.org/abs/2502.17673</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>艾伯：水下机器人检测的图像混合</title>
      <link>https://arxiv.org/abs/2502.17706</link>
      <description><![CDATA[ARXIV：2502.17706V1公告类型：新 
摘要：我们提出了一个融合管道的图像，\ textit {iburd}，该图像创建了逼真的合成图像，以帮助训练深水探测器，以用于水下自动驾驶汽车（AUV），以进行海洋碎屑检测任务。具体而言，艾伯德使用碎片对象的源图像，其注释和海上环境的目标背景图像生成了水下碎片及其像素级注释的两个图像。借助Poisson编辑和样式转移技术，Iburd甚至能够将透明的对象牢固地融合到任意背景中，并使用目标背景图像的模糊度量自动调整混合图像的样式。这些在实际的水下背景中生成的海洋碎片的图像解决了在具有挑战性的水下条件下，深度学习视力算法所面临的数据稀缺和数据品种问题，并可以使AUV用于环境清理任务。艾伯德的定量和机器人评估都证明了拟议方法在机器人碎片中检测的疗效。]]></description>
      <guid>https://arxiv.org/abs/2502.17706</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>对比性视觉数据增加</title>
      <link>https://arxiv.org/abs/2502.17709</link>
      <description><![CDATA[ARXIV：2502.17709V1公告类型：新 
摘要：大型多模型模型（LMM）通常很难识别新颖的概念，因为它们依赖于预训练的知识，并且具有有限的捕获微妙的视觉细节的能力。培训中特定于领域的知识差距也使它们容易混淆视觉相似，通常歪曲或低资源概念。为了帮助LMM更好地使细微差别的视觉特征与语言保持一致，提高了他们识别和理由的新颖或稀有概念的能力，我们提出了一种对比性视觉数据增强（CODA）策略。 CODA提取目标概念的关键对比文本和视觉特征与已知的概念被误认为为已知概念，然后使用多模式生成模型生成目标的合成数据。如人类注释者所验证的那样，实施了提取的功能和增强图像的自动过滤，以确保其质量。我们显示了尾声对低资源概念以及包括Inaturalist和Sun在内的各种场景识别数据集的有效性和效率。我们还收集了小说《小说》，这是一个由新发现的动物物种组成的基准数据集，这些动物物种保证不会被LMMS看到。 LLAVA-1.6这三个数据集上的1-shot更新结果表明，CODA显着将SOTA视觉数据增强策略提高了12.3％（小说类），5.1％（Sun）和6.0％（inatat）的准确性（INAT）绝对获得。]]></description>
      <guid>https://arxiv.org/abs/2502.17709</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于得分的生成建模可以有效地处理医学图像分类吗？</title>
      <link>https://arxiv.org/abs/2502.17727</link>
      <description><![CDATA[ARXIV：2502.17727V1公告类型：新 
摘要：近年来深度学习的显着成功促使了医学图像分类和诊断任务的应用。虽然分类模型在对更简单的数据集进行分类（例如MNIST或自然图像（例如Imagenet））时表现出了鲁棒性，但在复杂的医学图像数据集中并未始终如一地观察到这种弹性，因为数据更稀缺并且缺乏多样性。此外，自然图像数据集的先前发现表明，数据可能性和分类准确性之间存在潜在的权衡。在这项研究中，我们探讨了将基于得分的生成模型作为医学图像的分类器，特别是乳房X线照片图像的使用。我们的发现表明，我们提出的生成分类器模型不仅在CBIS-DDSM，INBREAST和VIN-DR MAMMO数据集上实现了卓越的分类结果，而且还引入了一种在更广泛的环境中进行图像分类的新方法。我们的代码可在https://github.com/sushmitasarker/sgc_for_medical_image_classification上公开获取]]></description>
      <guid>https://arxiv.org/abs/2502.17727</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>任务图的最大似然估计，以理解为中心视频的程序活动理解</title>
      <link>https://arxiv.org/abs/2502.17753</link>
      <description><![CDATA[ARXIV：2502.17753V1公告类型：新 
摘要：我们介绍了一种基于梯度的方法，用于从程序活动中学习任务图，从而改善了手工制作的方法。我们的方法通过最大可能性直接优化边缘权重，从而使整合到神经体系结构中。我们验证了对CaptainCook4D，Egoper和Egoprocel的方法，以 +14.5％， +10.2％和 +13.6％的F1得分提高。我们基于功能的方法来预测文本/视频嵌入的任务图，展示了新兴的视频理解能力。我们还在理解自我exo4d的基准的过程中达到了最高的性能，并显着改善了在线错误检测（insembly101-o的 +19.8％，epic-tent-o +6.4％）。代码：https：//github.com/fpv-iplab/differentiable-task-graph-learning。]]></description>
      <guid>https://arxiv.org/abs/2502.17753</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AI驱动的3D空间转录组学</title>
      <link>https://arxiv.org/abs/2502.17761</link>
      <description><![CDATA[ARXIV：2502.17761V1公告类型：新 
摘要：组织结构和基因表达的综合三维图（3D）图对于阐明各种生物医学应用中组织的复杂性和异质性至关重要。但是，大多数空间转录组学（ST）方法仍然限于组织的二维（2D）部分。尽管当前的3D ST方法具有前景，但它们通常需要广泛的组织切片，很复杂，与非破坏性3D组织成像技术不兼容，并且通常缺乏可扩展性。在这里，我们提出了体积解析的转录组学表达（Vortex），这是一个利用3D组织形态和最小2D ST的AI框架，以预测体积3D ST。通过从异质组织样品中预处理3D形态 - 转录对，然后从特定感兴趣的最小值数据进行微调，涡流学到了基因表达的通用组织相关和样品特异性形态相关性。这种方法可以使密集，高通量和快速的3D ST无缝缩放到远远超出现有3D ST技术范围的大型组织量。通过为获得体积分子见解提供具有成本效益且最小的破坏性途径，我们预计涡流将加速生物标志物的发现以及我们对复杂组织中形态分子的关联和细胞状态的理解。可以在https://vortex-demo.github.io/上查看Interactive 3D ST卷]]></description>
      <guid>https://arxiv.org/abs/2502.17761</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用深度异常方案对帕克森模式进行分类的数字固定生物标志物</title>
      <link>https://arxiv.org/abs/2502.17762</link>
      <description><![CDATA[ARXIV：2502.17762V1公告类型：新 
摘要：动眼运动的改变构成了一种有希望的生物标志物，即使在前驱阶段，也可以检测和表征帕金森氏病（PD）。当前，仅使用全局和简化的眼动轨迹来近似眼动函数的复杂和隐藏的运动学关系。机器学习和视频分析的最新进展鼓励了眼动模式的新颖特征来量化PD。这些方案可以鉴定主要与PD相关的时空段。但是，他们依靠需要大型培训数据集并依赖于平衡的班级分布的判别模型。这项工作介绍了一种新颖的视频分析方案，该方案旨在用异常检测框架量化帕金森尼眼睛固定模式。与在标记类别中学习差异的经典深层歧视方案相反，所提出的方法集中在一类学习上，避免了大量数据的必要性。提出的方法仅着眼于帕金森的代表，将任何其他类样本视为分布的异常。在该疾病的不同阶段，评估了该方法的眼睛固定任务，共有13名对照受试者和13名患者。提出的数字生物标志物的平均灵敏度和特异性分别为0.97和0.63，其AUC-ROC为0.95。统计检验显示预测类别中的显着差异（p &lt;0.05），证明了患者和对照受试者之间的歧视。]]></description>
      <guid>https://arxiv.org/abs/2502.17762</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过匹配的预测和重新排列来改善基于变压器的线段检测</title>
      <link>https://arxiv.org/abs/2502.17766</link>
      <description><![CDATA[ARXIV：2502.17766V1公告类型：新 
摘要：基于经典变压器的线段检测方法取得了令人印象深刻的结果。但是，我们观察到，在预测期间，对一些准确检测到的线段分配了较低的置信得分，从而使它们排名较低并可能被抑制。此外，这些模型通常需要延长训练期才能实现强大的性能，这主要是由于需要进行两分匹配。在本文中，我们介绍了Rank-leletr，这是一种基于变压器的新型线段检测方法。我们的方法利用可学习的几何信息来通过在后验证步骤中提高高质量预测的置信度得分来完善预测线段的排名。我们还提出了一种新的线段建议方法，其中最接近线段的质心的特征点直接预测了位置，从而显着提高了训练效率和稳定性。此外，我们引入了线段排名损失，以稳定训练期间的排名，从而增强了模型的概括能力。实验结果表明，我们的方法在预测准确性方面优于基于其他变压器和基于CNN的其他方法，同时比以前的基于变压器的模型需要更少的训练时期。]]></description>
      <guid>https://arxiv.org/abs/2502.17766</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>合成：具有负担能力组成的新颖概念设计</title>
      <link>https://arxiv.org/abs/2502.17793</link>
      <description><![CDATA[ARXIV：2502.17793V1公告类型：新 
摘要：文本对图像（T2I）模型可实现快速概念设计，使其广泛用于AI驱动设计。尽管最近的研究着重于产生给定设计概念的语义和风格变化，但功能连贯性（将多个负担能力集成到单个相干概念中的集成）很大程度上被忽略了。在本文中，我们介绍了Synthia，这是一个基于所需负担的新颖，功能相干设计的框架。我们的方法利用了分层概念的本体论，该概念将概念分解为部分和负担能力，是功能连贯设计的关键构件。我们还基于本体论开发了一种课程学习方案，该方案对形成了微型T2I模型，以逐步学习负担能力组成的同时保持视觉新颖性。为了详细说明，我们（i）（i）逐渐提高了负担距离，指导模型从基本的概念交往关联到复杂的负担能力构成，这些组合将不同的负担的一部分整合到一种单一的，连贯的形式中，以及（ii）通过使用对比目标来推动视觉新颖性来推动视觉新颖性从现有概念中学习的表示。实验结果表明，合成的表现优于最先进的T2I模型，在人类评估中，新颖性和功能相干性的绝对增长分别为25.1％和14.7％。]]></description>
      <guid>https://arxiv.org/abs/2502.17793</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LAM：一击动画高斯头的大型头像模型</title>
      <link>https://arxiv.org/abs/2502.17796</link>
      <description><![CDATA[ARXIV：2502.17796V1公告类型：新 
摘要：我们提出了Lam，这是一种创新的大型化身模型，用于从单个图像中进行动画高斯头部重建。与以前需要对捕获的视频序列进行广泛培训或依靠辅助神经网络进行动画和渲染的方法不同，我们的方法会生成高斯头部，这些高斯头部立即可动画和呈现。具体来说，Lam在单个前传中创建了一个可动的高斯头，可以在没有其他网络或后处理步骤的情况下重新制定和渲染。这种功能使无缝集成到现有的渲染管道中，确保实时动画并在包括手机在内的各种平台上进行渲染。我们框架的核心是规范的高斯属性生成器，该发电机将火焰的典型点作为查询。这些点通过变压器与多尺度图像特征相互作用，以准确预测规范空间中的高斯属性。然后，可以使用标准线性混合皮肤（LBS）进行矫正的搅拌机，就像火焰模型一样，使用标准的线性混合皮（LBS），可以使用标准的线性混合皮（LBS）进行动画制作。我们的实验结果表明，LAM在现有基准测试方面的表现优于最先进的方法。]]></description>
      <guid>https://arxiv.org/abs/2502.17796</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Easy-Poly：用于3D多对象跟踪的简单多面体框架</title>
      <link>https://arxiv.org/abs/2502.17822</link>
      <description><![CDATA[ARXIV：2502.17822V1公告类型：新 
摘要：3D多对象跟踪（3D MOT）的最新进展主要依赖于通过检测管道跟踪。但是，这些方法通常会忽略3D检测过程中的潜在增强功能，从而导致高误报（FP），错过的检测（FN）和身份转换（IDS）（iDS），尤其是在挑战性的场景中，例如拥挤的场景，小对象配置和小型对象配置和不利天气条件。此外，数据预处理，关联机制，运动建模和生命周期管理的局限性阻碍了整体跟踪鲁棒性。为了解决这些问题，我们提出了Easy-Poly，这是一个实时的，基于过滤器的3D MOT框架，用于多个对象类别。我们的贡献包括：（1）利用多模式数据增强和精制SPCONV操作的增强提案生成器，可显着改善Nuscenes的地图和NDS； （2）一种动态轨道（DTO）数据关联算法，该算法通过最佳分配和多个假设处理来有效地管理不确定性和遮挡； （3）一种动态运动建模（DMM），结合了置信加权的卡尔曼过滤器和自适应噪声协方差，在具有挑战性的条件下增强了MOTA和AMOTA； （4）具有调节阈值的扩展生命周期管理系统，以减少ID开关和错误终止。实验结果表明，Easy-Poly优于最先进的方法，例如Poly-Mot和Fast-Poly，在MAP中获得了显着的收益（例如，使用groundkernel3d从63.30％到64.96％）和AMOTA（例如，从73.1.1。 ％至74.5％），同时也实时运行。这些发现突出显示了在不同情况下轻松的适应性和鲁棒性，使其成为自动驾驶和相关3D MOT应用的引人注目的选择。本文的源代码将在接受后发表。]]></description>
      <guid>https://arxiv.org/abs/2502.17822</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>