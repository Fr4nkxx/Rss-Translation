<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Fri, 19 Jul 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>ActionSwitch：流视频中同时发生动作的类别无关检测</title>
      <link>https://arxiv.org/abs/2407.12987</link>
      <description><![CDATA[arXiv:2407.12987v1 公告类型：新
摘要：在线时间动作定位 (On-TAL) 是一项关键任务，旨在动作结束后立即识别未修剪的流媒体视频中的动作实例——这是基于帧的在线动作检测 (OAD) 的一大飞跃。然而，检测重叠动作的挑战经常被忽视，即使这是流媒体视频中的常见场景。当前可以解决并发操作的方法严重依赖于类信息，从而限制了它们的灵活性。本文介绍了 ActionSwitch，这是第一个能够检测重叠动作的类无关 On-TAL 框架。通过消除对类信息的依赖，ActionSwitch 为各种情况提供了更广泛的适用性，包括同一类的重叠动作或类信息不可用的场景。这种方法由提出的“保守性损失”补充，它将保守的决策原则直接嵌入到 On-TAL 的损失函数中。我们的 ActionSwitch 在复杂数据集中实现了最先进的性能，包括针对具有挑战性的自我中心视图的 Epic-Kitchens 100 和由细粒度动作组成的 FineAction。]]></description>
      <guid>https://arxiv.org/abs/2407.12987</guid>
      <pubDate>Fri, 19 Jul 2024 06:17:36 GMT</pubDate>
    </item>
    <item>
      <title>AdaLog：使用自适应对数量化器的视觉变换器进行训练后量化</title>
      <link>https://arxiv.org/abs/2407.12951</link>
      <description><![CDATA[arXiv:2407.12951v1 公告类型：新
摘要：Vision Transformer (ViT) 已成为计算机视觉社区中最流行的基础骨干网络之一。尽管准确度很高，但在实际应用中部署它会带来重大挑战，包括高计算成本和推理延迟。最近，训练后量化 (PTQ) 技术已成为提高 ViT 效率的一种有前途的方法。然而，现有的 ViT PTQ 方法在遵循幂律分布的后 Softmax 和后 GELU 激活上存在不灵活的量化问题。为了解决这些问题，我们提出了一种新颖的非均匀量化器，称为自适应对数 AdaLog (AdaLog) 量化器。它优化了对数基以适应幂律分布的激活，同时允许硬件友好的量化和反量化。通过采用偏差重新参数化，AdaLog 量化器可适用于后 Softmax 和后 GELU 激活。此外，我们开发了一种高效的快速渐进组合搜索 (FPCS) 策略来确定 AdaLog 的最佳对数基数，以及均匀量化器的缩放因子和零点。在公共基准上进行的大量实验结果证明了我们的方法对于各种基于 ViT 的架构和视觉任务（包括分类、对象检测和实例分割）的有效性。代码可在 https://github.com/GoatWu/AdaLog 上找到。]]></description>
      <guid>https://arxiv.org/abs/2407.12951</guid>
      <pubDate>Fri, 19 Jul 2024 06:17:35 GMT</pubDate>
    </item>
    <item>
      <title>用于医学图像分割的潜在空间中的去噪扩散</title>
      <link>https://arxiv.org/abs/2407.12952</link>
      <description><![CDATA[arXiv:2407.12952v1 公告类型：新
摘要：扩散模型 (DPM) 在图像生成方面表现出色，通常优于其他生成模型。自推出以来，强大的噪声到图像去噪流程已扩展到各种判别任务，包括图像分割。在医学成像的情况下，图像通常是大型 3D 扫描，由于内存消耗大和迭代采样过程耗时，使用 DPM 分割一张图像变得极其低效。在这项工作中，我们提出了一种新颖的条件生成建模框架 (LDSeg)，它在潜在空间中执行扩散以进行医学图像分割。我们提出的框架利用了学习到的目标对象形状和源图像嵌入的固有低维潜在分布。潜在空间中的条件扩散不仅确保了多标签对象的 n 维图像分割的准确性，而且还缓解了传统基于 DPM 的分割的主要潜在问题：（1）内存消耗大，（2）采样过程耗时，（3）正向/反向过程中不自然的噪声注入。LDSeg 在三种具有不同成像模式的医学图像数据集上实现了最先进的分割精度。此外，我们表明，与传统的确定性分割模型相比，我们提出的模型对噪声的鲁棒性明显更强，这可能有助于解决医学成像领域的域转移问题。代码可在以下网址获得：https://github.com/LDSeg/LDSeg。]]></description>
      <guid>https://arxiv.org/abs/2407.12952</guid>
      <pubDate>Fri, 19 Jul 2024 06:17:35 GMT</pubDate>
    </item>
    <item>
      <title>用于复合情绪识别的时间标签分层网络</title>
      <link>https://arxiv.org/abs/2407.12973</link>
      <description><![CDATA[arXiv:2407.12973v1 Announce Type: new 
摘要：情绪识别在近几十年来受到了越来越多的关注。虽然七种基本情绪的识别技术已经取得了重大进展，但现有的方法仍然难以解决实际应用中常见的复合情绪识别。本文介绍了我们在第七届现场情绪行为分析（ABAW）比赛中取得的成果。在比赛中，我们选择了经过广泛验证的预训练的ResNet18和Transformer作为基本网络框架。考虑到情绪随时间的连续性，我们提出了一个时间金字塔结构网络用于帧级情绪预测。此外。同时，为了解决复合情绪识别数据不足的问题，我们利用DFEW数据库中的细粒度标签构建比赛中情绪类别的训练数据。考虑到各种复杂情绪的效价唤醒特点，我们在标签空间构建了一个由粗到细的分类框架。]]></description>
      <guid>https://arxiv.org/abs/2407.12973</guid>
      <pubDate>Fri, 19 Jul 2024 06:17:35 GMT</pubDate>
    </item>
    <item>
      <title>基于文本和特征的自然复合多模态情绪识别模型</title>
      <link>https://arxiv.org/abs/2407.12927</link>
      <description><![CDATA[arXiv:2407.12927v1 公告类型：新
摘要：多模态情绪识别 (ER) 系统通常依赖从不同模态（例如视觉、音频和文本）中提取的特征来预测七种基本情绪。然而，复合情绪经常出现在现实世界中，更难预测。由于多种模态增加了不确定性，复合多模态 ER 在视频中变得更具挑战性。
此外，基于特征的标准模型可能无法完全捕捉理解复合情绪所需的复杂和微妙的线索。
%%%%
由于相关线索可以以文本形式提取，我们主张将所有模态（例如视觉和音频）文本化，以利用大型语言模型 (LLM) 的能力。这些模型可以理解模态之间的复杂相互作用和复杂情绪的微妙之处。尽管训练 LLM 需要大规模数据集，但最近大量预训练的 LLM（例如 BERT 和 LLaMA）可以轻松针对复合 ER 等下游任务进行微调。
本文比较了视频中复合 ER 的两种多模态建模方法——基于标准特征与基于文本。在具有挑战性的 C-EXPR-DB 数据集上对复合 ER 进行了实验，并与 MELD 数据集上的基本 ER 的结果进行了对比。
我们的代码可用]]></description>
      <guid>https://arxiv.org/abs/2407.12927</guid>
      <pubDate>Fri, 19 Jul 2024 06:17:34 GMT</pubDate>
    </item>
    <item>
      <title>GenRC：从稀疏图像集合生成 3D 房间</title>
      <link>https://arxiv.org/abs/2407.12939</link>
      <description><![CDATA[arXiv:2407.12939v1 公告类型：新
摘要：稀疏 RGBD 场景完成是一项具有挑战性的任务，尤其是在考虑整个场景中一致的纹理和几何形状时。与依赖人为设计的文本提示或预定义相机轨迹的现有解决方案不同，我们提出了 GenRC，这是一种无需训练的自动化管道，用于完成具有高保真纹理的房间规模 3D 网格。为此，我们首先将稀疏 RGBD 图像投影到高度不完整的 3D 网格上。我们没有迭代生成新视图来填补空白，而是利用我们提出的 E-Diffusion 生成视图一致的全景 RGBD 图像，以确保全局几何和外观一致性。此外，我们通过文本反转来代替人为设计的文本提示，从而保持输入输出场景风格的一致性。为了弥合数据集之间的领域差距，E-Diffusion 利用在大型数据集上训练的模型来生成不同的外观。尽管 GenRC 并未在这些数据集上进行训练，也没有使用预定义的相机轨迹，但 GenRC 在 ScanNet 和 ARKitScenes 数据集上的大多数外观和几何指标下都优于最先进的方法。项目页面：\href{https://minfenli.github.io/GenRC}{此 https URL}]]></description>
      <guid>https://arxiv.org/abs/2407.12939</guid>
      <pubDate>Fri, 19 Jul 2024 06:17:34 GMT</pubDate>
    </item>
    <item>
      <title>利用视觉变换器实现高效细粒度图像识别的全局-局部相似性</title>
      <link>https://arxiv.org/abs/2407.12891</link>
      <description><![CDATA[arXiv:2407.12891v1 公告类型：新
摘要：细粒度识别涉及从下级宏观类别对图像进行分类，由于类间差异较小，因此具有挑战性。为了克服这个问题，大多数方法执行判别性特征选择，由特征提取主干启用，然后进行高级特征细化步骤。​​最近，许多研究表明视觉转换器作为细粒度识别主干的潜力，但它们使用其注意机制来选择判别性标记可能会在计算上很昂贵。在这项工作中，我们提出了一种新颖且计算成本低廉的度量来识别图像中的判别区域。我们比较了 CLS 标记给出的图像全局表示、转换器用于分类的可学习标记和各个补丁的局部表示之间的相似性。我们选择相似度最高的区域来获取裁剪图像，然后通过相同的转换器编码器进行转发。最后，为了做出更稳健的预测，原始表示和裁剪表示的高级特征被进一步细化。通过广泛的实验评估，我们证明了我们提出的方法的有效性，在各种数据集的准确性方面获得了良好的结果。此外，与其他方法相比，我们的方法以低得多的计算成本实现了这些结果。代码和检查点可在以下位置获得：\url{https://github.com/arkel23/GLSim}。]]></description>
      <guid>https://arxiv.org/abs/2407.12891</guid>
      <pubDate>Fri, 19 Jul 2024 06:17:33 GMT</pubDate>
    </item>
    <item>
      <title>DreamStory：通过法学硕士指导的多主题一致性扩散实现开放域故事可视化</title>
      <link>https://arxiv.org/abs/2407.12899</link>
      <description><![CDATA[arXiv:2407.12899v1 公告类型：新
摘要：故事可视化旨在创建与文本叙述相对应的视觉上引人注目的图像或视频。尽管最近扩散模型的进展产生了有希望的结果，但现有方法仍然难以仅基于故事创建连贯的主题一致帧序列。为此，我们提出了 DreamStory，这是一个自动开放域故事可视化框架，它利用 LLM 和新颖的多主题一致扩散模型。DreamStory 由 (1) 充当故事导演的 LLM 和 (2) 用于在图像中生成一致多主题的创新多主题一致扩散模型 (MSD) 组成。首先，DreamStory 使用 LLM 为与故事一致的主题和场景生成描述性提示，注释每个场景的主题以供后续的主题一致生成。其次，DreamStory 利用这些详细的主题描述来创建主题的肖像，这些肖像及其相应的文本信息作为多模态锚点（指导）。最后，MSD 使用这些多模态锚点生成具有一致多主题的故事场景。具体来说，MSD 包括 Masked Mutual Self-Attention (MMSA) 和 Masked Mutual Cross-Attention (MMCA) 模块。MMSA 和 MMCA 模块分别确保与参考图像和文本的外观和语义一致性。这两个模块都采用掩蔽机制来防止主题混合。为了验证我们的方法并促进故事可视化的进步，我们建立了一个基准 DS-500，它可以评估故事可视化框架的整体性能、主题识别准确性和生成模型的一致性。大量实验验证了 DreamStory 在主观和客观评价中的有效性。请访问我们的项目主页 https://dream-xyz.github.io/dreamstory。]]></description>
      <guid>https://arxiv.org/abs/2407.12899</guid>
      <pubDate>Fri, 19 Jul 2024 06:17:33 GMT</pubDate>
    </item>
    <item>
      <title>ChatBCG：人工智能可以读懂你的幻灯片吗？</title>
      <link>https://arxiv.org/abs/2407.12875</link>
      <description><![CDATA[arXiv:2407.12875v1 公告类型：新
摘要：GPT4o 和 Gemini Flash 等多模态模型在推理和总结任务方面表现出色，其性能接近人类水平。然而，我们发现，当被要求执行非常具体的“阅读和估计”任务时，这些模型的表现不如人类，尤其是在商业卡片中的可视化图表方面。本文评估了 GPT 4o 和 Gemini Flash-1.5 在回答有关标记图表（数据在图表上有清晰的注释）和未标记图表（数据没有清晰的注释，必须从 X 轴和 Y 轴推断）上数据的简单问题时的准确性。我们得出的结论是，如果卡片包含任何复杂或未标记的图表，这些模型目前无法准确地端到端读取卡片。即使用户创建了一副只带标签的图表，该模型也只能完美地从头到尾读取 15 张带标签的图表中的 7-8 张。如需查看幻灯片图表的完整列表，请访问 https://www.repromptai.com/chat_bcg]]></description>
      <guid>https://arxiv.org/abs/2407.12875</guid>
      <pubDate>Fri, 19 Jul 2024 06:17:32 GMT</pubDate>
    </item>
    <item>
      <title>GeoGuide：扩散模型的几何指导</title>
      <link>https://arxiv.org/abs/2407.12889</link>
      <description><![CDATA[arXiv:2407.12889v1 公告类型：新
摘要：扩散模型是图像生成最有效的方法之一。这主要是因为，与 GAN 不同，它们可以在训练期间轻松调节以生成具有所需类别或属性的元素。但是，引导预先训练的扩散模型从以前未标记的数据中生成元素要困难得多。ADM-G 引导方法给出了一种可能的解决方案。尽管 ADM-G 成功地从给定的类中生成元素，但与最初以此类为条件的模型相比，存在显着的质量差距。特别是，ADM-G 引导扩散模型获得的 FID 分数几乎比类条件引导低三倍。我们证明这个问题部分是由于 ADM-G 在去噪过程的最后阶段提供的指导最少。为了解决这个问题，我们提出了 GeoGuide，这是一种基于追踪扩散模型轨迹与数据流形距离的引导模型。 GeoGuide 的主要思想是在后向去噪过程中进行归一化调整。实验表明，GeoGuide 在 FID 分数和生成图像质量方面都优于概率方法 ADM-G。]]></description>
      <guid>https://arxiv.org/abs/2407.12889</guid>
      <pubDate>Fri, 19 Jul 2024 06:17:32 GMT</pubDate>
    </item>
    <item>
      <title>博世街道数据集：用于自动驾驶的带成像雷达的多模态数据集</title>
      <link>https://arxiv.org/abs/2407.12803</link>
      <description><![CDATA[arXiv:2407.12803v1 公告类型：新
摘要：本文介绍了博世街道数据集 (BSD)，这是一种新型多模式大规模数据集，旨在促进高度自动驾驶 (HAD) 和高级驾驶辅助系统 (ADAS) 研究。与现有数据集不同，BSD 提供了高分辨率成像雷达、激光雷达和摄像头传感器的独特集成，提供了前所未有的 360 度覆盖范围，以弥补当前高分辨率雷达数据可用性的差距。BSD 涵盖城市、农村和高速公路环境，可以详细探索基于雷达的物体检测和传感器融合技术。该数据集旨在促进博世与现有和未来合作伙伴之间的学术和研究合作。这旨在促进共同努力开发尖端的 HAD 和 ADAS 技术。本文描述了数据集的关键属性，包括其可扩展性、雷达分辨率和标记方法。主要产品还包括传感器模式的初始基准和针对大量数据分析和性能评估而定制的开发套件，彰显了我们致力于为 HAD 和 ADAS 研究界贡献宝贵资源的承诺。]]></description>
      <guid>https://arxiv.org/abs/2407.12803</guid>
      <pubDate>Fri, 19 Jul 2024 06:17:31 GMT</pubDate>
    </item>
    <item>
      <title>Dark Transformer：用于黑暗中动作识别的视频转换器</title>
      <link>https://arxiv.org/abs/2407.12805</link>
      <description><![CDATA[arXiv:2407.12805v1 公告类型：新
摘要：在不利的光照条件下识别人类行为对计算机视觉提出了重大挑战，在视觉监控和夜间驾驶方面有着广泛的应用。现有方法分别处理动作识别和暗增强，限制了端到端学习时空表示进行视频动作分类的潜力。本文介绍了一种基于视频变换器的新型低光环境下动作识别方法 Dark Transformer。Dark Transformer 利用跨域设置中的时空自注意机制来增强跨域动作识别。通过扩展视频变换器以学习跨域知识，Dark Transformer 在基准动作识别数据集（包括 InFAR、XD145 和 ARID）上实现了最先进的性能。所提出的方法在解决不利光照条件下动作识别的挑战方面显示出巨大的潜力，为现实世界的应用提供了实际意义。]]></description>
      <guid>https://arxiv.org/abs/2407.12805</guid>
      <pubDate>Fri, 19 Jul 2024 06:17:31 GMT</pubDate>
    </item>
    <item>
      <title>边缘 CNN 和视觉转换器知识蒸馏的最佳权衡</title>
      <link>https://arxiv.org/abs/2407.12808</link>
      <description><![CDATA[arXiv:2407.12808v1 公告类型：新
摘要：本文讨论了卷积神经网络 (CNN) 和视觉变换器 (ViT) 架构的知识蒸馏 (KD) 过程的四个方面，特别是在处理能力受限的边缘设备上执行时。首先，我们对 CNN 和 ViT 架构之间的 KD 过程进行比较分析，旨在阐明为教师和学生采用不同架构配置的可行性和有效性，同时评估其性能和效率。其次，我们探讨了在保持恒定的 KD 持续时间的情况下改变学生模型的大小对准确性和推理速度的影响。第三，我们研究了使用更高分辨率图像对准确性、内存占用和计算工作量的影响。最后，我们研究了在 KD 之后通过将学生模型微调到特定的下游任务所获得的性能改进。通过实证评估与分析，该研究为AI从业者提供了最大化边缘设备上KD过程有效性的最佳策略的见解。]]></description>
      <guid>https://arxiv.org/abs/2407.12808</guid>
      <pubDate>Fri, 19 Jul 2024 06:17:31 GMT</pubDate>
    </item>
    <item>
      <title>SS-ADA：用于语义分割的半监督主动域自适应框架</title>
      <link>https://arxiv.org/abs/2407.12788</link>
      <description><![CDATA[arXiv:2407.12788v1 公告类型：新
摘要：语义分割在智能汽车中起着重要作用，提供有关环境的像素级语义信息。然而，当语义分割模型应用于新的驾驶场景时，标记预算昂贵且耗时。为了降低成本，已经提出了半监督语义分割方法来利用大量未标记图像。尽管如此，它们的性能仍然达不到实际应用所需的准确性，这通常通过监督学习来实现。一个显着的缺点是他们通常随机选择未标记的图像进行注释，而忽略了对模型训练的样本价值的评估。在本文中，我们提出了一种新的半监督主动域自适应（SS-ADA）语义分割框架，该框架采用图像级获取策略。SS-ADA将主动学习集成到半监督语义分割中，以使用来自目标域的有限数量的标记数据实现监督学习的准确性。此外，我们设计了一种基于 IoU 的类别加权策略，以使用来自主动学习的注释来缓解类别不平衡问题。我们在合成到真实和真实到真实的领域自适应设置上进行了广泛的实验。结果证明了我们方法的有效性。当使用实时分割模型时，SS-ADA 仅使用 25% 的目标标记数据就可以达到甚至超过其监督学习对手的准确率。SS-ADA 的代码可在 https://github.com/ywher/SS-ADA 上找到。]]></description>
      <guid>https://arxiv.org/abs/2407.12788</guid>
      <pubDate>Fri, 19 Jul 2024 06:17:30 GMT</pubDate>
    </item>
    <item>
      <title>用于文本视频检索的多粒度和多模态特征交互方法</title>
      <link>https://arxiv.org/abs/2407.12798</link>
      <description><![CDATA[arXiv:2407.12798v1 公告类型：新
摘要：文本到视频检索 (TVR) 任务的关键在于学习每对文本（由单词组成）和视频（由音频和图像帧组成）表示之间的独特相似性。然而，在视频和文本的表示对齐中存在一些问题，例如文本以及每个单词对于视频帧的重要性不同。此外，在帧携带的有效信息很少的情况下，音频通常会为 TVR 携带额外或关键信息。因此，在 TVR 任务中，文本的多粒度表示（包括整个句子和每个单词）和音频的模式是有益的，但在大多数现有作品中未得到充分利用。为了解决这个问题，我们提出了一种新的多粒度特征交互模块 MGFI，由文本框架和单词框架组成，用于视频文本表示对齐。此外，我们引入了音频和文本跨模态特征交互模块CMFI，解决了视频中帧间表达不足的问题。在MSR-VTT、MSVD、DiDeMo等基准数据集上的实验表明，所提方法优于现有的最佳方法。]]></description>
      <guid>https://arxiv.org/abs/2407.12798</guid>
      <pubDate>Fri, 19 Jul 2024 06:17:30 GMT</pubDate>
    </item>
    </channel>
</rss>