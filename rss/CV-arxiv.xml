<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Fri, 02 Aug 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>使用参考引导变压器改善图像去雨</title>
      <link>https://arxiv.org/abs/2408.00258</link>
      <description><![CDATA[arXiv:2408.00258v1 公告类型：新
摘要：图像去雨是计算机视觉中一项关键任务，可以提高可见性并增强户外视觉系统的鲁棒性。虽然去雨方法的最新进展取得了显著的效果，但挑战仍然是产生高质量和视觉上令人愉悦的去雨结果。在本文中，我们提出了一种参考引导的去雨滤波器，这是一种使用参考干净图像作为指导来增强去雨结果的变压器网络。我们利用所提出的模块的功能进一步细化现有方法去雨的图像。我们在三个数据集上验证了我们的方法，并表明我们的模块可以提高现有基于先验、基于 CNN 和基于变压器的方法的性能。]]></description>
      <guid>https://arxiv.org/abs/2408.00258</guid>
      <pubDate>Fri, 02 Aug 2024 06:19:00 GMT</pubDate>
    </item>
    <item>
      <title>任务适配器：针对少量样本动作识别的图像模型的任务特定适配</title>
      <link>https://arxiv.org/abs/2408.00249</link>
      <description><![CDATA[arXiv:2408.00249v1 公告类型：新
摘要：现有的少样本动作识别研究大多对预训练图像模型进行微调，并在特征级别设计复杂的时间对齐模块。然而，由于视频样本稀缺，简单地对预训练模型进行完全微调可能会导致过度拟合。此外，我们认为，仅依靠提取良好的抽象特征，对任务特定信息的探索是不够的。在本文中，我们提出了一种简单但有效的任务特定自适应方法（Task-Adapter），用于少样本动作识别。通过将提出的 Task-Adapter 引入主干的最后几层并保持原始预训练模型的参数不变，我们可以缓解完全微调引起的过拟合问题，并将任务特定机制推进到特征提取过程中。在每个任务适配器中，我们重复使用冻结的自注意力层，在给定任务的不同视频中执行特定于任务的自注意力，以捕获类之间的独特信息和类内的共享信息，这有助于特定于任务的适应性并增强查询特征和支持原型之间的后续度量测量。实验结果一致证明了我们提出的任务适配器在四个标准小样本动作识别数据集上的有效性。特别是在时间挑战性 SSv2 数据集上，我们的方法远远优于最先进的方法。]]></description>
      <guid>https://arxiv.org/abs/2408.00249</guid>
      <pubDate>Fri, 02 Aug 2024 06:18:59 GMT</pubDate>
    </item>
    <item>
      <title>LoopSparseGS：基于循环的稀疏视图友好型高斯分布</title>
      <link>https://arxiv.org/abs/2408.00254</link>
      <description><![CDATA[arXiv:2408.00254v1 公告类型：新
摘要：尽管原始 3D 高斯分层 (3DGS) 实现了逼真的新型视图合成 (NVS) 性能，但其渲染质量在稀疏输入视图的情况下会显著下降。这种性能下降主要是由于从稀疏输入生成的初始点数量有限、训练过程中监督不足以及对过大高斯椭球的正则化不足造成的。为了解决这些问题，我们提出了 LoopSparseGS，这是一个基于循环的 3DGS 框架，用于稀疏新型视图合成任务。具体来说，我们提出了一种基于循环的渐进高斯初始化 (PGI) 策略，该策略可以在训练过程中使用渲染的伪图像迭代地对初始化点云进行加密。然后，利用来自运动结构的稀疏可靠深度和基于窗口的密集单目深度，通过提出的深度对齐正则化 (DAR) 提供精确的几何监督。此外，我们引入了一种新颖的稀疏友好采样 (SFS) 策略来处理导致大像素误差的过大高斯椭圆体。在四个数据集上进行的综合实验表明，LoopSparseGS 优于现有的最先进的稀疏输入新视图合成方法，适用于具有各种图像分辨率的室内、室外和物体级场景。]]></description>
      <guid>https://arxiv.org/abs/2408.00254</guid>
      <pubDate>Fri, 02 Aug 2024 06:18:59 GMT</pubDate>
    </item>
    <item>
      <title>OmniParser 用于纯视觉的 GUI 代理</title>
      <link>https://arxiv.org/abs/2408.00203</link>
      <description><![CDATA[arXiv:2408.00203v1 公告类型：新 
摘要：大型视觉语言模型最近取得的成功显示出在推动用户界面上运行的代理系统方面的巨大潜力。然而，我们认为，由于缺乏一种强大的屏幕解析技术，像 GPT-4V 这样的多模态模型作为跨不同应用程序的多个操作系统上的通用代理的威力被大大低估了：1) 可靠地识别用户界面内的可交互图标，2) 理解屏幕截图中各种元素的语义并准确地将预期操作与屏幕上的相应区域关联起来。为了填补这些空白，我们引入了 \textsc{OmniParser}，这是一种将用户界面屏幕截图解析为结构化元素的综合方法，它显著增强了 GPT-4V 生成可准确基于界面相应区域的操作的能力。我们首先使用流行的网页和图标描述数据集策划了一个可交互的图标检测数据集。这些数据集用于微调专门的模型：一个用于解析屏幕上可交互区域的检测模型和一个用于提取检测到的元素的功能语义的标题模型。 \textsc{OmniParser} 显著提高了 GPT-4V 在 ScreenSpot 基准测试中的表现。而在 Mind2Web 和 AITW 基准测试中，仅使用屏幕截图输入的 \textsc{OmniParser} 的表现优于需要屏幕截图之外的额外信息的 GPT-4V 基线。]]></description>
      <guid>https://arxiv.org/abs/2408.00203</guid>
      <pubDate>Fri, 02 Aug 2024 06:18:58 GMT</pubDate>
    </item>
    <item>
      <title>一种用于长距离盲虹膜识别的嵌入式驱动架构</title>
      <link>https://arxiv.org/abs/2408.00210</link>
      <description><![CDATA[arXiv:2408.00210v1 公告类型：新
摘要：盲虹膜图像往往导致虹膜识别率下降，这是由于远距离虹膜识别过程中未知的退化造成的。目前，很少有现有文献提供针对此问题的解决方案。针对这一问题，我们提出了一种用于远距离盲虹膜识别的先验嵌入驱动架构。我们首先提出了一种盲虹膜图像恢复网络，称为 Iris-PPRGAN。为了有效地恢复盲虹膜的纹理，Iris-PPRGAN 包括用作先验解码器的生成对抗网络 (GAN) 和用作编码器的 DNN。为了更有效地提取虹膜特征，我们通过修改 InsightFace 的瓶颈模块提出了一种鲁棒的虹膜分类器，称为 Insight-Iris。首先通过 Iris-PPRGAN 恢复低质量的盲虹膜图像，然后通过 Insight-Iris 对恢复的虹膜图像进行识别。在公开的CASIA-Iris-distance数据集上的实验结果表明，我们提出的方法在数量和质量上都明显优于最先进的盲虹膜修复方法，具体来说，经过我们的方法处理后，远距离盲虹膜图像的识别率达到90％，与未修复的图像相比提高了大约10个百分点。]]></description>
      <guid>https://arxiv.org/abs/2408.00210</guid>
      <pubDate>Fri, 02 Aug 2024 06:18:58 GMT</pubDate>
    </item>
    <item>
      <title>CC-SAM：具有跨特征注意和上下文的 SAM，用于超声图像分割</title>
      <link>https://arxiv.org/abs/2408.00181</link>
      <description><![CDATA[arXiv:2408.00181v1 公告类型：新
摘要：Segment Anything 模型 (SAM) 在自然图像分割领域取得了显著成功，但其在医学成像领域的部署遇到了挑战。具体来说，该模型在处理具有低对比度、模糊边界、复杂形态和小尺寸物体的医学图像时遇到困难。为了应对这些挑战并提高 SAM 在医学领域的表现，我们引入了全面的修改。首先，我们将冻结的卷积神经网络 (CNN) 分支合并为图像编码器，它通过新颖的变分注意融合模块与 SAM 的原始视觉变换器 (ViT) 编码器协同作用。这种集成增强了模型捕获局部空间信息的能力，这在医学图像中通常是至关重要的。此外，为了进一步优化 SAM 的医学成像，我们在 ViT 分支中引入了特征和位置适配器，从而改进了编码器的表示。我们发现，与当前用于微调 SAM 进行超声医学分割的提示策略相比，使用文本描述作为 SAM 的文本提示有助于显著提高性能。利用 ChatGPT 的自然语言理解功能，我们生成提示，为 SAM 提供上下文信息和指导，使其能够更好地理解超声医学图像的细微差别并提高其分割准确性。我们的方法整体上代表着朝着使通用图像分割模型在医学领域更具适应性和效率迈出了重大一步。]]></description>
      <guid>https://arxiv.org/abs/2408.00181</guid>
      <pubDate>Fri, 02 Aug 2024 06:18:57 GMT</pubDate>
    </item>
    <item>
      <title>S-SYNTH：基于知识的皮肤图像合成生成</title>
      <link>https://arxiv.org/abs/2408.00191</link>
      <description><![CDATA[arXiv:2408.00191v1 公告类型：新
摘要：医学成像中人工智能 (AI) 技术的发展需要访问大规模和多样化的数据集进行训练和评估。在皮肤病学中，由于患者群体、照明条件和采集系统特性存在显著差异，获取此类数据集仍然具有挑战性。在这项工作中，我们提出了 S-SYNTH，这是第一个基于知识的、适应性强的开源皮肤模拟框架，可使用受解剖学启发的多层、多组分皮肤和生长病变模型快速生成合成皮肤、3D 模型和数字渲染图像。皮肤模型允许控制皮肤外观的变化，例如皮肤颜色、毛发的存在、病变形状和血液分数等参数。我们使用该框架来研究可能的变化对皮肤病变分割人工智能模型的开发和评估的影响，并表明使用合成数据获得的结果遵循与真实皮肤病图像相似的比较趋势，同时减轻现有数据集的偏差和局限性，包括数据集规模小、缺乏多样性和代表性不足。]]></description>
      <guid>https://arxiv.org/abs/2408.00191</guid>
      <pubDate>Fri, 02 Aug 2024 06:18:57 GMT</pubDate>
    </item>
    <item>
      <title>取得平衡：基于即时不确定性的用户交互实现长期视频对象分割</title>
      <link>https://arxiv.org/abs/2408.00169</link>
      <description><![CDATA[arXiv:2408.00169v1 公告类型：新
摘要：在本文中，我们介绍了一种视频对象分割 (VOS) 变体，它将交互式和半自动方法连接起来，称为惰性视频对象分割 (ziVOS)。与以离线方式（即预先录制的序列）处理视频对象分割的两个任务相比，我们提出通过 ziVOS 来定位在线录制的序列。在这里，我们努力在分割过程中即时征求用户反馈，以在长期场景中取得性能和稳健性之间的平衡。因此，我们的目标是最大限度地延长感兴趣对象的跟踪时间，同时要求最少的用户校正以在较长时间内保持跟踪。我们提出了一个有竞争力的基线，即 Lazy-XMem，作为 ziVOS 未来工作的参考。我们提出的方法使用跟踪状态的不确定性估计来确定是否需要用户交互来改进模型的预测。为了定量评估我们的方法的性能和用户的工作量，我们引入了补充指标以及该领域已经建立的指标。我们使用最近推出的 LVOS 数据集评估我们的方法，该数据集提供了大量长期视频。我们的代码可在 https://github.com/Vujas-Eteph/LazyXMem 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2408.00169</guid>
      <pubDate>Fri, 02 Aug 2024 06:18:56 GMT</pubDate>
    </item>
    <item>
      <title>基于实例感知部分分割的自动精子形态分析</title>
      <link>https://arxiv.org/abs/2408.00112</link>
      <description><![CDATA[arXiv:2408.00112v1 公告类型：新
摘要：传统的精子形态分析基于繁琐的手动注释。大量精子的自动形态分析需要对每个精子部分进行准确分割并进行定量形态评估。最先进的实例感知部分分割网络遵循“检测然后分割”范式。然而，由于精子的形状纤细，它们的分割会因 ROI 对齐期间的边界框裁剪和调整大小而遭受大量上下文丢失和特征失真。此外，由于精子尾部形状长而弯曲且宽度不均匀，因此对精子尾部的形态测量要求很高。本文介绍了自动定量测量精子形态参数的自动化技术。一种基于注意力机制的新型实例感知部分分割网络旨在通过合并特征金字塔网络提取的特征来细化初步分割的掩模，从而重建边界框外丢失的上下文并修复扭曲的特征。还提出了一种基于中心线的尾部形态自动测量方法，其中设计了异常值过滤方法和端点检测算法来精确重建尾部端点。实验结果表明，所提出的网络比最先进的自上而下的 RP-R-CNN 性能高出 9.2% [AP]_vol^p，并且所提出的尾部形态自动测量方法对长度、宽度和曲率的测量精度分别达到 95.34%、96.39% 和 91.2%。]]></description>
      <guid>https://arxiv.org/abs/2408.00112</guid>
      <pubDate>Fri, 02 Aug 2024 06:18:55 GMT</pubDate>
    </item>
    <item>
      <title>证明基于学习的关键点检测和姿势估计方法的稳健性</title>
      <link>https://arxiv.org/abs/2408.00117</link>
      <description><![CDATA[arXiv:2408.00117v1 公告类型：新
摘要：这项工作解决了基于视觉的两阶段 6D 物体姿态估计的局部鲁棒性的认证问题。两阶段物体姿态估计方法通过首先采用深度神经网络驱动的关键点回归，然后应用透视 n 点 (PnP) 技术实现了卓越的准确性。尽管取得了进步，但这些方法的鲁棒性认证仍然很少。本研究旨在填补这一空白，重点关注系统级的局部鲁棒性——在语义输入扰动中保持稳健估计的能力。核心思想是将局部鲁棒性的认证转化为分类任务的神经网络验证。挑战在于开发与现成验证工具一致的模型、输入和输出规范。为了方便验证，我们修改了关键点检测模型，用更适合验证过程的非线性操作代替非线性操作。我们没有像通常那样向图像中注入随机噪声，而是采用图像的凸包表示作为输入规范，以更准确地描述语义扰动。此外，通过进行敏感性分析，我们将稳健性标准从姿势传播到关键点精度，然后制定最佳误差阈值分配问题，以便设置最大允许的关键点偏差阈值。将每个像素视为一个单独的类，这些阈值会产生线性的、类似于分类的输出规范。在某些条件下，我们证明了我们的认证框架的主要组成部分既合理又完整，并通过对现实扰动的广泛评估来验证其效果。据我们所知，这是第一项在现实世界场景中给定图像来认证大规模基于关键点的姿势估计的稳健性的研究。]]></description>
      <guid>https://arxiv.org/abs/2408.00117</guid>
      <pubDate>Fri, 02 Aug 2024 06:18:55 GMT</pubDate>
    </item>
    <item>
      <title>从属性到自然语言：基于文本的人身再识别研究综述与展望</title>
      <link>https://arxiv.org/abs/2408.00096</link>
      <description><![CDATA[arXiv:2408.00096v1 公告类型：新
摘要：基于文本的行人重新识别（Re-ID）是复杂多模态分析领域的一个具有挑战性的课题，其最终目的是通过仔细检查属性/自然语言描述来识别特定的行人。尽管应用领域广泛，例如安全监控、视频检索、人员跟踪和社交媒体分析，但从技术角度总结基于文本的行人重新识别的全面评论却明显不足。为了解决这一差距，我们建议引入一个涵盖评估、策略、架构和优化维度的分类法，对基于文本的行人重新识别任务进行全面调查。我们首先为基于文本的行人重新识别奠定基础，阐明与基于属性/自然语言的识别相关的基本概念。然后对现有的基准数据集和指标进行彻底检查。随后，我们进一步深入研究了基于文本的行人重识别研究中采用的流行特征提取策略，然后简要总结了该领域内常见的网络架构。我们还仔细研究了基于文本的行人重识别中用于模型优化和模态对齐的流行损失函数。最后，我们简要总结了我们的研究结果，指出了基于文本的行人重识别中的挑战。为了应对这些挑战，我们概述了未来开放集基于文本的行人重识别的潜在途径，并提出了基于文本的行人图像生成引导重识别 (TBPGR) 的基本架构。]]></description>
      <guid>https://arxiv.org/abs/2408.00096</guid>
      <pubDate>Fri, 02 Aug 2024 06:18:54 GMT</pubDate>
    </item>
    <item>
      <title>WAS：艺术文本分割的数据集和方法</title>
      <link>https://arxiv.org/abs/2408.00106</link>
      <description><![CDATA[arXiv:2408.00106v1 公告类型：新
摘要：准确的文本分割结果对于文本相关的生成任务至关重要，例如文本图像生成、文本编辑、文本删除和文本样式转换。最近，一些场景文本分割方法在分割常规文本方面取得了重大进展。然而，这些方法在包含艺术文本的场景中表现不佳。因此，本文将重点放在更具挑战性的艺术文本分割任务上，并构建了一个真实的艺术文本分割数据集。该任务的一个挑战是艺术文本的局部笔画形状多变、多样性和复杂性。我们提出了一种具有分层动量查询的解码器，以防止模型忽略特殊形状的笔画区域。另一个挑战是全局拓扑结构的复杂性。我们进一步设计了一个骨架辅助头来引导模型关注全局结构。此外，为了增强文本分割模型的泛化性能，我们提出了一种基于大型多模态模型和扩散模型的训练数据合成策略。实验结果表明，我们提出的方法和合成数据集可以显著提高艺术文本分割的性能，并在其他公共数据集上取得最先进的结果。]]></description>
      <guid>https://arxiv.org/abs/2408.00106</guid>
      <pubDate>Fri, 02 Aug 2024 06:18:54 GMT</pubDate>
    </item>
    <item>
      <title>评估深度学习模型中的迁移学习对自定义野生动物数据集进行分类：YOLOv8 能否超越其他架构？</title>
      <link>https://arxiv.org/abs/2408.00002</link>
      <description><![CDATA[arXiv:2408.00002v1 公告类型：新
摘要：生物多样性在维持生态系统平衡方面起着至关重要的作用。然而，偷猎和无意的人类活动导致许多物种种群数量下降。因此，需要主动监测以保护这些濒危物种。目前以人为主导的监测技术容易出错，而且劳动密集型。因此，我们研究了卷积神经网络 (CNN) 和迁移学习等深度学习方法的应用，这些方法可以帮助实现濒危物种监测过程的自动化。为此，我们利用 iNaturalist 和 ZooChat 等值得信赖的在线数据库创建了我们的自定义数据集。为了为我们的用例选择最佳模型，我们在自定义野生动物数据集上比较了不同架构（如 DenseNet、ResNet、VGGNet 和 YOLOv8）的性能。迁移学习通过冻结预训练的权重并仅用为我们的数据集设计的自定义全连接层替换输出层来减少训练时间。我们的结果表明，YOLOv8 的表现更佳，训练准确率达到 97.39%，F1 得分达到 96.50%，超越了其他模型。我们的研究结果表明，将 YOLOv8 融入保护工作中，可以凭借其高准确率和高效率彻底改变野生动物监测，并有可能改变全球濒危物种的监测和保护方式。]]></description>
      <guid>https://arxiv.org/abs/2408.00002</guid>
      <pubDate>Fri, 02 Aug 2024 06:18:53 GMT</pubDate>
    </item>
    <item>
      <title>具有情境感知的局部高斯溅射编辑</title>
      <link>https://arxiv.org/abs/2408.00083</link>
      <description><![CDATA[arXiv:2408.00083v1 公告类型：新
摘要：最近，使用扩散先验，文本引导的单个 3D 对象的生成取得了巨大成功。然而，这些方法不适用于对象插入和替换任务，因为它们不考虑背景，导致环境内的照明不匹配。为了弥补这一差距，我们引入了一种用于 3D 高斯溅射 (3DGS) 表示的照明感知 3D 场景编辑管道。我们的主要观察是，通过最先进的条件 2D 扩散模型进行的修复与照明中的背景一致。为了利用训练有素的扩散模型的先验知识进行 3D 对象生成，我们的方法采用了具有修复视图的从粗到细的对象优化管道。在第一个粗步骤中，我们在给定理想修复视图的情况下实现图像到 3D 的提升。该过程采用来自视图条件扩散模型的 3D 感知扩散先验，从而保留条件图像中存在的照明。为了获得理想的修复图像，我们引入了一种锚视图提议 (AVP) 算法来找到最能代表目标区域场景照明的单一视图。在第二个纹理增强步骤中，我们引入了一种新颖的深度引导修复分数蒸馏采样 (DI-SDS)，它使用修复扩散先验增强几何和纹理细节，超出了第一个粗步骤中 3D 感知扩散先验知识的范围。DI-SDS 不仅提供细粒度的纹理增强，而且还促使优化以尊重场景照明。我们的方法有效地实现了具有全局照明一致性的局部编辑，而无需明确建模光传输。我们通过评估包含明确高光和阴影的真实场景中的编辑来证明我们方法的稳健性，并与最先进的文本到 3D 编辑方法进行了比较。]]></description>
      <guid>https://arxiv.org/abs/2408.00083</guid>
      <pubDate>Fri, 02 Aug 2024 06:18:53 GMT</pubDate>
    </item>
    <item>
      <title>视觉扩散模型中的复制：调查与展望</title>
      <link>https://arxiv.org/abs/2408.00001</link>
      <description><![CDATA[arXiv:2408.00001v1 公告类型：新
摘要：视觉扩散模型彻底改变了创意人工智能领域，产生了高质量和多样化的内容。然而，它们不可避免地会记住训练图像或视频，随后在推理过程中复制它们的概念、内容或风格。这种现象引起了人们对生成输出中的隐私、安全和版权的重大担忧。在这次调查中，我们首次全面回顾了视觉扩散模型中的复制，通过系统地将现有研究分类为揭示、理解和缓解这种现象，标志着对该领域的新贡献。具体而言，揭示主要指用于检测复制实例的方法。理解涉及分析导致这种现象的潜在机制和因素。缓解侧重于制定减少或消除复制的策略。除了这些方面之外，我们还回顾了关注其现实世界影响的论文。例如，在医疗保健领域，由于与患者数据相关的隐私问题，复制问题极其令人担忧。最后，本文讨论了当前面临的挑战，例如检测和基准测试复制的难度，并概述了未来的发展方向，包括开发更强大的缓解技术。通过综合不同研究的见解，本文旨在让研究人员和从业者更深入地了解人工智能技术与社会公益之间的交汇点。我们在 https://github.com/WangWenhao0716/Awesome-Diffusion-Replication 发布了这个项目。]]></description>
      <guid>https://arxiv.org/abs/2408.00001</guid>
      <pubDate>Fri, 02 Aug 2024 06:18:52 GMT</pubDate>
    </item>
    </channel>
</rss>