<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>CS.CV更新arxiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.cv更新arxiv.org e-print存档。</description>
    <lastBuildDate>Fri, 07 Mar 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>Egolife：迈向以自我为中心的生活助理</title>
      <link>https://arxiv.org/abs/2503.03803</link>
      <description><![CDATA[ARXIV：2503.03803V1公告类型：新 
摘要：我们介绍了Egolife，这是一个旨在开发以Egentric Life Assistant的项目，该项目伴随并通过AI驱动的可穿戴眼镜提高了个人效率。为了为这位助手奠定基础，我们进行了一项全面的数据收集研究，其中有六名参与者一起生活了一周，不断使用AI眼镜记录他们的日常活动，包括讨论，购物，烹饪，社交和娱乐 - 用于多模式电子中心视频捕获，以及同步的第三人称视频视频参考文献。这项工作导致了Egolife数据集，这是一个全面的300小时以人际，人际关系，多视图和多模式的日常生活数据集，并具有密集的注释。利用此数据集，我们介绍了EgolifeQa，这是一套长期以来，以生活为导向的问题，索问题的任务，旨在通过解决诸如召回过去相关事件，监视健康习惯和提供个性化建议的实用问题，以在日常生活中提供有意义的帮助。为了解决（1）为以eg中心数据开发强大的视觉审计模型的关键技术挑战，（2）实现身份识别，以及（3）促进对广泛的时间信息的长篇文化问题回答，我们介绍了Egobutler，Egobutler，Egobutler，Egogpt和Egogpt和Egorag组成的集成系统。 Egogpt是一种在以自我为中心数据集中训练的Omni-Modal模型，在以自我为中心的视频理解方面实现了最先进的性能。 Egorag是一个基于检索的组件，支持回答超长的问题。我们的实验研究验证了它们的工作机制，并揭示了关键因素和瓶颈，从而指导未来的改进。通过释放我们的数据集，模型和基准，我们旨在刺激以Egentric AI助手的进一步研究。]]></description>
      <guid>https://arxiv.org/abs/2503.03803</guid>
      <pubDate>Fri, 07 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>将视觉语言模型中的几何理解的组成部分解耦</title>
      <link>https://arxiv.org/abs/2503.03840</link>
      <description><![CDATA[ARXIV：2503.03840V1公告类型：新 
摘要：了解几何形状在很大程度上取决于视觉。在这项工作中，我们评估了最先进的视觉语言模型（VLM）是否可以理解简单的几何概念。我们使用认知科学的范式，该范式将对简单几何形状的视觉理解与经常与推理和世界知识等许多其他功能相结合。我们将模型绩效与来自美国的人类成年人以及对不受亚马逊土著群体正规教育的人类成年人的先前研究进行了比较。我们发现，VLM始终表现不佳，尽管他们在某些概念上取得了比其他概念更成功。我们还发现，VLM几何理解比人类的理解更易碎，并且在任务需要精神旋转时并不强大。这项工作突出了人类和机器中几何理解的起源的有趣差异，例如从正规教育中使用的印刷材料与与物理世界的互动或两者的结合 - 以及了解这些差异的一小步。]]></description>
      <guid>https://arxiv.org/abs/2503.03840</guid>
      <pubDate>Fri, 07 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>对视觉基础模型的任务不合时宜的攻击</title>
      <link>https://arxiv.org/abs/2503.03842</link>
      <description><![CDATA[ARXIV：2503.03842V1公告类型：新 
摘要：机器学习中安全性的研究主要集中在特定于特定于任务的攻击上，其中通过优化特定于下游任务的损失函数来获得对抗性示例。同时，机器学习实践者采用公开可用的预培训的视觉基础模型已成为标准实践，有效地在各种应用程序中共享了常见的骨干架构，例如分类，细分，深度估计，检索，问题解决方案等。研究对此类基础模型的攻击及其对多个下游任务的影响仍然充满探讨。这项工作提出了一个通用框架，该框架通过最大程度地破坏了使用基础模型获得的特征表示来制定任务无关的对手示例。我们通过测量此攻击对多个下游任务的影响及其在模型之间的可传递性来广泛评估由流行视觉基础模型获得的功能表示的安全性。]]></description>
      <guid>https://arxiv.org/abs/2503.03842</guid>
      <pubDate>Fri, 07 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Nexar Dashcam碰撞预测数据集和挑战</title>
      <link>https://arxiv.org/abs/2503.03848</link>
      <description><![CDATA[ARXIV：2503.03848V1公告类型：新 
摘要：本文介绍了Nexar Dashcam碰撞预测数据集和挑战，旨在支持交通事件分析，碰撞预测和自动驾驶汽车安全方面的研究。该数据集由1,500个带注释的视频剪辑组成，每个剪辑长约40秒，捕获了各种各样的现实交通情况。视频标有事件类型（碰撞/近距离驾驶与正常驾驶），环境条件（照明条件和天气）以及场景类型（城市，农村，高速公路等）。对于碰撞和近碰撞案例，提供了其他时间标签，包括事件的确切力矩和警报时间，标记碰撞首先可预测的何时。
  为了推进事故预测的研究，我们介绍了Nexar Dashcam碰撞预测挑战，这是该数据集的公共竞争。参与者的任务是开发机器学习模型，以预测迫在眉睫的碰撞的可能性，给定视频。使用事故发生前的多个间隔（即500毫秒，1000毫秒和事件发生前1500毫秒）计算的平均精度（AP）评估模型性能，强调了早期和可靠的预测的重要性。
  该数据集是在开放许可下发布的，并限制了对不道德使用的限制，从而确保了负责任的研究和创新。]]></description>
      <guid>https://arxiv.org/abs/2503.03848</guid>
      <pubDate>Fri, 07 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>IC-Mapper：以实例为中心的时空建模用于在线矢量地图构造</title>
      <link>https://arxiv.org/abs/2503.03882</link>
      <description><![CDATA[ARXIV：2503.03882V1公告类型：新 
摘要：基于视觉数据的在线矢量图构造可以绕过传统地图构造所需的数据收集，后处理和手动注释的过程，从而大大提高了地图构建效率。但是，现有工作将在线映射任务视为本地范围感知任务，从而忽略了地图构造所需的空间可扩展性。我们提出了一个以实例为中心的在线映射框架的IC-mapper，其中包括两个主要组件：1）以实例为中心的时间关联模块：对于相邻帧的检测查询，我们在功能和几何维度中都测量它们，以获得跨框架之间实例之间的匹配对应关系。 2）以实例为中心的空间融合模块：我们从空间维度上对历史全局映射进行点采样，并将其与与当前帧相对应的实例的检测结果集成在一起，以实现映射的实时扩展和更新。根据Nuscenes数据集，我们评估了我们的检测，跟踪和全局映射指标的方法。实验结果证明了IC贴剂与其他最先进方法的优越性。代码将在https://github.com/brickzhuantou/ic-mapper上发布。]]></description>
      <guid>https://arxiv.org/abs/2503.03882</guid>
      <pubDate>Fri, 07 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>神经描述符：使用多项式贴片对稳健局部表面描述符的自我监督学习</title>
      <link>https://arxiv.org/abs/2503.03907</link>
      <description><![CDATA[ARXIV：2503.03907V1公告类型：新 
摘要：经典形状描述符，例如热内核签名（HKS），波核标志（WKS）和方向直方图（SHOT）的签名，虽然广泛用于形状分析，但对网格连接性，采样模式和拓扑噪声表现出敏感性。尽管差异几何形状通过其差异不变性理论提供了一种有希望的替代方法，从理论上讲，这些理论可以保证是强大的形状描述符，但这些不变的分离网格上的计算通常会导致不稳定的数值近似值，从而限制了其实际效用。我们提出了一种自我监督的学习方法，用于从3D表面提取几何特征。我们的方法将合成数据的生成与旨在学习采样不变特征的神经结构相结合。通过将我们的功能集成到现有的形状通信框架中，我们在包括浮士德，Scape，Topkids和Sherec&#39;16在内的标准基准上的性能提高了，对拓扑噪声和部分形状表现出了特殊的鲁棒性。]]></description>
      <guid>https://arxiv.org/abs/2503.03907</guid>
      <pubDate>Fri, 07 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>外科手术：对外科视频解剖学分割和检测的基础模型进行微调</title>
      <link>https://arxiv.org/abs/2503.03942</link>
      <description><![CDATA[ARXIV：2503.03942V1公告类型：新 
摘要：背景：我们通过在零拍和微调之后检查其对器官/组织的语义分割能力来评估SAM 2的手术场景理解。方法：我们利用五个公共数据集评估和微调SAM 2在外科视频/图像中分割解剖组织。微调应用于图像编码器和蒙版解码器。我们将培训子集从每个课程中的50个样本中限制在50个样本中，以更好地模拟使用数据采集的实际约束。使用加权平均骰子系数（WMDC）评估了数据集大小对微调性能的影响，并且还将结果与先前报道的最先进的（SOTA）结果进行了比较。结果：与基线SAM 2相比，Surgisam 2是一种微调的SAM 2模型，显示了分割性能的显着改善，相对WMDC的增益获得了17.9％的相对WMDC增益。从1个提示点增加到10，并将数据量表从50/class培训到400/类/类/类增强的性能；在验证子集上，最佳的WMDC为0.92，每类10个提示点和400个样本实现。在测试子集上，使用10分提示，该模型在24/30（80％）的类别中优于24/30（80％）的先验SOTA方法。值得注意的是，Surgisam 2有效地进行了有效的器官类别，在其中7/9（77.8％）上实现了SOTA。结论：SAM 2在手术场景细分方面取得了显着的零射击和微调的性能，超过了几个器官类别的不同数据集的先前的SOTA模型。这表明可以实现自动/半自动注释管道的巨大潜力，从而减轻了促进几种手术应用的注释负担。]]></description>
      <guid>https://arxiv.org/abs/2503.03942</guid>
      <pubDate>Fri, 07 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>护卫门：通过保护后门保护恶意扩散编辑</title>
      <link>https://arxiv.org/abs/2503.03944</link>
      <description><![CDATA[ARXIV：2503.03944V1公告类型：新 
摘要：扩散模型的不断增长彻底改变了图像编辑，但也引起了人们对未经授权修改的重大关注，例如误导和窃。现有的对策在很大程度上依赖于旨在破坏扩散模型输出的对抗扰动。但是，发现这些方法很容易通过简单的图像预处理技术（例如压缩和噪声）进行中和。为了解决这一限制，我们提出了Guarddoor，这是一种新颖而强大的保护机制，促进了图像所有者和模型提供者之间的协作。具体而言，参与机制的模型提供商微调图像编码器嵌入了保护性后门，从而使图像所有者可以要求将不可察觉的触发器附加到其图像上。当未经授权的用户尝试通过此扩散模型编辑这些受保护的图像时，该模型会产生毫无意义的输出，从而降低了恶意图像编辑的风险。我们的方法证明了针对图像预处理操作的鲁棒性增强，并且对于大规模部署而言是可扩展的。这项工作强调了模型提供商和图像所有者之间合作框架的潜力，以保护生成AI时代的数字内容。]]></description>
      <guid>https://arxiv.org/abs/2503.03944</guid>
      <pubDate>Fri, 07 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>粗糙：与粗大的伪造标签的合作伪标记，用于越野语义分段</title>
      <link>https://arxiv.org/abs/2503.03947</link>
      <description><![CDATA[ARXIV：2503.03947V1公告类型：新 
摘要：自主越野导航由于多种多样的非结构化环境而面临挑战，需要通过几何和语义理解来强大的感知。但是，稀缺标记的语义数据限制了跨域的概括。模拟数据有帮助，但引入了域的适应问题。我们提出了一个粗糙的，一个半监督的域适应框架，用于越野语义分割，利用稀疏，粗大的内域标签和密集标记的偏域数据。使用预审前的视觉变压器，我们用互补的像素级和贴片级解码器桥接域间隙，并通过未标记的数据的协作伪标记策略增强了域差距。对RUGD和Rellis-3D数据集的评估分别显示出9.7 \％和8.4 \％的显着改善，而仅使用粗数据。在多生物群落环境中对现实世界中越野车数据的测试进一步证明了粗糙的适用性。]]></description>
      <guid>https://arxiv.org/abs/2503.03947</guid>
      <pubDate>Fri, 07 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>视网膜网络：由大型视觉模型提供动力的视网膜临床偏好对话助手</title>
      <link>https://arxiv.org/abs/2503.03987</link>
      <description><![CDATA[ARXIV：2503.03987V1公告类型：新 
摘要：最近，多模式的大语言模型（MLLM）对其处理和分析非文本数据（例如图像，视频和音频）的显着能力引起了极大的关注。值得注意的是，已经探索了包括LLAVA-MED在内的几种通用域MLLM对医疗领域的改编。但是，这些医学适应在理解和解释视网膜图像方面仍然不足。相比之下，医学专家强调了定量分析在疾病检测和解释中的重要性。这突显了通用域和医疗域MLLM之间的差距：虽然通用域MLLM在广泛的应用中表现出色，但它们缺乏在医疗领域精确诊断和解释性任务所需的专业知识。为了应对这些挑战，我们介绍了\ textit {RetinalGpt}，这是视网膜图像的临床优选定量分析的多模式对话助手。具体而言，我们通过编译一个大型的视网膜图像数据集，开发新的数据管道并采用自定义的视觉指导调整来增强视网膜分析和丰富医学知识来实现​​这一目标。特别是，在8个基准视网膜数据集中，视网膜疾病的诊断较大，视网膜差优于通用域中的MLLM。除了疾病诊断之外，视网膜还具有定量分析和病变定位，这代表了利用LLM的开创性步骤，用于可解释的端到端临床研究框架。该代码可从https://github.com/retinal-research/retinalgpt获得]]></description>
      <guid>https://arxiv.org/abs/2503.03987</guid>
      <pubDate>Fri, 07 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DSV-LFS：统一LLM驱动的语义提示具有视觉特征，可用于稳健的几次分割</title>
      <link>https://arxiv.org/abs/2503.04006</link>
      <description><![CDATA[ARXIV：2503.04006V1公告类型：新 
摘要：很少有语义分割（FSS）的目的是使模型仅使用有限的标记示例来分割新颖/看不见的对象类。但是，当前的FSS方法经常由于不完整和有偏见的特征表示而在概括中遇到困难，尤其是当支持图像未捕获目标类别的全部外观变异性时。为了改善FSS管道，我们提出了一个新颖的框架，该框架利用大型语言模型（LLMS）将通用类语义信息调整为查询图像。此外，该框架采用密集的像素匹配来确定查询和支持图像之间的相似性，从而提高了FSS性能。受基于推理的分割框架的启发，我们的方法名为DSV-LFS，向LLM词汇引入了其他令牌，允许多模式LLM从类描述中生成“语义提示”。同时，一个密集的匹配模块标识查询和支持图像之间的视觉相似性，从而生成“视觉提示”。然后，共同使用这些提示来指导基于及时的解码器，以准确地分割查询图像。基准数据集上的全面实验Pascal- $ 5^{i} $和可可$ 20^{i} $表明，我们的框架可以实现最先进的性能，从而有很大的差距证明了对各种场景的新颖性和鲁棒性的出色概括。源代码可在\ href {https://github.com/aminpdik/dsv-lfs} {https://github.com/aminpdik/dsv-lfs}中获得。]]></description>
      <guid>https://arxiv.org/abs/2503.04006</guid>
      <pubDate>Fri, 07 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>NSBM-GAT：一般交通崩溃风险预测的非平稳块最大和图形注意框架</title>
      <link>https://arxiv.org/abs/2503.04018</link>
      <description><![CDATA[ARXIV：2503.04018V1公告类型：新 
摘要：准确预测单个车辆的交通崩溃风险对于增强车辆安全至关重要。尽管对流量崩溃风险预测非常关注，但现有研究面临两个主要挑战：首先，由于崩溃之前的单个车辆数据缺乏，大多数模型都依赖于研究人员认为危险的假设场景。这引起了对它们对实际碰撞条件的适用性的怀疑。其次，从仪表板视频中学到了一些崩溃风险预测框架。尽管这样的视频捕捉了单个车辆的爆炸前行为，但它们通常缺乏有关周围车辆运动的关键信息。但是，车辆及其周围车辆之间的相互作用在撞车事件中具有很大影响。为了克服这些挑战，我们提出了一种新型的非平稳性极值理论（EVT），其中使用图形注意力网络以非线性方式优化了协变量函数。 EVT组件通过概率分布结合了崩溃的随机性质，从而增强了模型的解释性。值得注意的是，非线性协变量功能使该模型能够捕获目标车辆与其多个周围车辆之间的交互行为，从而促进了跨不同驾驶任务的崩溃风险预测。我们在实际崩溃之前使用100组车辆轨迹数据训练和测试我们的模型，这是在合并和编织段的三年内通过无人机收集的。我们证明，我们的模型成功地学习了崩溃的微级前体，并借助非线性协变量函数拟合了更准确的分布。我们在测试数据集上的实验表明，提出的模型通过同时提供后端和侧环崩溃的更准确的预测来优于现有模型。]]></description>
      <guid>https://arxiv.org/abs/2503.04018</guid>
      <pubDate>Fri, 07 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>TextDoctor：统一文档图像通过补丁金字塔扩散模型插图</title>
      <link>https://arxiv.org/abs/2503.04021</link>
      <description><![CDATA[ARXIV：2503.04021V1公告类型：新 
摘要：现实世界中文本文档的数字版本通常会遇到原始文档的环境腐蚀，低质量扫描或人类干扰等问题。现有的文档修复和介绍方法通常在概括地看不见的文档样式和处理高分辨率图像方面努力。为了应对这些挑战，我们介绍了TextDoctor，这是一种新颖的统一文档图像介入方法。受到人类阅读行为的启发，TextDoctor恢复了补丁的基本文本元素，然后将扩散模型应用于整个文档图像，而不是对特定文档类型的培训模型。为了处理不同的文本大小并避免在高分辨率文档中常见的有内存的问题，我们建议使用结构金字塔预测和贴片金字塔扩散模型。这些技术利用多尺度输入和金字塔贴片来提高全球和本地覆盖的质量。在七个公共数据集上进行的广泛的定性和定量实验验证了TextDoctor在还原各种类型的高分辨率文档图像方面优于最先进的方法。]]></description>
      <guid>https://arxiv.org/abs/2503.04021</guid>
      <pubDate>Fri, 07 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>自我监督的大规模点云完成考古场地修复</title>
      <link>https://arxiv.org/abs/2503.04030</link>
      <description><![CDATA[ARXIV：2503.04030V1公告类型：新 
摘要：点云完成有助于恢复部分不完整的点云遭受遮挡。当前的自我监督方法无法为缺少表面和可用点不平衡分布的大对象提供高保真度完成。在本文中，我们提出了一种新颖的方法，用于恢复具有有限和不平衡地面真相的大规模点云。使用粗糙的边界注释对感兴趣的区域，我们将原始点云投影到多中心的预测（MCOP）图像中，其中片段被投影到5个通道的图像（RGB，深度和旋转）。原始点云的完成将减少为介绍MCOP图像中缺少的像素。由于缺乏完整的结构和现有零件的不平衡分布，我们开发了一种自我保护的方案，该方案学会用类似于现有的“完整”补丁的点填充MCOP图像。采用特殊损失来进一步增强完成的MCOP图像的规律性和一致性，该图像映射回3D以形成最终的修复。广泛的实验证明了我们方法在秘鲁完成600多个不完整和不平衡的考古结构方面的优越性。]]></description>
      <guid>https://arxiv.org/abs/2503.04030</guid>
      <pubDate>Fri, 07 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GaussianGraph：3D基于高斯的场景图生成用于开放世界的场景理解</title>
      <link>https://arxiv.org/abs/2503.04034</link>
      <description><![CDATA[ARXIV：2503.04034V1公告类型：新 
摘要：3D高斯脱落（3DG）的最新进展已显着改善了语义场景的理解，从而使自然语言查询能够在场景中定位对象。但是，现有的方法主要集中于将压缩夹的特征嵌入到3D高斯，患有低对象分割精度和缺乏空间推理能力。为了解决这些局限性，我们提出了高斯格拉普（Gaussiangraph），这是一个新颖的框架，通过整合自适应语义聚类和场景图生成来增强基于3DGS的场景理解。我们引入了“控制遵循”聚类策略，该策略会动态适应场景尺度和特征分布，避免了特征压缩并显着提高了分割精度。此外，我们通过整合从2D基础模型中提取的对象属性和空间关系来丰富场景表示。为了解决空间关系中的不准确性，我们提出了3D校正模块，该模块通过空间一致性验证过滤不可用的关系，从而确保了可靠的场景图构造。在三个数据集上进行的大量实验表明，高斯图在语义分割和对象接地任务中的最先进方法优于最先进的方法，为复杂的场景理解和交互提供了强大的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2503.04034</guid>
      <pubDate>Fri, 07 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>