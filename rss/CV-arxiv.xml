<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Fri, 13 Dec 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>轻量级语义分割网络的特征细化模块</title>
      <link>https://arxiv.org/abs/2412.08670</link>
      <description><![CDATA[arXiv:2412.08670v1 公告类型：新
摘要：低计算复杂度和高分割精度对于现实世界的语义分割任务都至关重要。然而，为了加快模型推理速度，大多数现有方法倾向于设计具有非常有限参数数量的轻量级网络，由于网络的表示能力下降导致精度大幅下降。为了解决这个问题，本文提出了一种新的语义分割方法，以提高轻量级网络获取语义信息的能力。具体来说，提出了一个特征细化模块（FRM），从主干生成的多阶段特征图中提取语义，并利用变换器块捕获非局部上下文信息。在 Cityscapes 和 Bdd100K 数据集上，实验结果表明，所提出的方法在精度和计算成本之间取得了良好的平衡，尤其是对于 Cityscapes 测试集，实现了 80.4% 的 mIoU，而只需要 214.82 GFLOP。]]></description>
      <guid>https://arxiv.org/abs/2412.08670</guid>
      <pubDate>Fri, 13 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有语义和上下文细化的深度语义分割网络</title>
      <link>https://arxiv.org/abs/2412.08671</link>
      <description><![CDATA[arXiv:2412.08671v1 公告类型：新
摘要：语义分割是多媒体处理中的一项基本任务，可用于分析、理解、编辑图像和视频的内容等。为了加速多媒体数据的分析，现有的分割研究倾向于通过逐步降低特征图的空间分辨率来提取语义信息。然而，这种方法在恢复高级特征图的分辨率时会引入错位问题。在本文中，我们设计了一个语义细化模块 (SRM) 来解决分割网络中的这个问题。具体而言，SRM 旨在学习上采样特征图中每个像素的变换偏移，由高分辨率特征图和相邻偏移引导。通过将这些偏移应用于上采样特征图，SRM 增强了分割网络的语义表示，特别是对于对象边界周围的像素。此外，还提出了一个上下文细化模块 (CRM) 来捕获跨空间和通道维度的全局上下文信息。为了平衡通道和空间之间的维度，我们汇总了主干网络所有四个阶段的语义图，以丰富通道上下文信息。这些提议模块的有效性在三个广泛使用的数据集（Cityscapes、Bdd100K 和 ADE20K）上得到了验证，与最先进的方法相比，它们表现出了卓越的性能。此外，本文将这些模块扩展到轻量级分割网络，在仅使用 137.9 GFLOP 的情况下，在 Cityscapes 验证集上实现了 82.5% 的 mIoU。]]></description>
      <guid>https://arxiv.org/abs/2412.08671</guid>
      <pubDate>Fri, 13 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Coherent3D：通过三平面融合实现连贯的 3D 肖像视频重建</title>
      <link>https://arxiv.org/abs/2412.08684</link>
      <description><![CDATA[arXiv:2412.08684v1 公告类型：新
摘要：单图像 3D 肖像重建的最新突破使远程呈现系统能够实时从单个摄像头传输 3D 肖像视频，从而使远程呈现变得民主化。然而，每帧 3D 重建表现出时间不一致性并且会忘记用户的外表。另一方面，自我重现方法可以通过驱动从单个参考图像构建的 3D 头像来渲染连贯的 3D 肖像，但无法忠实地保留用户的每帧外观（例如，瞬时面部表情和灯光）。因此，这两个框架都不是民主化 3D 远程呈现的理想解决方案。在这项工作中，我们解决了这个难题，并提出了一种新颖的解决方案，既能保持连贯的身份，又能保持动态的每帧外观，以实现最佳的真实感。为此，我们提出了一种新的基于融合的方法，通过将参考视图中的标准 3D 先验与每帧输入视图中的动态外观融合在一起，充分利用两者的优势，生成时间稳定的 3D 视频，并忠实重建用户的每帧外观。我们的基于编码器的方法仅使用由表情调节的 3D GAN 生成的合成数据进行训练，在工作室和野外数据集上实现了最先进的 3D 重建和时间一致性。https://research.nvidia.com/labs/amri/projects/coherent3d]]></description>
      <guid>https://arxiv.org/abs/2412.08684</guid>
      <pubDate>Fri, 13 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ChatDyn：街景中语言驱动的多主体动态生成</title>
      <link>https://arxiv.org/abs/2412.08685</link>
      <description><![CDATA[arXiv:2412.08685v1 公告类型：新
摘要：根据特定指令生成交通参与者的真实且可交互的动态对于街道场景模拟至关重要。然而，目前缺乏一种全面的方法来生成不同类型的参与者（包括车辆和行人）的真实动态，以及它们之间不同类型的交互。在本文中，我们介绍了 ChatDyn，这是第一个能够基于语言指令在街道场景中生成交互式、可控且真实的参与者动态的系统。为了通过复杂的语言实现精确控制，ChatDyn 采用了多 LLM 代理角色扮演方法，该方法利用自然语言输入来规划不同交通参与者的轨迹和行为。为了根据规划生成逼真的细粒度动态，ChatDyn 设计了两个新颖的执行器：PedExecutor，一个统一的多任务执行器，可在不同的任务规划下生成逼真的行人动态；VehExecutor，一种基于物理转换的策略，可生成物理上合理的车辆动态。大量实验表明，ChatDyn 可以生成包含多辆车和行人的逼真驾驶场景动态，并且在子任务上的表现明显优于以前的方法。代码和模型将在 https://vfishc.github.io/chatdyn 上提供。]]></description>
      <guid>https://arxiv.org/abs/2412.08685</guid>
      <pubDate>Fri, 13 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>VisionArena：230K 真实世界用户-VLM 对话，带有偏好标签</title>
      <link>https://arxiv.org/abs/2412.08687</link>
      <description><![CDATA[arXiv:2412.08687v1 公告类型：新
摘要：随着视觉语言模型 (VLM) 的采用和功能的不断增加，需要能够捕捉真实用户-VLM 交互的基准。为此，我们创建了 VisionArena，这是一个包含 230K 个用户和 VLM 之间真实对话的数据集。VisionArena 收集自 Chatbot Arena（一个用户与 VLM 交互并提交偏好投票的开源平台），涵盖 73K 个唯一用户、45 个 VLM 和 138 种语言。我们的数据集包含三个子集：VisionArena-Chat，用户和 VLM 之间的 200k 个单轮和多轮对话；VisionArena-Battle，30K 个对话，比较两个匿名 VLM 与用户偏好投票；以及 VisionArena-Bench，一个包含 500 个不同用户提示的自动基准，可有效近似实时 Chatbot Arena 模型排名。此外，我们还重点介绍了用户提出的问题类型、回答风格对偏好的影响以及模型经常失败的领域。我们发现，字幕和幽默等开放式任务高度依赖于风格，而当前的 VLM 在空间推理和规划任务方面表现不佳。最后，我们展示了在 VisionArena-Chat 上对同一基础模型进行微调后，其表现优于 Llava-Instruct-158K，在 MMMU 上提高了 17 点，在 WildVision 基准上提高了 46 点。数据集位于 https://huggingface.co/lmarena-ai]]></description>
      <guid>https://arxiv.org/abs/2412.08687</guid>
      <pubDate>Fri, 13 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Euclid：利用合成高保真视觉描述增强多模态法学硕士 (LLM)</title>
      <link>https://arxiv.org/abs/2412.08737</link>
      <description><![CDATA[arXiv:2412.08737v1 公告类型：新
摘要：多模态大型语言模型 (MLLM) 近年来取得了快速进展，但在低级视觉感知 (LLVP) 方面仍然举步维艰——尤其是准确描述图像几何细节的能力。这种能力对于机器人、医学图像分析和制造等领域的应用至关重要。在本文中，我们首先介绍 Geoperception，这是一个旨在评估 MLLM 准确转录图像中二维几何信息的能力的基准。利用这个基准，我们展示了领先的 MLLM 的局限性，然后进行了一项全面的实证研究，以探索提高其几何任务性能的策略。我们的研究结果强调了某些模型架构、训练技术和数据策略的好处，包括使用高保真合成数据和使用数据课程进行多阶段训练。值得注意的是，我们发现数据课程使模型能够学习具有挑战性的几何理解任务，而这些任务是它们无法从头开始学习的。利用这些见解，我们开发了 Euclid，这是一组专门针对强大的低级几何感知而优化的模型。尽管 Euclid 仅使用合成多模态数据进行训练，但它对新型几何形状表现出了很强的泛化能力。例如，在某些 Geoperception 基准任务上，Euclid 的表现比最好的闭源模型 Gemini-1.5-Pro 高出 58.56%，在所有任务上平均高出 10.65%。]]></description>
      <guid>https://arxiv.org/abs/2412.08737</guid>
      <pubDate>Fri, 13 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DocVLM：让你的 VLM 成为高效的阅读器</title>
      <link>https://arxiv.org/abs/2412.08746</link>
      <description><![CDATA[arXiv:2412.08746v1 公告类型：新
摘要：视觉语言模型 (VLM) 在各种视觉任务中表现出色，但在文档理解方面面临挑战，这需要细粒度的文本处理。虽然典型的视觉任务在低分辨率输入下表现良好，但阅读密集型应用程序需要高分辨率，从而导致计算开销很大。在 VLM 提示中使用 OCR 提取的文本部分解决了这个问题，但与全分辨率文本相比表现不佳，因为它缺乏最佳性能所需的完整视觉上下文。我们引入了 DocVLM，这是一种将基于 OCR 的模态集成到 VLM 中以增强文档处理同时保留原始权重的方法。我们的方法采用 OCR 编码器来捕获文本内容和布局，将它们压缩为一组紧凑的学习查询，并合并到 VLM 中。对领先的 VLM 的全面评估表明，DocVLM 显着减少了对高分辨率图像进行文档理解的依赖。在有限标记方案 (448$\times$448) 下，具有 64 个学习查询的 DocVLM 与 InternVL2 集成时可将 DocVQA 结果从 56.0% 提高到 86.6%，与 Qwen2-VL 集成时可将 84.4% 提高到 91.2%。在 LLaVA-OneVision 中，DocVLM 使用的图像标记减少了 80%，但结果得到了改善。减少的标记使用量可以有效地处理多个页面，在 DUDE 上显示出令人印象深刻的零样本结果，在 MP-DocVQA 上显示出最先进的性能，凸显了 DocVLM 在需要高性能和高效率的应用程序中的巨大潜力。]]></description>
      <guid>https://arxiv.org/abs/2412.08746</guid>
      <pubDate>Fri, 13 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>主动对抗防御：利用视觉语言模型中的快速调整来检测未见的后门图像</title>
      <link>https://arxiv.org/abs/2412.08755</link>
      <description><![CDATA[arXiv:2412.08755v1 公告类型：新
摘要：后门攻击通过将隐藏的触发器嵌入输入中构成严重威胁，导致模型将其错误分类为目标标签。虽然大量研究集中在通过权重微调来减轻对象识别模型中的这些攻击，但对直接检测后门样本的关注却少得多。鉴于训练中使用的大量数据集，手动检查后门触发器是不切实际的，即使是最先进的防御机制也无法完全消除其影响。为了解决这一差距，我们引入了一种突破性的方法来检测训练和推理过程中看不见的后门图像。利用视觉语言模型 (VLM) 中提示调整的变革性成功，我们的方法训练可学习的文本提示，以区分干净的图像和隐藏后门触发器的图像。实验证明了该方法的卓越有效性，在两个著名的数据集中检测看不见的后门触发器时达到了令人印象深刻的 86% 的平均准确率，为后门防御树立了新的标准。]]></description>
      <guid>https://arxiv.org/abs/2412.08755</guid>
      <pubDate>Fri, 13 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>超越知识孤岛：任务指纹识别助力医学成像 AI 民主化</title>
      <link>https://arxiv.org/abs/2412.08763</link>
      <description><![CDATA[arXiv:2412.08763v1 公告类型：新
摘要：医学成像人工智能领域目前正在经历快速变革，系统性研究越来越多地转化为临床实践。尽管取得了这些成功，但研究仍受到知识孤岛的影响，阻碍了协作和进步：现有知识分散在出版物中，许多细节仍未公布，而隐私法规限制了数据共享。本着人工智能民主化的精神，我们提出了一个在医学图像分析领域安全知识转移的框架。我们方法的关键是数据集“指纹”，即特征分布的结构化表示，可以量化任务相似性。我们通过转移神经架构、预训练、增强策略和多任务学习，在 71 个不同的任务和 12 种医学成像模式下测试了我们的方法。根据综合分析，我们的方法优于传统的识别相关知识的方法，并促进了协作模型训练。我们的框架促进了医学成像领域人工智能的民主化，并可能成为促进更快科学进步的宝贵工具。]]></description>
      <guid>https://arxiv.org/abs/2412.08763</guid>
      <pubDate>Fri, 13 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLaVA-Zip：具有内在图像信息的自适应视觉标记压缩</title>
      <link>https://arxiv.org/abs/2412.08771</link>
      <description><![CDATA[arXiv:2412.08771v1 公告类型：新
摘要：利用指令跟踪数据的多模态大型语言模型 (MLLM)（例如 LLaVA）在业界取得了巨大进步。这些模型的一个主要限制是视觉标记消耗了大型语言模型 (LLM) 中最大标记限制的很大一部分，导致在提示包含多个图像或视频时计算需求增加且性能下降。行业解决方案通常通过增加计算能力来缓解此问题，但这种方法在资源有限的学术环境中不太可行。在本研究中，我们提出了基于 LLaVA-1.5 的动态特征图缩减 (DFMR) 来解决视觉标记过载的挑战。DFMR 动态压缩视觉标记，释放标记容量。我们的实验结果表明，将 DFMR 集成到 LLaVA-1.5 中可显著提高 LLaVA 在不同视觉标记长度下的性能，为扩展 LLaVA 以处理资源受限的学术环境中的多图像和视频场景提供了一种有前途的解决方案，并且它还可以应用于行业环境中的数据增强，以帮助缓解在持续的预训练阶段开放域图像-文本对数据集的稀缺性。]]></description>
      <guid>https://arxiv.org/abs/2412.08771</guid>
      <pubDate>Fri, 13 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ProtoOcc：使用双分支编码器-原型查询解码器进行准确、高效的 3D 占用预测</title>
      <link>https://arxiv.org/abs/2412.08774</link>
      <description><![CDATA[arXiv:2412.08774v1 公告类型：新 
摘要：在本文中，我们介绍了 ProtoOcc，这是一种新颖的 3D 占用预测模型，旨在通过对场景的深度语义理解来预测 3D 体素的占用状态和语义类别。ProtoOcc 由两个主要组件组成：双分支编码器 (DBE) 和原型查询解码器 (PQD)。DBE 通过双分支结构跨多个尺度组合 3D 体素和 BEV 表示来生成新的 3D 体素表示。这种设计通过为 BEV 表示提供较大的感受野，同时为体素表示保持较小的感受野，从而提高了性能和计算效率。PQD 引入了原型查询来加速解码过程。场景自适应原型源自输入样本的 3D 体素特征，而场景无关原型则是通过在训练阶段将场景自适应原型应用于指数移动平均线来计算的。通过使用这些基于原型的查询进行解码，我们可以直接在单个步骤中预测 3D 占用率，而无需迭代 Transformer 解码。此外，我们提出了稳健原型学习，它将噪声注入原型生成过程并训练模型在训练阶段进行去噪。ProtoOcc 在 Occ3D-nuScenes 基准上以 45.02% mIoU 实现了最先进的性能。对于单帧方法，它在 NVIDIA RTX 3090 上达到 39.56% mIoU，推理速度为 12.83 FPS。我们的代码可以在 https://github.com/SPA-junghokim/ProtoOcc 找到。]]></description>
      <guid>https://arxiv.org/abs/2412.08774</guid>
      <pubDate>Fri, 13 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用外显记忆进行生成建模</title>
      <link>https://arxiv.org/abs/2412.08781</link>
      <description><![CDATA[arXiv:2412.08781v1 公告类型：新
摘要：最近的研究表明，深度生成扩散模型中的去噪过程隐式学习并记忆数据分布中的语义信息。这些发现表明，捕获更复杂的数据分布需要更大的神经网络，从而导致计算需求大幅增加，这反过来又成为扩散模型训练和推理的主要瓶颈。为此，我们引入了带有 \textbf{E}xplicit \textbf{M}emory (GMem) 的 \textbf{G}enerative \textbf{M}modeling，在扩散模型的训练和采样阶段都利用外部存储库。这种方法保留了数据分布中的语义信息，减少了对神经网络学习能力的依赖，并在不同的数据集中进行推广。结果很显著：我们的 GMem 提高了训练、采样效率和生成质量。例如，在分辨率为 $256 \times 256$ 的 ImageNet 上，GMem 将 SiT 训练速度提高了超过 $46.7\times$，在不到 $150K$ 步内实现了经过 $7M$ 步训练的 SiT 模型的性能。与现有最有效的方法 REPA 相比，GMem 仍提供了 $16\times$ 的速度提升，在 $250K$ 步内达到 5.75 的 FID 分数，而 REPA 需要超过 $4M$ 步。此外，我们的方法实现了最先进的生成质量，在 ImageNet $256\times256$ 上无需无分类器指导的 FID 分数为 {3.56}。我们的代码可在 \url{https://github.com/LINs-lab/GMem} 上找到。]]></description>
      <guid>https://arxiv.org/abs/2412.08781</guid>
      <pubDate>Fri, 13 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DALI：通过分布级和实例级伪标签去噪实现域自适应 LiDAR 对象检测</title>
      <link>https://arxiv.org/abs/2412.08806</link>
      <description><![CDATA[arXiv:2412.08806v1 公告类型：新
摘要：使用 LiDAR 点云进行物体检测依赖于大量人工注释的样本，以训练底层检测器的深度神经网络。但是，为大规模数据集生成 3D 边界框注释可能成本高昂且耗时。或者，无监督域自适应 (UDA) 使给定的物体检测器能够对具有未标记训练数据集的新数据进行操作，方法是将从训练标记的 \textit{源域} 数据中学到的知识转移到新的未标记的 \textit{目标域}。伪标签策略通常用于 UDA，该策略涉及使用来自预训练模型的目标域预测边界框来训练 3D 物体检测器。然而，这些伪标签通常会引入噪音，影响性能。在本文中，我们介绍了域自适应 LIdar (DALI) 物体检测框架来解决分布和实例级别的噪声问题。首先，开发了一种训练后尺寸归一化 (PTSN) 策略，通过在网络训练后确定无偏尺度来减轻伪标签尺寸分布中的偏差。为了解决伪标签和相应点云之间的实例级噪声，开发了两种伪点云生成 (PPCG) 策略，即射线约束和无约束，以为每个实例生成伪点云，确保训练期间伪标签和伪点之间的一致性。我们在公开可用且流行的数据集 KITTI、Waymo 和 nuScenes 上证明了我们方法的有效性。我们表明，所提出的 DALI 框架实现了最先进的结果，并且在大多数领域自适应任务上都优于领先方法。我们的代码可在 \href{https://github.com/xiaohulugo/T-RO2024-DALI}{https://github.com/xiaohulugo/T-RO2024-DALI} 获得。]]></description>
      <guid>https://arxiv.org/abs/2412.08806</guid>
      <pubDate>Fri, 13 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Labits：基于事件相机的连续密集轨迹估计的分层双向时间表面表示</title>
      <link>https://arxiv.org/abs/2412.08849</link>
      <description><![CDATA[arXiv:2412.08849v1 公告类型：新
摘要：事件相机为传统的基于帧的传感器提供了一种引人注目的替代方案，可以捕捉具有高时间分辨率和低延迟的动态场景。移动物体沿其轨迹触发具有精确时间戳的事件，从而实现平滑的连续时间估计。然而，很少有工作试图优化事件表示构建过程中的信息丢失，这给这项任务设置了上限。充分利用事件相机需要同时保留细粒度时间信息、稳定且特征化的二维视觉特征以及时间一致的信息密度的表示，这是现有表示中未解决的挑战。我们引入了 Labits：分层双向时间表面，这是一种简单而优雅的表示，旨在保留所有这些特征。此外，我们提出了一个专用模块来提取活动像素局部光流 (APLOF)，显着提高性能。与 MultiFlow 数据集上以前的最先进技术相比，我们的方法实现了轨迹端点误差 (TEPE) 的 49% 的显著减少。代码将在接受后发布。]]></description>
      <guid>https://arxiv.org/abs/2412.08849</guid>
      <pubDate>Fri, 13 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ViUniT：可视化单元测试，实现更强大的可视化编程</title>
      <link>https://arxiv.org/abs/2412.08859</link>
      <description><![CDATA[arXiv:2412.08859v1 公告类型：新
摘要：基于编程的推理任务方法大大扩展了模型可以回答的有关视觉场景的问题类型。然而，在基准视觉推理数据上，当模型回答正确时，它们 33% 的时间都会产生错误的程序。这些模型往往出于错误的原因而正确，并有可能在新数据上出现意外故障。单​​元测试在确保代码正确性方面起着基础性作用，可用于修复此类故障。我们提出了可视化单元测试 (ViUniT)，这是一个通过自动生成单元测试来提高视觉程序可靠性的框架。在我们的框架中，单元测试表示为一个新的图像和答案对，旨在验证针对给定查询生成的程序的逻辑正确性。我们的方法利用语言模型以图像描述和预期答案的形式创建单元测试，并通过图像合成来生成相应的图像。我们对有效的视觉单元测试套件的构成进行了全面分析，探索了单元测试生成、采样策略、图像生成方法以及改变程序和单元测试的数量。此外，我们介绍了视觉单元测试的四种应用：最佳程序选择、答案拒绝、重新提示和强化学习的无监督奖励公式。在视觉问答和图像文本匹配的三个数据集上使用两个模型进行的实验表明，ViUniT 将模型性能提高了 11.4%。值得注意的是，它使 7B 开源模型的平均性能比 gpt-4o-mini 高出 7.7%，并将因错误原因而正确的程序的发生率降低了 40%。]]></description>
      <guid>https://arxiv.org/abs/2412.08859</guid>
      <pubDate>Fri, 13 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>