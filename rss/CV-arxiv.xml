<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Mon, 13 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>翻译中发现：增强人脸验证中 AI 可解释性的语义方法</title>
      <link>https://arxiv.org/abs/2501.05471</link>
      <description><![CDATA[arXiv:2501.05471v1 公告类型：新
摘要：计算机视觉中机器学习模型的复杂性日益增加，特别是在人脸验证中，这需要开发可解释的人工智能 (XAI) 来增强可解释性和透明度。这项研究扩展了以前的工作，将从人类认知过程衍生的语义概念集成到 XAI 框架中，以弥合模型输出和人类理解之间的理解差距。我们提出了一种结合全局和局部解释的新方法，使用由用户选择的面部标志定义的语义特征通过大型语言模型 (LLM) 生成相似性图和文本解释。该方法通过定量实验和用户反馈进行了验证，证明了可解释性的提高。结果表明，与传统方法相比，我们基于语义的方法，尤其是最详细的方法，提供了对模型决策更细致入微的理解。用户研究强调了我们对语义解释的偏好，而不是传统的基于像素的热图，强调了以人为本的人工智能可解释性的好处。这项工作有助于持续努力创建 XAI 框架，使 AI 模型行为与人类的认知过程保持一致，从而在关键应用中培养信任和接受度。]]></description>
      <guid>https://arxiv.org/abs/2501.05471</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>2024 年 Waymo 开放数据集挑战赛 3D 语义分割赛道第二名解决方案</title>
      <link>https://arxiv.org/abs/2501.05472</link>
      <description><![CDATA[arXiv:2501.05472v1 公告类型：新
摘要：3D 语义分割是驾驶感知中最关键的任务之一。基于学习的模型能够准确感知密集的 3D 周围环境，这通常可确保自动驾驶汽车的安全运行。然而，现有的基于 LiDAR 的 3D 语义分割数据库由连续获取的 LiDAR 扫描组成，这些扫描具有长尾性且缺乏训练多样性。在本报告中，我们介绍了 MixSeg3D，它是强点云分割模型与先进的 3D 数据混合策略的复杂组合。具体而言，我们的方法将 MinkUNet 系列与 LaserMix 和 PolarMix 相结合，这两种场景级数据增强方法沿自我场景的倾斜度和方位角方向混合 LiDAR 点云。通过实证实验，我们证明了 MixSeg3D 相对于基线和现有技术的优越性。我们团队在 2024 年 Waymo 开放数据集挑战赛的 3D 语义分割赛道中取得了第二名。]]></description>
      <guid>https://arxiv.org/abs/2501.05472</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>点云中语义信息的隐式引导与显式表示：综述</title>
      <link>https://arxiv.org/abs/2501.05473</link>
      <description><![CDATA[arXiv:2501.05473v1 公告类型：新
摘要：点云是一种重要的 3D 表示方法，广泛应用于自动驾驶、测量、电力、建筑和游戏等行业，并且已经对其准确性和弹性进行了严格的研究。从场景中提取语义信息可以增强人类的理解和机器的感知。通过将二维场景中的语义信息与三维点云相结合，研究人员旨在提高各种任务的精度和效率。本文全面回顾了点云中语义信息集成的各种应用和最新进展。我们探讨了点云中语义信息的双重作用，包括隐式指导和显式表示，涵盖传统和新兴任务。此外，我们还对针对特定任务的公开数据集进行了比较分析，并提出了值得注意的观察结果。最后，我们讨论了充分利用点云中的语义信息时可能在未来出现的几个挑战和潜在问题，并提供了我们对这些障碍的看法。对基于语义的点云任务相关的文章进行了分类整理，并持续跟进不同领域的相关成果，可通过https://github.com/Jasmine-tjy/Semantic-based-Point-Cloud-Tasks进行访问。]]></description>
      <guid>https://arxiv.org/abs/2501.05473</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过全局-本地协作传播实现无需调整的长视频生成</title>
      <link>https://arxiv.org/abs/2501.05484</link>
      <description><![CDATA[arXiv:2501.05484v1 公告类型：新
摘要：制作高保真、连贯的长视频是一种备受追捧的愿望。虽然最近的视频扩散模型显示出了良好的潜力，但它们仍在努力解决时空不一致性和高计算资源需求的问题。我们提出了一种无需调整的长视频生成方法 GLC-Diffusion。它通过全局-局部协作去噪建立去噪轨迹来模拟长视频去噪过程，以确保整体内容一致性和帧间时间连贯性。此外，我们引入了一种噪声重新初始化策略，该策略将局部噪声改组与频率融合相结合，以提高全局内容一致性和视觉多样性。此外，我们提出了一个视频运动一致性细化 (VMCR) 模块，该模块计算像素和频率损失的梯度，以增强视觉一致性和时间平滑度。大量实验，包括对不同长度的视频（\textit{例如}，3 倍和 6 倍长）的定量和定性评估，表明我们的方法可以有效地与现有的视频扩散模型相结合，生成优于以前方法的连贯、高保真的长视频。]]></description>
      <guid>https://arxiv.org/abs/2501.05484</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>OVO-Bench：您的视频法学硕士距离现实世界的在线视频理解还有多远？</title>
      <link>https://arxiv.org/abs/2501.05510</link>
      <description><![CDATA[arXiv:2501.05510v1 公告类型：新
摘要：时间感知，即根据提出问题的时间戳进行动态推理的能力，是离线和在线视频 LLM 之间的关键区别。与依赖完整视频进行静态事后分析的离线模型不同，在线模型会逐步处理视频流并根据提出问题的时间戳动态调整其响应。尽管时间感知非常重要，但现有基准尚未对其进行充分评估。为了填补这一空白，我们提出了 OVO-Bench（Online-VideO-Benchmark），这是一种新颖的视频基准，强调了时间戳对于高级在线视频理解能力基准测试的重要性。OVO-Bench 在三种不同情况下评估视频 LLM 推理和响应特定时间戳事件的能力：（1）向后追踪：追溯过去的事件来回答问题。 (2) 实时理解：理解当前时间戳上发生的事件并做出响应。 (3) 前向主动响应：延迟响应，直到有足够的未来信息可用来准确回答问题。 OVO-Bench 包含 12 个任务，包含 644 个独特视频和大约 2,800 个人工策划的细粒度元注释，并带有精确的时间戳。我们将自动生成流程与人工策划相结合。利用这些高质量样本，我们进一步开发了评估流程，以系统地查询视频时间线上的视频 LLM。对九个视频 LLM 的评估表明，尽管在传统基准上有所进步，但当前模型在在线视频理解方面仍举步维艰，与人类代理相比存在显著差距。我们希望 OVO-Bench 能够推动视频 LLM 的进步，并激发未来在线视频推理的研究。我们的基准和代码可在 https://github.com/JoeLeelyf/OVO-Bench 上访问。]]></description>
      <guid>https://arxiv.org/abs/2501.05510</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过结合视觉对应性改进零样本对象级变化检测</title>
      <link>https://arxiv.org/abs/2501.05555</link>
      <description><![CDATA[arXiv:2501.05555v1 公告类型：新
摘要：在许多涉及视觉检查或摄像机监控的应用中，检测可能在不同视图中的两幅图像之间的对象级变化是一项核心任务。现有的变化检测方法有三个主要限制：（1）缺乏对不包含变化的图像对的评估，导致未报告的假阳性率；（2）缺乏对应关系（即，定位变化前后的区域）；（3）不同领域的零样本泛化能力较差。为了解决这些问题，我们引入了一种新方法，该方法利用变化对应关系（a）在训练期间提高变化检测准确性，（b）在测试时最大限度地减少假阳性。也就是说，我们利用对象添加或删除位置的监督标签来监督变化检测器，从而大大提高了其准确性。我们的工作也是第一个使用估计的单应性和匈牙利算法来预测检测到的变化对之间的对应关系的工作。我们的模型比现有方法表现出更优异的性能，在分布内和零样本基准中实现了变化检测和变化对应准确度方面的最先进的结果。]]></description>
      <guid>https://arxiv.org/abs/2501.05555</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>自动驾驶的视觉语言模型：基于 CLIP 的动态场景理解</title>
      <link>https://arxiv.org/abs/2501.05566</link>
      <description><![CDATA[arXiv:2501.05566v1 公告类型：新
摘要：场景理解对于提高驾驶员安全性、为自动驾驶汽车 (AV) 决策生成以人为本的解释以及利用人工智能 (AI) 进行回顾性驾驶视频分析至关重要。本研究使用对比语言-图像预训练 (CLIP) 模型开发了一个动态场景检索系统，该系统可以针对边缘设备上的实时部署进行优化。所提出的系统优于最先进的上下文学习方法，包括 GPT-4o 的零样本功能，特别是在复杂场景中。通过对本田场景数据集进行帧级分析，该数据集包含约 80 小时的带注释驾驶视频集合，捕捉各种现实世界的道路和天气条件，我们的研究强调了 CLIP 模型在从自然语言监督中学习视觉概念方面的稳健性。结果还表明，微调 CLIP 模型（例如 ViT-L/14 和 ViT-B/32）可显著改善场景分类，实现最高 F1 得分 91.1%。这些结果表明该系统能够快速准确地识别场景，可用于满足高级驾驶辅助系统 (ADAS) 的关键要求。这项研究表明 CLIP 模型有潜力为动态场景理解和分类提供可扩展且高效的框架。此外，这项工作通过加深对驾驶员行为、道路状况和安全关键场景的理解，为先进的自动驾驶汽车技术奠定了基础，标志着朝着更智能、更安全、更具情境感知的自动驾驶系统迈出了重要一步。]]></description>
      <guid>https://arxiv.org/abs/2501.05566</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>无人水面航行器上的近似监督物体距离估计</title>
      <link>https://arxiv.org/abs/2501.05567</link>
      <description><![CDATA[arXiv:2501.05567v1 公告类型：新
摘要：无人水面航行器 (USV) 和船只在海上作业中越来越重要，但由于传感器昂贵且复杂，它们的部署受到限制。激光雷达、雷达和深度相机要么价格昂贵，要么产生稀疏点云，要么噪声大，需要大量校准。在这里，我们介绍了一种使用监督物体检测在 USV 中近似距离估计的新方法。我们收集了一个数据集，其中包含带有手动注释的边界框和相应距离测量的图像。利用这些数据，我们提出了一个物体检测模型的专门分支，不仅可以检测物体，还可以预测它们与 USV 的距离。该方法提供了一种经济高效且直观的传统距离测量技术替代方案，更接近人类的估计能力。我们展示了它在海上辅助系统中的应用，该系统向操作员发出附近物体（如船只、浮标或其他水上危险）的警报。]]></description>
      <guid>https://arxiv.org/abs/2501.05567</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>HFMF：分层融合与多流模型相结合，实现 Deepfake 检测</title>
      <link>https://arxiv.org/abs/2501.05631</link>
      <description><![CDATA[arXiv:2501.05631v1 公告类型：新
摘要：深度生成模型的快速发展导致了令人难以置信的逼真的合成图像的产生，这些合成图像与现实世界的数据越来越难以区分。变分模型、扩散模型和生成对抗网络的广泛使用使得生成令人信服的假图像和视频变得更加容易，这对检测和减轻错误信息的传播提出了重大挑战。因此，开发有效的方法来检测人工智能生成的假货已成为一个紧迫的问题。在我们的研究中，我们提出了 HFMF，这是一个全面的两阶段深度伪造检测框架，它利用分层跨模态特征融合和多流特征提取来增强对最先进的生成人工智能模型生成的图像的检测性能。我们方法的第一个组成部分通过分层特征融合机制集成了视觉 Transformers 和卷积网络。我们框架的第二个组件结合了对象级信息和经过微调的卷积网络模型。然后，我们通过集成深度神经网络融合这两个组件的输出，从而实现强大的分类性能。我们证明了我们的架构在保持校准和互操作性的同时，在各种数据集基准上实现了卓越的性能。]]></description>
      <guid>https://arxiv.org/abs/2501.05631</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LPRnet：用于 LiDAR 和摄影测量点云的自监督配准网络</title>
      <link>https://arxiv.org/abs/2501.05669</link>
      <description><![CDATA[arXiv:2501.05669v1 公告类型：新 
摘要：LiDAR和摄影测量分别是用于点云获取的主动和被动遥感技术，具有互补优势和异构性。由于传感机制、空间分布和坐标系的根本差异，它们的点云在密度、精度、噪声和重叠方面表现出显著差异。再加上大规模场景缺乏地面真实值，整合异构点云是一项极具挑战性的任务。本文提出了一种基于掩蔽自动编码器的自监督配准网络，专注于异构LiDAR和摄影测量点云。该方法的核心是引入一种多尺度掩蔽训练策略，在自监督下从异构点云中提取稳健的特征。为了进一步提高配准性能，设计了一个旋转平移嵌入模块来有效捕获精确刚性变换所必需的关键特征。基于 Transformer 的架构以稳健的表示为基础，无缝集成了局部和全局特征，促进了不同点云数据集之间的精确对齐。所提出的方法对 LiDAR 和摄影测量点云都具有强大的特征提取能力，解决了在场景级别获取地面真实值的挑战。在两个真实数据集上进行的实验验证了所提出方法在解决异构点云配准问题方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2501.05669</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于跨模态检索的深度可逆一致性学习</title>
      <link>https://arxiv.org/abs/2501.05686</link>
      <description><![CDATA[arXiv:2501.05686v1 公告类型：新
摘要：跨模态检索（CMR）通常涉及学习共同表示以直接测量多模态样本之间的相似性。大多数现有的 CMR 方法通常假设多模态样本成对并采用联合训练来学习共同表示，从而限制了 CMR 的灵活性。尽管一些方法对每种模态采用独立的训练策略来提高 CMR 的灵活性，但它们利用随机初始化的正交矩阵来指导表示学习，这并不是最优的，因为它们假设类间样本彼此独立，从而限制了样本表示和真实标签之间语义对齐的潜力。为了解决这些问题，我们提出了一种用于跨模态检索的新方法，称为深度可逆一致性学习（DRCL）。DRCL 包括两个核心模块，即选择性先验学习（SPL）和可逆语义一致性学习（RSC）。更具体地说，SPL 首先学习每种模态的变换权重矩阵，并根据质量得分选择最佳矩阵作为先验，这极大地避免了盲目选择从低质量模态中学习到的先验。然后，RSC 采用模态不变表征重塑机制 (MRR)，通过先验的广义逆矩阵从样本语义标签中重塑潜在的模态不变表征。由于标签缺乏模态特定信息，我们利用重塑特征来指导表征学习，从而最大程度地保持语义一致性。此外，RSC 中引入了一种特征增强机制 (FA)，以鼓励模型在更广泛的数据分布中学习以实现多样性。最后，在五个广泛使用的数据集上进行的大量实验以及与 15 个最新基线的比较证明了我们的 DRCL 的有效性和优越性。]]></description>
      <guid>https://arxiv.org/abs/2501.05686</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>UniQ：具有任务特定查询的统一解码器，用于高效场景图生成</title>
      <link>https://arxiv.org/abs/2501.05687</link>
      <description><![CDATA[arXiv:2501.05687v1 公告类型：新 
摘要：场景图生成（SGG）是一种场景理解任务，旨在识别对象实体并推理给定图像中它们的关系。与基于大型物体检测器（例如 Faster R-CNN）的现行两阶段方法相比，单阶段方法集成了一组固定大小的可学习查询来联合推理关系三元组。该范例表现出稳健的性能，同时显着降低了参数和计算开销。然而，单阶段方法的挑战源于弱纠缠问题，其中涉及关系的实体既需要三元组内共享的耦合特征，也需要解耦的视觉特征。以前的方法要么采用单个解码器进行耦合三元组特征建模，要么采用多个解码器进行单独的视觉特征提取，但未能同时考虑两者。在本文中，我们介绍了 UniQ，这是一种具有任务特定查询架构的统一解码器，其中任务特定查询分别为主语、宾语和谓词生成解耦的视觉特征，统一解码器支持关系三元组中的耦合特征建模。在 Visual Genome 数据集上的实验结果表明，UniQ 的性能优于单阶段和双阶段方法。]]></description>
      <guid>https://arxiv.org/abs/2501.05687</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>eKalibr：基于事件第一原理的事件摄像机动态固有校准</title>
      <link>https://arxiv.org/abs/2501.05688</link>
      <description><![CDATA[arXiv:2501.05688v1 公告类型：新
摘要：近年来，生物启发事件相机因其高动态范围和低延迟特性而具有巨大潜力，引起了广泛的研究关注。与标准相机类似，事件相机需要精确的内在校准，以促进进一步的高级视觉应用，例如姿势估计和映射。虽然已经提出了几种事件相机的校准方法，但大多数方法要么是 (i) 工程驱动的，严重依赖于传统的基于图像的校准管道，要么是 (ii) 不方便的，需要复杂的仪器。为此，我们提出了一种准确、方便的事件相机内在校准方法，名为 eKalibr，它建立在精心设计的基于事件的圆网格模式识别算法之上。为了从事件中提取目标模式，我们执行基于事件的法向流估计以识别由圆边缘生成的潜在事件，并对其进行空间聚类。随后，使用法向流匹配和分组与相同网格圆相关的事件簇，以便随后进行时变椭圆估计。拟合的椭圆中心是时间同步的，用于最终的网格模式识别。我们进行了广泛的实验来评估 eKalibr 在模式提取和内在校准方面的性能。eKalibr 的实现在 (https://github.com/Unsigned-Long/eKalibr) 上开源，以造福研究界。]]></description>
      <guid>https://arxiv.org/abs/2501.05688</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于知识蒸馏的视觉问答语言先验问题解决</title>
      <link>https://arxiv.org/abs/2501.05690</link>
      <description><![CDATA[arXiv:2501.05690v1 公告类型：新
摘要：先前的研究指出，视觉问答 (VQA) 模型倾向于依赖语言先验来预测答案。在这种情况下，预测通常依赖于语言捷径，而不是对多模态知识的全面掌握，这削弱了它们的泛化能力。在本文中，我们提出了一种新方法，即 KDAR，利用知识蒸馏来解决 VQA 任务中的先验依赖困境。具体来说，训练有素的老师的软标签促进的正则化效应被用来惩罚对最常见答案的过度拟合。起到正则化作用的软标签还提供语义指导，缩小候选答案的范围。此外，我们设计了一种自适应的样本重加权学习策略，通过动态调整每个样本的重要性来进一步减轻偏差。实验结果表明，我们的方法在 OOD 和 IID 设置中都提高了性能。我们的方法在 VQA-CPv2 分布外 (OOD) 基准上实现了最先进的性能，显著优于以前最先进的方法。]]></description>
      <guid>https://arxiv.org/abs/2501.05690</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>EmotiCrafter：基于效价-唤醒模型的文本到情感图像生成</title>
      <link>https://arxiv.org/abs/2501.05710</link>
      <description><![CDATA[arXiv:2501.05710v1 公告类型：新
摘要：最近的研究表明，情绪可以增强用户的认知并影响信息交流。虽然对视觉情绪分析的研究很广泛，但在帮助用户生成情感丰富的图像内容方面所做的工作有限。现有的情感图像生成工作依赖于离散的情绪类别，因此很难准确捕捉复杂而微妙的情绪细微差别。此外，这些方法难以控制基于文本提示的生成图像的具体内容。在这项工作中，我们引入了连续情绪图像内容生成（C-EICG）的新任务，并提出了 EmotiCrafter，这是一种基于文本提示和 Valence-Arousal 值生成图像的情绪图像生成模型。具体而言，我们提出了一种新颖的情绪嵌入映射网络，将 Valence-Arousal 值嵌入文本特征中，从而能够根据预期的输入提示捕捉特定情绪。此外，我们引入了一个损失函数来增强情感表达。实验结果表明，我们的方法有效地生成具有所需内容的代表特定情绪的图像，并且优于现有技术。]]></description>
      <guid>https://arxiv.org/abs/2501.05710</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>