<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Fri, 01 Nov 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>VL-Cache：用于视觉语言模型推理加速的稀疏性和模态感知 KV 缓存压缩</title>
      <link>https://arxiv.org/abs/2410.23317</link>
      <description><![CDATA[arXiv:2410.23317v1 公告类型：新
摘要：视觉语言模型 (VLM) 在一系列多功能任务中表现出色。加速 VLM 的一个关键挑战是存储和访问对长视觉上下文（例如图像或视频）进行编码的大型键值 (KV) 缓存。虽然现有的 KV 缓存压缩方法对大型语言模型 (LLM) 有效，但直接将它们迁移到 VLM 会导致准确性和加速不理想。为了弥补这一差距，我们提出了 VL-Cache，这是一种专为加速 VLM 推理而量身定制的新型 KV 缓存压缩方法。在本文中，我们首先通过区分预填充和解码阶段的视觉和文本标记来研究 VLM 注意力的独特稀疏模式。基于这些观察，我们引入了一种层自适应稀疏感知缓存预算分配方法，该方法有效地将有限的缓存预算分配到不同的层上，从而在不影响准确性的情况下进一步减少 KV 缓存大小。此外，我们开发了一种模态感知 token 评分策略，以更好地评估 token 重要性。在多个基准数据集上的经验结果表明，仅保留 10% 的 KV 缓存即可实现与全缓存相当的准确率。在速度基准测试中，我们的方法将生成 100 个 token 的端到端延迟提高了 2.33 倍，将解码速度提高了 7.08 倍，同时将 GPU 中 KV 缓存的内存占用减少了 90%。]]></description>
      <guid>https://arxiv.org/abs/2410.23317</guid>
      <pubDate>Fri, 01 Nov 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CLIPErase：CLIP 中视觉文本关联的有效忘却</title>
      <link>https://arxiv.org/abs/2410.23330</link>
      <description><![CDATA[arXiv:2410.23330v1 公告类型：新
摘要：机器反学习 (MU) 作为一种无需完全重新训练过程即可从训练模型中删除特定数据的方法，已引起广泛关注。虽然在文本和图像分类等单模态领域取得了进展，但多模态模型中的反学习仍然相对未被充分探索。在这项工作中，我们解决了 CLIP 中反学习的独特挑战，CLIP 是一种将视觉和文本表示对齐的著名多模态模型。我们引入了 CLIPErase，这是一种新颖的方法，可以解开并有选择地忘记视觉和文本关联，确保反学习不会影响模型性能。CLIPErase 由三个关键模块组成：一个破坏遗忘集中关联的遗忘模块、一个保留保留集上的性能的保留模块和一个与原始模型保持一致的一致性模块。在四个 CLIP 下游任务中对 CIFAR-100 和 Flickr30K 数据集进行的大量实验表明，CLIPErase 可以有效地忘记多模态样本的零样本任务中的指定关联，同时在取消学习后保留模型在保留集上的性能。]]></description>
      <guid>https://arxiv.org/abs/2410.23330</guid>
      <pubDate>Fri, 01 Nov 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MoLE：通过混合低阶专家增强以人为中心的文本到图像的传播</title>
      <link>https://arxiv.org/abs/2410.23332</link>
      <description><![CDATA[arXiv:2410.23332v1 公告类型：新
摘要：文本到图像的扩散因其出色的图像生成能力而引起了广泛关注。然而，当涉及到以人为中心的文本到图像生成时，特别是在面部和手部的背景下，由于训练先验不足，结果往往不够自然。我们从两个角度缓解了这项工作中的问题。1）从数据方面，我们仔细收集了一个以人为中心的数据集，其中包括超过一百万张高质量的场景中的人像图像和两组特定的面部和手部特写图像。这些数据集共同提供了丰富的先验知识库，以增强扩散模型以人为中心的图像生成能力。2）在方法论方面，我们提出了一种简单而有效的方法，称为低秩专家混合（MoLE），将分别在特写手部和面部图像上训练的低秩模块视为专家。这一概念的灵感来自于我们对低秩细化的观察，其中由定制的特写数据集训练的低秩模块在以适当的比例应用时有可能增强相应的图像部分。为了验证 MoLE 在以人为本的图像生成方面相对于最先进技术的优势，我们构建了两个基准并使用不同的指标和人工研究进行评估。数据集、模型和代码发布于 https://sites.google.com/view/mole4diffuser/。]]></description>
      <guid>https://arxiv.org/abs/2410.23332</guid>
      <pubDate>Fri, 01 Nov 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用线性判别分析和卷积神经网络的域分解图像分类算法</title>
      <link>https://arxiv.org/abs/2410.23359</link>
      <description><![CDATA[arXiv:2410.23359v1 公告类型：新
摘要：在许多现代计算机应用问题中，图像数据的分类起着重要作用。在许多不同的监督机器学习模型中，卷积神经网络 (CNN) 和线性判别分析 (LDA) 及其复杂的变体是流行的技术。在这项工作中，针对不同的图像分类问题对两种不同的域分解 CNN 模型进行了实验比较。这两种模型都受到域分解方法的启发，此外还结合了迁移学习策略。与相应的、没有迁移学习的组合全局 CNN 模型相比，所得到的模型显示出更好的分类准确率，此外，还有助于加快训练过程。此外，提出了一种新的分解 LDA 策略，该策略也依赖于定位方法并与小型神经网络模型相结合。与应用于整个输入数据的全局 LDA 相比，所提出的分解 LDA 方法对所考虑的测试问题显示出更高的分类准确率。]]></description>
      <guid>https://arxiv.org/abs/2410.23359</guid>
      <pubDate>Fri, 01 Nov 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>遥感领域的多语言视觉语言预训练</title>
      <link>https://arxiv.org/abs/2410.23370</link>
      <description><![CDATA[arXiv:2410.23370v1 公告类型：新
摘要：基于对比语言-图像预训练 (CLIP) 的方法如今被广泛用于支持涉及遥感数据的视觉和语言任务，例如跨模态检索。CLIP 对这一特定领域的适应依赖于使用标准对比目标进行模型微调，使用现有的人工标记的图像标题数据集，或使用对应于从遥感图像上的其他注释（例如对象类）得出的图像标题对的合成数据。不同预训练机制的使用受到的关注较少，只有少数例外考虑了多语言输入。这项工作为遥感领域提出了一种新颖的视觉和语言模型，探索了多语言 CLIP 模型的微调，并测试了基于对齐单个输入图像的局部和全局表示的自监督方法以及标准 CLIP 目标的使用。模型训练依赖于组装预先存在的遥感图像数据集和英文字幕，然后使用自动机器翻译将其翻译成另外九种语言。我们表明翻译数据确实很有帮助，例如，在英语方面也能提高性能。我们最终的模型被命名为遥感多语言 CLIP (RS-M-CLIP)，它在各种视觉和语言任务中获得了最先进的结果，包括跨模态和多语言图像文本检索，或零样本图像分类。]]></description>
      <guid>https://arxiv.org/abs/2410.23370</guid>
      <pubDate>Fri, 01 Nov 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>TPP-Gaze：利用神经时间点过程对空间和时间中的凝视动态进行建模</title>
      <link>https://arxiv.org/abs/2410.23409</link>
      <description><![CDATA[arXiv:2410.23409v1 公告类型：新
摘要：注意力引导我们的目光注视场景的正确位置，并根据当前的处理需求在该位置停留应有的时间，然后再转移到下一个位置。因此，凝视部署至关重要是一个时间过程。现有的计算模型在预测观察者视觉扫描路径的空间方面（看哪里）取得了重大进展，同时经常将注意力动态的时间方面（何时）置于背景中。在本文中，我们提出了 TPP-Gaze，这是一种基于神经时间点过程 (TPP) 的扫描路径动力学模型的新颖且有原则的方法，它联合学习注视位置和持续时间的时间动态，将深度学习方法与点过程理论相结合。我们在五个公开可用的数据集上进行了广泛的实验。我们的结果表明，与最先进的方法相比，所提出的模型具有整体优越的性能。源代码和训练好的模型可在以下网址公开获取：https://github.com/phuselab/tppgaze。]]></description>
      <guid>https://arxiv.org/abs/2410.23409</guid>
      <pubDate>Fri, 01 Nov 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>EchoFM：通用超声心动图分析的基础模型</title>
      <link>https://arxiv.org/abs/2410.23413</link>
      <description><![CDATA[arXiv:2410.23413v1 公告类型：新
摘要：基础模型最近因其在多个任务和数据分布中的通用性和适应性而备受关注。尽管医学基础模型已经出现，但心脏成像（尤其是超声心动图视频）的解决方案仍未被探索。在本文中，我们介绍了 EchoFM，这是一个专门用于表示和分析超声心动图视频的基础模型。在 EchoFM 中，我们提出了一个自监督学习框架，该框架通过时空一致的掩蔽策略和周期驱动的对比学习来捕获空间和时间变异模式。该框架可以有效地捕捉超声心动图的时空动态，并在没有任何标签的情况下学习代表性视频特征。我们在一个广泛的数据集上对我们的模型进行了预训练，该数据集包含超过 290,000 个超声心动图视频，涵盖不同成像模式下的 26 个扫描视图，最多有 2000 万帧图像。经过预训练的 EchoFM 可轻松适应和微调以适应各种下游任务，作为强大的骨干模型。我们的评估是针对超声心动图检查程序后的四个下游任务系统设计的。实验结果表明，EchoFM 在所有下游任务中都超越了最先进的方法，包括专门的超声心动图方法、自监督预训练模型和通用预训练基础模型。]]></description>
      <guid>https://arxiv.org/abs/2410.23413</guid>
      <pubDate>Fri, 01 Nov 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>因果关系驱动的模型稳健性审计</title>
      <link>https://arxiv.org/abs/2410.23494</link>
      <description><![CDATA[arXiv:2410.23494v1 公告类型：新
摘要：深度神经网络 (DNN) 的稳健性审计提供了一种方法来揭示模型对具有挑战性的现实世界成像条件的敏感性，这些条件会显著降低 DNN 在野外的性能。此类条件通常是环境、传感器或处理管道固有的多种因素综合作用的结果，可能会导致难以分类的复杂图像失真。当稳健性审计仅限于一组预先确定的成像效果或失真时，结果无法（轻易）转移到现实世界的条件下，其中图像损坏可能更复杂或更微妙。为了应对这一挑战，我们提出了一种新的替代稳健性审计方法，该方法使用因果推理来测量 DNN 对导致复杂失真的成像过程因素的敏感性。我们的方法使用因果模型来明确编码关于领域相关因素及其相互作用的假设。然后，通过对多个视觉任务中的自然图像和渲染图像进行大量实验，我们表明我们的方法能够使用观测域数据可靠地估计每个因素对 DNN 性能的因果影响。这些因果影响直接将 DNN 敏感性与感兴趣域中成像管道的可观测属性联系起来，从而降低在该域中部署时出现意外 DNN 故障的风险。]]></description>
      <guid>https://arxiv.org/abs/2410.23494</guid>
      <pubDate>Fri, 01 Nov 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LBurst：基于学习的机器人突发特征提取，用于低光环境下的 3D 重建</title>
      <link>https://arxiv.org/abs/2410.23522</link>
      <description><![CDATA[arXiv:2410.23522v1 公告类型：新
摘要：无人机彻底改变了航空成像、测绘和灾难恢复领域。然而，无人机在低光照条件下的部署受到其机载摄像头产生的图像质量的限制。在本文中，我们提出了一种学习架构，通过查找突发特征来改善低光照条件下的 3D 重建。我们的方法通过检测和描述低信噪比图像中的高质量真实特征和较少的杂散特征来增强视觉重建。我们证明我们的方法能够处理毫勒克斯照明下的具有挑战性的场景，这使其成为无人机在夜间和极低光照应用（如地下采矿和搜索和救援行动）中运行的重要一步。]]></description>
      <guid>https://arxiv.org/abs/2410.23522</guid>
      <pubDate>Fri, 01 Nov 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>来来回回：扩散模型中噪声、图像及其反演之间的关系</title>
      <link>https://arxiv.org/abs/2410.23530</link>
      <description><![CDATA[arXiv:2410.23530v1 公告类型：新 
摘要：去噪扩散概率模型 (DDPM) 在从随机噪声合成新图像方面实现了最先进的性能，但它们缺乏将数据编码为特征的有意义的潜在空间。最近基于 DDPM 的编辑技术试图通过将图像反转回其近似的凝视噪声来缓解此问题。在这项工作中，我们研究了初始高斯噪声、从中生成的样本以及通过反转过程获得的相应潜在编码之间的关系。首先，我们通过将潜在表示流形定位在初始噪声和生成的样本之间来解释它们的空间距离关系以显示 DDIM 反转技术的不准确性。然后，我们展示了扩散训练过程中初始高斯噪声与其对应代之间的特殊关系，表明生成图像的高级特征迅速稳定，在整个训练过程中保持噪声和代之间的空间距离关系一致。]]></description>
      <guid>https://arxiv.org/abs/2410.23530</guid>
      <pubDate>Fri, 01 Nov 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>语言引导的分层细粒度图像伪造检测与定位</title>
      <link>https://arxiv.org/abs/2410.23556</link>
      <description><![CDATA[arXiv:2410.23556v1 公告类型：新 
摘要：CNN 合成和图像编辑领域生成的图像的伪造属性差异很大，这种差异使得统一的图像伪造检测和定位 (IFDL) 具有挑战性。为此，我们提出了一种用于 IFDL 表示学习的分层细粒度公式。具体来说，我们首先用不同级别的多个标签表示被操纵图像的伪造属性。然后，我们使用它们之间的层次依赖关系在这些级别执行细粒度分类。因此，鼓励算法学习综合特征和不同伪造属性固有的层次结构。在这项工作中，我们提出了一种语言引导的分层细粒度 IFDL，表示为 HiFi-Net++。具体来说，HiFi-Net++ 包含四个组件：多分支特征提取器、语言引导的伪造定位增强器以及分类和定位模块。多分支特征提取器的每个分支都学习在一个级别上对伪造属性进行分类，而定位和分类模块分别分割像素级伪造区域并检测图像级伪造。此外，语言引导的伪造定位增强器 (LFLE) 包含通过对比语言图像预训练 (CLIP) 学习的图像和文本编码器，用于进一步丰富 IFDL 表示。LFLE 将专门设计的文本和给定的图像作为多模态输入，然后生成视觉嵌入和操作分数图，用于进一步提高 HiFi-Net++ 操作定位性能。最后，我们构建了一个分层细粒度数据集以方便我们的研究。我们通过使用不同的基准来证明我们的方法在 IFDL 和伪造属性分类任务上对 $8$ 的有效性。我们的源代码和数据集可用。]]></description>
      <guid>https://arxiv.org/abs/2410.23556</guid>
      <pubDate>Fri, 01 Nov 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>短语解耦跨模态分层匹配和视觉基础的渐进位置校正</title>
      <link>https://arxiv.org/abs/2410.23570</link>
      <description><![CDATA[arXiv:2410.23570v1 Announce Type: new 
摘要：视觉接地由于在各类视觉语言任务中的广泛应用而受到广泛关注。尽管视觉接地研究取得了重大进展，但现有方法忽略了不同层级的文本与图像特征之间的关联对跨模态匹配的促进作用。本文提出一种短语解耦的跨模态层级匹配与渐进位置修正的视觉接地方法。首先通过解耦后的句子短语生成mask，构建文本与图像层级匹配机制，突出不同层级之间的关联在跨模态匹配中的作用。此外，基于层级匹配机制定义相应的目标物体位置渐进修正策略，实现对文本中描述的目标物体的精确定位。该方法可以随着目标物体文本描述确定性的提高而不断优化调整目标物体的边界框位置。本设计探索了不同层次特征之间的关联性，突出了与目标物体及其位置相关的特征在目标定位中的作用。通过实验在不同数据集上验证了所提方法，并通过与最新方法的性能比较验证了其优越性。]]></description>
      <guid>https://arxiv.org/abs/2410.23570</guid>
      <pubDate>Fri, 01 Nov 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用结构相似性和 Kolmogorov-Arnold 网络对三铰链脑回进行解剖嵌入</title>
      <link>https://arxiv.org/abs/2410.23598</link>
      <description><![CDATA[arXiv:2410.23598v1 公告类型：新
摘要：三铰链脑回（3HG）是一种新定义的折叠模式，是皮质折叠中来自三个方向的脑回的结合。许多研究表明，3HG 可以作为构建大脑网络或连接组的可靠节点，因为它们同时具有不同个体大脑和群体之间的共性和个性。然而，3HG 是在个体空间内识别和验证的，由于缺乏跨主体的对应性，因此很难直接用作大脑网络节点。3HG 对应关系代表了大脑组织结构的内在规律，传统的基于图像的配准方法往往会失败，因为需要充分尊重个体的解剖特性。为了应对这一挑战，我们提出了一种新颖的自监督框架，用于对 3HG 进行解剖特征嵌入，以建立不同大脑之间的对应关系。该框架的核心组件是基于最近开发的用于解剖特征嵌入的 Kolmogorov-Arnold 网络 (KAN) 构建结构相似性增强的多跳特征编码策略。大量实验表明，当不存在一一映射时，我们的方法可以有效地建立稳健的跨主题对应关系。]]></description>
      <guid>https://arxiv.org/abs/2410.23598</guid>
      <pubDate>Fri, 01 Nov 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用多模态深度神经网络将语言与视觉美学区分开来</title>
      <link>https://arxiv.org/abs/2410.23603</link>
      <description><![CDATA[arXiv:2410.23603v1 公告类型：新
摘要：当我们体验到视觉刺激是美丽的时，这种体验有多少来自我们无法描述的感知计算，有多少来自我们可以轻松转化为自然语言的概念知识？通过行为范式或神经成像将视觉引起的情感和审美体验中的感知与语言区分开来通常在经验上是难以解决的。在这里，我们通过对单峰视觉、单峰语言和多峰（语言对齐）深度神经网络 (DNN) 模型的学习表示进行线性解码来预测自然图像的人类美感评级，从而绕过这一挑战。我们表明，单峰视觉模型（例如 SimCLR）解释了这些评级中绝大多数可解释的方差。语言对齐视觉模型（例如 SLIP）相对于单峰视觉的收益较小。以视觉嵌入为条件生成字幕（通过 CLIPCap）的单模态语言模型（例如 GPT2）没有进一步的进步。单独的字幕嵌入产生的预测准确度低于图像和字幕嵌入的组合（连接）。总之，这些结果表明，无论我们最终找到什么词来描述我们对美的体验，前馈感知的不可言喻的计算可能为这种体验提供足够的基础。]]></description>
      <guid>https://arxiv.org/abs/2410.23603</guid>
      <pubDate>Fri, 01 Nov 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>增强视觉转换器的上下文感知标记选择和打包</title>
      <link>https://arxiv.org/abs/2410.23608</link>
      <description><![CDATA[arXiv:2410.23608v1 公告类型：新
摘要：近年来，视觉转换器的长程注意力机制推动了各种计算机视觉任务的重大性能突破。然而，传统的自注意力机制同时处理信息性和非信息性标记，存在效率低下和不准确的问题。虽然已经引入了稀疏注意力机制来通过修剪涉及注意力的标记来缓解这些问题，但它们往往缺乏上下文感知和智能。这些机制经常在批量训练的不同输入中应用统一的标记选择策略，或者只为推理阶段优化效率。为了克服这些挑战，我们提出了一种新算法：选择和打包注意力（SPA）。SPA 使用由选择标签监督的低成本门控层动态选择信息性标记，并将这些标记打包到新批次中，从而可以在并行化的 GPU 批量训练和推理中使用可变数量的标记。在不同数据集和计算机视觉任务上进行的大量实验表明，SPA 具有卓越的性能和效率，包括对象检测中 0.6 mAP 的提高和计算成本的降低 16.4%。]]></description>
      <guid>https://arxiv.org/abs/2410.23608</guid>
      <pubDate>Fri, 01 Nov 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>