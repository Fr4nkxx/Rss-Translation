<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>CS.CV更新arxiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.cv更新arxiv.org e-print存档。</description>
    <lastBuildDate>Tue, 11 Mar 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>CASP：基于注意力稀疏性的大型多模型压缩</title>
      <link>https://arxiv.org/abs/2503.05936</link>
      <description><![CDATA[ARXIV：2503.05936V1公告类型：新 
摘要：在这项工作中，我们为大型多模型（LMM）提出了一种极端的压缩技术。尽管先前的研究已探索了量化作为大型语言模型（LLMS）的有效训练后压缩方法，但多模型模型的低位压缩仍未得到探索。多模式模型中输入的冗余性质导致高度稀疏的注意矩阵。我们从理论上和实验上证明了注意力矩阵的稀疏性限制了查询和钥匙重量矩阵的压缩误差。基于此，我们介绍了CASP，这是一种用于LMM的模型压缩技术。我们的方法在查询和关键权重矩阵上执行数据感知的低级分解，然后根据最佳位分配过程对所有层进行量化。 CASP与任何量化技术兼容，并增强了最先进的2位量化方法（AQLM和QUIP＃）的图像和视频语言基准平均为21％。]]></description>
      <guid>https://arxiv.org/abs/2503.05936</guid>
      <pubDate>Tue, 11 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>贝叶斯田地：任务驱动的开放式语义高斯碎片</title>
      <link>https://arxiv.org/abs/2503.05949</link>
      <description><![CDATA[ARXIV：2503.05949V1公告类型：新 
摘要：开放式的语义映射需要（i）确定正确的粒度以表示场景（例如，如何定义对象），以及（ii）在多个2D观察中融合了多个2D观察的语义知识，将其与高实际的3D重建 - 与高实际性且低模拟足迹融合在一起。尽管大多数相关的作品通过将原始素组合在一起的类似语义（根据一些手动调谐阈值）将第一个问题绕过第一个问题，但我们认识到对象粒度是任务依赖性的，并开发了任务驱动的语义映射方法。为了解决第二个问题，当前的做法是通过多个视图的平均视觉嵌入向量。取而代之的是，我们根据基础视觉语言基础模型的属性展示了使用概率方法的好处，并利用贝叶斯更新来汇总场景的多个观察结果。结果是贝叶斯字段，这是一种由任务驱动的和概率的开放式语义映射方法。为了启用高保真对象和密集的场景表示，贝叶斯字段使用了3D高斯人，我们将它们聚集到与任务相关的对象中，从而可以轻松的3D对象提取和减少内存使用。我们在https：//github.com/mit-spark/bayesian-fields上发布贝叶斯田地。]]></description>
      <guid>https://arxiv.org/abs/2503.05949</guid>
      <pubDate>Tue, 11 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>您的视频语言模型是可靠的法官吗？</title>
      <link>https://arxiv.org/abs/2503.05977</link>
      <description><![CDATA[ARXIV：2503.05977V1公告类型：新 
摘要：随着视频语言模型（VLMS）在各种情况下获得更多应用程序，对其性能的强大和可扩展评估的需求变得越来越关键。基于人类专家的VLM的传统评估具有一致性和可伸缩性的局限性，这引起了对自动方法的兴趣，例如使用VLM来评估VLM。但是，VLMS作为法官的可靠性仍未得到充实。现有方法通常依靠单个VLM作为评估者。但是，这种方法可能是不可靠的或有偏见的，因为这种模型可能缺乏完全理解内容的能力，并且可能具有固有的偏见，最终损害了评估可靠性。一种补救措施是应用集体思想的原则，汇总了来自多个VLM的评估以提高可靠性。这项研究调查了这种方法的功效，尤其是当法官库既包含可靠和不可靠的模型时）。我们的发现表明，从这种混合池中纳入集体判断并不一定会提高最终评估的准确性。包含较不可靠的法官会引入噪音，从而破坏结果的总体可靠性。为了探索影响评估可靠性的因素，我们对表现不佳的VLM法官，视频闭合，并观察到仅提高理解能力的能力不足以使VLM法官更加可靠。这些发现强调了集体思想方法的局限性，并强调了对可以解释各个模型可靠性的更高级方法的必要性。我们的研究促进了VLM的更可靠评估方法的发展]]></description>
      <guid>https://arxiv.org/abs/2503.05977</guid>
      <pubDate>Tue, 11 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MagicInfinite：用您的言语和声音生成无限说话的视频</title>
      <link>https://arxiv.org/abs/2503.05978</link>
      <description><![CDATA[ARXIV：2503.05978V1公告类型：新 
摘要：我们提出了MagicInfinite，这是一种新颖的扩散变压器（DIT）框架，它克服了传统的肖像画限制，在各种角色类型 - 现实的人，全身人物和风格的动漫角色中提供了高保真的结果。它支持各种面部姿势，包括背面视图，并用输入掩码为单个或多个字符动画，以在多字符场景中使用精确的扬声器名称。我们的方法通过三个创新解决了关键挑战：（1）带有滑动窗口降级策略的3D全注意机制，从而使无限的视频产生具有各种角色样式的时间连贯性和视觉质量； （2）一种两阶段的课程学习方案，集成了唇同同步的音频，表达动力学的文本以及用于身份保存的参考图像，从而可以对长序列进行灵活的多模式控制； （3）具有自适应损失功能的特定区域面具，以平衡全球文本控制和本地音频指导，并支持特定于扬声器的动画。通过我们的创新统一步骤和CFG蒸馏技术提高了效率，实现了20倍推理速度的提高：在10秒内生成10秒的540x540p视频，或在8 h100 gpus的30秒内在30秒内生成720x720p，而无需质量损失。对我们的新基准的评估表明，魔术师在不同情况下的音频同步，身份保存和运动自然性方面具有优势。它可在https://www.hedra.com/上公开获取，并在https://magicinfinite.github.io/上提供示例。]]></description>
      <guid>https://arxiv.org/abs/2503.05978</guid>
      <pubDate>Tue, 11 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在视觉模型中集成频域表示和低级别适应性</title>
      <link>https://arxiv.org/abs/2503.06003</link>
      <description><![CDATA[ARXIV：2503.06003V1公告类型：新 
摘要：情境意识应用程序在很大程度上依赖于视觉和文本数据的实时处理以提供可行的见解。视觉语言模型（VLM）已成为通过将视觉输入与自然语言描述联系起来来解释复杂环境的重要工具。但是，这些模型通常会面临计算挑战，尤其是在需要在实际环境中有效执行的情况下。这项研究提出了一种新颖的视觉语言模型（VLM）框架，该框架利用频域转换和低级别适应性（LORA）来增强特征可提取，可伸缩性和效率。与仅依靠空间域表示的传统VLM不同，我们的方法包含了离散的傅立叶变换（DFT）低级别功能，同时保留了预审预测的空间重量，从而在噪音或低的可见性场景中实现了稳健的性能。我们使用具有不同级别的高斯噪声的基准数据集评估了有关字幕生成和视觉问题回答（VQA）任务的建议模型。定量结果表明，我们的模型可以实现与最新VLM相当的评估指标，例如剪辑VIT-L/14和SIGLIP。定性分析进一步表明，我们的模型提供了更详细且相关的响应，特别是对于由安装在无人接地车上（UGV）上的真实摄像机捕获的现实世界图像。]]></description>
      <guid>https://arxiv.org/abs/2503.06003</guid>
      <pubDate>Tue, 11 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>带基于图的编码的端到端HOI重建变压器</title>
      <link>https://arxiv.org/abs/2503.06012</link>
      <description><![CDATA[ARXIV：2503.06012V1公告类型：新 
摘要：随着人类对象相互作用（HOI）应用的多样化以及捕获人网格的成功，HOI重建引起了广泛的关注。现有的主流HOI重建方法通常依赖于显式建模人与对象之间的相互作用。但是，这种方式导致了强调全球结构的3D网格重建和细粒度的接触重建之间的自然冲突，该重建的重点是本地细节。为了解决显式建模的局限性，我们提出了使用基于图的编码（HOI-TG）的端到端HOI重建变压器。它隐含地通过利用自我发育机制来了解人与物体之间的相互作用。在变压器体系结构中，我们设计了图形残差块，以在不同空间结构的顶点之间汇总拓扑。这种双重重点有效地平衡了全球和地方代表。没有铃铛和口哨，Hoi-TG可以表现出最先进的性能和Intercap数据集。尤其是在具有挑战性的Intercap数据集上，我们的方法将人类和对象网格的重建结果分别提高了8.9％和8.6％。]]></description>
      <guid>https://arxiv.org/abs/2503.06012</guid>
      <pubDate>Tue, 11 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>迈向无歧义的空间基础模型：重新思考和脱钩深度歧义</title>
      <link>https://arxiv.org/abs/2503.06014</link>
      <description><![CDATA[ARXIV：2503.06014V1公告类型：新 
摘要：深度歧义是空间场景理解中的一个基本挑战，尤其是在单深度估计未能捕获完整3D结构的透明场景中。现有模型，仅限于确定性预测，忽略了现实世界中的多层深度。为了解决这个问题，我们介绍了从单个预测到多种假设的空间基础模型的范式转变。我们首先提出\ texttt {MD-3K}，这是一种基准测试，通过多层空间关系标签和新指标，通过多层空间关系标签和基础模型中的深度偏见。为了解决深度歧义，我们提出了Laplacian Visual Pressing（LVP），这是一种无训练的光谱提示技术，通过Laplacian转换的RGB输入从预训练的模型中提取隐藏的深度。通过将LVP的深度与基于标准的RGB估计值集成，我们的方法引发了多层深度，而无需模型重新培训。广泛的实验验证了LVP在零拍的多层深度估计中的有效性，从而解开了更健壮和全面的几何形成视觉发电，3D接地的空间推理以及时间上一致的视频级别深度推断。我们的基准和代码将在https://github.com/xiaohao-xu/ambiguity-in-space上找到。]]></description>
      <guid>https://arxiv.org/abs/2503.06014</guid>
      <pubDate>Tue, 11 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>朝着通用文本驱动的CT图像分段</title>
      <link>https://arxiv.org/abs/2503.06030</link>
      <description><![CDATA[ARXIV：2503.06030V1公告类型：新 
摘要：计算机断层扫描（CT）广泛用于器官和病变的准确可视化和分割。尽管深度学习模型（例如卷积神经网络（CNN）和视觉变压器（VIT））显着改善了CT图像分析，但当应用于多样化的现实世界临床数据时，它们的性能通常会下降。尽管基础模型提供了更广泛，更适应性的解决方案，但由于获得大规模的，体素级的注释的挑战，其潜力受到限制。为了应对这些挑战，出现了使用视觉或文本提示的提示模型。视觉促进方法（例如任何模型（SAM））仍然需要大量的手动输入，并且在应用于临床方案时可能会引入歧义。取而代之的是，使用文本提示的基础模型提供了一种更通用和临床相关的方法。值得注意的是，当前的文本推出模型（例如夹子驱动的通用模型）仅限于在培训和努力处理现实世界临床应用程序复杂而多样的情况下已经遇到的文本提示。我们提出了OpenVocabct，而不是通过自然成像训练的微调模型，这是一种在大规模的3D CT图像上预处理的视觉模型，用于通用文本驱动的分段。使用大尺度的CT率数据集，我们使用大型语言模型将诊断报告分解为细粒度的，器官级的描述，以进行多个晶体对比度学习。我们评估了跨9个公共器官和肿瘤分段的公共数据集的下游细分任务的OpenVocabct，这证明了与现有方法相比，我们的模型的出色性能。所有代码，数据集和模型将在https://github.com/ricklisz/openvocabct上公开发布。]]></description>
      <guid>https://arxiv.org/abs/2503.06030</guid>
      <pubDate>Tue, 11 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于深学习</title>
      <link>https://arxiv.org/abs/2503.06038</link>
      <description><![CDATA[ARXIV：2503.06038V1公告类型：新 
摘要：剩余移动（RMO）为旅行时间断层扫描提供了关键信息。当前适合RMO的行业标准方法涉及扫描高阶多项式方程。但是，这种分析方法无法准确捕获局部盐分，从而导致层析成像的迭代效率较低。基于学习的基于学习的图像分割方法可有效捕获局部变化；但是，他们遇到了挑战，例如稀缺可靠的培训样本和后期处理的高复杂性。为了解决这些问题，本研究提出了一种基于深度学习的级联选择方法。它使用分割网络和基于趋势回归的后处理技术来区分准确且健壮的RMO。此外，引入了数据综合方法，从而使分割网络能够在合成数据集上进行培训，以有效地选择现场数据。此外，提出了一组指标来量化自动选择的RMO的质量。基于模型和真实数据的实验结果表明，与基于ELLBLANCE的方法相比，我们的方法可以达到更大的拾取密度和准确性。]]></description>
      <guid>https://arxiv.org/abs/2503.06038</guid>
      <pubDate>Tue, 11 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过双流适配器改进SAM以进行伪装的对象检测</title>
      <link>https://arxiv.org/abs/2503.06042</link>
      <description><![CDATA[ARXIV：2503.06042V1公告类型：新 
摘要：段的任何模型（SAM）在自然图像上显示出令人印象深刻的通用分割性能，但其在伪装对象检测（COD）上的性能不令人满意。在本文中，我们建议对RGB-D输入执行伪装的对象检测。在保持SAM体系结构完整的同时，将双流适配器扩展到图像编码器上，以从RGB图像和深度图像中学习潜在的互补信息，并微调蒙版解码器及其深度复制品以执行双流膜掩码预测。在实践中，双流适配器以平行方式嵌入图像编码器的注意力块中，以促进两种类型的图像嵌入的细化和校正。为了减轻双流嵌入不会直接相互作用的双流嵌入引起的通道差异，我们使用双向知识蒸馏（包括模型蒸馏器和模态蒸馏器）增强了双流嵌入的嵌入的关联。此外，为了预测RGB和深度注意图的掩模，我们将两种类型的图像嵌入方式融合在一起，它们与及时嵌入式共同学习以更新初始提示，然后将它们馈入掩模解码器，以同步图像嵌入的一致性并嵌入及时。四个COD基准的实验结果表明，我们的SAM-cod在SAM上实现了出色的检测性能，并通过给定的微调范式获得了最先进的结果。]]></description>
      <guid>https://arxiv.org/abs/2503.06042</guid>
      <pubDate>Tue, 11 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>螺旋桨：探索积分时空一致的视频生成的数据集和方法</title>
      <link>https://arxiv.org/abs/2503.06053</link>
      <description><![CDATA[ARXIV：2503.06053V1公告类型：新 
摘要：时空的一致性是视频生成中的关键研究主题。合格的生成的视频段必须确保情节的合理性和连贯性，同时在各种观点跨保持对象和场景的视觉一致性。先前的研究，尤其是在开源项目中，主要集中于时间或空间一致性或其基本组合，例如在提示后对摄像机移动的描述附加而不限制该运动的结果。但是，相机运动可能会将新对象引入场景或消除现有物体，从而覆盖并影响前面的叙述。尤其是在具有众多相机运动的视频中，多个图之间的相互作用变得越来越复杂。本文介绍并研究了积分时空的一致性，考虑到情节进步和摄像机技术之间的协同作用以及先前内容对后续生成的长期影响。我们的研究涵盖了数据集构建到模型的开发。最初，我们构建了一个DropletVideo-10m数据集，该数据集由动态摄像机运动和对象动作组成1000万个视频。每个视频的平均标题为206个单词，详细介绍了各种相机运动和情节发展。此后，我们开发并训练了滴液列车模型，该模型在视频生成过程中保持时空连贯性方面表现出色。可以在https://dropletx.github.io上访问DropletVideo数据集和模型。]]></description>
      <guid>https://arxiv.org/abs/2503.06053</guid>
      <pubDate>Tue, 11 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>病理学提前的多个实例学习用于减轻乳腺癌灾难性遗忘的整体幻灯片图像分类</title>
      <link>https://arxiv.org/abs/2503.06056</link>
      <description><![CDATA[ARXIV：2503.06056V1公告类型：新 
摘要：在组织病理学中，整个幻灯片图像的智能诊断（WSI）对于自动化和客观诊断至关重要，减少病理学家的工作量。但是，诊断模型通常面临忘记来自不同来源数据集的增量培训期间忘记先前学习的数据的挑战。为了解决这个问题，我们提出了一个新的框架Pagmil，以减轻乳腺癌分类中的灾难性遗忘。我们的框架将两个关键组件引入了通用MIL模型体系结构。首先，它在选择MIL的更准确和多样的代表性贴片之前，利用微观病理学。其次，它为每个任务训练单独的分类头，并使用宏观的病理先验知识，将缩略图作为及时指南（PG）选择适当的分类头。我们评估了在几个公共乳腺癌数据集中Pagmil的持续学习表现。 Pagmil在当前任务的执行与以前的任务的保留之间取得了更好的平衡，表现优于其他连续学习方法。我们的代码将在接受后开源。]]></description>
      <guid>https://arxiv.org/abs/2503.06056</guid>
      <pubDate>Tue, 11 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多模式LLM中的多层视觉特征融合：方法，分析和最佳实践</title>
      <link>https://arxiv.org/abs/2503.06063</link>
      <description><![CDATA[ARXIV：2503.06063V1公告类型：新 
摘要：近年来，多模式大型语言模型（MLLM）在增强模型性能中起着越来越重要的作用。但是，MLLMS中多层视觉特征的集成仍然没有充满反感，尤其是在最佳层选择和融合策略方面。现有方法通常依赖于任意设计选择，从而导致次优结果。在本文中，我们系统地研究了多层视觉特征融合的两个核心方面：（1）选择最有效的视觉层，以及（2）使用语言模型确定最佳的融合方法。我们的实验表明，在结合多个阶段的视觉特征的同时，可以改善概括，并结合了同一阶段的其他特征，通常会导致性能下降。此外，我们发现在输入阶段的多层视觉特征的直接融合始终在各种配置中产生卓越和更稳定的性能。我们将所有代码公开可用：https：//github.com/eit-nlp/layer_select_fuse_for_mllm。]]></description>
      <guid>https://arxiv.org/abs/2503.06063</guid>
      <pubDate>Tue, 11 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>洛拉专家的混合物，一种新颖的值得信赖的视频摘要算法</title>
      <link>https://arxiv.org/abs/2503.06064</link>
      <description><![CDATA[ARXIV：2503.06064V1公告类型：新 
摘要：随着视频共享平台上用户生成内容的指数增长，促进有效搜索和浏览视频的挑战引起了人们的极大关注。为了增强用户迅速找到和审查相关视频的能力，创建简洁而有益的视频摘要变得越来越重要。 Video-lalama是生成视频摘要的有效工具，但它无法有效地统一和优化时间和空间特征的建模，并且需要大量的计算资源和时间。因此，我们建议Milora-Visum更有效地捕获视频数据中固有的复杂时间动力学和空间关系，并控制训练的参数数量。通过将传统的低级适应（LORA）扩展到精致的专家范式中，Milora-Visum结合了专门针对视频摘要任务的双重时间空间适应机制。这种方法动态整合了专业的洛拉专家，每个专家都经过微调以解决不同的时间或空间维度。对Videoxum和ActivityNet数据集的广泛评估表明，与最新模型相比，Milora-Visum可以实现最佳的汇总性能，同时保持较低的计算成本。提出的Experts策略与双重适应机制相结合，突出了该模型增强视频摘要功能的潜力，尤其是在需要效率和精度的大规模应用中。]]></description>
      <guid>https://arxiv.org/abs/2503.06064</guid>
      <pubDate>Tue, 11 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>换台：带有软本地化的双二十个变压器框架，用于端到端自动停车</title>
      <link>https://arxiv.org/abs/2503.06071</link>
      <description><![CDATA[ARXIV：2503.06071V1公告类型：新 
摘要：近年来，完全可区分的端到端自动驾驶系统已成为智能运输领域的研究热点。在各种研究指示中，自动停车尤其重要，因为它旨在在复杂的环境中实现精确的停车场。在本文中，我们提出了一个纯粹基于视觉的变压器模型，用于使用专家轨迹训练的端到端自动停车。给定相机捕获的数据作为输入，提出的模型直接输出未来的轨迹坐标。实验结果表明，与同一类型的当前最新端到端轨迹预测算法相比，我们模型的各种误差降低了约50％。因此，我们的方法为完全可区分的自动停车提供了有效的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2503.06071</guid>
      <pubDate>Tue, 11 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>