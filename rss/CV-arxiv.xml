<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Tue, 24 Dec 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>基于机器学习的腹腔镜胃底折叠术体内缝合的自动评估</title>
      <link>https://arxiv.org/abs/2412.16195</link>
      <description><![CDATA[arXiv:2412.16195v1 公告类型：新
摘要：使用人工智能 (AI) 自动评估手术技能可为受训者提供即时反馈。捕捉双手工具运动后，得出的运动学指标是腹腔镜任务表现的可靠预测指标。实施自动工具跟踪需要耗时的人工注释。我们使用 Segment Anything 模型 (SAM) 开发了基于 AI 的工具跟踪，以消除对人工注释者的需求。在这里，我们描述了一项研究，该研究评估了我们的工具跟踪模型在胃底折叠术中腹腔镜缝合任务的自动评估中的实用性。自动工具跟踪模型被应用于猪肠尼森胃底折叠术的录制视频。外科医生被分为新手 (PGY1-2) 和专家 (PGY3-5，主治医生)。每个缝合步骤的开始和结束都被分割，并提取了左右工具的运动。截止频率为 24 Hz 的低通滤波器可消除噪音。使用监督和无监督模型评估性能，并通过消融研究比较结果。使用逻辑回归、随机森林、支持向量分类器和 XGBoost 提取和分析运动特征 - RMS 速度、RMS 加速度、RMS 急动度、总路径长度和双手灵活性。执行 PCA 以减少特征。对于无监督学习，训练了带有分类器（例如 1-D CNN 和传统模型）的去噪自动编码器 (DAE) 模型。提取了 28 名参与者（9 名新手、19 名专家）的数据。使用 PCA 和随机森林的监督学习实现了 0.795 的准确率和 0.778 的 F1 分数。无监督 1-D CNN 取得了出色的结果，准确率为 0.817，F1 得分为 0.806，无需进行运动特征计算。我们展示了一种能够自动进行性能分类的 AI 模型，无需人工注释。]]></description>
      <guid>https://arxiv.org/abs/2412.16195</guid>
      <pubDate>Tue, 24 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过三维卷积变分自动编码器实现 EELS 光谱图像中的稳健光谱异常检测</title>
      <link>https://arxiv.org/abs/2412.16200</link>
      <description><![CDATA[arXiv:2412.16200v1 公告类型：新
摘要：我们引入了一种三维卷积变分自动编码器 (3D-CVAE)，用于电子能量损失谱成像 (EELS-SI) 数据中的自动异常检测。我们的方法利用 EELS-SI 数据的完整三维结构来检测细微的光谱异常，同时保留整个数据立方体的空间和光谱相关性。通过采用负对数似然损失和对体光谱进行训练，该模型学会了重建无缺陷材料的体特征。在探索异常检测方法时，我们评估了我们的 3D-CVAE 方法和主成分分析 (PCA)，使用旨在模拟材料缺陷的 Fe L 边峰移测试它们的性能。我们的结果表明，3D-CVAE 实现了卓越的异常检测，并在各种偏移量下保持一致的性能。该方法展示了正常光谱和异常光谱之间的明显双峰分离，从而实现了可靠的分类。进一步的分析验证了低维表示对数据中的异常具有鲁棒性。虽然与 PCA 相比，性能优势会随着异常浓度的降低而减弱，但我们的方法即使在具有挑战性的噪声主导的光谱区域中也能保持较高的重建质量。这种方法为无监督自动检测 EELS-SI 数据中的光谱异常提供了一个强大的框架，对于分析复杂的材料系统特别有价值。]]></description>
      <guid>https://arxiv.org/abs/2412.16200</guid>
      <pubDate>Tue, 24 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于方面的小样本学习</title>
      <link>https://arxiv.org/abs/2412.16202</link>
      <description><![CDATA[arXiv:2412.16202v1 公告类型：新
摘要：我们通过引入方面的概念来概括小样本学习的公式。在传统的小样本学习公式中，有一个基本假设，即单个“真实”标签定义每个数据点的内容。此标签作为查询对象和支持集中的对象之间的比较的基础。但是，当要求人类专家在没有预定义标签集的情况下执行相同任务时，他们通常会将支持集中的其余数据点视为上下文。此上下文指定了抽象级别和可以进行比较的方面。在这项工作中，我们引入了一种新颖的架构和训练程序，该架构和训练程序在给定查询和支持集的情况下开发上下文，并实现不限于预定类集的基于方面的小样本学习。我们证明我们的方法能够在几何形状和精灵数据集上形成和使用方面进行小样本学习。与传统的小样本学习相比，结果验证了我们的方法的可行性。]]></description>
      <guid>https://arxiv.org/abs/2412.16202</guid>
      <pubDate>Tue, 24 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>你的世界模拟器是一个好的故事讲述者吗？基于连续事件的未来长视频生成基准</title>
      <link>https://arxiv.org/abs/2412.16211</link>
      <description><![CDATA[arXiv:2412.16211v1 公告类型：新
摘要：当前最先进的视频生成模型可以制作具有高度逼真细节的商业级视频。然而，他们仍然难以连贯地呈现提示指定的故事中的多个连续事件，这可以预见是未来长视频生成场景的必备能力。例如，顶级 T2V 生成模型仍然无法生成“如何将大象放进冰箱”这个简短故事的视频。虽然现有的细节导向基准主要关注美学质量和时空一致性等细粒度指标，但它们无法评估模型处理事件级故事呈现的能力。为了解决这一差距，我们引入了 StoryEval，这是一个以故事为导向的基准，专门用于评估文本到视频 (T2V) 模型的故事完成能力。StoryEval 有 423 个提示，涵盖 7 个类别，每个提示代表由 2-4 个连续事件组成的短篇故事。我们采用先进的视觉语言模型（例如 GPT-4V 和 LLaVA-OV-Chat-72B）来验证生成的视频中每个事件的完成情况，并采用一致投票法来提高可靠性。我们的方法确保与人工评估高度一致，对 11 个模型的评估揭示了其挑战性，没有一个模型的平均故事完成率超过 50%。StoryEval 为推进 T2V 模型提供了新的基准，并强调了开发下一代解决方案以生成连贯的故事驱动视频的挑战和机遇。]]></description>
      <guid>https://arxiv.org/abs/2412.16211</guid>
      <pubDate>Tue, 24 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ManiVideo：利用灵巧且通用的抓握技术生成手部物体操作视频</title>
      <link>https://arxiv.org/abs/2412.16212</link>
      <description><![CDATA[arXiv:2412.16212v1 公告类型：新
摘要：本文介绍了一种新方法 ManiVideo，该方法可根据给定的手和物体运动序列生成一致且时间连贯的双手手部物体操作视频。ManiVideo 的核心思想是构建多层遮挡 (MLO) 表示，从无遮挡法线图和遮挡置信度图中学习 3D 遮挡关系。通过以两种形式将 MLO 结构嵌入到 UNet 中，该模型增强了灵巧手部物体操作的 3D 一致性。为了进一步实现物体的通用抓取，我们集成了大型 3D 物体数据集 Objaverse，以解决视频数据的稀缺性，从而促进广泛物体一致性的学习。此外，我们提出了一种创新的训练策略，可以有效地整合多个数据集，支持以人为中心的手部物体操作视频生成等下游任务。通过大量实验，我们证明我们的方法不仅实现了具有合理手部-物体交互和可泛化物体的视频生成，而且还优于现有的 SOTA 方法。]]></description>
      <guid>https://arxiv.org/abs/2412.16212</guid>
      <pubDate>Tue, 24 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AdvIRL：基于强化学习的 3D NeRF 模型对抗攻击</title>
      <link>https://arxiv.org/abs/2412.16213</link>
      <description><![CDATA[arXiv:2412.16213v1 公告类型：新
摘要：人工智能模型在关键应用中的部署日益增多，使它们面临对抗性攻击的重大风险。虽然 2D 视觉模型中的对抗性漏洞已经得到广泛研究，但 3D 生成模型（如神经辐射场 (NeRF)）的威胁形势仍未得到充分探索。这项工作引入了 \textit{AdvIRL}，这是一个使用即时神经图形基元 (Instant-NGP) 和强化学习制作对抗性 NeRF 模型的新框架。与之前的方法不同，\textit{AdvIRL} 生成的对抗性噪声在各种 3D 变换（包括旋转和缩放）下仍然保持稳健，从而能够在现实场景中实施有效的黑盒攻击。我们的方法在从小物体（例如香蕉）到大环境（例如灯塔）等各种场景中都得到了验证。值得注意的是，针对性攻击实现了高置信度错误分类，例如将香蕉标记为蛞蝓，将卡车标记为大炮，这表明对抗性 NeRF 带来的实际风险。除了攻击之外，\textit{AdvIRL} 生成的对抗性模型可以作为对抗性训练数据来增强视觉系统的鲁棒性。\textit{AdvIRL} 的实现在 \url{https://github.com/Tommy-Nguyen-cpu/AdvIRL/tree/MultiView-Clean} 上公开可用，确保可重复性并促进未来的研究。]]></description>
      <guid>https://arxiv.org/abs/2412.16213</guid>
      <pubDate>Tue, 24 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 LLM 辅助文本描述和跨模态共嵌入在 Google Ads 中进行零样本图像审核</title>
      <link>https://arxiv.org/abs/2412.16215</link>
      <description><![CDATA[arXiv:2412.16215v1 公告类型：新
摘要：我们提出了一种可扩展且灵活的 Google 广告图片内容审核方法，解决了审核大量内容多样且政策不断变化的广告的挑战。所提出的方法利用人工策划的文本描述和跨模态文本-图像共嵌入来实现违反政策的广告图像的零样本分类，从而无需大量监督训练数据和人工标记。通过利用大型语言模型 (LLM) 和用户专业知识，系统生成并完善了一组代表政策指南的全面文本描述。在推理过程中，传入图像和文本描述之间的共嵌入相似性可作为政策违规检测的可靠信号，从而实现高效且适应性强的广告内容审核。评估结果表明，该框架在显著提高违反政策内容的检测方面是有效的。]]></description>
      <guid>https://arxiv.org/abs/2412.16215</guid>
      <pubDate>Tue, 24 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>自适应校准：脉冲神经网络的统一转换框架</title>
      <link>https://arxiv.org/abs/2412.16219</link>
      <description><![CDATA[arXiv:2412.16219v1 公告类型：新
摘要：脉冲神经网络 (SNN) 被视为传统人工神经网络 (ANN) 的节能替代品，但性能差距仍然是一个挑战。虽然通过 ANN 到 SNN 的转换缩小了这一差距，但仍然需要大量的计算资源，并且无法确保转换后的 SNN 的能源效率。为了解决这个问题，我们提出了一个统一的免训练转换框架，可显着提高转换后的 SNN 的性能和效率。受生物神经系统的启发，我们提出了一种新颖的自适应放电神经元模型 (AdaFire)，该模型可动态调整不同层的放电模式，以显着减少不均匀误差 - 转换后的 SNN 在有限推理时间步内的主要误差来源。我们进一步介绍了两种提高效率的技术：用于减少脉冲操作的灵敏度脉冲压缩 (SSC) 技术和用于减少延迟的输入感知自适应时间步 (IAT) 技术。这些方法共同使我们的方法实现了最先进的性能，同时在 CIFAR-10、CIFAR-100 和 ImageNet 数据集上分别实现了高达 70.1%、60.3% 和 43.1% 的显著节能。在 2D、3D、事件驱动分类任务、对象检测和分割任务中进行的大量实验证明了我们的方法在各个领域的有效性。代码可在以下位置获取：https://github.com/bic-L/burst-ann2snn。]]></description>
      <guid>https://arxiv.org/abs/2412.16219</guid>
      <pubDate>Tue, 24 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GALOT：通过可优化的零样本文本到图像生成进行生成主动学习</title>
      <link>https://arxiv.org/abs/2412.16227</link>
      <description><![CDATA[arXiv:2412.16227v1 公告类型：新
摘要：主动学习 (AL) 是机器学习中的一个重要方法，强调识别和利用最具信息量的样本进行有效的模型训练。然而，AL 的一个重大挑战是它依赖于有限的标记数据样本和数据分布，从而导致性能有限。为了解决这一限制，本文通过设计一个新颖的框架，将零样本文本到图像 (T2I) 合成和主动学习相结合，该框架可以有效地使用文本描述来训练机器学习 (ML) 模型。具体来说，我们利用 AL 标准来优化文本输入，以生成更具信息量和多样性的数据样本，这些样本由从文本制作的伪标签注释，然后作为主动学习的合成数据集。这种方法通过提供信息丰富的训练样本，降低了数据收集和注释的成本，同时提高了模型训练的效率，从而实现了从文本描述到视觉模型的新型端到端 ML 任务。通过全面的评估，我们的框架比传统的 AL 方法表现出持续且显著的改进。]]></description>
      <guid>https://arxiv.org/abs/2412.16227</guid>
      <pubDate>Tue, 24 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>TopView：利用深度学习，从未校准的街道级图像中以鸟瞰视角矢量化道路使用者</title>
      <link>https://arxiv.org/abs/2412.16229</link>
      <description><![CDATA[arXiv:2412.16229v1 公告类型：新
摘要：生成道路使用者的鸟瞰图有利于各种应用，包括导航、检测代理冲突和测量空间占用率，以及利用公制测量不同物体之间距离的能力。在这项研究中，我们介绍了一种简单的方法，用于从图像中估计鸟瞰图，而无需事先了解给定相机的内在和外在参数。该模型基于通过学习给定场景的消失点将物体从各种视野正交投影到鸟瞰图。此外，我们利用学习到的消失点和轨迹线将道路使用者的 2D 边界框转换为 3D 边界信息。引入的框架已应用于多个应用程序，以从摄像机馈送生成实时地图并分析城市规模的社交距离违规行为。引入的框架在各种未校准的摄像头中对道路使用者进行地理定位方面表现出了很高的有效性。它还为城市建模技术的新改进和准确模拟建筑环境铺平了道路，这可以通过依赖深度学习和计算机视觉使基于代理的建模受益。]]></description>
      <guid>https://arxiv.org/abs/2412.16229</guid>
      <pubDate>Tue, 24 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>可废止的视觉蕴涵：基准、评估器和奖励驱动的优化</title>
      <link>https://arxiv.org/abs/2412.16232</link>
      <description><![CDATA[arXiv:2412.16232v1 公告类型：新
摘要：我们引入了一项称为可废止视觉蕴涵 (DVE) 的新任务，其目标是允许根据额外更新修改图像前提和文本假设之间的蕴涵关系。虽然这个概念在自然语言推理中已经很成熟，但在视觉蕴涵中仍未得到探索。在高层次上，DVE 使模型能够改进其初始解释，从而提高各种应用中的准确性和可靠性，例如检测图像中的误导性信息、增强视觉问答和改进自主系统中的决策过程。现有指标不能充分捕捉更新带来的蕴涵关系的变化。为了解决这个问题，我们提出了一种新颖的推理感知评估器，旨在使用成对对比学习和分类信息学习来捕捉更新引起的蕴涵强度的变化。此外，我们引入了一种奖励驱动的更新优化方法，以进一步提高多模态模型生成的更新的质量。实验结果证明了我们提出的评估器和优化方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2412.16232</guid>
      <pubDate>Tue, 24 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过双金字塔网络进行基于 WiFi CSI 的时间活动检测</title>
      <link>https://arxiv.org/abs/2412.16233</link>
      <description><![CDATA[arXiv:2412.16233v1 公告类型：新
摘要：我们解决了基于 WiFi 的时间活动检测的挑战，并提出了一种集成时间信号语义编码器和局部敏感响应编码器的高效双金字塔网络。时间信号语义编码器将特征学习分为高频和低频分量，使用新颖的符号掩码注意机制来强调重要区域并淡化不重要的区域，并使用 ContraNorm 融合特征。局部敏感响应编码器无需学习即可捕获波动。然后使用新的交叉注意融合机制将这些特征金字塔组合起来。我们还引入了一个数据集，其中包含 553 个 WiFi CSI 样本中的 2,114 多个活动片段，每个片段持续约 85 秒。大量实验表明，我们的方法优于具有挑战性的基线。代码和数据集可在 https://github.com/AVC2-UESTC/WiFiTAD 获得。]]></description>
      <guid>https://arxiv.org/abs/2412.16233</guid>
      <pubDate>Tue, 24 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用专门的生成基元进行交互式场景创作</title>
      <link>https://arxiv.org/abs/2412.16253</link>
      <description><![CDATA[arXiv:2412.16253v1 公告类型：新
摘要：生成高质量的 3D 数字资产通常需要复杂设计工具的专业知识。我们引入了 Specialized Generative Primitives，这是一个生成框架，允许非专家用户以无缝、轻量级和可控的方式创作高质量的 3D 场景。每个原语都是一个高效的生成模型，可以捕获来自现实世界的单个样例的分布。使用我们的框架，用户可以捕获环境的视频，然后借助 3D Gaussian Splatting，我们将其转换为高质量且明确的外观模型。然后，用户在语义感知特征的指导下选择感兴趣的区域。为了创建生成原语，我们将生成细胞自动机调整为单样例训练和可控生成。我们通过对稀疏体素进行操作将生成任务与外观模型分离，并通过后续的稀疏补丁一致性步骤恢复高质量输出。每个基元可以在 10 分钟内完成训练，并可用于以完全合成的方式以交互方式创作新场景。我们展示了交互式课程，其中从现实世界场景中提取各种基元并进行控制，以在几分钟内创建 3D 资源和场景。我们还展示了基元的其他功能：处理各种 3D 表示以控制生成、传输外观和编辑几何图形。]]></description>
      <guid>https://arxiv.org/abs/2412.16253</guid>
      <pubDate>Tue, 24 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PromptLA：面向黑盒文本到图像扩散模型的完整性验证</title>
      <link>https://arxiv.org/abs/2412.16257</link>
      <description><![CDATA[arXiv:2412.16257v1 公告类型：新
摘要：当前的文本到图像 (T2I) 扩散模型可以生成高质量的图像，而仅被授权将该模型用于良性目的的恶意用户可能会修改其模型以生成导致有害社会影响的图像。因此，验证 T2I 扩散模型的完整性至关重要，尤其是当它们部署为黑盒服务时。为此，考虑到生成模型输出的随机性以及与它们交互的高成本，我们通过生成图像特征分布的差异来捕获对模型的修改。我们提出了一种基于学习自动机的新型提示选择算法，用于高效准确地验证 T2I 扩散模型的完整性。与基线相比，大量实验证明了我们的算法对现有完整性违规的有效性、稳定性、准确性和泛化性。据我们所知，本文是第一篇解决 T2I 扩散模型完整性验证的研究，为实践中人工智能应用的版权讨论和保护铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2412.16257</guid>
      <pubDate>Tue, 24 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LEARN：多任务领域自适应小样本学习的统一框架</title>
      <link>https://arxiv.org/abs/2412.16275</link>
      <description><![CDATA[arXiv:2412.16275v1 公告类型：新
摘要：计算机视觉中的小样本学习和领域自适应子领域在最新算法和数据集的可用性方面都取得了重大进展。每个子领域都开发了框架；但是，构建一个将两者结合起来的通用系统或框架是尚未探索的事情。作为我们研究的一部分，我们提出了第一个统一的框架，该框架结合了 3 个不同任务（图像分类、对象检测和视频分类）的小样本学习设置的领域自适应。我们的框架是高度模块化的，能够根据算法支持小样本学习，包括/不包括领域自适应。此外，我们框架最重要的可配置特性是增量 $n$ 样本任务的即时设置，以及可选的功能来配置系统以扩展到传统的多次任务。随着当前小样本学习方法更加注重自监督学习 (SSL)，我们的系统还支持多种 SSL 预训练配置。为了测试我们框架的功能，我们针对不同任务和问题设置的各种算法和数据集提供了基准测试。代码是开源的，已在此处公开：https://gitlab.kitware.com/darpa_learn/learn]]></description>
      <guid>https://arxiv.org/abs/2412.16275</guid>
      <pubDate>Tue, 24 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>