<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Fri, 20 Dec 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>推进车牌识别：使用 VehiclePaliGemma 的多任务视觉语言模型</title>
      <link>https://arxiv.org/abs/2412.14197</link>
      <description><![CDATA[arXiv:2412.14197v1 公告类型：新
摘要：车牌识别 (LPR) 涉及利用摄像头和计算机视觉读取车辆牌照的自动化系统。然后可以将通过 LPR 收集的车牌与数据库进行比较，以识别被盗车辆、未投保司机、犯罪嫌疑人等。LPR 系统在节省警察等机构的时间方面发挥着重要作用。过去，LPR 严重依赖于光学字符识别 (OCR)，该技术已被广泛用于识别图像中的字符。通常，收集到的车牌图像受到各种限制，包括噪声、模糊、天气条件和近距离字符，使得识别变得复杂。现有的 LPR 方法仍然需要显着改进，尤其是对于扭曲的图像。为了填补这一空白，我们建议利用视觉语言模型 (VLM)，例如 OpenAI GPT4o、Google Gemini 1.5、Google PaliGemma（Pathways 语言和图像模型 + Gemma 模型）、Meta Llama 3.2、Anthropic Claude 3.5 Sonnet、LLaVA、NVIDIA VILA 和 moondream2 来识别这种具有相似字符的不清楚的车牌。本文评估了 VLM 解决上述问题的能力。此外，我们推出了“VehiclePaliGemma”，这是一款经过微调的开源 PaliGemma VLM，旨在在具有挑战性的条件下识别车牌。我们使用在复杂条件下收集的马来西亚车牌数据集，将我们提出的 VehiclePaliGemma 与最先进的方法和其他 VLM 进行了比较。结果表明，VehiclePaliGemma 取得了优异的性能，准确率达到 87.6%。此外，它能够使用 A100-80GB GPU 以每秒 7 帧的速度预测汽车的车牌。最后，我们探索了 VehiclePaliGemma 模型的多任务处理能力，以准确识别包含多辆不同型号和颜色的汽车的车牌，这些车牌的位置和方向各不相同。]]></description>
      <guid>https://arxiv.org/abs/2412.14197</guid>
      <pubDate>Fri, 20 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>提高 YOLOv8 在相机陷阱物体检测中的泛化性能</title>
      <link>https://arxiv.org/abs/2412.14211</link>
      <description><![CDATA[arXiv:2412.14211v1 公告类型：新
摘要：相机陷阱已成为野生动物保护中不可或缺的工具，为监测和研究自然栖息地中的野生动物提供了非侵入性手段。利用物体检测算法自动识别相机陷阱图像中的物种对于研究和保护目的至关重要。然而，泛化问题很普遍，即训练后的模型无法将其学习应用于前所未见的数据集。本论文探讨了对 YOLOv8 物体检测算法的增强，以解决泛化问题。该研究深入研究了基线 YOLOv8 模型的局限性，强调了它在现实环境中泛化的困难。为了克服这些限制，提出了增强措施，包括加入全局注意机制 (GAM) 模块、改进的多尺度特征融合和联合上的明智交集 (WIoUv3) 作为边界框回归损失函数。全面的评估和消融实验表明，改进后的模型能够抑制背景噪音、关注物体属性，并在新环境中表现出强大的泛化能力。拟议的增强功能不仅解决了相机陷阱数据集固有的挑战，还为在现实世界的保护场景中更广泛地应用铺平了道路，最终有助于有效管理野生动物种群和栖息地。]]></description>
      <guid>https://arxiv.org/abs/2412.14211</guid>
      <pubDate>Fri, 20 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于高效逼真图像去雾的蒸馏池化变压器编码器</title>
      <link>https://arxiv.org/abs/2412.14220</link>
      <description><![CDATA[arXiv:2412.14220v1 公告类型：新
摘要：本文提出了一种轻量级神经网络，利用蒸馏池化变压器编码器（DPTE-Net）实现逼真的图像去雾。最近，虽然视觉变压器（ViT）在各种视觉任务中取得了巨大成功，但它们的自注意力（SA）模块的复杂性与图像分辨率成二次方增长，阻碍了它们在资源受限设备上的适用性。为了克服这个问题，提出的 DPTE-Net 用高效的池化机制取代了传统的 SA 模块，显着降低了计算需求，同时保留了 ViT 的学习能力。为了进一步增强语义特征学习，实施了基于蒸馏的训练过程，将丰富的知识从更大的教师网络转移到 DPTE-Net。此外，DPTE-Net 在生成对抗网络 (GAN) 框架内进行训练，利用 GAN 在图像恢复中的强大泛化能力，并采用传输感知损失函数来动态适应不同的雾霾密度。在各种基准数据集上的实验结果表明，与最先进的方法相比，所提出的 DPTE-Net 可以实现具有竞争力的去雾性能，同时保持较低的计算复杂度，使其成为资源有限的应用的有前途的解决方案。这项工作的代码可在 https://github.com/tranleanh/dpte-net 上找到。]]></description>
      <guid>https://arxiv.org/abs/2412.14220</guid>
      <pubDate>Fri, 20 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ViTmiX：通过混合可视化方法增强 Vision Transformer 的可解释性</title>
      <link>https://arxiv.org/abs/2412.14231</link>
      <description><![CDATA[arXiv:2412.14231v1 公告类型：新
摘要：视觉变换器 (ViT) 的最新进展已在各种视觉识别任务中表现出色，这要归功于它们能够通过自注意力机制捕获图像中的长距离依赖关系。然而，ViT 模型的复杂性需要强大的可解释性方法来揭示其决策过程。可解释人工智能 (XAI) 通过提供对模型预测的洞察，在提高模型透明度和可信度方面发挥着至关重要的作用。当前基于可视化技术（例如逐层相关性传播 (LRP) 和基于梯度的方法）的 ViT 可解释性方法已显示出有希望但有时有限的结果。在本研究中，我们探索了一种混合多种可解释性技术的混合方法，以克服这些限制并增强 ViT 模型的可解释性。我们的实验表明，与单个方法相比，这种混合方法显着提高了 ViT 模型的可解释性。我们还对现有技术进行了修改，例如使用几何平均值进行混合，这在对象分割任务中显示出显着的结果。为了量化可解释性增益，我们通过应用 Pigeonhole 原理引入了一种新颖的事后可解释性度量。这些发现强调了改进和优化 ViT 模型可解释性方法的重要性，为可靠的基于 XAI 的细分铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2412.14231</guid>
      <pubDate>Fri, 20 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用视觉专家增强描述性字幕以实现多模式感知</title>
      <link>https://arxiv.org/abs/2412.14233</link>
      <description><![CDATA[arXiv:2412.14233v1 公告类型：新
摘要：训练大型多模态模型 (LMM) 依赖于连接图像和语言的描述性图像标题。现有方法要么从 LMM 模型中提取标题，要么从互联网图像或人工构建标题。我们建议利用现成的视觉专家来增强图像标题，这些专家最初是从带注释的图像中训练出来的，而不是用于图像标题。
我们的方法称为 DCE，它探索对象的低级和细粒度属性（例如深度、情感和细粒度类别）和对象关系（例如相对位置和人与对象交互 (HOI)），并将属性组合到描述性标题中。实验表明，这样的视觉专家能够提高视觉理解任务的性能以及受益于更准确的视觉理解的推理。我们将发布源代码和管道，以便其他视觉专家可以轻松地合并到管道中。 DCE 管道和数据集的完整源代码将在 \url{https://github.com/syp2ysy/DCE} 提供。]]></description>
      <guid>https://arxiv.org/abs/2412.14233</guid>
      <pubDate>Fri, 20 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉中的分割学习，用于最小化语义分割延迟</title>
      <link>https://arxiv.org/abs/2412.14272</link>
      <description><![CDATA[arXiv:2412.14272v1 公告类型：新
摘要：在本文中，我们提出了一种新颖的方法，使用分割学习 (SL) 来最小化语义分割中的推理延迟，该方法可满足资源受限设备的实时计算机视觉 (CV) 应用的需求。语义分割对于自动驾驶汽车和智能城市基础设施等应用至关重要，但由于计算和通信负载高，面临着巨大的延迟挑战。传统的集中式处理方法对于这种情况效率低下，通常会导致不可接受的推理延迟。SL 通过在边缘设备和中央服务器之间划分深度神经网络 (DNN) 提供了一种有前途的替代方案，实现了本地化数据处理并减少了传输所需的数据量。我们的贡献包括带宽分配的联合优化、边缘设备 DNN 的切割层选择以及中央服务器的处理资源分配。我们研究了并行和串行数据处理场景，并提出了低复杂度的启发式解决方案，在降低计算要求的同时保持近乎最佳的性能。数值结果表明，我们的方法有效地减少了推理延迟，证明了 SL 在动态、资源受限的环境中改进实时 CV 应用的潜力。]]></description>
      <guid>https://arxiv.org/abs/2412.14272</guid>
      <pubDate>Fri, 20 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PixelMan：通过像素操作和生成，使用扩散模型进行一致的对象编辑</title>
      <link>https://arxiv.org/abs/2412.14283</link>
      <description><![CDATA[arXiv:2412.14283v1 公告类型：新
摘要：最近的研究探索了扩散模型 (DM) 在一致对象编辑方面的潜力，旨在修改对象的位置、大小和构图等，同时保持对象和背景的一致性而不改变其纹理和属性。当前的推理时间方法通常依赖于 DDIM 反转，这本质上会损害效率和编辑图像可实现的一致性。最近的方法还利用能量引导，迭代更新预测的噪声，并可以将潜伏因素从原始图像中驱离，从而导致失真。在本文中，我们提出了 PixelMan，一种无需反转和训练即可通过像素操作和生成实现一致性对象编辑的方法，其中我们直接在像素空间中的目标位置创建源对象的副本，并引入一种有效的采样方法，以迭代方式将操作后的对象协调到目标位置并修复其原始位置，同时通过将要生成的编辑图像锚定到像素操作后的图像以及在推理过程中引入各种一致性保持优化技术来确保图像一致性。基于基准数据集的实验评估以及广泛的视觉比较表明，在短短 16 个推理步骤中，PixelMan 在多个一致性对象编辑任务上的表现优于一系列最先进的基于训练和无需训练的方法（通常需要 50 步）。]]></description>
      <guid>https://arxiv.org/abs/2412.14283</guid>
      <pubDate>Fri, 20 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>TRecViT：循环视频转换器</title>
      <link>https://arxiv.org/abs/2412.14294</link>
      <description><![CDATA[arXiv:2412.14294v1 公告类型：新
摘要：我们提出了一种用于视频建模的新型模块。它依赖于时间-空间-通道分解，每个维度都有专用模块：门控线性循环单元 (LRU) 随时间执行信息混合，自注意力层随空间执行混合，MLP 随通道执行混合。由此产生的架构 TRecViT 在稀疏和密集任务上表现良好，在监督或自监督机制下进行训练。值得注意的是，我们的模型是因果关系，在大型视频数据集 (SSv2、Kinetics400) 上的表现优于或与纯注意力模型 ViViT-L 相当，同时参数减少了 $3\times$，内存占用减少了 $12\times$，FLOP 数减少了 $5\times$。代码和检查点将在 https://github.com/google-deepmind/trecvit 上在线提供。]]></description>
      <guid>https://arxiv.org/abs/2412.14294</guid>
      <pubDate>Fri, 20 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过对比时隙实现时间一致的以对象为中心的学习</title>
      <link>https://arxiv.org/abs/2412.14295</link>
      <description><![CDATA[arXiv:2412.14295v1 公告类型：新
摘要：无监督的以对象为中心的视频学习是一种很有前途的方法，可以从大量未标记的视频集合中提取结构化表示。为了支持自主控制等下游任务，这些表示必须具有组合性和时间一致性。现有的基于循环处理的方法通常缺乏跨帧的长期稳定性，因为它们的训练目标不强制时间一致性。在这项工作中，我们为视频对象中心模型引入了一种新的对象级时间对比损失，明确促进了时间一致性。我们的方法显着提高了学习到的以对象为中心的表示的时间一致性，产生了更可靠的视频分解，从而促进了具有挑战性的下游任务，例如无监督对象动态预测。此外，我们的损失增加的归纳偏差极大地改善了对象发现，从而在合成和现实世界数据集上都获得了最先进的结果，甚至优于利用运动掩码作为额外线索的弱监督方法。]]></description>
      <guid>https://arxiv.org/abs/2412.14295</guid>
      <pubDate>Fri, 20 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>对比无源域自适应中被忽视的内容：在邻域环境中利用基于源的潜在增强</title>
      <link>https://arxiv.org/abs/2412.14301</link>
      <description><![CDATA[arXiv:2412.14301v1 公告类型：新 
摘要：无源域自适应 (SFDA) 涉及调整最初使用标记数据集 ({\em 源域}) 训练的模型，以便在自适应期间不依赖任何源数据的情况下在未标记数据集 ({\em 目标域}) 上有效执行。当两个域之间存在数据分布的显著差异并且存在有关源模型的训练数据的隐私问题时，这种适应尤其重要。在适应期间无法访问源数据使得分析估计域差距变得具有挑战性。为了解决这个问题，已经提出了各种技术，例如无监督聚类、对比学习和持续学习。在本文中，我们首先基于对比学习对 SFDA 进行广泛的理论分析，主要是因为它与其他技术相比表现出了卓越的性能。在获得的见解的启发下，我们随后介绍了一种针对对比 SFDA 量身定制的简单但高效的潜在增强方法。这种增强方法利用查询样本邻域内潜在特征的分散性，在源预训练模型的指导下，增强正键的信息量。我们的方法基于单个 InfoNCE 对比损失，在广泛认可的基准数据集上优于最先进的 SFDA 方法。]]></description>
      <guid>https://arxiv.org/abs/2412.14301</guid>
      <pubDate>Fri, 20 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>个性化生成低光图像去噪与增强</title>
      <link>https://arxiv.org/abs/2412.14327</link>
      <description><![CDATA[arXiv:2412.14327v1 公告类型：新
摘要：虽然如今的智能手机相机可以拍出令人惊叹的好照片，但由于光子散粒噪声和传感器读取噪声的基本限制，它们在低光照条件下的表现仍然不能完全令人满意。与传统方法相比，生成图像恢复方法已经显示出有希望的结果，但当信噪比 (SNR) 较低时，它们会产生幻觉内容。认识到用户智能手机上个性化照片库的可用性，我们提出了个性化生成去噪 (PGD)，通过构建针对不同用户定制的扩散模型。我们的核心创新是身份一致的物理缓冲区，可从图库中提取人的物理属性。这种 ID 一致的物理缓冲区提供了强大的先验，可以与扩散模型集成以恢复退化的图像，而无需微调。在广泛的低光测试场景中，我们表明与现有的基于扩散的去噪方法相比，PGD 实现了卓越的图像去噪和增强性能。]]></description>
      <guid>https://arxiv.org/abs/2412.14327</guid>
      <pubDate>Fri, 20 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用带适配器的扩散实现联合语音手势和富有表现力的说话面孔生成</title>
      <link>https://arxiv.org/abs/2412.14333</link>
      <description><![CDATA[arXiv:2412.14333v1 公告类型：新
摘要：近来，同声手势和说话头部生成方面的进展令人印象深刻，但大多数方法只关注两个任务中的一项。那些试图同时生成两者的方法通常依赖于单独的模型或网络模块，这增加了训练的复杂性，并忽略了面部和身体运动之间的内在关系。为了应对这些挑战，在本文中，我们提出了一种新颖的模型架构，可在单个网络中联合生成面部和身体运动。这种方法利用模态之间的共享权重，并通过适配器实现对公共潜在空间的适应。我们的实验表明，所提出的框架不仅保持了最先进的同声手势和说话头部生成性能，而且还显着减少了所需的参数数量。]]></description>
      <guid>https://arxiv.org/abs/2412.14333</guid>
      <pubDate>Fri, 20 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有已知和未知对象的动态语义 VSLAM</title>
      <link>https://arxiv.org/abs/2412.14359</link>
      <description><![CDATA[arXiv:2412.14359v1 公告类型：新
摘要：传统的视觉同步定位和映射 (VSLAM) 系统假设静态环境，这使得它们在高度动态的环境中无效。为了克服这个问题，许多方法集成了来自深度学习模型的语义信息来识别图像中的动态区域。然而，这些方法面临着一个很大的限制，因为监督模型无法识别训练数据集中未包含的对象。本文介绍了一种基于特征的新型语义 VSLAM，能够在已知和未知物体存在的情况下检测动态特征。通过使用无监督分割网络，我们实现了未标记的分割，然后利用对象检测器来识别其中的任何已知类。然后，我们将其与计算出的高梯度光流信息配对，接下来识别已知和未知对象类的静态与动态分割。还引入了一致性检查模块，以进一步细化并最终将其分类为静态特征和动态特征。使用公共数据集进行的评估表明，当图像中存在未知物体时，我们的方法比传统 VSLAM 具有更优异的性能，而当图像仅包含已知物体时，我们的方法仍可与领先的语义 VSLAM 技术的性能相匹配]]></description>
      <guid>https://arxiv.org/abs/2412.14359</guid>
      <pubDate>Fri, 20 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用视觉语言模型生成超现实主义图像</title>
      <link>https://arxiv.org/abs/2412.14366</link>
      <description><![CDATA[arXiv:2412.14366v1 公告类型：新
摘要：生成式人工智能的最新进展使得创建不同类型的内容（包括文本、图像和代码）变得方便。在本文中，我们探索使用视觉语言生成模型（包括 DALL-E、Deep Dream Generator 和 DreamStudio）生成超现实主义运动风格的图像。我们的研究从在各种图像生成设置和不同模型下的图像生成开始。主要目标是确定最适合生成此类图像的模型和设置。此外，我们旨在了解使用编辑过的基础图像对生成的结果图像的影响。通过这些实验，我们评估了所选模型的性能，并获得了有关它们生成此类图像的能力的宝贵见解。我们的分析表明，当使用 ChatGPT 生成的提示时，Dall-E 2 表现最佳。]]></description>
      <guid>https://arxiv.org/abs/2412.14366</guid>
      <pubDate>Fri, 20 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SEREP：用于野外捕获和重定向的语义面部表情表示</title>
      <link>https://arxiv.org/abs/2412.14371</link>
      <description><![CDATA[arXiv:2412.14371v1 公告类型：新
摘要：由于捕捉条件、脸型和表情各异，在野外进行单眼面部表情捕捉具有挑战性。大多数当前方法依赖于线性 3D 可变形模型，该模型在顶点位移级别独立于身份表示面部表情。我们提出了 SEREP（语义表情表示），该模型在语义层面将表情与身份区分开来。它首先使用循环一致性损失从未配对的 3D 面部表情中学习表情表示。然后，我们使用一种依赖于领域自适应的新型半监督方案训练模型来预测单眼图像中的表情。此外，我们引入了 MultiREX，这是一个基准，解决了表情捕捉任务缺乏评估资源的问题。我们的实验表明，SEREP 优于最先进的方法，可以捕捉具有挑战性的表情并将其转移到新的身份中。]]></description>
      <guid>https://arxiv.org/abs/2412.14371</guid>
      <pubDate>Fri, 20 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>