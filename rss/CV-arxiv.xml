<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Wed, 28 Aug 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>通过基于 CLIP 的直接优化重新审视图像字幕训练范式</title>
      <link>https://arxiv.org/abs/2408.14547</link>
      <description><![CDATA[arXiv:2408.14547v1 公告类型：新
摘要：传统的图像字幕训练方法包括使用教师强制对网络进行预训练，然后使用自我批判序列训练进行微调，以最大化手工制作的字幕指标。然而，当尝试优化 CLIP-Score 和 PAC-Score 等现代和更高质量的指标时，这种训练方法经常会遇到不稳定的情况，无法获得生成流畅且信息丰富的字幕所需的真正描述能力。在本文中，我们提出了一种新的训练范式，称为直接基于 CLIP 的优化 (DiCO)。我们的方法联合学习和优化从具有高度人类相关性的可学习字幕评估器中提炼出的奖励模型。这是通过直接在字幕制作者内部解决加权分类问题来实现的。同时，DiCO 可防止与原始模型的分歧，从而确保保持流畅性。与现有方法相比，DiCO 不仅在生成的字幕中表现出更高的稳定性和质量，而且与人类偏好更加一致，尤其是在现代指标方面。此外，它在传统指标中保持了有竞争力的性能。我们的源代码和经过训练的模型可在 https://github.com/aimagelab/DiCO 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2408.14547</guid>
      <pubDate>Wed, 28 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>探索合成数据取代真实数据的潜力</title>
      <link>https://arxiv.org/abs/2408.14559</link>
      <description><![CDATA[arXiv:2408.14559v1 公告类型：新
摘要：合成数据取代真实数据的潜力在数据饥渴的人工智能中产生了对合成数据的巨大需求。当合成数据与来自测试域以外域的少量真实图像一起用于训练时，这种潜力甚至更大。我们发现这种潜力取决于（i）跨域真实图像的数量和（ii）评估训练模型的测试集。我们引入了两个新指标，即 train2test 距离和 $\text{AP}_\text{t2t}$，以评估使用合成数据的跨域训练集表示测试实例特征与训练性能之间的关系的能力。利用这些指标，我们深入研究了影响合成数据潜力的因素，并揭示了一些关于合成数据如何影响训练性能的有趣动态。我们希望这些发现能够鼓励更广泛地使用合成数据。]]></description>
      <guid>https://arxiv.org/abs/2408.14559</guid>
      <pubDate>Wed, 28 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>伪装物体检测及其他研究综述</title>
      <link>https://arxiv.org/abs/2408.14562</link>
      <description><![CDATA[arXiv:2408.14562v1 公告类型：新
摘要：伪装物体检测 (COD) 是指识别和分割无缝融入周围环境的物体的任务，这对计算机视觉系统提出了重大挑战。近年来，COD 因其在监视、野生动物保护、自主系统等领域的潜在应用而受到广泛关注。虽然存在几份关于 COD 的调查，但它们在涵盖的论文数量和范围方面往往存在局限性，特别是考虑到自 2023 年中期以来该领域的快速进步。为了填补这一空白，我们提出了迄今为止最全面的 COD 综述，涵盖了理论框架和对该领域的实际贡献。本文从传统和深度学习方法的角度探讨了四个领域的各种 COD 方法，包括图像级和视频级解决方案。我们彻底研究了 COD 与其他伪装场景方法之间的相关性，从而为后续分析奠定了理论基础。除了对象级检测之外，我们还总结了实例级任务的扩展方法，包括伪装实例分割、计数和排名。此外，我们概述了 COD 任务中常用的基准和评估指标，对图像和视频领域的基于深度学习的技术进行了全面评估，同时考虑了定性和定量性能。最后，我们讨论了当前 COD 模型的局限性，并提出了 9 个有希望的未来研究方向，重点是解决固有挑战并探索新颖、有意义的技术。对于那些感兴趣的人，可以在 https://github.com/ChunmingHe/awesome-concealed-object-segmentation 找到与 COD 相关的技术、数据集和其他资源的精选列表]]></description>
      <guid>https://arxiv.org/abs/2408.14562</guid>
      <pubDate>Wed, 28 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DIAGen：使用生成模型进行多样化图像增强</title>
      <link>https://arxiv.org/abs/2408.14584</link>
      <description><![CDATA[arXiv:2408.14584v1 公告类型：新
摘要：简单的数据增强技术（例如旋转和翻转）被广泛用于增强计算机视觉模型的泛化能力。但是，这些技术通常无法修改类的高级语义属性。为了解决这一限制，研究人员探索了生成增强方法，例如最近提出的 DA-Fusion。尽管取得了一些进展，但变化仍然主要局限于纹理变化，因此在视角、环境、天气条件甚至类别级语义属性（例如，狗的品种变化）等方面都存在不足。为了克服这一挑战，我们提出了基于 DA-Fusion 的 DIAGen。首先，我们将高斯噪声应用于使用文本反转学习的对象的嵌入，以使用预训练的扩散模型的知识来实现​​生成多样化。其次，我们利用文本到文本生成模型的一般知识来指导扩散模型的图像生成，并提供不同的类特定提示。最后，我们引入了一种加权机制来减轻生成质量较差的样本的影响。在各种数据集上的实验结果表明，DIAGen 不仅增强了语义多样性，而且还提高了后续分类器的性能。DIAGen 相对于标准增强和 DA-Fusion 基线的优势在分布外的样本中尤为明显。]]></description>
      <guid>https://arxiv.org/abs/2408.14584</guid>
      <pubDate>Wed, 28 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于全局-局部蒸馏网络的不完整模态视听说话人跟踪</title>
      <link>https://arxiv.org/abs/2408.14585</link>
      <description><![CDATA[arXiv:2408.14585v1 公告类型：新
摘要：在说话人跟踪研究中，整合和补充多模态数据是提高跟踪系统准确性和鲁棒性的关键策略。然而，由于遮挡、声学噪声和传感器故障导致的噪声观测，不完整模态的跟踪仍然是一个具有挑战性的问题。特别是当多个模态中存在缺失数据时，现有多模态融合方法的性能往往会下降。为此，我们提出了一种基于全局-局部蒸馏的跟踪器 (GLDTracker)，用于鲁棒的视听说话人跟踪。GLDTracker 由师生蒸馏模型驱动，能够灵活地融合来自每个模态的不完整信息。教师网络处理由摄像头和麦克风阵列捕获的全局信号，学生网络处理受视觉遮挡和缺失音频通道影响的局部信息。通过将知识从老师传递给学生，学生网络可以更好地适应具有不完整观察的复杂动态场景。在学生网络中，构建了一个基于生成对抗网络的全局特征重构模块，用于从缺少局部信息的特征嵌入中重构全局特征。此外，引入了多模态多级融合注意力机制，将不完整特征和重构特征进行整合，充分利用视听特征和全局-局部特征的互补性和一致性。在 AV16.3 数据集上的实验结果表明，提出的 GLDTracker 优于现有的最先进的视听跟踪器，在标准和不完整模态数据集上均实现了领先的性能，凸显了其在复杂条件下的优越性和鲁棒性。代码和模型将可用。]]></description>
      <guid>https://arxiv.org/abs/2408.14585</guid>
      <pubDate>Wed, 28 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MMR：评估大型多模态模型的阅读能力</title>
      <link>https://arxiv.org/abs/2408.14594</link>
      <description><![CDATA[arXiv:2408.14594v1 公告类型：新 
摘要：大型多模态模型 (LMM) 在理解各种类型的图像（包括富文本图像）方面表现出令人印象深刻的能力。大多数现有的富文本图像基准都是简单的基于提取的问答，许多 LMM 现在很容易获得高分。这意味着当前的基准无法准确反映不同模型的性能，一个自然的想法是建立一个新的基准来评估它们的复杂推理和空间理解能力。在这项工作中，我们提出了 11 个不同任务中的多模态阅读 (MMR) 基准，以评估 LMM 对富文本图像的理解。MMR 是第一个在语言模型的帮助下基于人工注释构建的富文本图像基准。通过评估包括 GPT-4o 在内的几种最先进的 LMM，它揭示了现有 LMM 的有限能力，凸显了我们基准的价值。]]></description>
      <guid>https://arxiv.org/abs/2408.14594</guid>
      <pubDate>Wed, 28 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PVAFN：用于 3D 物体检测的具有多池增强的点体素注意力融合网络</title>
      <link>https://arxiv.org/abs/2408.14600</link>
      <description><![CDATA[arXiv:2408.14600v1 公告类型：新
摘要：点和体素表示的集成在基于 LiDAR 的 3D 物体检测中变得越来越普遍。然而，这种组合通常难以有效地捕获语义信息。此外，仅依赖感兴趣区域内的点特征可能会导致信息丢失和局部特征表示的限制。为了应对这些挑战，我们提出了一种新型的两阶段 3D 物体检测器，称为点体素注意融合网络 (PVAFN)。PVAFN 利用注意机制在特征提取阶段改进多模态特征融合。在细化阶段，它利用多池策略有效地整合多尺度和区域特定信息。点体素注意机制自适应地结合点云和基于体素的鸟瞰图 (BEV) 特征，从而产生更丰富的物体表示，有助于减少错误检测。此外，还引入了多池增强模块来增强模型的感知能力。该模块采用聚类池化和金字塔池化技术来有效捕获关键几何细节和细粒度形状结构，从而增强局部和全局特征的整合。在 KITTI 和 Waymo 数据集上进行的大量实验表明，所提出的 PVAFN 实现了具有竞争力的性能。代码和模型将可用。]]></description>
      <guid>https://arxiv.org/abs/2408.14600</guid>
      <pubDate>Wed, 28 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>3D 点云网络修剪：当某些权重不重要时</title>
      <link>https://arxiv.org/abs/2408.14601</link>
      <description><![CDATA[arXiv:2408.14601v1 公告类型：新
摘要：点云是众多应用中使用的关键几何数据结构。采用称为点云神经网络 (PC-NN) 的深度神经网络来处理 3D 点云，极大地推动了依赖 3D 几何数据来提高任务效率的领域。扩大神经网络模型和 3D 点云的大小对最小化计算和内存需求带来了重大挑战。这对于满足现实世界应用程序的苛刻要求至关重要，这些应用程序优先考虑最低能耗和低延迟。因此，由于 PCNN 对参数的敏感性，研究 PCNN 中的冗余至关重要但具有挑战性。此外，传统的修剪方法面临困难，因为这些网络严重依赖权重和点。尽管如此，我们的研究揭示了一个有希望的现象，可以改进标准的 PCNN 修剪技术。我们的研究结果表明，仅保留最高幅度权重的前 p% 对于保持准确性至关重要。例如，从 PointNet 模型中剪枝 99% 的权重仍可获得接近基础水平的准确率。具体来说，在 ModelNet40 数据集中，PointNet 模型的基础准确率为 87.5%，仅保留 1% 的权重仍可实现 86.8% 的准确率。代码位于：https://github.com/apurba-nsu-rnd-lab/PCNN_Pruning]]></description>
      <guid>https://arxiv.org/abs/2408.14601</guid>
      <pubDate>Wed, 28 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>比较分析：使用迁移学习从视频中识别暴力</title>
      <link>https://arxiv.org/abs/2408.14659</link>
      <description><![CDATA[arXiv:2408.14659v1 公告类型：新
摘要：动作识别已成为计算机视觉领域的热门话题。然而，计算机视觉在视频处理中的主要应用集中在相对简单的动作检测上，而对暴力检测等复杂事件的研究相对较少。本研究重点是在复杂数据集上对各种深度学习技术进行基准测试。接下来，使用更大的数据集来测试数据量增加带来的提升。数据集大小从 500 个视频增加到 1,600 个视频，导致四个模型的平均准确率显着提高了 6%。]]></description>
      <guid>https://arxiv.org/abs/2408.14659</guid>
      <pubDate>Wed, 28 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>物理上可行的语义分割</title>
      <link>https://arxiv.org/abs/2408.14672</link>
      <description><![CDATA[arXiv:2408.14672v1 公告类型：新
摘要：最先进的语义分割模型通常以数据驱动的方式进行优化，仅最小化训练数据上的每像素分类目标。这种纯数据驱动的范式通常会导致荒谬的分割，尤其是当输入图像的域与训练期间遇到的域不同时。例如，最先进的模型可能会将标签“道路”分配给位于分别标记为“天空”的段上方的段，尽管我们对物理世界的了解表明，这种配置对于前向直立相机捕获的图像是不可行的。我们的方法，物理上可行的语义分割（PhyFea），从语义分割数据集的训练集中提取控制空间类关系的明确物理约束，并强制执行可微分损失函数，惩罚违反这些约束的行为以提高预测可行性。与我们在 ADE20K、Cityscapes 和 ACDC 中用作基线的每个最先进网络相比，PhyFea 在 mIoU 方面实现了显著的性能提升，尤其是在 ADE20K 上提高了 $1.5\%$，在 ACDC 上提高了 $2.1\%$。]]></description>
      <guid>https://arxiv.org/abs/2408.14672</guid>
      <pubDate>Wed, 28 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>gWaveNet：使用自定义内核集成深度学习方法对噪声卫星数据中的重力波进行分类</title>
      <link>https://arxiv.org/abs/2408.14674</link>
      <description><![CDATA[arXiv:2408.14674v1 公告类型：新
摘要：大气重力波发生在地球大气层中，是由重力和浮力相互作用引起的。这些波对大气的各个方面都有深远的影响，包括降水模式、云层形成、臭氧分布、气溶胶和污染物扩散。因此，了解重力波对于理解和监测各种大气行为的变化至关重要。使用机器学习技术从卫星数据中识别重力波的研究有限。特别是，如果不应用噪声消除技术，它仍然是一个未被充分探索的研究领域。本研究提出了一种新颖的内核设计，旨在识别卫星图像中的重力波。所提出的内核无缝集成到深度卷积神经网络中，称为 gWaveNet。我们提出的模型在无需任何特征工程的情况下，在从嘈杂的卫星数据中检测包含重力波的图像方面表现出令人印象深刻的能力。实证结果表明，我们的模型优于相关方法，训练准确率超过 98%，测试准确率超过 94%，这是迄今为止重力波探测的最佳结果。我们在 https://rb.gy/qn68ku 上开源了我们的代码。]]></description>
      <guid>https://arxiv.org/abs/2408.14674</guid>
      <pubDate>Wed, 28 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>捕捉和诊断：一种用于识别野生植物疾病的先进多模式检索系统</title>
      <link>https://arxiv.org/abs/2408.14723</link>
      <description><![CDATA[arXiv:2408.14723v1 公告类型：新
摘要：植物病害识别是一项关键任务，可确保作物健康并减轻病害造成的损害。在潜在病害进一步蔓延之前，迫切需要一种方便的工具来使农民能够根据查询图片或可疑植物的文本描述进行诊断，从而开始治疗。在本文中，我们开发了一个多模态植物病害图像检索系统，以支持基于图像或文本提示的病害搜索。具体来说，我们利用最大的野生植物病害数据集 PlantWild，其中包括 89 个类别的 18,000 多张图像，以提供与查询相关的潜在病害的全面视图。此外，在开发的系统中实现了跨模态检索，这得益于一种新颖的基于 CLIP 的视觉语言模型，该模型将病害描述和病害图像编码到同一个潜在空间中。我们的检索系统建立在检索器之上，允许用户上传植物疾病图像或疾病描述，以从疾病数据集中检索具有相似特征的相应图像，从而为最终用户推荐候选疾病。]]></description>
      <guid>https://arxiv.org/abs/2408.14723</guid>
      <pubDate>Wed, 28 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GeoTransfer：通过迁移学习实现可推广的少样本多视图重建</title>
      <link>https://arxiv.org/abs/2408.14724</link>
      <description><![CDATA[arXiv:2408.14724v1 公告类型：新 
摘要：本文提出了一种稀疏 3D 重建的新方法，利用神经辐射场 (NeRF) 的表达能力及其特征的快速传输来学习准确的占用场。现有的稀疏输入的 3D 重建方法仍然难以捕捉复杂的几何细节，并且在处理遮挡区域时会受到限制。另一方面，NeRF 在建模复杂场景方面表现出色，但不提供提取有意义几何图形的方法。我们提出的方法通过传输 NeRF 特征中编码的信息来获得准确的占用场表示，从而兼具两全其美的优势。我们利用预先训练的、可泛化的最先进的 NeRF 网络来捕获详细的场景辐射信息，并快速传输这些知识以训练可泛化的隐式占用网络。此过程有助于利用可泛化 NeRF 先验中编码的场景几何知识并对其进行细化以学习占用场，从而促进更精确的 3D 空间泛化表示。迁移学习方法可将训练时间大幅缩短几个数量级（即从几天缩短到 3.5 小时），无需从头开始训练可泛化的稀疏表面重建方法。此外，我们引入了一种新的体积渲染权重损失，有助于学习准确的占用场，以及一种有助于全局平滑占用场的正常损失。我们在 DTU 数据集上评估了我们的方法，并在重建精度方面展示了最先进的性能，尤其是在具有稀疏输入数据和遮挡区域的具有挑战性的场景中。此外，我们通过在 Blended MVS 数据集上展示定性结果而无需任何再训练来证明我们方法的泛化能力。]]></description>
      <guid>https://arxiv.org/abs/2408.14724</guid>
      <pubDate>Wed, 28 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>OctFusion：基于八叉树的 3D 形状生成扩散模型</title>
      <link>https://arxiv.org/abs/2408.14732</link>
      <description><![CDATA[arXiv:2408.14732v1 公告类型：新
摘要：扩散模型已成为 3D 生成的流行方法。然而，对于扩散模型来说，有效生成多样化和高质量的 3D 形状仍然具有挑战性。在本文中，我们介绍了 OctFusion，它可以在单个 Nvidia 4090 GPU 上在 2.5 秒内生成具有任意分辨率的 3D 形状，并且保证提取的网格是连续的和流形的。OctFusion 的关键组件是基于八叉树的潜在表示和随附的扩散模型。该表示结合了隐式神经表示和显式空间八叉树的优点，并通过基于八叉树的变分自动编码器进行学习。所提出的扩散模型是一个统一的多尺度 U-Net，它能够在不同的八叉树级别之间共享权重和计算，并避免了广泛使用的级联扩散方案的复杂性。我们在 ShapeNet 和 Objaverse 数据集上验证了 OctFusion 的有效性，并在形状生成任务上实现了最先进的性能。我们通过生成用于纹理网格生成的高质量色场和以文本提示、草图或类别标签为条件的高质量 3D 形状，证明了 OctFusion 的可扩展性和灵活性。我们的代码和预训练模型可在 \url{https://github.com/octree-nn/octfusion} 上找到。]]></description>
      <guid>https://arxiv.org/abs/2408.14732</guid>
      <pubDate>Wed, 28 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用基于文本的查询和条件建模的个性化视频摘要</title>
      <link>https://arxiv.org/abs/2408.14743</link>
      <description><![CDATA[arXiv:2408.14743v1 公告类型：新
摘要：YouTube 和 Vimeo 等平台上视频内容的激增对有效定位相关信息提出了重大挑战。自动视频摘要旨在通过以浓缩形式提取和呈现关键内容来解决这一问题。本论文探讨了通过集成基于文本的查询和条件建模来增强视频摘要，以根据用户需求定制摘要。传统方法通常会产生固定的摘要，可能不符合个人要求。为了克服这个问题，我们提出了一种多模式深度学习方法，该方法结合了文本查询和视觉信息，将它们融合在模型架构的不同级别。准确度和 F1 分数等评估指标可评估生成的摘要的质量。该论文还研究了使用上下文词嵌入和专门的注意力网络改进基于文本的查询表示。这增强了对查询的语义理解，从而产生了更好的视频摘要。为了模拟类似人类的摘要，即考虑视觉连贯性和故事情节一致性等抽象因素，我们引入了一种条件建模方法。该方法使用多个随机变量和联合分布来捕获关键的摘要组件，从而产生更像人类且可解释的摘要。为了解决完全监督学习中的数据稀缺问题，该论文提出了一种片段级伪标记方法。这种自监督方法可以生成额外的数据，即使在有限的人工标记数据集下也能提高模型性能。总之，这项研究旨在通过结合基于文本的查询、改进查询表示、引入条件建模和解决数据稀缺问题来增强自动视频摘要，从而创建更有效和个性化的视频摘要。]]></description>
      <guid>https://arxiv.org/abs/2408.14743</guid>
      <pubDate>Wed, 28 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>