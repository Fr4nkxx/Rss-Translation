<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Mon, 29 Jul 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>可回答性领域：通过扩散模型估计可回答的位置</title>
      <link>https://arxiv.org/abs/2407.18497</link>
      <description><![CDATA[arXiv:2407.18497v1 公告类型：新
摘要：在人工智能和机器人技术不断进步的时代，让机器与周围环境互动并理解周围环境是一项重要的研究工作。在本文中，我们提出了 Answerability Fields，这是一种预测复杂室内环境中可回答性的新方法。利用 3D 问答数据集，我们构建了一个全面的 Answerability Fields 数据集，涵盖了来自 ScanNet 的各种场景和问题。使用扩散模型，我们成功地推断和评估了这些 Answerability Fields，证明了对象及其位置在回答场景中的问题方面的重要性。我们的结果展示了 Answerability Fields 在指导场景理解任务方面的有效性，为其在增强智能代理与其环境之间的交互方面的应用奠定了基础。]]></description>
      <guid>https://arxiv.org/abs/2407.18497</guid>
      <pubDate>Tue, 30 Jul 2024 03:16:25 GMT</pubDate>
    </item>
    <item>
      <title>重新审视事件生成模型：使用隐式神经表征进行事件到视频重建的自监督学习</title>
      <link>https://arxiv.org/abs/2407.18500</link>
      <description><![CDATA[arXiv:2407.18500v1 公告类型：新
摘要：在保持高时间分辨率和动态范围的同时从事件数据重建强度帧对于弥合基于事件和基于帧的计算机视觉之间的差距至关重要。以前的方法依赖于对合成数据的监督学习，这种数据缺乏可解释性，并且存在过度拟合事件模拟器设置的风险。最近，人们积极研究基于自监督学习 (SSL) 的方法，该方法主要利用每帧光流通过光度恒常性来估计强度。然而，在光流不准确的情况下，它们容易出错。本文提出了一种新颖的 SSL 事件到视频重建方法，称为 EvINR，它消除了对标记数据或光流估计的需求。我们的核心思想是通过直接解决事件生成模型来重建强度帧，该模型本质上是一个偏微分方程 (PDE)，它描述了如何根据随时间变化的亮度信号生成事件。具体来说，我们利用隐式神经表征（INR）来表示事件生成方程的解，该表征以时空坐标（x，y，t）为输入并预测强度值。INR 参数化为完全连接的多层感知器（MLP），可以通过事件监督的时间导数进行优化。为了使 EvINR 满足在线要求，我们提出了几种加速技术，可大大加快训练过程。综合实验表明，我们的 EvINR 比以前的 SSL 方法高出 38% 均方误差（MSE），并且与 SoTA 监督方法相当或优于后者。项目页面：https://vlislab22.github.io/EvINR/。]]></description>
      <guid>https://arxiv.org/abs/2407.18500</guid>
      <pubDate>Tue, 30 Jul 2024 03:16:25 GMT</pubDate>
    </item>
    <item>
      <title>用于阿尔茨海默病亚型诊断的渐进式单模态到多模态分类框架</title>
      <link>https://arxiv.org/abs/2407.18466</link>
      <description><![CDATA[arXiv:2407.18466v1 公告类型：新
摘要：当前阿尔茨海默病 (AD) 的临床诊断框架涉及从多个诊断阶段获得的多种模态，每种模态都有不同的用途和成本。先前的 AD 诊断研究主要集中于如何直接融合多种模态以进行端到端的单阶段诊断，这实际上需要很高的数据获取成本。此外，这些方法中很大一部分在诊断 AD 时没有考虑临床指南，无法提供准确的亚型诊断。在本文中，通过探索多种模态之间的相互关联，我们提出了一种新颖的渐进式 AD 亚型诊断框架，旨在基于早期低成本阶段中更易于访问的模态而不是所有阶段的模态给出诊断结果。具体而言，首先，我们设计 1) 一个文本解缠网络，以更好地处理初始阶段收集的表格数据，以及 2) 一个模态融合模块，用于分别融合多模态特征。其次，我们将早期低成本阶段获得的模态特征与后期高成本阶段获得的模态特征进行对齐，以便在后期无需实际获取模态的情况下提供准确诊断，从而节省成本。此外，我们遵循临床指南，在每个阶段对齐特征以实现亚型诊断。第三，我们利用渐进式分类器，该分类器可以逐步包括其他获得的模态（如果需要）进行诊断，以实现诊断成本和诊断性能之间的平衡。我们在大型多样化公共和家庭数据集（总共 8280 个）上评估了我们提出的框架，并取得了优于最先进方法的性能。我们的代码将在接受后发布。]]></description>
      <guid>https://arxiv.org/abs/2407.18466</guid>
      <pubDate>Tue, 30 Jul 2024 03:16:24 GMT</pubDate>
    </item>
    <item>
      <title>SMPISD-MTPNet：使用多任务感知网络的场景语义先验辅助红外船舶检测</title>
      <link>https://arxiv.org/abs/2407.18487</link>
      <description><![CDATA[arXiv:2407.18487v1 公告类型：新 
摘要：近年来，由于红外图像对恶劣天气的鲁棒性，红外船舶检测（IRSD）受到越来越多的关注。然而，在复杂的场景中可能会出现大量的误报。为了应对这些挑战，我们提出了场景语义先验辅助多任务感知网络（SMPISD-MTPNet），它包括三个阶段：场景语义提取、深度特征提取和预测。在场景语义提取阶段，我们使用场景语义提取器（SSE）通过基于专家知识提取的特征来引导网络。在深度特征提取阶段，使用主干网络提取深度特征。这些特征随后由融合网络集成，增强了对不同大小目标的检测能力。在预测阶段，我们利用多任务感知模块，其中包括基于梯度的模块和场景分割模块，能够精确检测复杂场景中的小目标和暗目标。对于训练过程，我们引入了软微调训练策略来抑制数据增强造成的失真。此外，由于缺乏针对场景标记的公开数据集，我们引入了带场景分割的红外船舶数据集 (IRSDSS)。最后，我们评估网络并将其与最先进的 (SOTA) 方法进行比较，表明 SMPISD-MTPNet 优于现有方法。本研究的源代码和数据集可在 https://github.com/greekinRoma/KMNDNet 上访问。]]></description>
      <guid>https://arxiv.org/abs/2407.18487</guid>
      <pubDate>Tue, 30 Jul 2024 03:16:24 GMT</pubDate>
    </item>
    <item>
      <title>抑郁症患者积极和消极情绪的神经调节改变：使用积极/消极情绪图谱从 fMRI 获得的见解</title>
      <link>https://arxiv.org/abs/2407.18492</link>
      <description><![CDATA[arXiv:2407.18492v1 公告类型：新
摘要：背景：尽管人们已经注意到抑郁症患者在处理情绪方面表现出差异，但积极和消极情绪的精确神经调节机制仍然难以捉摸。FMRI 是一种尖端医学成像技术，以其高空间分辨率和动态时间信息而闻名，特别适合研究抑郁症的神经动力学。方法：为了解决这一空白，我们的研究首先利用 fMRI 描绘健康个体中与积极和消极情绪相关的激活区域，从而创建积极情绪图谱 (PEA) 和消极情绪图谱 (NEA)。随后，我们使用这些图谱检查了抑郁症患者的神经影像学变化，并基于机器学习评估了它们的诊断性能。结果：我们的研究结果表明，基于 PEA 和 NEA 的抑郁症患者的分类准确率超过 0.70，与全脑图谱相比有显着提高。此外，ALFF 分析揭示了抑郁症患者和健康对照者在 NEA 期间在八个功能簇中存在显著差异，重点关注左楔叶、扣带回和顶上小叶。相比之下，PEA 揭示了十五个簇之间的更明显差异，涉及右侧梭状回、海马旁回和顶下小叶。局限性：由于样本量有限和抑郁症患者的亚型有限，疗效可能需要在未来进一步验证。结论：这些发现强调了情绪调节和抑郁之间的复杂相互作用，展示了抑郁症患者的 PEA 和 NEA 的显著变化。这项研究加深了我们对抑郁症情绪调节的理解，对诊断和治疗评估具有重要意义。]]></description>
      <guid>https://arxiv.org/abs/2407.18492</guid>
      <pubDate>Tue, 30 Jul 2024 03:16:24 GMT</pubDate>
    </item>
    <item>
      <title>基于参考的 3D 语义感知框架，用于精确的局部面部属性编辑</title>
      <link>https://arxiv.org/abs/2407.18392</link>
      <description><![CDATA[arXiv:2407.18392v2 公告类型：新
摘要：面部属性编辑在合成具有特定特征的真实面部同时保持逼真的外观方面起着至关重要的作用。尽管取得了进展，但在实现精确的 3D 感知属性修改方面仍然存在挑战，这对于从不同角度一致且准确地表示面部至关重要。当前的方法难以解决语义纠缠，并且缺乏在保持图像完整性的同时合并属性的有效指导。为了解决这些问题，我们引入了一个新颖的框架，该框架融合了基于潜在和基于参考的编辑方法的优势。我们的方法采用 3D GAN 反转技术将参考图中的属性嵌入到三平面空间中，确保 3D 一致性和从多个角度逼真的观看效果。我们利用混合技术和预测语义掩码来定位精确的编辑区域，并将它们与参考图中的上下文指导合并。然后应用由粗到细的修复策略来保持非目标区域的完整性，从而显着增强真实感。我们的评估表明，它在各种编辑任务中都表现出色，验证了我们的框架在真实且适用的面部属性编辑中的有效性。]]></description>
      <guid>https://arxiv.org/abs/2407.18392</guid>
      <pubDate>Tue, 30 Jul 2024 03:16:23 GMT</pubDate>
    </item>
    <item>
      <title>视觉变换器的混合非线性量化</title>
      <link>https://arxiv.org/abs/2407.18437</link>
      <description><![CDATA[arXiv:2407.18437v1 公告类型：新
摘要：大多数量化方法都是为了减小 Vision Transformers 的模型大小而提出的，但大多数方法都忽略了非线性操作的量化。只有少数作品解决了非线性操作的量化问题，但它们在所有非线性操作中应用了单一量化方法。我们相信，通过对每个非线性操作采用不同的量化方法可以进一步改善这一点。因此，为了从已知方法中为每个非线性层分配误差最小的量化方法，我们提出了一种混合非线性量化，该量化考虑了通过 SQNR 差异度量测量的逐层量化敏感度。结果表明，对于 ViT、DeiT 和 Swin 模型，我们的方法在 8 位和 6 位设置下均优于 I-BERT、FQ-ViT 和 I-ViT，平均分别高出 0.6%p 和 19.6%p。在训练时间有限的情况下，我们的方法比 I-BERT 和 I-ViT 分别高出 0.6%p 和 20.8%p。我们计划在 https://gitlab.com/ones-ai/mixed-non-linear-quantization 发布我们的代码。]]></description>
      <guid>https://arxiv.org/abs/2407.18437</guid>
      <pubDate>Tue, 30 Jul 2024 03:16:23 GMT</pubDate>
    </item>
    <item>
      <title>HybridDepth：利用焦点深度和单图像先验实现移动 AR 的稳健深度融合</title>
      <link>https://arxiv.org/abs/2407.18443</link>
      <description><![CDATA[arXiv:2407.18443v1 公告类型：新
摘要：我们提出了 HYBRIDDEPTH，这是一种强大的深度估计管道，可解决移动 AR 深度估计的独特挑战，例如尺度模糊性、硬件异质性和通用性。HYBRIDDEPTH 利用移动设备上可用的相机功能。它有效地结合了焦点深度 (DFF) 方法固有的尺度精度与强大的单图像深度先验所实现的泛化能力。通过利用移动相机的焦平面，我们的方法可以准确地从聚焦像素中捕获深度值，并应用这些值来计算比例和移位参数，以将相对深度转换为度量深度。我们将我们的管道作为一个端到端系统进行测试，使用新开发的移动客户端来捕获焦点堆栈，然后将其发送到 GPU 驱动的服务器进行深度估计。
通过全面的定量和定性分析，我们证明 HYBRIDDEPTH 不仅在常见数据集 (DDFF12、NYU Depth v2) 和真实世界 AR 数据集 ARKitScenes 中的表现优于最先进 (SOTA) 模型，而且还表现出强大的零样本泛化能力。例如，在 NYU Depth v2 上训练的 HYBRIDDEPTH 在 DDFF12 上实现了与在 DDFF12 上训练的现有模型相当的性能。它在 ARKitScenes 数据集上的零样本性能也优于所有 SOTA 模型。此外，我们对我们的模型和 ARCore 框架进行了定性比较，表明我们的模型输出的深度图在结构细节和度量精度方面明显更准确。该项目的源代码可在 github 上找到。]]></description>
      <guid>https://arxiv.org/abs/2407.18443</guid>
      <pubDate>Tue, 30 Jul 2024 03:16:23 GMT</pubDate>
    </item>
    <item>
      <title>纺织品异常检测：地毯自动质量检测最新技术的评估</title>
      <link>https://arxiv.org/abs/2407.18450</link>
      <description><![CDATA[arXiv:2407.18450v1 公告类型：新
摘要：在本研究中，对最先进的无监督检测模型进行了评估，目的是自动对羊毛地毯进行异常检查。创建了四种独特地毯纹理的自定义数据集，以彻底测试模型及其在检测复杂纹理中的细微异常方面的稳健性。由于制造用例中在线检查系统的要求，本研究中的重要指标是检测异常区域的准确性、错误检测的数量以及每个模型的实时性能推理时间。在评估的模型中，发现基于学生-教师网络的方法平均产生最高的检测准确率和最低的错误检测率。当在多类数据集上进行训练时，发现这些模型产生的结果与单类训练相当，甚至更好。最后，在检测速度方面，除生成模型外，所有其他评估模型在 GPU 上的推理时间都相当，平均每张图像 0.16 秒。在 CPU 上，这些模型中的大多数通常产生的结果分别是 GPU 推理时间的 1.5 到 2 倍。]]></description>
      <guid>https://arxiv.org/abs/2407.18450</guid>
      <pubDate>Tue, 30 Jul 2024 03:16:23 GMT</pubDate>
    </item>
    <item>
      <title>SMiCRM：机械分子图像的基准数据集</title>
      <link>https://arxiv.org/abs/2407.18338</link>
      <description><![CDATA[arXiv:2407.18338v1 公告类型：新
摘要：光学化学结构识别 (OCSR) 系统旨在从化学分子图像中提取分子结构信息，通常以分子图或 SMILES 的形式。虽然已经为此目的开发了许多工具，但由于图像中可能存在不同类型的噪声，因此仍然存在挑战。具体来说，我们专注于“箭头推动”图，这是一种典型的化学图像类型，用于展示机械步骤中的电子流动。我们提出了化学反应机制中分子图像的结构分子标识符 (SMiCRM)，这是一个旨在使用箭头推动注释对化学分子的机器识别能力进行基准测试的数据集。它包含 453 张图像，涵盖了广泛的有机化学反应，每个反应都用分子结构和机械箭头说明。SMiCRM 提供了丰富的带注释的分子图像集合，以增强 OCSR 方法的基准测试过程。该数据集包括每幅图像的机器可读分子标识以及显示化学反应过程中电子流动的机械箭头。它为测试分子识别技术提供了更真实、更具挑战性的任务，完成这项任务可以大大丰富计算机提取的化学反应数据中的机械信息。]]></description>
      <guid>https://arxiv.org/abs/2407.18338</guid>
      <pubDate>Tue, 30 Jul 2024 03:16:22 GMT</pubDate>
    </item>
    <item>
      <title>无符号距离场的神经表面检测</title>
      <link>https://arxiv.org/abs/2407.18381</link>
      <description><![CDATA[arXiv:2407.18381v1 公告类型：新
摘要：可以使用传统算法（例如 Marching Cubes）从有符号距离场 (SDF) 中提取表面。但是，由于它们依赖于表面上的符号翻转，因此这些算法不能直接用于无符号距离场 (UDF)。在这项工作中，我们引入了一种深度学习方法来获取 UDF 并将其本地转换为 SDF，以便可以使用现有算法对其进行有效的三角测量。我们表明它在表面检测方面比现有方法具有更好的准确性。此外，它可以很好地推广到看不见的形状和数据集，同时可以并行化。我们还通过将该方法与 DualMeshUDF 结合使用来展示该方法的灵活性，DualMeshUDF 是一种最先进的双网格划分方法，可以在 UDF 上运行，从而改善其结果并消除调整其参数的需要。]]></description>
      <guid>https://arxiv.org/abs/2407.18381</guid>
      <pubDate>Tue, 30 Jul 2024 03:16:22 GMT</pubDate>
    </item>
    <item>
      <title>UOUO：用于测量视觉语言模型知识视野的非语境化不常见对象</title>
      <link>https://arxiv.org/abs/2407.18391</link>
      <description><![CDATA[arXiv:2407.18391v1 公告类型：新
摘要：较小规模的视觉语言模型 (VLM) 通常声称在通用领域的视觉基础和问答基准测试中表现与较大的模型相当，同时在计算效率和存储方面具有优势。然而，它们处理属于数据分布长尾的稀有对象的能力尚不明确。为了严格评估这方面，我们引入了“非语境化不常见对象”(UOUO) 基准。该基准测试侧重于系统地测试具有大参数计数和小参数计数的 VLM 在稀有和特殊对象上的表现。我们的综合分析表明，虽然较小的 VLM 在常见数据集上保持了竞争性能，但它们在涉及不常见对象的任务上表现明显不佳。我们还提出了一种先进的、可扩展的数据收集和清理管道，确保 UOUO 基准测试提供高质量、具有挑战性的实例。这些发现强调了在评估 VLM 的真实能力时需要考虑长尾分布。]]></description>
      <guid>https://arxiv.org/abs/2407.18391</guid>
      <pubDate>Tue, 30 Jul 2024 03:16:22 GMT</pubDate>
    </item>
    <item>
      <title>通过知识提炼利用基础模型进行多目标跟踪：将 DINOv2 特征提炼到 FairMOT</title>
      <link>https://arxiv.org/abs/2407.18288</link>
      <description><![CDATA[arXiv:2407.18288v1 公告类型：新
摘要：多目标跟踪 (MOT) 是一种计算机视觉任务，已在各个领域得到应用。MOT 中的一些常见限制是不同的物体外观、遮挡或拥挤的场景。为了应对这些挑战，机器学习方法得到了广泛部署，利用了大型数据集、复杂的模型和大量的计算资源。由于实际限制，访问上述内容并不总是一种选择。然而，随着著名人工智能公司最近发布基础模型，预训练模型已经使用最先进的方法在大量数据集和资源上进行了训练。这项工作试图通过使用知识蒸馏来利用一种名为 DINOv2 的基础模型。所提出的方法使用师生架构，其中 DINOv2 是老师，FairMOT 主干 HRNetv2 W18 是学生。结果表明，尽管所提出的方法在某些情况下显示出改进，但它并不总是优于原始 FairMOT 模型。这些发现凸显了在知识领域应用基础模型的潜力和局限性]]></description>
      <guid>https://arxiv.org/abs/2407.18288</guid>
      <pubDate>Tue, 30 Jul 2024 03:16:21 GMT</pubDate>
    </item>
    <item>
      <title>MARINE：用于检测动物视频中罕见捕食者与猎物相互作用的计算机视觉模型</title>
      <link>https://arxiv.org/abs/2407.18289</link>
      <description><![CDATA[arXiv:2407.18289v1 公告类型：新
摘要：捕食者和猎物之间的相遇在生态系统中起着至关重要的作用，但它们的稀有性使得它们很难在视频记录中被发现。尽管动作识别 (AR) 和时间动作检测 (AD) 方面的进步，尤其是基于 Transformer 的模型和视觉基础模型，已经在人类动作数据集上取得了高性能，但动物视频仍然相对研究不足。本论文通过提出模型 MARINE 来解决这一差距，该模型利用针对快速动物动作设计的基于运动的帧选择和 DINOv2 特征提取以及可训练的分类头进行动作识别。MARINE 在识别鱼类视频中的捕食者攻击方面优于 VideoMAE，无论是在小型和特定的珊瑚礁数据集（81.53% 对 52.64% 的准确率）上，还是在更广泛的动物王国数据集的子集（94.86% 对 83.14% 的准确率）上。在动物王国代表性样本的多标签设置中，MARINE 实现了 23.79% 的 mAP，在现有基准中处于中等水平。此外，在珊瑚礁数据集的 AD 任务中，尽管 t-IoU 阈值降低至 25%，但 MARINE 实现了 80.78% 的 AP（而 VideoMAE 为 34.89%）。因此，尽管还有改进空间，但 MARINE 提供了一个有效的入门框架，可应用于动物记录的 AR 和 AD 任务，从而为自然生态系统的研究做出贡献。]]></description>
      <guid>https://arxiv.org/abs/2407.18289</guid>
      <pubDate>Tue, 30 Jul 2024 03:16:21 GMT</pubDate>
    </item>
    <item>
      <title>2024年视觉生成的几个问题</title>
      <link>https://arxiv.org/abs/2407.18290</link>
      <description><![CDATA[arXiv:2407.18290v1 Announce Type: new 
摘要：本文不提出任何新算法，而是根据作者的个人理解概述了视觉生成领域的各种问题。这些问题的核心在于如何分解视觉信号，所有其他问题都与这一核心问题密切相关，并源于不合适的信号分解方法。本文旨在引起研究人员对视觉信号分解重要性的关注。]]></description>
      <guid>https://arxiv.org/abs/2407.18290</guid>
      <pubDate>Tue, 30 Jul 2024 03:16:21 GMT</pubDate>
    </item>
    </channel>
</rss>