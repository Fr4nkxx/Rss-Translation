<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Thu, 19 Dec 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>iRBSM：深度隐式 3D 乳房形状模型</title>
      <link>https://arxiv.org/abs/2412.13244</link>
      <description><![CDATA[arXiv:2412.13244v1 公告类型：新
摘要：我们提出了第一个女性乳房的深度隐式 3D 形状模型，该模型建立在最近提出的雷根斯堡乳房形状模型 (RBSM) 的基础上并进行了改进。与基于 PCA 的前身相比，我们的模型采用隐式神经表征；因此，它可以在原始 3D 乳房扫描上进行训练，并且无需进行计算要求高的非刚性配准——这项任务对于没有特征的乳房形状来说尤其困难。由此产生的模型称为 iRBSM，可捕捉详细的表面几何形状，包括乳头和肚脐等精细结构，具有很强的表现力，并且在不同的表面重建任务中优于 RBSM。最后，利用 iRBSM，我们提出了一个原型应用程序，仅从一张图像即可 3D 重建乳房形状。模型和代码可在 https://rbsm.re-mic.de/implicit 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2412.13244</guid>
      <pubDate>Thu, 19 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CompactFlowNet：移动设备上高效的实时光流估计</title>
      <link>https://arxiv.org/abs/2412.13273</link>
      <description><![CDATA[arXiv:2412.13273v1 公告类型：新
摘要：我们提出了 CompactFlowNet，这是第一个用于光流预测的实时移动神经网络，它涉及确定初始帧中每个像素相对于后续帧中相应像素的位移。光流是各种视频相关任务的基本构建块，例如视频恢复、运动估计、视频稳定、对象跟踪、动作识别和视频生成。虽然当前最先进的方法优先考虑准确性，但它们往往忽略了速度和内存使用方面的限制。现有的轻量级模型通常专注于减小尺寸，但仍然表现出高延迟、质量显著受损或针对高性能 GPU 进行了优化，导致移动设备上的性能不佳。本研究旨在通过提出一种新颖的移动设备兼容架构以及对训练管道的增强来开发移动优化的光流模型，从而优化模型以减轻重量、降低内存利用率并提高速度，同时保持最小错误。我们的方法在极具挑战性的 KITTI 和 Sintel 基准测试中表现出优于或可与最先进的轻量级模型相媲美的性能。此外，它实现了显著加快的推理速度，从而在 iPhone 8 上实现了实时运行效率，同时超越了更先进的移动设备上的实时性能水平。]]></description>
      <guid>https://arxiv.org/abs/2412.13273</guid>
      <pubDate>Thu, 19 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>图像配准是一个几何深度学习任务</title>
      <link>https://arxiv.org/abs/2412.13294</link>
      <description><![CDATA[arXiv:2412.13294v1 公告类型：新
摘要：数据驱动的可变形图像配准方法主要依赖于处理网格状输入的操作。然而，对图像应用可变形变换会导致扭曲空间偏离刚性网格结构。因此，具有顺序变形的数据驱动方法必须在每个变形步骤之间应用网格重采样操作。虽然重采样引起的伪影在高分辨率图像中可以忽略不计，但稀疏高维特征网格的重采样会引入影响变形建模过程的误差。从变形场的拉格朗日参考系中汲取灵感，我们的工作引入了一种数据驱动的可变形图像配准的新范式，利用几何深度学习原理对不需要网格的变形进行建模。具体来说，我们将图像特征建模为一组在欧几里得空间中自由移动的节点，在图操作下更新它们的坐标，并动态地重新调整它们的局部邻域。我们采用此公式构建多分辨率可变形配准模型，其中变形层在每个分辨率上迭代细化整体变换，而无需在特征网格上进行中间重采样操作。我们研究了我们的方法在多个医学成像配准任务中完全变形地捕捉大变形的能力。具体来说，我们将我们的方法 (GeoReg) 应用于受试者间脑部 MR 图像和吸气-呼气肺部 CT 图像的配准，其性能与当前最先进的方法相当。我们相信，我们的贡献开辟了研究途径，通过明确建模架构内的变换来减少当前学习配准范式的黑箱性质。]]></description>
      <guid>https://arxiv.org/abs/2412.13294</guid>
      <pubDate>Thu, 19 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FastVLM：视觉语言模型的高效视觉编码</title>
      <link>https://arxiv.org/abs/2412.13303</link>
      <description><![CDATA[arXiv:2412.13303v1 公告类型：新
摘要：缩放输入图像分辨率对于增强视觉语言模型 (VLM) 的性能至关重要，特别是在文本丰富的图像理解任务中。然而，由于堆叠的自注意力层导致的大量标记和高编码延迟，流行的视觉编码器（如 ViT）在高分辨率下变得效率低下。在不同的操作分辨率下，VLM 的视觉编码器可以沿着两个轴进行优化：减少编码延迟并最小化传递给 LLM 的视觉标记数量，从而降低整体延迟。基于对图像分辨率、视觉延迟、标记数和 LLM 大小之间相互作用的全面效率分析，我们引入了 FastVLM，这是一种在延迟、模型大小和准确性之间实现优化权衡的模型。FastVLM 结合了 FastViTHD，这是一种新型混合视觉编码器，旨在输出更少的标记并显着减少高分辨率图像的编码时间。与之前的方法不同，FastVLM 仅通过缩放输入图像就实现了视觉 token 数量和图像分辨率之间的最佳平衡，从而无需进行额外的 token 修剪并简化了模型设计。在 LLaVA-1.5 设置中，FastVLM 在 VLM 基准测试中保持了与之前工作类似的性能，同时将第一个 token 时间 (TTFT) 提高了 3.2$\times$。与最高分辨率 (1152$\times$1152) 下的 LLaVa-OneVision 相比，FastVLM 在 SeedBench 和 MMMU 等关键基准测试中实现了相当的性能，使用相同的 0.5B LLM，但 TTFT 速度提高了 85$\times$，视觉编码器小了 3.4$\times$。]]></description>
      <guid>https://arxiv.org/abs/2412.13303</guid>
      <pubDate>Thu, 19 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>BadSAD：针对深度半监督异常检测的清洁标签后门攻击</title>
      <link>https://arxiv.org/abs/2412.13324</link>
      <description><![CDATA[arXiv:2412.13324v1 公告类型：新
摘要：图像异常检测 (IAD) 在工业检测、医学成像和安全等应用中至关重要。尽管深度半监督异常检测 (DeepSAD) 等深度学习模型取得了进展，但这些模型仍然容易受到后门攻击，带来了重大的安全挑战。在本文中，我们介绍了 BadSAD，这是一种专门针对 DeepSAD 模型设计的新型后门攻击框架。我们的方法涉及两个关键阶段：触发器注入，其中微妙的触发器嵌入到正常图像中，以及潜在空间操纵，将中毒图像定位和聚集在正常图像附近，使触发器看起来是良性的。在基准数据集上进行的大量实验验证了我们攻击策略的有效性，凸显了后门攻击对基于深度学习的异常检测系统构成的严重风险。]]></description>
      <guid>https://arxiv.org/abs/2412.13324</guid>
      <pubDate>Thu, 19 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>为亚马逊视觉搜索系统带来多模态性</title>
      <link>https://arxiv.org/abs/2412.13364</link>
      <description><![CDATA[arXiv:2412.13364v1 公告类型：新
摘要：图像到图像匹配在计算机视觉社区中得到了很好的研究。先前的研究主要集中于训练深度度量学习模型，以匹配查询图像和图库图像之间的视觉模式。在这项研究中，我们表明纯图像到图像匹配会受到与局部视觉模式匹配导致的误报的影响。为了缓解这个问题，我们建议利用视觉语言预训练研究的最新进展。具体来说，我们在深度度量学习中引入了额外的图像文本对齐损失，作为图像到图像匹配损失的约束。通过在文本（例如产品标题）和图像对之间进行额外的对齐，该模型可以明确地从两种模态中学习概念，从而避免匹配低级视觉特征。我们逐步开发了两个变体，即 3 塔和 4 塔模型，其中后者需要再输入一个短文本查询。通过大量实验，我们表明这种变化可以显着改善图像到图像匹配问题。我们进一步利用该模型进行多模态搜索，该模型同时接受图像和重构文本查询以提高搜索质量。离线和在线实验都表明主要指标有显著改善。具体来说，我们发现 3 塔模型的图像匹配点击率相对提高了 4.95%，而 4 塔模型的图像匹配点击率则提高了 1.13%。]]></description>
      <guid>https://arxiv.org/abs/2412.13364</guid>
      <pubDate>Thu, 19 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于 3D 物体识别的目标视角不变对抗扰动</title>
      <link>https://arxiv.org/abs/2412.13376</link>
      <description><![CDATA[arXiv:2412.13376v1 公告类型：新
摘要：对抗性攻击对 3D 物体识别提出了重大挑战，尤其是在涉及多视图分析的场景中，其中可以从不同角度观察物体。本文介绍了视图不变对抗性扰动 (VIAP)，这是一种用于制作在多个视点上仍然有效的稳健对抗性示例的新方法。与传统方法不同，VIAP 能够利用单一通用扰动进行有针对性的攻击，能够操纵识别系统将对象分类为特定的、预先确定的标签。利用 121 个不同渲染的 3D 对象的 1,210 张图像的数据集，我们展示了 VIAP 在有针对性和无针对性设置中的有效性。我们的无针对性扰动成功地产生了对 3D 变换具有鲁棒性的奇异对抗性噪声，而有针对性的攻击取得了卓越的效果，在各种 epsilon 值上的 top-1 准确率超过 95%。这些发现凸显了 VIAP 在实际应用中的潜力，例如测试 3D 识别系统的稳健性。所提出的方法为视角不变对抗性稳健性树立了新标杆，推动了 3D 物体识别对抗性机器学习领域的发展。]]></description>
      <guid>https://arxiv.org/abs/2412.13376</guid>
      <pubDate>Thu, 19 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Marigold-DC：利用引导扩散实现零样本单目深度完成</title>
      <link>https://arxiv.org/abs/2412.13389</link>
      <description><![CDATA[arXiv:2412.13389v1 公告类型：新
摘要：深度补全将稀疏深度测量升级为由传统图像引导的密集深度图。现有的针对这种高度不适定任务的方法在严格约束的设置下运行，并且在应用于训练域之外的图像或可用的深度测量稀疏、分布不规则或密度变化时往往会遇到困难。受到单目深度估计最新进展的启发，我们将深度补全重新定义为由稀疏测量引导的图像条件深度图生成。我们的方法 Marigold-DC 建立在用于单目深度估计的预训练潜在扩散模型的基础上，并通过与去噪扩散的迭代推理同时运行的优化方案将深度观测注入测试时间指导。该方法在各种环境中表现出出色的零样本泛化能力，甚至可以有效处理极其稀疏的指导。我们的结果表明，当代单目深度先验极大地增强了深度补全：最好将任务视为在稀疏深度的指导下从（密集）图像像素中恢复密集深度；而不是在图像的指导下修复（稀疏）深度。项目网站：https://MarigoldDepthCompletion.github.io/]]></description>
      <guid>https://arxiv.org/abs/2412.13389</guid>
      <pubDate>Thu, 19 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MMHMR：用于手部网格恢复的生成式蒙版建模</title>
      <link>https://arxiv.org/abs/2412.13393</link>
      <description><![CDATA[arXiv:2412.13393v1 公告类型：新
摘要：由于复杂的关节、自遮挡和深度模糊性，从单个 RGB 图像重建 3D 手部网格具有挑战性。传统的判别方法学习从 2D 图像到单个 3D 网格的确定性映射，通常会遇到 2D 到 3D 映射中固有的模糊性问题。为了应对这一挑战，我们提出了 MMHMR，这是一种用于手部网格恢复的新型生成式掩蔽模型，它通过从模糊的 2D 到 3D 映射过程的概率分布中学习和采样来合成合理的 3D 手部网格。 MMHMR 由两个关键组件组成：(1) VQ-MANO，它将 3D 手部关节编码为潜在空间中的离散姿势标记；(2) 上下文引导的 Masked Transformer，它随机屏蔽姿势标记并学习它们的联合分布，以损坏的标记序列、图像上下文和 2D 姿势线索为条件。这种学习到的分布有助于在推理过程中进行置信度引导采样，从而产生具有低不确定性和高精度的网格重建。对基准和真实世界数据集的广泛评估表明，MMHMR 在 3D 手部网格重建方面实现了最先进的准确性、稳健性和真实性。项目网站：https://m-usamasaleem.github.io/publication/MMHMR/mmhmr.html]]></description>
      <guid>https://arxiv.org/abs/2412.13393</guid>
      <pubDate>Thu, 19 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大规模分布变化：地球观测中的分布外检测</title>
      <link>https://arxiv.org/abs/2412.13394</link>
      <description><![CDATA[arXiv:2412.13394v1 公告类型：新
摘要：训练强大的深度学习模型对于地球观测至关重要，因为全球部署的模型经常面临分布变化，从而降低性能，尤其是在低数据区域。分布外 (OOD) 检测通过识别与分布内 (ID) 数据不同的输入来解决这一挑战。然而，现有的方法要么假设可以访问 OOD 数据，要么损害主要任务性能，使其不适合实际部署。我们提出了 TARDIS，一种用于可扩展地理空间部署的事后 OOD 检测方法。核心新颖之处在于通过整合来自 ID 数据和未知分布的信息来生成替代标签，从而实现大规模 OOD 检测。我们的方法采用预先训练的模型、ID 数据和 WILD 样本，根据内部激活将后者分解为替代 ID 和替代 OOD 标签，并将二元分类器拟合为 OOD 检测器。我们在 EuroSAT 和 xBD 数据集上验证了 TARDIS，在 17 个实验设置中涵盖了协变量和语义偏移，结果表明它在 13 种情况下分配替代 ID 和 OOD 样本时的表现接近理论上限。为了展示可扩展性，我们在 Fields of the World 数据集上部署了 TARDIS，为大规模部署提供了对预训练模型行为的可行见解。代码可在 https://github.com/microsoft/geospatial-ood-detection 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2412.13394</guid>
      <pubDate>Thu, 19 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用扩散先验进行零样本低光图像增强</title>
      <link>https://arxiv.org/abs/2412.13401</link>
      <description><![CDATA[arXiv:2412.13401v1 公告类型：新
摘要：在增强具有挑战性的劣化源中的图像时，平衡美学质量与保真度是计算摄影的核心目标。在本文中，我们讨论了低光图像增强 (LLIE)，这是一项暗图像通常包含有限可见信息的任务。扩散模型以其强大的图像增强能力而闻名，是解决此问题的自然选择。然而，它们的深度生成先验也会导致幻觉，引入不存在的元素或大幅改变原始场景的视觉语义。在这项工作中，我们引入了一种新颖的零样本方法来控制和改进扩散模型的生成行为，以完成暗到亮图像转换任务。定量指标和定性分析都证明了我们的方法在低光图像增强任务中比现有的最先进方法具有更优越的性能。]]></description>
      <guid>https://arxiv.org/abs/2412.13401</guid>
      <pubDate>Thu, 19 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FlashVTG：用于视频时间基础的特征分层和自适应分数处理网络</title>
      <link>https://arxiv.org/abs/2412.13441</link>
      <description><![CDATA[arXiv:2412.13441v1 公告类型：新
摘要：文本引导的视频时间定位 (VTG) 旨在根据文本描述在未修剪的视频中定位相关片段，包括两个子任务：时刻检索 (MR) 和亮点检测 (HD)。尽管以前的典型方法已经取得了令人称赞的结果，但检索短视频时刻仍然具有挑战性。这主要是由于依赖稀疏和有限的解码器查询，这严重限制了预测的准确性。此外，由于以前的方法根据孤立的预测对预测进行排名，而忽略了更广泛的视频上下文，因此经常会出现次优结果。为了解决这些问题，我们引入了 FlashVTG，这是一个具有时间特征分层 (TFL) 模块和自适应分数细化 (ASR) 模块的框架。 TFL 模块取代了传统的解码器结构，以捕捉跨多个时间尺度的细微视频内容变化，而 ASR 模块则通过整合相邻时刻的上下文和多时间尺度特征来提高预测排名。大量实验表明，FlashVTG 在 MR 和 HD 的四个广泛采用的数据集上均实现了最佳性能。具体来说，在 QVHighlights 数据集上，它将 MR 的 mAP 提高了 5.8%，将 HD 的 mAP 提高了 3.3%。对于短时刻检索，FlashVTG 将 mAP 提高到之前 SOTA 性能的 125%。所有这些改进都是在不增加训练负担的情况下实现的，突显了其有效性。我们的代码可在 https://github.com/Zhuo-Cao/FlashVTG 上找到。]]></description>
      <guid>https://arxiv.org/abs/2412.13441</guid>
      <pubDate>Thu, 19 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DarkIR：强大的低光图像恢复</title>
      <link>https://arxiv.org/abs/2412.13443</link>
      <description><![CDATA[arXiv:2412.13443v1 公告类型：新
摘要：由于环境昏暗和长时间曝光的普遍使用，夜间或黑暗条件下的摄影通常会受到噪音、弱光和模糊问题的影响。虽然在这些条件下去模糊和低光图像增强 (LLIE) 是相关的，但图像恢复中的大多数方法都是单独解决这些任务的。在本文中，我们提出了一种高效且强大的神经网络，用于多任务低光图像恢复。我们没有遵循基于 Transformer 的模型的当前趋势，而是提出了新的注意机制来增强高效 CNN 的感受野。与以前的方法相比，我们的方法降低了参数和 MAC 操作方面的计算成本。我们的模型 DarkIR 在流行的 LOLBlur、LOLv2 和 Real-LOLBlur 数据集上取得了新的最先进结果，能够在现实世界的夜晚和黑暗图像上进行推广。代码和模型位于 https://github.com/cidautai/DarkIR]]></description>
      <guid>https://arxiv.org/abs/2412.13443</guid>
      <pubDate>Thu, 19 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ConDo：绝对姿态回归的持续域扩展</title>
      <link>https://arxiv.org/abs/2412.13452</link>
      <description><![CDATA[arXiv:2412.13452v1 公告类型：新
摘要：视觉定位是一个基本的机器学习问题。绝对姿势回归 (APR) 训练场景相关模型，以有效地将输入图像映射到预定义场景中的相机姿势。然而，许多应用程序的环境不断变化，部署后会出现新姿势或场景条件（天气、几何）下的推理数据。在固定数据集上训练 APR 会导致过度拟合，使其在具有挑战性的新数据上灾难性地失败。这项工作提出了持续域扩展 (ConDo)，它不断收集未标记的推理数据以更新已部署的 APR。ConDo 不是应用对 APR 无效的标准无监督域自适应方法，而是通过从与场景无关的定位方法中提取知识，有效地从未标记的数据中学习。通过从历史和新收集的数据中均匀采样数据，ConDo 可以有效地扩展 APR 的泛化域。构建了具有各种场景类型的大规模基准测试，以评估实际（长期）数据变化下的模型。ConDo 在架构、场景类型和数据变化方面始终显著优于基线。在具有挑战性的场景中（图 1），它将定位误差降低了 7 倍以上（14.8m vs 1.7m）。分析表明，ConDo 对计算预算、重放缓冲区大小和教师预测噪声具有很强的鲁棒性。与模型重新训练相比，ConDo 实现了类似的性能，但速度提高了 25 倍。]]></description>
      <guid>https://arxiv.org/abs/2412.13452</guid>
      <pubDate>Thu, 19 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>预训练密度感知姿态变换器以实现基于 LiDAR 的稳健 3D 人体姿态估计</title>
      <link>https://arxiv.org/abs/2412.13454</link>
      <description><![CDATA[arXiv:2412.13454v1 公告类型：新 
摘要：随着自动驾驶的快速发展，基于激光雷达的 3D 人体姿态估计 (3D HPE) 正在成为研究重点。然而，由于激光雷达捕获点云的噪声和稀疏性，稳健的人体姿态估计仍然具有挑战性。大多数现有方法使用时间信息、多模态融合或 SMPL 优化来纠正有偏差的结果。在这项工作中，我们尝试仅通过建模低质量点云的内在属性来获取 3D HPE 的足够信息。因此，提出了一种简单而强大的方法，它为点云的建模和增强提供了见解。具体而言，我们首先提出一种简洁有效的密度感知姿势变换器 (DAPT) 来获得稳定的关键点表示。通过使用一组联合锚点和精心设计的交换模块，从具有不同密度的点云中提取有效信息。然后利用一维热图来表示关键点的精确位置。其次，提出了一种全面的 LiDAR 人体合成与增强方法对模型进行预训练，使其能够获得更好的人体先验。我们通过随机采样人体位置和方向以及通过添加激光级遮挡来模拟遮挡，从而增加了点云的多样性。我们在多个数据集上进行了广泛的实验，包括带 IMU 注释的 LidarHuman26M、SLOPER4D 和手动注释的 Waymo Open Dataset v2.0 (Waymo)、HumanM3。我们的方法在所有场景中都展示了 SOTA 性能。特别是，与 Waymo 上的 LPFormer 相比，我们将平均 MPJPE 降低了 $10.0mm$。与 SLOPER4D 上的 PRN 相比，我们将平均 MPJPE 显著降低了 $20.7mm$。]]></description>
      <guid>https://arxiv.org/abs/2412.13454</guid>
      <pubDate>Thu, 19 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>