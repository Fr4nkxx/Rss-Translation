<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>CS.CV更新arxiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.cv更新arxiv.org e-print存档。</description>
    <lastBuildDate>Mon, 03 Mar 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>Videoa11y：用于可访问的视频说明的方法和数据集</title>
      <link>https://arxiv.org/abs/2502.20480</link>
      <description><![CDATA[ARXIV：2502.20480V1公告类型：新 
摘要：视频描述对于盲目和低视力（BLV）用户访问视觉内容至关重要。但是，由于培训数据集中人类注释质量的限制，当前用于生成描述的人工智能模型通常会缺乏，从而导致描述无法完全满足BLV用户的需求。为了解决这一差距，我们介绍了VideoA11Y，该方法利用多模式模型（MLLM）和视频可访问性指南来生成针对BLV个体量身定制的描述。使用此方法，我们策划了VideoA11Y-40K，这是针对BLV用户描述的40,000个视频的最大，最全面的数据集。在15个视频类别中进行了严格的实验，其中涉及347名观察参与者，40名BLV参与者和7个专业描述者，表明VideoA11Y描述的表现优于新手人体注释，并且与训练有素的人类注释有关，以清晰，准确性，客观性，描述性和用户满意度具有训练的人类注释。我们使用标准标准和自定义指标评估了VideoA11Y-40K上的模型，表明该数据集上的MLLMS微调可产生高质量的可访问描述。代码和数据集可在https://people-robots.github.io/videoa11y上找到。]]></description>
      <guid>https://arxiv.org/abs/2502.20480</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>egonormia：基准对身体社会规范的理解</title>
      <link>https://arxiv.org/abs/2502.20490</link>
      <description><![CDATA[ARXIV：2502.20490V1公告类型：新 
摘要：人类活动由规范调节。当在现实世界中采取行动时，人类不仅遵循规范，而且还要考虑不同规范之间的权衡，但是，机器通常在规范理解和推理上明确监督，尤其是当规范基于物理和社会环境中时，就经常受到训练。为了改善和评估视觉模型（VLMS）的规范推理能力，我们提出了Egonormia $ \ | \ epsilon \ | $，由1,853个以人类互动为中心的视频组成，每个视频都有两个相关问题，这些问题既评估了规范性行动的预测和正当性。规范行动包括七个类别：安全，隐私，亲近，礼貌，合作，协调/积极性以及沟通/透明度。为了大规模编译该数据集，我们提出了一条新型管道，利用视频采样，自动答案生成，过滤和人类验证。我们的工作表明，当前最新的视觉模型缺乏强大的规范理解，对自我态度的评分最高为45％（与92％的人类基础相比）。我们对每个维度的性能的分析强调了安全，隐私以及应用于现实世界代理商时缺乏协作和沟通能力的重大风险。我们还表明，通过一种基于检索的生成方法，可以使用自我工艺来增强VLMS中的规范推理。]]></description>
      <guid>https://arxiv.org/abs/2502.20490</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>可口可乐：对比字幕人学习胸部X射线视力语言理解的强大时间结构</title>
      <link>https://arxiv.org/abs/2502.20509</link>
      <description><![CDATA[ARXIV：2502.20509V1公告类型：新 
摘要：事实证明，视觉模型对医学图像分析具有很大的好处，因为它们从图像和报告中都学到了丰富的语义。先前的努力专注于更好地对齐图像和文本表示，以增强图像理解。然而，尽管对先前图像的明确引用在胸部X射线（CXR）报告中很常见，但是将进程描述与图像对中语义差异的一致性保持不足。在这项工作中，我们提出了两个组件来解决此问题。 （1）CXR报告处理管道以提取时间结构。它使用大型语言模型（LLM）处理报告，以分开描述和比较上下文，并从报告中提取细粒度的注释。 （2）CXR的对比字幕模型，即可口可乐，以学习如何描述图像及其时间进步。可口可乐结合了一个新型的区域跨意识模块，以识别配对的CXR图像之间的局部差异。广泛的实验表明，与以前的方法相比，可口可乐对两种进展分析和报告产生的优越性。值得注意的是，在MS-CXR-T进展分类上，可口可乐在五个肺部条件下获得了65.0％的平均测试准确性，表现优于先前的最先前的ART（SOTA）模型Biovil-T，高于4.8％。它还可以在模仿CXR上获得24.2％的Radgraph F1，这与Med-Gemini基础模型相当。]]></description>
      <guid>https://arxiv.org/abs/2502.20509</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>最好的脚：强大的脚部重建</title>
      <link>https://arxiv.org/abs/2502.20511</link>
      <description><![CDATA[ARXIV：2502.20511V1公告类型：新 
摘要：准确的3D脚重建对于个性化矫形器，数字医疗保健和虚拟配件至关重要。但是，现有方法在不完整的扫描和解剖变化方面遇到困难，尤其是在用户移动性受到限制的自动扫描场景中，因此很难捕获拱形和脚跟等区域。我们提出了一种新颖的端到端管道，该管道优化了从结构（SFM）重建的结构。它首先使用SE（3）规范化模块来解决扫描对齐歧义，然后通过基于注意的基于注意的基于注意的网络进行综合增强点云。我们的方法在保留临床验证的解剖学忠诚度的同时，实现了重建指标的最新性能。通过将合成训练数据与学习的几何先验相结合，我们可以在现实世界中捕获条件下进行健壮的脚部重建，从而为基于移动的3D扫描提供了新的机会，可以在医疗保健和零售中进行新的机会。]]></description>
      <guid>https://arxiv.org/abs/2502.20511</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>建模合并以增强医学成像分类模型的鲁棒性</title>
      <link>https://arxiv.org/abs/2502.20516</link>
      <description><![CDATA[ARXIV：2502.20516V1公告类型：新 
摘要：模型合并是合并多个模型以增强模型性能的有效策略，并且比集合学习更有效，因为它不会将额外的计算引入推理中。但是，有限的研究探讨了合并过程是否可以在一个模型中发生并增强模型的鲁棒性，这在医学图像领域尤其重要。在本文中，我们是第一个提出模型合并（INMERGE）的人，这是一种新颖的方法，可以通过在训练过程中选择性合并单个卷积神经网络（CNN）深层层中的相似卷积内核来增强模型的鲁棒性。我们还分析揭示了影响如何进行模型合并的重要特征，这是社区的深刻参考。我们证明了该技术对4个普遍数据集上不同CNN体系结构的可行性和有效性。所提出的侵入训练的模型超过了典型的训练模型。该代码将公开。]]></description>
      <guid>https://arxiv.org/abs/2502.20516</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>关于个体差异在当前的计算图像美学方法中的作用</title>
      <link>https://arxiv.org/abs/2502.20518</link>
      <description><![CDATA[ARXIV：2502.20518V1公告类型：新 
摘要：图像美学评估（IAA）评估图像美学，这是一项任务，这是图像多样性和用户主观性复杂的任务。当前的方法在两个阶段解决了这一点：通用IAA（GIAA）模型估计平均美学得分，而个人IAA（PIAA）模型使用转移学习适应GIAA，以纳入用户主观性。但是，缺乏对GIAA和PIAA之间转移学习的理论理解，尤其是关于群体组成，群体规模，群体和个人之间的美学差异以及人口相关性的影响。这项工作为IAA建立了理论基础，提出了一个统一模型，该模型以分布形式以个人和小组评估编码个体特征。我们表明，从GIAA转移到PIAA涉及外推，而反向涉及插值，这通常对机器学习更有效。具有不同组组成的实验，包括按组大小和脱节人口组合进行的子采样，甚至揭示了GIAA的显着性能差异，这表明平均得分并不能完全消除个人主观性。绩效变化和GINI指数分析揭示了教育是影响美学差异的主要因素，其次是摄影和艺术经验，在艺术品中观察到的个人主观性比照片更强。我们的模型独特地支持GIAA和PIAA，从而增强了跨人口统计的概括。]]></description>
      <guid>https://arxiv.org/abs/2502.20518</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>更细致的差异不确定性可以加速化学组织病理学成像</title>
      <link>https://arxiv.org/abs/2502.20532</link>
      <description><![CDATA[ARXIV：2502.20532V1公告类型：新 
摘要：无标签的化学成像具有改善数字病理工作流程的巨大希望。但是，数据采集速度仍然是光滑临床过渡的限制因素。为了解决这一差距，我们提出了一种自适应策略：快速扫描整个组织的低信息（LI）含量，识别具有高分子不确定性（AU）的区域，并有选择地将它们以更好的质量重新形成，以捕获更高的信息（HI）细节。主要的挑战在于区分可以通过HI成像来减轻的高AU区域和无法减轻的区域。但是，由于现有的不确定性框架无法将这种AU子类别分开，因此我们提出了一种基于事后潜在空间分析的细粒度分离方法，以使无法从不可抵消的高AU区域分辨出来。我们将方法应用于乳房组织的有效图像红外光谱数据，与随机基线相比，使用获得的HI数据实现了出色的分割性能。这代表了第一个算法研究，该研究集中在动态图像空间（Li-to-Hi）内的细粒度AU分解，并在简化组织病理学方面进行了新的应用。]]></description>
      <guid>https://arxiv.org/abs/2502.20532</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LISARD：学习图像相似性以防御灰色框对抗攻击</title>
      <link>https://arxiv.org/abs/2502.20562</link>
      <description><![CDATA[ARXIV：2502.20562V1公告类型：新 
摘要：最新的防御机制通常是在白盒攻击的背景下评估的，这是不现实的，因为它假设攻击者可以访问目标网络的梯度。为了防止这种情况，对抗训练（AT）和对抗性蒸馏（AD）在训练阶段包括对抗性示例，对抗性纯化使用生成模型来重建给分类器给出的所有图像。本文考虑了一个更现实的评估方案：灰色框攻击，假设攻击者知道用于训练目标网络的体系结构和数据集，但无法访问其梯度。我们提供了经验证据，表明模型容易受到灰色框攻击的影响，并提出了Lisard，这是一种防御机制，不会增加计算和时间成本，但可以对灰色框和白盒子的攻击提供稳健性，而无需添加AT。我们的方法近似于用扰动和干净的图像的嵌入形成的互相关矩阵，同时进行分类学习，同时进行对角线矩阵。我们的结果表明，Lisard可以有效地防止灰色盒子攻击，可以在多个体系结构中使用，并将其弹性赋予对白色盒子方案。同样，在删除和/或移至灰色盒子设置时，最先进的广告模型在大大的表现不佳，这突出了现有方法在各种条件下（除了白色盒子设置）中缺乏鲁棒性。所有源代码均可在https://github.com/joana-cabral/lisard上找到。]]></description>
      <guid>https://arxiv.org/abs/2502.20562</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>HazardNet：用于实时交通安全性检测到边缘设备的小型视觉语言模型</title>
      <link>https://arxiv.org/abs/2502.20572</link>
      <description><![CDATA[ARXIV：2502.20572V1公告类型：新 
摘要：在当代城市环境中，交通安全仍然是一个至关重要的问题，这是由于车辆的增加和道路网络的复杂性而加剧了。传统的关键安全事件检测系统主要依赖于基于传感器的方法和常规的机器学习算法，需要大量数据收集和复杂的培训过程，以遵守交通安全法规。本文介绍了Hazardnet，这是一种小型视觉语言模型，旨在通过利用高级语言和视觉模型的推理能力来提高交通安全。我们通过微调预先训练的QWEN2-VL-2B模型来构建HazardNet，该模型以其在开源替代方案中的出色性能和紧凑的大小为20亿个参数而选择。这有助于促进具有高效推理吞吐量在边缘设备上的部署。此外，我们提出了Hazardqa，这是一个新颖的视觉问题答案（VQA）数据集，该数据集是专门针对涉及安全至关重要事件的现实世界情景培训Hazardnet构建的。我们的实验结果表明，微型危险网的表现优于基本模型，其F1得分提高了89％，并且与较大的模型（例如GPT-4O）相比，在某些情况下具有可比的结果，在某些情况下提高了6％。这些进步强调了Hazardnet在提供实时，可靠的交通安全事件检测方面的潜力，从而有助于减少事故和改善城市环境中的交通管理。 HazardNet模型和HazArdQA数据集均可分别在https://huggingface.co/tami3/hazardnet和https://huggingface.co/datasetsets/tami3/hazardqa上获得。]]></description>
      <guid>https://arxiv.org/abs/2502.20572</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>城市交叉点的视觉推理：用于交通冲突检测的GPT-4O的鉴定</title>
      <link>https://arxiv.org/abs/2502.20573</link>
      <description><![CDATA[ARXIV：2502.20573V1公告类型：新 
摘要：由于复杂性，频繁的冲突和盲点，未信号的城市交叉点中的交通控制提出了重大挑战。这项研究探讨了利用多模式大型语言模型（MLLM）（例如GPT-4O）的能力，通过直接使用四足相交的鸟类视频视频来提供逻辑和视觉推理。在这种提出的方​​法中，GPT-4O充当智能系统，可检测冲突并为驾驶员提供解释和建议。微调模型的准确度为77.14％，而对微型GPT-4O的真实预测值的手动评估显示，模型生成的解释的精确度为89.9％，对于推荐的下一步动作，对模型生成的解释的精度为92.3％。这些结果强调了使用视频作为输入进行实时流量管理的可行性，从而为交叉点流量管理和操作提供了可扩展且可操作的见解。本研究中使用的代码可在https://github.com/sarimasri3/traffic-intersection-conflict-detection-using-images.git上获得。]]></description>
      <guid>https://arxiv.org/abs/2502.20573</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Instaface：带有单个图像推理的具有身份的面部编辑</title>
      <link>https://arxiv.org/abs/2502.20577</link>
      <description><![CDATA[ARXIV：2502.20577V1公告类型：新 
摘要：面部外观编辑对于数字化身，AR/VR和个性化内容创建，推动现实的用户体验至关重要。但是，通过生成模型保留身份是具有挑战性的，尤其是在数据可用性有限的情况下。传统方法通常需要多个图像，并且仍然在不自然的面部移动，不一致的头发对准或过度平滑效果方面挣扎。为了克服这些挑战，我们介绍了一种新颖的基于扩散的框架Instaface，以生成逼真的图像，同时仅使用单个图像保留身份。 Instaface的核心，我们引入了一个高效的指导网络，该网络通过集成了多个基于3DMM的条件，而无需引入其他可训练的参数来利用3D透视。此外，为了确保最大的身份保留，背景，头发和其他上下文功能（如配件），我们引入了一个新型模块，该模块利用了面部识别模型和预训练的视觉语言模型的特征嵌入。定量评估表明，我们的方法在身份保存，光真相和对姿势，表达和照明的有效控制方面优于几种最先进的方法。]]></description>
      <guid>https://arxiv.org/abs/2502.20577</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用分层稀疏自动编码器解释剪辑</title>
      <link>https://arxiv.org/abs/2502.20578</link>
      <description><![CDATA[ARXIV：2502.20578V1公告类型：新 
摘要：稀疏自动编码器（SAE）可用于检测和转向神经网络中的可解释特征，具有理解复杂的多模式表示的潜力。鉴于它们能够发现可解释的功能，SAE对于分析大规模视觉模型（例如剪辑和siglip）特别有价值，这些模型是现代系统中的基本构建基础，但仍具有挑战性的解释和控制。但是，当前的SAE方法通过同时优化重建质量和稀疏性而受到限制，因为它们依赖于激活抑制或刚性稀疏性约束。为此，我们介绍了Matryoshka Sae（MSAE），这是一种新的体系结构，同时以多种粒度学习层次结构表示，从而可以直接优化两个指标而没有妥协。 MSAE在重建质量和稀疏性之间建立了新的最新帕累托前沿，达到0.99余弦相似性，而差异的差异小于0.1个方差的比例小于0.1，同时保持〜80％的稀疏性。最后，我们通过从其表示中提取超过120个语义概念来表明MSAE作为解释和控制剪辑的工具，以在Celeba等下游任务中执行基于概念的相似性搜索和偏见分析。]]></description>
      <guid>https://arxiv.org/abs/2502.20578</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>RTGEN：实时生成检测变压器</title>
      <link>https://arxiv.org/abs/2502.20622</link>
      <description><![CDATA[ARXIV：2502.20622V1公告类型：新 
摘要：虽然开放式对象检测器在推理过程中需要预定义的类别，但生成对象检测器通过赋予模型具有文本生成功能来克服此限制。但是，现有的生成对象检测方法将自回归语言模型直接附加到对象检测器上，以生成每个检测到的对象的文本。这种直接的设计导致结构冗余和增加的处理时间。在本文中，我们提出了一个实时生成检测变压器（RTGEN），这是一种具有简洁的编码器架构架构的实时生成对象检测器。具体而言，我们介绍了一种新型的区域语言解码器（RL-Decoder），该解码器创新地将非自动回忆性语言模型整合到检测解码器中，从而使对象和文本信息并发处理。通过这些有效的设计，RTGEN的推理速度为60.41 fps。此外，RTGEN在LVIS数据集上获得18.6 MAP，以3.5 MAP优于先前的SOTA方法。]]></description>
      <guid>https://arxiv.org/abs/2502.20622</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>T2ICOUNT：增强零击计数的跨模式理解</title>
      <link>https://arxiv.org/abs/2502.20625</link>
      <description><![CDATA[ARXIV：2502.20625V1公告类型：新 
摘要：零拍对对象计数的目的是计算文本描述指定的任意对象类别的实例。现有方法通常依赖于剪辑等视觉模型，但通常对文本提示表现出有限的敏感性。我们提出了T2icount，这是一个基于扩散的框架，它利用了预处理的扩散模型的丰富先验知识和细粒度的视觉理解。一步降解可确保效率，但会导致文本敏感性降低。为了应对这一挑战，我们提出了一个层次的语义校正模块，该模块逐渐完善了文本图像特征对齐方式，以及代表性的区域连贯性损失，该损失通过利用从DENADY U-NET中提取的交叉注意力图来提供可靠的监督信号。此外，我们观察到当前的基准主要集中在图像中的多数对象上，可能掩盖了模型的文本灵敏度。为了解决这个问题，我们为FSC​​147的重新注册子集做出了挑战，以更好地评估文本指导的计数能力。广泛的实验表明，我们的方法在不同的基准测试中实现了卓越的性能。代码可在https://github.com/cha15yq/t2icount上找到。]]></description>
      <guid>https://arxiv.org/abs/2502.20625</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Tractcloud-fov：基于不完整的视野的扩散MRI中基于深度学习的强大拖拉术</title>
      <link>https://arxiv.org/abs/2502.20637</link>
      <description><![CDATA[ARXIV：2502.20637V1公告类型：新 
摘要：拖拉术分类将从扩散MRI重建为解剖学定义的纤维区域的流线分类，用于临床和研究应用。但是，临床扫描通常具有不完整的视野（FOV），其中大脑区域被部分成像，导致部分或截短的纤维区域。为了应对这一挑战，我们介绍了Tractcloud-fov，这是一个深度学习框架，在不完整的FOV条件下稳健地划分了拖拉机。我们提出了一种新颖的训练策略，即FOV切割增强（FOV-CA），其中我们合成切割拖拉图，以模拟现实世界中的下部FOV截止场景。这种数据增强方法丰富了具有逼真的截断流线的训练集，从而使模型能够实现卓越的概括。我们在合成剪切的拖拉术和两个现实生活中的数据集上评估了所提出的拖拉线果。 TractCloud-Fov在流线分类精度，概括能力，解剖学描述和计算效率方面，在所有测试数据集上的最新方法显着优于几种最新方法。总体而言，TractCloud-fov在扩散MRI中具有不完整的FOV的有效且一致的拖拉术分析。]]></description>
      <guid>https://arxiv.org/abs/2502.20637</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>