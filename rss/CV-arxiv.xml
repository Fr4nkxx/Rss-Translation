<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>CS.CV更新arxiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.cv更新arxiv.org e-print存档。</description>
    <lastBuildDate>Mon, 14 Apr 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>ScreenSpot-Pro：专业高分辨率计算机使用的GUI接地</title>
      <link>https://arxiv.org/abs/2504.07981</link>
      <description><![CDATA[ARXIV：2504.07981V1公告类型：新 
摘要：多模式大语言模型（MLLM）的最新进展已导致开发GUI代理的一般任务，例如Web浏览和手机使用。但是，它们在专业领域中的应用仍然不足。这些专业的工作流对GUI感知模型引入了独特的挑战，包括高分辨率显示器，较小的目标尺寸和复杂的环境。在本文中，我们介绍了ScreenSpot-Pro，这是一种新的基准测试，旨在严格评估MLLM在高分辨率专业环境中的接地能力。该基准包括来自带有专家注释的各种专业领域的真实高分辨率图像。它涵盖了五个行业和三个操作系统的23个应用程序。现有的GUI接地模型在此数据集上的性能很差，最佳模型仅达到18.9％。我们的实验表明，从策略上降低搜索区域会提高准确性。基于此见解，我们提出了Screenseeker，这是一种视觉搜索方法，它利用强大规划师的GUI知识来指导级联的搜索，以48.1％的速度实现了最先进的性能，没有任何其他培训。我们希望我们的基准和发现能够推动为专业应用的GUI代理的开发。可以在https://gui-agent.github.io/grounding-leaderboard上找到代码，数据和排行榜。]]></description>
      <guid>https://arxiv.org/abs/2504.07981</guid>
      <pubDate>Mon, 14 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>我们是否有统一的图像产生和理解？ GPT-4O图像产生能力的实证研究</title>
      <link>https://arxiv.org/abs/2504.08003</link>
      <description><![CDATA[ARXIV：2504.08003V1公告类型：新 
摘要：OpenAI的多模式GPT-4O在图像生成和编辑中表现出了非凡的功能，但其实现世界知识知识的语义综合的能力 - 无需整合域知识，上下文推理和指导依从性 - 依据 - 尚未证实。在这项研究中，我们系统地评估了三个关键维度的这些功能：（1）全球指导依从性，（2）精细颗粒的编辑精度和（3）产后推理。尽管现有基准强调了GPT-4O在图像生成和编辑中的强大功能，但我们的评估揭示了GPT-4O的持续局限性：该模型经常默认用于指令的字面解释，不一致地应用知识限制，并在有条件的推理任务中挣扎。这些发现挑战了有关GPT-4O统一的理解和发电能力的普遍假设，从而在其动态知识整合中揭示了很大的差距。我们的研究要求开发更强大的基准和训练策略，这些策略超出了表面层面的一致性，强调了情境感知和推理的多模式生成。]]></description>
      <guid>https://arxiv.org/abs/2504.08003</guid>
      <pubDate>Mon, 14 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于多功能测试时间适应的自我引导</title>
      <link>https://arxiv.org/abs/2504.08010</link>
      <description><![CDATA[ARXIV：2504.08010V1公告类型：新 
摘要：在本文中，我们试图为各种任务开发一种多功能测试时间适应（TTA）目标 - 跨图像，对象和像素级预测的分类和回归。我们通过一个自我引导的方案实现了这一目标，该方案优化了测试图像（作为目标）及其变质视图之间的预测一致性。关键挑战在于设计有效的增强/恶化，即：i）保留图像的几何信息，例如对象大小和位置，这对于对象/像素级任务上的TTA至关重要，ii）为TTA提供了足够的学习信号。为此，我们分析了共同分布的变化如何影响傅立叶域中空间频率的图像信息功率，并揭示低频组件具有高功率并掩盖这些组件提供更多的学习信号，而掩盖高频组件则不能。鉴于此，我们将图像的低频幅度随机掩盖在其傅立叶域中以进行增强。同时，我们还通过噪声注入来增强图像，以通过增强那里的信息能力来补偿高频的缺少学习信号。实验表明，无论是独立还是作为插件模块，我们的方法都在分类，分割和3D单眼检测任务中都具有Transfesser和CNN模型的较高结果。]]></description>
      <guid>https://arxiv.org/abs/2504.08010</guid>
      <pubDate>Mon, 14 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SRVP：使用基于注意的时空相关融合的强烈回忆视频预测模型</title>
      <link>https://arxiv.org/abs/2504.08012</link>
      <description><![CDATA[ARXIV：2504.08012V1公告类型：新 
摘要：视频预测（VP）通过利用过去框架的空间表示和时间上下文来生成未来的帧。基于传统的复发性神经网络（RNN）模型增强了记忆细胞结构，以捕获长时间的时空状态，但逐渐损失了物体外观细节。为了解决这个问题，我们提出了强大的回忆VP（SRVP）模型，该模型集成了标准注意力（SA）和增强功能注意力（RFA）模块。这两个模块均采用缩放的点产物注意来提取时间上下文和空间相关性，然后将其融合以增强时空表示。在三个基准数据集上的实验表明，SRVP减轻基于RNN的模型中的图像质量降解，同时实现与无RNN架构相当的预测性能。]]></description>
      <guid>https://arxiv.org/abs/2504.08012</guid>
      <pubDate>Mon, 14 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DGFAMBA：学习流量分解的视觉域概括状态空间</title>
      <link>https://arxiv.org/abs/2504.08019</link>
      <description><![CDATA[ARXIV：2504.08019V1公告类型：新 
摘要：域的概括旨在从源域中学习表示形式，可以将其推广到任意看不见的目标域。视觉领域概括的基本挑战是由戏剧性风格变化引起的域间隙，而图像内容稳定。由Vmamba举例说明的选择性状态空间的领域展示了其代表内容时的全球接受领域。但是，很少探索为选择性状态空间开发域不变属性的方式。在本文中，我们提出了一种新型的流量分解状态空间模型，称为DG-Famba，用于视觉领域的概括。为了维持域的一致性，我们通过流量分解来创新地绘制样式增强和原始状态嵌入。在这个潜在的流动空间中，每个样式嵌入的每个状态均由潜在概率路径指定。通过在潜在空间中对齐这些概率路径，无论样式差异如何，状态嵌入能够表示相同的内容分布。在各种视觉域概括设置上进行的广泛实验表明其最先进的性能。]]></description>
      <guid>https://arxiv.org/abs/2504.08019</guid>
      <pubDate>Mon, 14 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过双曲线状态空间幻觉学习细粒域的概括</title>
      <link>https://arxiv.org/abs/2504.08020</link>
      <description><![CDATA[ARXIV：2504.08020V1公告类型：新 
摘要：细粒域的概括（FGDG）旨在学习细粒度的表示，在仅在源域数据上训练时，可以很好地将其概括为看不见的目标域。与通用域的概括相比，FGDG尤其具有挑战性，因为细粒类别只能通过一些微妙而微小的模式来辨别。这种模式在由照明，颜色等引起的跨域样式转移下尤其脆弱。为了推动这种边界，本文提出了一种新型的双曲状态空间幻觉（HSSH）方法。它由两个关键组成部分组成，分别是状态空间幻觉（SSH）和双曲歧管一致性（HMC）。 SSH首先通过推断并幻觉来丰富状态嵌入的样式多样性。然后，前后风格的幻觉状态嵌入被投影到双曲线歧管中。双曲状态空间对高阶统计进行了建模，并可以更好地辨别细粒模式。最后，双曲线距离被最小化，以便可以消除样式变化对细粒模式的影响。三个FGDG基准测试的实验证明了其最先进的性能。]]></description>
      <guid>https://arxiv.org/abs/2504.08020</guid>
      <pubDate>Mon, 14 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>传授人类的微妙差异随扩散</title>
      <link>https://arxiv.org/abs/2504.08046</link>
      <description><![CDATA[ARXIV：2504.08046V1公告类型：新 
摘要：人类的专业知识取决于识别细微的视觉差异的能力，例如区分疾病，物种或天体现象。我们提出了一种新的方法来教新手如何区分专用领域中细微的类别。我们的方法使用生成模型来可视化特征的最小变化，即相反事实之间的过渡到过渡，甚至在数据稀疏，示例不合格的域，并且类别边界也不容易通过文本解释。通过操纵扩散模型的调理空间，我们提出的方法扩散分离类别结构与实例身份相结构，即使在具有挑战性的域中也可以使高保真综合。跨六个领域的实验也显示出准确的过渡，即使跨类别的示例有限且未配对的示例。用户研究证实，我们生成的反事实在教学专业知识中的表现优于未配对的示例，显示了生成模型的专业视觉学习潜力。]]></description>
      <guid>https://arxiv.org/abs/2504.08046</guid>
      <pubDate>Mon, 14 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>斑块分布建模框架自适应余弦估计器（PADIM-ACE）用于合成孔径雷达图像中异常检测和定位</title>
      <link>https://arxiv.org/abs/2504.08049</link>
      <description><![CDATA[ARXIV：2504.08049V1公告类型：新 
摘要：这项工作为合成孔径雷达图像（SAR）中的异常检测和定位提供了一种新的方法，并扩展了现有的斑块分布建模框架（PADIM）。我们介绍了自适应余弦估计器（ACE）检测统计量。 Padim在推理时使用Mahalanobis距离，这是一个无限的度量。 ACE相反，使用余弦相似性度量，提供有界的异常检测分数。跨多个SAR数据集评估了所提出的方法，其性能指标在图像和像素水平上包括接收器操作曲线（AUROC）下的区域，旨在提高SAR成像异常检测和定位的性能。该代码公开可用：https：//github.com/advanced-vision-vision-and-learning-lab/padim-lace。]]></description>
      <guid>https://arxiv.org/abs/2504.08049</guid>
      <pubDate>Mon, 14 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多任务学习，具有多功能三重损失，以改进对象检测</title>
      <link>https://arxiv.org/abs/2504.08054</link>
      <description><![CDATA[ARXIV：2504.08054V1公告类型：新 
摘要：传统上，Triplet损失仅依赖类标签，并且不使用多种类型的注释可用的多任务场景中使用所有可用信息。本文介绍了多功能三重损失（MATL）框架，该框架通过将其他注释（例如边界框信息）与损失公式中的类标签结合在一起，从而扩展了三胞胎损失。通过使用这些互补注释，MATL改善了需要分类和本地化的任务的多任务学习。对空中野生动植物图像数据集的实验表明，MATL在分类和本地化中都胜过常规的三重态损失。这些发现突出了使用所有可用注释在多任务学习框架中使用三重损失的好处。]]></description>
      <guid>https://arxiv.org/abs/2504.08054</guid>
      <pubDate>Mon, 14 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Stei-PCN：通过时空编码和推断的有效的纯卷积网络用于交通预测</title>
      <link>https://arxiv.org/abs/2504.08061</link>
      <description><![CDATA[ARXIV：2504.08061V1公告类型：新 
摘要：流量数据表现出复杂的时间，空间和时空相关性。大多数模型都使用独立的模块来分别提取时间和空间相关性或关节模块，以同步提取它们，而无需考虑时空相关性。此外，考虑关节时空相关性（时间，空间和时空相关性）的模型通常会在准确性和计算效率方面面临重大挑战，这些挑战阻止了此类模型证明关节时空相关结构的预期优势。为了解决这些问题，本文提出了一个有效的纯卷积网络，用于通过时空编码和推断（Stei-PCN）进行交通预测。该模型基于绝对空间和时间坐标以及相对的空间距离和时间距离编码，介绍并设计了动态邻接矩阵推断模块，并使用绘图卷积网络与门控机制相结合，以捕获局部同步的关节空间 - 周期性相关性。此外，使用三层时间扩张的因果卷积网络来捕获远程时间相关性。最后，通过多视图协作预测模块，该模型集成了封闭式激活的原始，局部同步的联合空间时空和远程时间特征，以实现全面的预测。这项研究在流数据集（PEMS03/04/07/08）和速度数据集（PEMS-Bay）上进行了广泛的实验，涵盖了多个预测范围。结果表明，Stei-PCN在训练速度和推理速度上都表现出竞争性的计算效率，并且在大多数评估指标上都表现出优越或略微低于最先进的（SOTA）模型。]]></description>
      <guid>https://arxiv.org/abs/2504.08061</guid>
      <pubDate>Mon, 14 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>X-DECODE：具有课程优化和域均衡的极端去蓝色</title>
      <link>https://arxiv.org/abs/2504.08072</link>
      <description><![CDATA[ARXIV：2504.08072V1公告类型：新 
摘要：恢复严重模糊的图像在计算机视觉中仍然是一个重大挑战，影响了自动驾驶，医学成像和摄影的应用。本文介绍了一种基于课程学习的新型培训策略，以提高深度学习模型的鲁棒性，以实现极端图像造影。与仅在低至中度模糊水平上训练的常规方法不同，我们的方法通过随着时间的推移引入较高的模糊严重程度的图像来逐渐增加难度，从而使模型可以逐步适应。此外，我们在训练过程中整合了感知和铰链损失，以增强细节恢复并提高训练稳定性。我们尝试了各种课程学习策略，并探讨了火车测试域间隙对脱张性能的影响。极端gopro数据集的实验结果表明，在SSIM中，我们的方法优于下一个最佳方法，而极端kitti数据集的实验表明，我们的方法在SSIM中的下一个优于下一个最佳状态。消融研究表明，线性课程的进展优于逐步的，乙状结肠和指数性进度，而诸如训练模糊百分比和损失函数等高参数设置都在解决极端模糊伪影方面起着重要作用。数据集和代码可在https://github.com/raptor-msstate/xdecode上找到]]></description>
      <guid>https://arxiv.org/abs/2504.08072</guid>
      <pubDate>Mon, 14 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>对比gaussian：高保真3D一代与对比度学习和高斯裂开</title>
      <link>https://arxiv.org/abs/2504.08100</link>
      <description><![CDATA[ARXIV：2504.08100V1公告类型：新 
摘要：从单视图像创建3D内容是一个具有挑战性的问题，近年来引起了人们的关注。当前方法通常利用预先训练的2D扩散模型的得分蒸馏采样（SDS）来生成多视图3D表示。尽管某些方法通过平衡生成速度和模型质量取得了显着的进步，但它们的性能通常受到扩散模型输出的视觉不一致的限制。在这项工作中，我们提出了对比的高斯，将对比度学习整合到生成过程中。通过使用感知损失，我们有效地区分了正和负样本，利用视觉上的不一致来提高3D代质量。为了进一步增强样品分化并改善对比度学习，我们结合了一个超分辨率模型，并引入了另一个数量感知的三重态损失，以解决训练过程中不同样本分布的问题。我们的实验表明，我们的方法实现了优越的纹理保真度并提高了几何一致性。]]></description>
      <guid>https://arxiv.org/abs/2504.08100</guid>
      <pubDate>Mon, 14 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>迈向无约束的2D姿势估计人脊柱</title>
      <link>https://arxiv.org/abs/2504.08110</link>
      <description><![CDATA[ARXIV：2504.08110V1公告类型：新 
摘要：我们提供SpinEtrack，这是第一个用于不受限制设置中2D脊柱姿势估算的综合数据集，以满足运动分析，医疗保健和现实动画的关键需求。现有的姿势数据集通常将脊柱简化为一个刚性段，俯瞰精确运动分析所需的细微迹线。相比之下，Spinetrack注释了两个互补子集中的九个详细的脊柱关键点：一种合成集，其中包括使用Unreal Engine创建的25K注释，该杂货通过使用OpenSim进行生物力学对齐，以及一个由超过33K的现实设置组成的，该集合由超过33K的注释，通过一台通过活跃的学习管道策划的33K注释，可通过遍历自动化的人类供应。这种集成的方法可确保在解剖学上一致的标签，即使是为了具有挑战性的野外图像。我们进一步引入了脊柱，使用知识蒸馏和解剖正则化策略来扩展最先进的身体姿势估计量，以共同预测身体和脊柱关键。我们在一般和特定运动环境中的实验验证了Spinetrack对精确脊柱构成估计的有效性，为野外的晚期生物力学分析和3D脊柱重建建立了稳健的基础。]]></description>
      <guid>https://arxiv.org/abs/2504.08110</guid>
      <pubDate>Mon, 14 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>诗：通过MLLM控件精确的对象级编辑</title>
      <link>https://arxiv.org/abs/2504.08111</link>
      <description><![CDATA[ARXIV：2504.08111V1公告类型：新 
摘要：扩散模型已显着改善了文本形象的生成，从文本描述中产生了高质量的现实图像。除了产生外，对象级图像编辑仍然是一个具有挑战性的问题，需要精确的修改，同时保持视觉连贯性。现有的基于文本的教学编辑方法与本地化形状和布局转换相努力，通常会引入意想不到的全球变化。基于图像相互作用的方法提供了更好的准确性，但需要手动人工努力来提供精确的指导。为了减少这种手动努力，同时保持高图像编辑精度，在本文中，我们提出了诗歌，这是使用多模式大语言模型（MLLM）精确对象级编辑的框架。诗歌利用MLLM分析教学提示并在转换前后生成精确的对象掩码，从而无需大量的用户输入即可获得细粒度的控制。该结构化推理阶段指导基于扩散的编辑过程，以确保准确的对象定位和转换。为了评估我们的方法，我们介绍了基于Pascal VOC 2012的基准数据集介绍Vocedits，并增强了教学编辑提示，地面真相转换和精确的对象掩码。实验结果表明，与基于相互作用的方法相比，诗的精度和可靠性优于现有的基于文本的图像编辑方法，同时减少手动努力。]]></description>
      <guid>https://arxiv.org/abs/2504.08111</guid>
      <pubDate>Mon, 14 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于合成孔径雷达图像异常检测（SARIAD）算法的基准测试套件</title>
      <link>https://arxiv.org/abs/2504.08115</link>
      <description><![CDATA[ARXIV：2504.08115V1公告类型：新 
摘要：异常检测是计算机视觉和机器学习中的关键研究挑战，其应用在从质量控制到雷达成像的许多领域中进行。在雷达成像中，特别是合成孔径雷达（SAR），可用于分类，检测和分割感兴趣的对象。但是，没有在SAR图像上开发和基准这些方法的方法。为了解决这个问题，我们引入了SAR图像异常检测（SARIAD）。与Anomalib（用于异常检测的深度学习库Anomalib）一起，Sariad提供了一套全面的算法和数据集，用于评估和开发SAR图像的异常检测方法。 SARIAD专门集成了多个SAR数据集以及工具，可以有效地将各种异常检测算法应用于SAR图像。提供了几个异常检测指标和可视化。总体而言，SARIAD充当基准测试SAR模型和数据集的中央软件包，以允许在SAR Imagery中在异常检测领域进行可重现的研究。该软件包公开可用：https：//github.com/advanced-vision-vision-and-learning-lab/sariad。]]></description>
      <guid>https://arxiv.org/abs/2504.08115</guid>
      <pubDate>Mon, 14 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>