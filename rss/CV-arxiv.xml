<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>CS.CV更新arxiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.cv更新arxiv.org e-print存档。</description>
    <lastBuildDate>Mon, 24 Mar 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>Adams Bashforth Moulton求解器用于倒置和编辑</title>
      <link>https://arxiv.org/abs/2503.16522</link>
      <description><![CDATA[ARXIV：2503.16522V1公告类型：新 
摘要：整流的流模型在图像和视频生成任务中取得了显着的性能。但是，现有的数值求解器面临快速采样和高准确解决方案之间的权衡，从而限制了它们在重建和编辑等下游应用中的有效性。为了应对这一挑战，我们提出了利用Adams-Bashforth-Moulton（ABM）预测器 - 矫正器方法，以增强整流流模型中ODE求解的准确性。具体来说，我们介绍了ABM溶剂，该溶剂剂集成了多步预测器校正方法，以减少局部截断误差并采用自适应步骤尺寸调整以提高采样速度。此外，为了在促进语义修改的同时有效地保留非编辑区域，我们引入了掩盖引导的特征注入模块。我们估计自相似性，以产生一个空间面具，该面具将保存的区域与可用于编辑的区域区分开来。在多个高分辨率图像数据集上进行的广泛实验验证了ABM溶剂可显着提高反转精度和编辑质量，超过现有的求解器，而无需其他培训或优化。]]></description>
      <guid>https://arxiv.org/abs/2503.16522</guid>
      <pubDate>Mon, 24 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>视力 - 语言实施方案，用于单眼深度估计</title>
      <link>https://arxiv.org/abs/2503.16535</link>
      <description><![CDATA[ARXIV：2503.16535V1公告类型：新 
摘要：深度估计是机器人感知和视觉任务中的核心问题，但是来自单个图像的3D重建呈现出固有的不确定性。当前的深度估计模型主要依赖于对监督培训的间形间关系，通常忽略相机本身提供的内在信息。我们提出了一种将相机模型及其物理特征体现为深度学习模型的方法，通过与道路环境的实时互动来计算场景深度。该模型只能使用相机的固有属性，而没有任何其他设备，可以基于立即的环境变化来实时计算体现场景深度。通过将体现的场景深度与RGB图像特征相结合，该模型可以在几何和视觉细节上获得全面的观点。此外，我们将包含环境内容和深度信息的文本描述作为场景理解的先验，丰富了模型对对象的感知。图像和语言的这种整合 - 两种固有的模棱两可的方式 - 利用它们的互补优势来进行单眼深度估计。体现语言和深度先验模型的实时性质可确保该模型可以在动态环境中不断调整其感知和行为。实验结果表明，体现的深度估计方法可以增强不同场景的模型性能。]]></description>
      <guid>https://arxiv.org/abs/2503.16535</guid>
      <pubDate>Mon, 24 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用视觉语言模型进行开放式摄影实例分割和跟踪</title>
      <link>https://arxiv.org/abs/2503.16538</link>
      <description><![CDATA[ARXIV：2503.16538V1公告类型：新 
摘要：本文介绍了一种新颖的方法，该方法通过将视觉模型（VLMS）与已建立的开放式视频检测方法（OVD），实例分段和跟踪的方法集成来利用视觉模型的能力（VLM）。我们利用VLM生成的结构化描述来识别可见的对象实例，收集相关的属性，并告知开放式摄影探测器，以提取传递给视频细分模型的相应边界框，以提供精确的细分蒙版和跟踪功能。一旦初始化，该模型就可以直接提取分割掩码，从而在最小的计算开销中实时处理图像流。可以根据需要在线更新曲目，通过生成新的结构化描述和相应的开放式摄影检测。这将VLM的描述能力与OVD的接地能力和像素级的理解和视频分割的速度相结合。我们跨数据集和机器人平台的评估证明了这种方法的广泛适用性，展示了其在动态环境中从非标准对象中提取特定于任务属性的能力。]]></description>
      <guid>https://arxiv.org/abs/2503.16538</guid>
      <pubDate>Mon, 24 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>防御生物医学图像的梯度反演攻击通过可学习的数据扰动</title>
      <link>https://arxiv.org/abs/2503.16542</link>
      <description><![CDATA[ARXIV：2503.16542V1公告类型：新 
摘要：共享医疗保健数据和临床研究合作的越来越多的需求引起了隐私问题。由于恶意攻击而导致的健康信息泄漏会导致严重的问题，例如误诊和患者识别问题。近年来，具有隐私机器学习（PPML）和隐私增强技术，尤其是联合学习（FL），作为平衡隐私保护与数据效用的创新解决方案；但是，它们也遭受了固有的隐私脆弱性。 Gradient inversion attacks constitute major threats to data sharing in federated learning.研究人员提出了许多防御梯度反演攻击的防御能力。但是，当前用于医疗保健数据的防御方法缺乏普遍性，即现有解决方案可能不适用于来自更广泛人群的数据。此外，使用非医疗保健数据对大多数现有的防御方法进行了测试，这引起了人们对其对现实医疗保健系统的适用性的担忧。在这项研究中，我们为联邦学习中的梯度反演攻击提供了防御。我们利用通用图像数据集和医疗图像数据集使用潜在数据扰动和最小值优化实现这一目标。将我们的方法与两个基准进行了比较，结果表明，我们的方法可以优于基准，而攻击者在对重建的图像进行分类时的准确性下降了12.5％。所提出的方法在原始图像和重建图像之间的平均平方误差（MSE）的同一水平相同级别（约90％的客户端分类精度）之间的平均平方误差（MSE）增加了12.4％。结果表明，可推广的防御对医疗保健数据的潜力。]]></description>
      <guid>https://arxiv.org/abs/2503.16542</guid>
      <pubDate>Mon, 24 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>深入CNN的建筑进步的全面调查：挑战，应用和新兴的研究方向</title>
      <link>https://arxiv.org/abs/2503.16546</link>
      <description><![CDATA[ARXIV：2503.16546V1公告类型：新 
摘要：深度卷积神经网络（CNN）具有明显的深入学习，在计算机视觉，自然语言处理，医学诊断，对象检测和语音识别方面取得突破。建筑创新，包括1D，2D和3D卷积模型，扩张和分组的卷积，深度可分离的卷积以及注意机制解决了特定领域的挑战，并提高了特征表示和计算效率。结构性改进，例如空间通道开发，多路径设计和特征映射增强功能，有助于稳健的层次特征提取和改进的概括，尤其是通过转移学习。有效的预处理策略，包括傅立叶变换，结构化变换，低精度计算和权重压缩，优化推理速度并促进在资源约束环境中的部署。这项调查提出了一种统一的分类法，该分类法根据空间剥削，多路径结构，深度，宽度，维度扩展，渠道增强和注意机制对CNN体系结构进行了分类。它系统地回顾了CNN在面部识别，姿势估计，行动识别，文本分类，统计语言建模，疾病诊断，放射学分析，加密货币情绪预测，1D数据处理，视频分析和语音识别方面的应用。除了巩固建筑的进步之外，审查还重点介绍了新兴的学习范例，例如射击，零射击，弱监督，联合学习框架和未来的研究方向，包括混合CNN转换器模型，视觉语言整合，生成性学习等。本综述，综述提供了对CNN的综合范围的范围，可在2015年及205号的范围内进行挑战，并提供了2025的范围。机会。]]></description>
      <guid>https://arxiv.org/abs/2503.16546</guid>
      <pubDate>Mon, 24 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>数学流：增强视觉数学问题MLLM的感知流程</title>
      <link>https://arxiv.org/abs/2503.16549</link>
      <description><![CDATA[ARXIV：2503.16549V1公告类型：新 
摘要：尽管各种任务的表现令人印象深刻，但多模式的大语言模型（MLLM）尚未完全证明其在可视数学问题解决问题中的潜力，尤其是在准确地感知和解释图中。受到人类典型过程的启发，我们假设从图中提取有意义信息的感知能力至关重要，因为它直接影响了随后的推理过程。为了验证这一假设，我们开发了Flowverse，这是一个综合基准，将问题解决过程中使用的所有信息分为四个组件，然后将其合并为六个问题版本以进行评估。我们在流动方面的初步结果表明，现有的MLLM在从图表中提取基本信息和理性属性并根据这些视觉输入进行复杂的推理时会显示出重大局限性。作为响应，我们引入了MathFlow，这是一种模块化问题的管道，将感知和推断分解为不同的阶段，从而独立优化了每个阶段。鉴于在当前MLLM中观察到的感知局限性，我们训练了MathFlow-P-7B作为专用感知模型。实验结果表明，当与各种封闭源和开源推理模型集成时，MathFlow-P-7b会产生可观的性能。这证明了数学流水线的有效性及其与各种推理框架的兼容性。流量基准和代码可在https://github.com/mathflow-zju/mathflow上找到。]]></description>
      <guid>https://arxiv.org/abs/2503.16549</guid>
      <pubDate>Mon, 24 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Reval：对大视力语言模型的可靠性和价值的理解评估</title>
      <link>https://arxiv.org/abs/2503.16566</link>
      <description><![CDATA[ARXIV：2503.16566V1公告类型：新 
摘要：大型视觉模型（LVLM）的快速演变突出了进行全面评估框架的必要性，这些框架跨越各个方面评估了这些模型。尽管现有的基准将重点放在特定方面，例如感知能力，认知能力和针对对抗性攻击的安全性，但它们通常缺乏对LVLMS的优势和局限性提供整体理解所需的广度和深度。为了解决此差距，我们引入了Reval，Reval是一个综合基准，旨在评估\ textbf {re}责任和\ textbf {val {val} ue lvlms。 REVAL encompasses over 144K image-text Visual Question Answering (VQA) samples, structured into two primary sections: Reliability, which assesses truthfulness (\eg, perceptual accuracy and hallucination tendencies) and robustness (\eg, resilience to adversarial attacks, typographic attacks, and image corruption), and Values, which evaluates ethical concerns (\eg, bias and moral理解），安全问题（\，例如，毒性和越狱漏洞）以及隐私问题（\ f，隐私意识和隐私泄漏）。我们评估了26个型号，包括主流开源LVLM和诸如GPT-4O和GEMINI-1.5-PRO的突出封闭源模型。我们的发现表明，尽管当前的LVLM在感知任务和避免毒性方面表现出色，但它们在对抗场景，隐私保护和道德推理中表现出很大的脆弱性。这些见解强调了未来改进的关键领域，指导了更安全，可靠和道德上一致的LVLM的发展。 Reval为研究人员提供了一个强大的框架，以系统地评估和比较LVLM，从而促进该领域的进步。]]></description>
      <guid>https://arxiv.org/abs/2503.16566</guid>
      <pubDate>Mon, 24 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>来自AI图像生成机器人控制的世界知识</title>
      <link>https://arxiv.org/abs/2503.16579</link>
      <description><![CDATA[ARXIV：2503.16579V1公告类型：新 
摘要：与世界机器人进行互动时，面临许多困难的问题，必须在需要做出选择的情况下做出决定时做出决策，通常没有明确定义的正确和错误答案。另一方面，人类通常可以依靠他们的知识和经验来填补空白。例如，组织新购买的农产品在冰箱中的简单任务涉及决定单独放置每件事的位置，如何有意义地将它们放在一起，例如将相关的内容放在一起，虽然没有明确的正确和错误的方法来完成这项任务。我们可以将有关如何将这些事情明确地进行到机器人的知识库中进行编码，但是考虑到机器人可能遇到的潜在任务和情况的数量，这很快就会变得不知所措。但是，现实世界的图像通常会隐式编码此类问题的答案，并可以显示对象的配置有意义或通常被人类使用。完整的冰箱图像可以提供大量有关通常如何相对于彼此和整个冰箱的内容的信息。现代生成系统能够生成现实世界中合理的图像，并且可以在机器人运行的环境下进行条件。在这里，我们调查了使用有关现代生成AI系统世界的隐性知识的想法，它们具有产生令人信服的现实世界图像解决未指定任务的能力。]]></description>
      <guid>https://arxiv.org/abs/2503.16579</guid>
      <pubDate>Mon, 24 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>UNIK3D：通用摄像头单眼3D估计</title>
      <link>https://arxiv.org/abs/2503.16591</link>
      <description><![CDATA[ARXIV：2503.16591V1公告类型：新 
摘要：单眼3D估计对于视觉感知至关重要。但是，当前方法通过依靠过度简化的假设（例如针孔摄像头模型或整流图像）而降低。这些局限性严重限制了它们的一般适用性，在使用鱼眼或全景图像的现实情况下导致性能差，并导致大量背景损失。为了解决这个问题，我们提出了Unik3D，这是一种能够对任何相机建模的单眼3D估计方法。我们的方法引入了球形3D表示，该表示可以更好地解开相机和场景几何形状，并实现了无约束的相机模型的准确度量3D重建。我们的摄像头组件以射线铅笔的新颖，独立于模型的表示，通过学习的球形谐波叠加来实现。我们还引入了角度损耗，该损失与摄像头模块设计一起防止了宽视觉摄像机的3D输出的收缩。对13个不同数据集进行了全面的零摄像评估，这表明了UNIK3D在3D，深度和摄像机指标中的最先进性能，在挑战的大型视野和全景设置方面具有很大的收益，同时保持了传统的常规小针域中的最高准确性。代码和型号可在github.com/lpiccinelli-eth/unik3d上找到。]]></description>
      <guid>https://arxiv.org/abs/2503.16591</guid>
      <pubDate>Mon, 24 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从单个图像中生成3D世界的食谱</title>
      <link>https://arxiv.org/abs/2503.16611</link>
      <description><![CDATA[ARXIV：2503.16611V1公告类型：新 
摘要：我们介绍了一种食谱，以通过将任务作为2D授课模型的秘密学习问题来从单个图像中产生沉浸式3D世界。这种方法需要最少的培训，并使用现有的生成模型。我们的过程涉及两个步骤：使用预训练的扩散模型生成相干全景图，并使用度量深度估计器将其提升为3D。然后，我们通过在渲染点云上调节介绍模型来填充未观察到的区域，需要微调的微调。在合成图像和真实图像上测试，我们的方法可产生适合VR显示的高质量3D环境。通过从一开始就明确对生成环境的3D结构进行建模，我们的方法始终超过了最先进的基于视频综合的方法，沿多个定量图像质量指标。项目页面：https：//katjaschwarz.github.io/worlds/]]></description>
      <guid>https://arxiv.org/abs/2503.16611</guid>
      <pubDate>Mon, 24 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>医学图像分割的进行性测试时间适应</title>
      <link>https://arxiv.org/abs/2503.16616</link>
      <description><![CDATA[arxiv：2503.16616v1公告类型：新 
摘要：我们为医学图像分割提出了一种模型，渐进的测试时间适应方法。在各种医疗数据集中保持模型性能是具有挑战性的，因为分布的变化是由不一致的成像协议和患者变化引起的。与需要多次通过目标数据的域适应方法（在临床环境中不切实际）不同 - 我们的方法在处理测试数据时逐渐适应了预验证的模型。我们的方法利用了对源数据训练的形状能量模型，该模型在源数据上分配了能量得分以分割图：低能代表分布式（准确）形状，而高能信号不分配（错误）预测。通过在测试时间最小化此能量评分，我们完善分割模型以与目标分布保持一致。为了验证有效性和适应性，我们评估了跨越心脏，脊髓和肺部分段的八个公共MRI（BSSFP，T1-和T2加权）和X射线数据集的框架。我们始终在定量和质量上都超过基本线。]]></description>
      <guid>https://arxiv.org/abs/2503.16616</guid>
      <pubDate>Mon, 24 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MobilePlantVit：一种用于通用植物疾病图像分类的移动友好型混合VIT</title>
      <link>https://arxiv.org/abs/2503.16628</link>
      <description><![CDATA[ARXIV：2503.16628V1公告类型：新 
摘要：植物疾病通过降低农作物产量和破坏农业可持续性，从而极大地威胁着全球粮食安全。 AI驱动的自动分类已成为一种有前途的解决方案，深度学习模型在植物疾病鉴定中表现出了令人印象深刻的表现。但是，由于较高的计算需求和资源限制，将这些模型部署在移动设备和边缘设备上仍然具有挑战性，这突出了对可访问的智能农业系统轻巧，准确的解决方案的需求。为了解决这个问题，我们建议MobilePlantVit，这是一种新型混合视觉变压器（VIT）架构，旨在广泛性植物疾病分类，可在保持高性能的同时优化资源效率。各种规模的不同植物疾病数据集进行了广泛的实验表明，我们的模型的有效性和强大的普遍性，实现了从80％到99％以上的测试精确度。值得注意的是，尽管有较高的参数计数，但我们的体系结构只有69万参数的效果优于MobileVitv1和MobileVitv2的最小版本。这些结果强调了我们方法对可持续和资源有效的智能农业系统中现实世界中的自动化植物疾病分类的潜力。所有代码都将在GitHub存储库中提供：https：//github.com/moshiurtonmoy/mobileplantvit]]></description>
      <guid>https://arxiv.org/abs/2503.16628</guid>
      <pubDate>Mon, 24 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>iflame：与有效的网格产生的全面关注和线性关注</title>
      <link>https://arxiv.org/abs/2503.16653</link>
      <description><![CDATA[ARXIV：2503.16653V1公告类型：新 
摘要：本文提出了Iflame，这是一种基于变压器的新型网络架构，用于网格生成。尽管基于注意力的模型在网格生成中表现出了显着的性能，但它们的二次计算复杂性限制了可扩展性，尤其是对于高分辨率3D数据。相反，线性注意机制提供了较低的计算成本，但通常很难捕获长期依赖性，从而导致了次优的结果。为了解决这一权衡，我们提出了一个交织的自回归网格生成框架，将线性注意力的效率与全部注意机制的表现力相结合。为了进一步提高效率并利用网格表示的固有结构，我们将这种交织方法整合到沙漏体系结构中，从而大大提高了效率。我们的方法减少了训练时间，同时实现了与纯粹的基于注意力的模型相当的性能。为了提高推论效率，我们实施了一种缓存算法，该算法几乎使速度翻了一番，并且与原始变压器相比，KV高速缓存的大小降低了八分之七。我们在Shapenet和Objaverse上评估了我们的框架，证明了其有效生成高质量3D网格的能力。我们的结果表明，提议的交织框架有效地平衡了计算效率和生成性能，从而使其成为网格生成的实用解决方案。在39K数据上，训练只需4 GPU，最多4K面孔的训练。]]></description>
      <guid>https://arxiv.org/abs/2503.16653</guid>
      <pubDate>Mon, 24 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>当较少足够的时候：自适应令牌降低以进行有效的图像表示</title>
      <link>https://arxiv.org/abs/2503.16660</link>
      <description><![CDATA[Arxiv：2503.16660V1公告类型：新 
摘要：Vision编码通常会产生大量的视觉令牌，提供信息丰富的表示形式，但大大增加了计算需求。这就提出了一个问题，即所有产生的令牌是否同样有价值，或者是否可以丢弃其中的一些代币以降低计算成本而不会损害质量。在本文中，我们介绍了一种新方法来确定功能实用程序的基础，即可以从更有价值的功能中重建较低的价值功能。我们通过将自动编码器与Gumbel-Softmax选择机制集成，从而实现此概念，该机制允许仅识别和保留最有用的视觉令牌。为了验证我们的方法，我们使用我们的方法和随机选择的功能选择的功能比较了LLAVA-NEXT模型的性能。我们发现，在基于OCR的任务上，可以以最小的性能损失来消除超过50％的视觉上下文，而随机丢弃相同比例的功能会显着影响模型功能。此外，在一般域任务中，即使仅随机保留30％的令牌可以实现与使用完整的视觉令牌相当的性能。我们的结果突出了一个有希望的自适应和高效多模式修剪的方向，可促进可扩展和低超过的推理而不会损害性能。]]></description>
      <guid>https://arxiv.org/abs/2503.16660</guid>
      <pubDate>Mon, 24 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>TextBite：逻辑页细分的历史捷克文档数据集</title>
      <link>https://arxiv.org/abs/2503.16664</link>
      <description><![CDATA[ARXIV：2503.16664V1公告类型：新 
摘要：逻辑页面细分是文档分析中的重要一步，可以使更好的语义表示，信息检索和文本理解。以前的方法依赖于OCR或精确的几何形状来定义通过文本或几何对象定义逻辑分割。为了避免需要OCR，我们将任务纯粹定义为图像域中的分割。此外，为了确保评估不受不会影响文本分割的几何变化影响，我们建议仅在评估度量中使用前景文本像素，而无视所有背景像素。为了支持逻辑文档细分中的研究，我们介绍了TextBite，TextBite是跨越18至20世纪的历史捷克文档数据集，其中包含来自报纸，词典和手写记录的各种布局。该数据集包含8,449个页面图像，具有78,863个逻辑和主题相干文本的注释段。我们提出了一组将文本区域检测和关系预测结合的基线方法。可以在https://github.com/dcgm/textbite-dataset上访问数据集，基准和评估框架。]]></description>
      <guid>https://arxiv.org/abs/2503.16664</guid>
      <pubDate>Mon, 24 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>