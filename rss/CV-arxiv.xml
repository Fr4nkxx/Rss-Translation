<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Thu, 05 Sep 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>使用自监督学习识别下水道管道中的缺陷</title>
      <link>https://arxiv.org/abs/2409.02140</link>
      <description><![CDATA[arXiv:2409.02140v1 公告类型：新
摘要：污水基础设施是最昂贵的现代投资之一，需要合格人员进行耗时的人工检查。我们的研究解决了对自动化解决方案的需求，而无需依赖大量标记数据。我们提出了一种用于下水道检查的自监督学习 (SSL) 的新应用，为缺陷检测提供了一种可扩展且经济高效的解决方案。我们使用的模型比文献中发现的其他方法至少小 5 倍，获得了具有竞争力的结果，并且在使用更大的架构进行训练时，使用 10% 的可用数据获得了具有竞争力的性能。我们的研究结果强调了 SSL 在资源有限的情况下彻底改变下水道维护的潜力。]]></description>
      <guid>https://arxiv.org/abs/2409.02140</guid>
      <pubDate>Thu, 05 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>EgoPressure：自我中心视觉中的手部压力和姿势估计数据集</title>
      <link>https://arxiv.org/abs/2409.02224</link>
      <description><![CDATA[arXiv:2409.02224v1 公告类型：新
摘要：估计自我中心视觉中的触摸接触和压力是增强现实、虚拟现实以及许多机器人应用中下游应用的核心任务，因为它可以提供有关手部与物体交互和物体操纵的精确物理洞察。然而，现有的接触压力数据集缺乏自我中心视图和手势，而这些对于 AR/VR 交互和机器人操作的现场操作期间的准确估计至关重要。在本文中，我们介绍了 EgoPressure，这是一个从自我中心角度进行的触摸接触和压力交互的新数据集，并补充了手势网格和每个接触的细粒度压力强度。我们数据集中的手势使用我们提出的基于多视图序列的方法进行优化，该方法处理来自我们的 8 个经过精确校准的 RGBD 摄像机的捕捉装置的镜头。 EgoPressure 包括 21 名参与者的 5.0 小时触摸接触和压力互动，这些互动由移动的 egocentric 摄像头和 7 个固定的 Kinect 摄像头捕捉，可提供 30 Hz 的 RGB 图像和深度图。此外，我们还提供了使用不同方式估计压力的基线，这将支持未来对数据集的开发和基准测试。总体而言，我们证明了压力和手势是互补的，这支持了我们更好地促进对 AR/VR 和机器人研究中手与物体交互的物理理解的意图。]]></description>
      <guid>https://arxiv.org/abs/2409.02224</guid>
      <pubDate>Thu, 05 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一种用于精神障碍检测的新型视听信息融合系统</title>
      <link>https://arxiv.org/abs/2409.02243</link>
      <description><![CDATA[arXiv:2409.02243v1 公告类型：新
摘要：精神障碍是全球医疗保健挑战的主要因素之一。研究表明，及时诊断和干预对于治疗各种精神障碍至关重要。然而，某些精神障碍的早期躯体化症状可能不会立即显现，常常导致疏忽和误诊。此外，传统的诊断方法需要花费大量的时间和成本。基于 fMRI 和 EEG 的深度学习方法提高了精神障碍检测过程的效率。然而，设备和训练有素的工作人员的成本通常很高。此外，大多数系统仅针对特定的精神障碍进行训练，而不是通用的。最近，生理研究表明，一些精神障碍（例如抑郁症和多动症）存在一些与言语和面部相关的症状。在本文中，我们关注精神障碍的情绪表达特征，并介绍一种基于视听信息输入的多模态精神障碍诊断系统。我们提出的系统基于时空注意力网络，创新性地使用计算密集度较低的预训练音频识别网络来微调视频识别模块以获得更好的结果。我们还首次将统一系统应用于多种精神障碍（ADHD 和抑郁症）。所提出的系统在真实的多模态 ADHD 数据集上实现了超过 80% 的准确率，并在抑郁症数据集 AVEC 2014 上取得了最先进的结果。]]></description>
      <guid>https://arxiv.org/abs/2409.02243</guid>
      <pubDate>Thu, 05 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>NoiseAttack：一种利用高斯白噪声进行规避性样本特定多目标后门攻击</title>
      <link>https://arxiv.org/abs/2409.02251</link>
      <description><![CDATA[arXiv:2409.02251v1 公告类型：新
摘要：在使用第三方数据进行深度学习开发时，后门攻击构成了重大威胁。在这些攻击中，可以操纵数据，导致训练有素的模型在应用特定触发模式时行为不当，从而为对手提供未经授权的优势。虽然大多数现有工作都侧重于设计可见和不可见的触发模式来毒害受害者类别，但它们通常会在后门攻击成功后产生单个目标类别，这意味着受害者类别只能根据对手预定义的值转换为另一个类别。在本文中，我们通过引入一种新颖的样本特定多目标后门攻击（即 NoiseAttack）来解决这个问题。具体来说，我们采用具有各种功率谱密度 (PSD) 的高斯白噪声 (WGN) 作为我们的基础触发器，并结合独特的训练策略来执行后门攻击。这项工作是同类中第一个发起视觉后门攻击的工作，目的是以最少的输入配置生成多个目标类别。此外，我们大量的实验结果表明，NoiseAttack 可以对流行的网络架构和数据集实现较高的攻击成功率，并绕过最先进的后门检测方法。我们的源代码和实验可在 https://github.com/SiSL-URI/NoiseAttack/tree/main 上找到。]]></description>
      <guid>https://arxiv.org/abs/2409.02251</guid>
      <pubDate>Thu, 05 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>如何确定黑盒视觉语言模型的首选图像分布？</title>
      <link>https://arxiv.org/abs/2409.02253</link>
      <description><![CDATA[arXiv:2409.02253v1 公告类型：新
摘要：大型基础模型已经彻底改变了该领域，但在优化专门的视觉任务的多模态模型方面仍然存在挑战。我们提出了一种新颖的、可推广的方法，通过测量不同输入提示的输出一致性来识别黑盒视觉语言模型 (VLM) 的首选图像分布。将其应用于不同类型的 3D 对象渲染类型，我们证明了它在需要精确解释复杂结构的各个领域的有效性，重点是计算机辅助设计 (CAD) 作为示例领域。我们使用带有人工反馈的上下文学习进一步完善 VLM 输出，显着提高了解释质量。为了解决专业领域缺乏基准的问题，我们引入了 CAD-VQA，这是一个用于评估 CAD 相关视觉问答任务中的 VLM 的新数据集。我们对 CAD-VQA 上最先进的 VLM 的评估建立了基准性能水平，为在需要专家级视觉解释的各个领域的复杂视觉推理任务中提高 VLM 能力提供了一个框架。我们在 \url{https://github.com/asgsaeid/cad_vqa} 上发布了数据集和评估代码。]]></description>
      <guid>https://arxiv.org/abs/2409.02253</guid>
      <pubDate>Thu, 05 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>视频中基于行动的注意力缺陷多动障碍诊断</title>
      <link>https://arxiv.org/abs/2409.02261</link>
      <description><![CDATA[arXiv:2409.02261v1 公告类型：新
摘要：注意力缺陷多动障碍 (ADHD) 会导致各个领域的严重损害。早期诊断和治疗 ADHD 可以显著改善生活质量和功能。最近，机器学习方法提高了 ADHD 诊断过程的准确性和效率。然而，现有方法所需的设备和训练有素的人员的成本通常很高。因此，我们首次将基于视频的帧级动作识别网络引入 ADHD 诊断。我们还记录了一个真实的多模态 ADHD 数据集，并从视频模态中​​提取了三个动作类用于 ADHD 诊断。整个过程的数据已报告给 CNTW-NHS 基金会信托基金，将由医疗顾问/专业人员审查，并将适时公布。]]></description>
      <guid>https://arxiv.org/abs/2409.02261</guid>
      <pubDate>Thu, 05 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习根据视频中记录的动作特征进行 ADHD 诊断</title>
      <link>https://arxiv.org/abs/2409.02274</link>
      <description><![CDATA[arXiv:2409.02274v1 公告类型：新
摘要：对 ADHD 诊断和治疗的需求正在大幅增加，现有的服务无法及时满足需求。在这项工作中，我们通过识别和分析原始视频记录，引入了一种用于 ADHD 诊断的新型动作识别方法。我们的主要贡献包括 1) 设计和实施一项测试，重点关注参与者的注意力和多动/冲动性，通过三个摄像头记录；2) 首次实现一种基于动作识别神经网络的新型机器学习 ADHD 诊断系统；3) 提出分类标准以提供诊断结果和 ADHD 动作特征分析。]]></description>
      <guid>https://arxiv.org/abs/2409.02274</guid>
      <pubDate>Thu, 05 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>交通工程问题的视觉语言模型评估与比较</title>
      <link>https://arxiv.org/abs/2409.02278</link>
      <description><![CDATA[arXiv:2409.02278v1 公告类型：新
摘要：视觉语言模型 (VLM) 的最新发展已显示出与图像理解相关的各种应用的巨大潜力。在本研究中，我们探索了用于基于视觉的交通工程任务（例如图像分类和物体检测）的最先进的 VLM 模型。图像分类任务涉及拥堵检测和裂缝识别，而对于物体检测，则识别了头盔违规行为。我们已经应用了 CLIP、BLIP、OWL-ViT、Llava-Next 和闭源 GPT-4o 等开源模型来评估这些最先进的 VLM 模型的性能，以利用语言理解能力来完成基于视觉的交通任务。这些任务是通过对 VLM 模型应用零样本提示来执行的，因为零样本提示涉及在不对这些任务进行任何训练的情况下执行任务。它消除了对注释数据集或特定任务进行微调的需要。虽然这些模型在图像分类任务中给出了与基准卷积神经网络 (CNN) 模型相当的结果，但对于对象定位任务，它仍然需要改进。因此，本研究对最先进的 VLM 模型进行了全面评估，突出了这些模型的优点和局限性，可作为未来改进和大规模实施的基础。]]></description>
      <guid>https://arxiv.org/abs/2409.02278</guid>
      <pubDate>Thu, 05 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>K-Origins：神经网络的更好的颜色量化</title>
      <link>https://arxiv.org/abs/2409.02281</link>
      <description><![CDATA[arXiv:2409.02281v1 公告类型：新
摘要：K-Origins 是一个神经网络层，旨在提高基于图像的网络性能，因为学习颜色或强度是有益的。超过 250 个编码器-解码器卷积网络在 16 位合成数据上进行了训练和测试，表明 K-Origins 在两种情况下提高了语义分割的准确性：低信噪比的对象检测，以及分割形状相同但颜色不同的多个对象。K-Origins 根据方程 $\textbf{Y}_k = \textbf{X}-\textbf{J}\cdot w_k$ 从输入特征 $\textbf{X}$ 生成输出特征，其中 $\textbf{J}$ 是一个 1 的矩阵。此外，我们训练了具有不同感受野的网络，以根据目标类别的维度确定最佳网络深度，这表明感受野长度应超过物体大小。通过确保足够的感受野长度并结合 K-Origins，我们可以实现更好的语义网络性能。]]></description>
      <guid>https://arxiv.org/abs/2409.02281</guid>
      <pubDate>Thu, 05 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>生化前列腺癌复发预测：快速思考与缓慢思考</title>
      <link>https://arxiv.org/abs/2409.02284</link>
      <description><![CDATA[arXiv:2409.02284v1 公告类型：新
摘要：前列腺癌生化复发时间对于前列腺切除术后患者进展的预后监测至关重要，可评估手术效果。在这项工作中，我们提出通过两阶段“快速思考和慢速思考”策略利用多实例学习来预测复发时间 (TTR)。第一阶段（“快速思考”）找到与生化复发最相关的 WSI 区域，第二阶段（“慢速思考”）利用更高分辨率的补丁来预测 TTR。我们的方法在我们的内部验证中显示平均 C 指数 ($Ci$) 为 0.733 ($\theta=0.059$)，在 LEOPARD 挑战验证集上显示 $Ci=0.603$。事后注意力可视化显示最专注的区域有助于 TTR 预测。]]></description>
      <guid>https://arxiv.org/abs/2409.02284</guid>
      <pubDate>Thu, 05 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大规模运动结构几何感知特征匹配</title>
      <link>https://arxiv.org/abs/2409.02310</link>
      <description><![CDATA[arXiv:2409.02310v1 公告类型：新
摘要：在多幅图像之间建立一致且密集的对应关系对于运动结构 (SfM) 系统至关重要。显著的视图变化，例如具有非常稀疏视图重叠的空对地，对对应关系求解器提出了更大的挑战。我们提出了一种基于优化的新型方法，通过引入几何线索和颜色线索，显著增强了现有的特征匹配方法。这有助于在大规模场景中重叠较少时填补空白。我们的方法将几何验证公式化为优化问题，指导无检测器方法中的特征匹配，并使用基于检测器的方法中的稀疏对应关系作为锚点。通过 Sampson 距离强制执行几何约束，我们的方法确保无检测器方法的更密集对应关系在几何上一致且更准确。这种混合策略显着提高了对应密度和准确性，缓解了多视图不一致，并显著提高了相机姿势准确性和点云密度。它在基准数据集上的表现优于最先进的特征匹配方法，并能够在具有挑战性的极端大规模设置中进行特征匹配。]]></description>
      <guid>https://arxiv.org/abs/2409.02310</guid>
      <pubDate>Thu, 05 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>你看到了什么共同点？通过生命之树学习分层原型以发现进化特征</title>
      <link>https://arxiv.org/abs/2409.02335</link>
      <description><![CDATA[arXiv:2409.02335v1 公告类型：新
摘要：生物学中的一大挑战是发现进化特征 - 生命树（也称为系统发育树）中具有共同祖先的一组物种所共有的特征。随着生物学中图像存储库的日益普及，有巨大的机会直接从原型层次结构形式的图像中发现进化特征。然而，当前基于原型的方法大多设计用于在扁平的类结构上运行，并且在发现分层原型时面临一些挑战，包括在内部节点学习过度特定特征的问题。为了克服这些挑战，我们引入了通过原型网络（HComP-Net）实现层次对齐共性的框架。我们通过经验表明，与鸟类、蝴蝶和鱼类数据集上的基线相比，HComP-Net 学习的原型准确、语义一致且可推广到未见过的物种。代码和数据集可在https://github.com/Imageomics/HComPNet获得。]]></description>
      <guid>https://arxiv.org/abs/2409.02335</guid>
      <pubDate>Thu, 05 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多元显著物体检测</title>
      <link>https://arxiv.org/abs/2409.02368</link>
      <description><![CDATA[arXiv:2409.02368v1 公告类型：新
摘要：我们引入了多元显著性物体检测 (PSOD)，这是一项新颖的任务，旨在为给定的输入图像生成多个合理的显著性分割结果。与为显著性物体生成单个分割掩码的传统 SOD 方法不同，这种新设置可以识别现实世界图像的固有复杂性，包括多个物体，以及由于不同的用户意图而导致的定义显著性物体的模糊性。为了研究这项任务，我们提出了两个新的 SOD 数据集“DUTS-MM”和“DUS-MQ”，以及新设计的评估指标。DUTS-MM 以 DUTS 数据集为基础，但从三个方面丰富了地面实况掩码注释，1) 提高了掩码质量，尤其是边界和细粒度结构；2) 缓解注释不一致问题；3) 为具有显著性模糊性的图像提供多个地面实况掩码。 DUTS-MQ 包含大约 10 万张带有人工标注偏好分数的图像-口罩对，能够学习人类在测量口罩质量方面的实际偏好。基于这两个数据集，我们基于混合专家 (MOE) 设计提出了一种简单而有效的多元 SOD 基线。它配备了两个预测头，可以使用不同的查询提示同时预测多个口罩，并预测每个口罩候选者的人类偏好分数。大量的实验和分析强调了我们提出的数据集的重要性，并肯定了我们的 PSOD 框架的有效性。]]></description>
      <guid>https://arxiv.org/abs/2409.02368</guid>
      <pubDate>Thu, 05 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过泰勒展开式展开视频动力学</title>
      <link>https://arxiv.org/abs/2409.02371</link>
      <description><![CDATA[arXiv:2409.02371v1 公告类型：新
摘要：从物理运动中汲取灵感，我们提出了一种新的视频自监督动态学习策略：视频时间微分实例识别 (ViDiDi)。ViDiDi 是一种简单且数据高效的策略，可轻松应用于现有的基于实例识别的自监督视频表示学习框架。从本质上讲，ViDiDi 通过视频帧序列的不同顺序的时间导数来观察视频的不同方面。这些导数与原始帧一起支持离散时间下底层连续动态的泰勒级数展开，其中高阶导数强调高阶运动特征。ViDiDi 学习单个神经网络，该网络按照平衡的交替学习算法将视频及其时间导数编码为一致的嵌入。通过学习原始帧和导数的一致表示，编码器被引导以强调静态背景上的运动特征并揭示原始帧中隐藏的动态。因此，视频表示可以更好地通过动态特征进行区分。我们将 ViDiDi 集成到现有的实例识别框架（VICReg、BYOL 和 SimCLR）中，以便在 UCF101 或 Kinetics 上进行预训练，并在包括视频检索、动作识别和动作检测在内的标准基准上进行测试。无需大型模型或大量数据集，性能即可显著提高。]]></description>
      <guid>https://arxiv.org/abs/2409.02371</guid>
      <pubDate>Thu, 05 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>探索扩散模型中的低维子空间以实现可控图像编辑</title>
      <link>https://arxiv.org/abs/2409.02374</link>
      <description><![CDATA[arXiv:2409.02374v1 公告类型：新
摘要：最近，扩散模型已成为一类强大的生成模型。尽管它们取得了成功，但人们对其语义空间的理解仍然有限。这使得在没有额外训练的情况下实现精确和解开的图像生成具有挑战性，尤其是在无监督的情况下。在这项工作中，我们从有趣的观察中提高了对其语义空间的理解：在一定范围的噪声水平中，（1）扩散模型中学习到的后验均值预测器（PMP）是局部线性的，（2）其雅可比矩阵的奇异向量位于低维语义子空间中。我们提供了坚实的理论基础来证明 PMP 中的线性和低秩性。这些见解使我们能够提出一种无监督、单步、无需训练的低秩可控图像编辑（LOCO Edit）方法，用于扩散模型中的精确局部编辑。 LOCO Edit 确定了具有良好特性的编辑方向：同质性、可转移性、可组合性和线性。LOCO Edit 的这些特性极大地受益于低维语义子空间。我们的方法可以进一步扩展到各种文本到图像扩散模型（T-LOCO Edit）中的无监督或文本监督编辑。最后，大量的实证实验证明了 LOCO Edit 的有效性和效率。代码将在 https://github.com/ChicyChen/LOCO-Edit 上发布。]]></description>
      <guid>https://arxiv.org/abs/2409.02374</guid>
      <pubDate>Thu, 05 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>