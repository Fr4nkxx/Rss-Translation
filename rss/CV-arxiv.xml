<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Wed, 05 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>FruitPAL：一个基于物联网的智能医疗水果消费自动监控框架</title>
      <link>https://arxiv.org/abs/2502.01643</link>
      <description><![CDATA[arXiv:2502.01643v1 公告类型：新
摘要：水果富含人体健康所必需的维生素和营养素。本研究介绍了两款全自动设备 FruitPAL 及其更新版本 FruitPAL 2.0，旨在促进安全食用水果，同时降低健康风险。这两款设备都利用了 15 种水果的高质量数据集，并使用先进的模型 YOLOv8 和 YOLOv5 V6.0 来提高检测准确性。得益于 YOLOv8 更高的准确性和快速的响应时间，原始 FruitPAL 设备可以识别各种水果类型，并在检测到过敏反应时通知护理人员。通知通过云传输到移动设备，确保实时更新和立即可访问。FruitPAL 2.0 在此基础上不仅检测水果，还估算其营养价值，从而鼓励健康消费。FruitPAL 2.0 在 YOLOv5 V6.0 模型上进行训练，可分析水果摄入量，为用户提供有价值的饮食见解。本研究旨在通过帮助个人做出明智的选择，平衡健康益处和过敏意识来促进水果消费。通过提醒用户注意潜在的过敏原，同时鼓励食用营养丰富的水果，这些设备既支持健康维护，又支持饮食意识。]]></description>
      <guid>https://arxiv.org/abs/2502.01643</guid>
      <pubDate>Wed, 05 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过图像语义编码利用稳定扩散进行单目深度估计</title>
      <link>https://arxiv.org/abs/2502.01666</link>
      <description><![CDATA[arXiv:2502.01666v1 公告类型：新
摘要：单目深度估计涉及从单个 RGB 图像预测深度，在自动驾驶、机器人导航、3D 重建等应用中起着至关重要的作用。基于学习的方法的最新进展显着提高了深度估计性能。生成模型，尤其是稳定扩散，通过对不同数据集进行大规模训练，在恢复精细细节和重建缺失区域方面表现出了巨大的潜力。然而，像 CLIP 这样依赖于文本嵌入的模型在需要丰富上下文信息的复杂户外环境中面临限制。这些限制降低了它们在这种具有挑战性的场景中的有效性。在这里，我们提出了一种新颖的基于图像的语义嵌入，它直接从视觉特征中提取上下文信息，显着改善了复杂环境中的深度预测。在 KITTI 和 Waymo 数据集上进行评估后，我们的方法实现了与最先进模型相当的性能，同时解决了 CLIP 嵌入在处理户外场景方面的缺点。通过直接利用视觉语义，我们的方法在深度估计任务中表现出增强的鲁棒性和适应性，展示了其应用于其他视觉感知任务的潜力。]]></description>
      <guid>https://arxiv.org/abs/2502.01666</guid>
      <pubDate>Wed, 05 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于生成式人工智能的语义通信：图像压缩和边缘优化的新方法</title>
      <link>https://arxiv.org/abs/2502.01675</link>
      <description><![CDATA[arXiv:2502.01675v1 公告类型：新
摘要：随着数字技术的进步，通信网络在处理智能设备产生的大量数据方面面临挑战。自动驾驶汽车、智能传感器和物联网系统需要新的范式。本论文通过集成语义通信和生成模型来优化图像压缩和边缘网络资源分配，以应对这些挑战。与以位为中心的系统不同，语义通信优先传输专门选择用来传达含义的有意义的数据，而不是获得原始数据的忠实表示。通信基础设施可以受益于带宽效率的显着提高和延迟的减少。这项工作的核心是使用生成对抗网络和去噪扩散概率模型设计保留语义的图像压缩。这些模型通过仅对语义相关的特征进行编码来压缩图像，从而以最少的传输实现高质量的重建。此外，还介绍了一个面向目标的边缘网络优化框架，利用信息瓶颈原理和随机优化来动态分配资源并提高效率。通过将语义通信集成到边缘网络中，这种方法平衡了计算效率和通信效率，使其适用于实时应用。该论文使用经典和语义评估指标将语义感知模型与传统图像压缩技术进行了比较。结果表明，将生成式人工智能与语义通信相结合可以创建更高效​​的面向语义目标的通信网络，以满足现代数据驱动型应用的需求。]]></description>
      <guid>https://arxiv.org/abs/2502.01675</guid>
      <pubDate>Wed, 05 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>HuViDPO：通过以人为中心的直接偏好优化增强视频生成</title>
      <link>https://arxiv.org/abs/2502.01690</link>
      <description><![CDATA[arXiv:2502.01690v1 公告类型：new 
摘要：随着AIGC技术的快速发展，基于扩散模型的文本转图像（T2I）和文本转视频（T2V）技术取得了重大进展。近年来，一些研究将直接偏好优化（DPO）策略引入到T2I任务中，显著增强了生成图像中的人类偏好。然而，现有的T2V生成方法缺乏一个结构良好且具有精确损失函数的流水线来指导使用DPO策略将生成的视频与人类偏好对齐。此外，配对视频偏好数据稀缺等挑战阻碍了有效的模型训练。同时，训练数据集的缺乏导致生成的视频灵活性不足和视频生成质量差的风险。基于这些问题，我们的工作依次提出了三个有针对性的解决方案。1）我们的工作首次将DPO策略引入T2V任务。通过推导一个精心构建的损失函数，我们利用人类反馈将视频生成与人类偏好对齐。我们将这种新方法称为 HuViDPO。2）我们的工作为每个动作类别构建小规模的人类偏好数据集并微调该模型，在降低训练成本的同时提高了生成视频的美学质量。3）我们采用首帧条件化策略，利用第一帧的丰富信息来指导后续帧的生成，增强视频生成的灵活性。同时，我们采用 SparseCausal Attention 机制来提升生成视频的质量。更多详细信息和示例可在我们的网站上访问：https://tankowa.github.io/HuViDPO. github.io/。]]></description>
      <guid>https://arxiv.org/abs/2502.01690</guid>
      <pubDate>Wed, 05 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CLIP-DQA：使用 CLIP 从全局和局部视角盲目评估去雾图像</title>
      <link>https://arxiv.org/abs/2502.01707</link>
      <description><![CDATA[arXiv:2502.01707v1 公告类型：新
摘要：盲去雾图像质量评估（BDQA）旨在在没有任何参考信息的情况下准确预测去雾图像的视觉质量，对于图像去雾算法的评估、比较和优化至关重要。现有的基于学习的 BDQA 方法已经取得了显著的成功，而 DQA 数据集的规模较小限制了它们的性能。为了解决这个问题，在本文中，我们提出将在大规模图像-文本对上进行预训练的对比语言-图像预训练（CLIP）应用于 BDQA 任务。具体而言，受人类视觉系统基于分层特征理解图像这一事实的启发，我们将去雾图像的全局和局部信息作为 CLIP 的输入。为了将去雾图像的输入分层信息准确地映射到质量分数中，我们通过快速学习调整了 CLIP 的视觉分支和语言分支。在两个真实的 DQA 数据集上的实验结果表明，我们提出的方法 CLIP-DQA 比现有的 BDQA 方法实现了更准确的质量预测。代码可在 https://github.com/JunFu1995/CLIP-DQA 上找到。]]></description>
      <guid>https://arxiv.org/abs/2502.01707</guid>
      <pubDate>Wed, 05 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>融合频域和跨视图注意力机制的多尺度特征融合框架，用于双视图 X 射线安全检查</title>
      <link>https://arxiv.org/abs/2502.01710</link>
      <description><![CDATA[arXiv:2502.01710v1 公告类型：新 
摘要：随着现代交通运输系统的快速发展和物流量的指数级增长，基于X射线的智能安检系统在公共安全中发挥着至关重要的作用。虽然单视角X射线设备被广泛部署，但由于视点依赖性强和特征表达不足，它很难在复杂的堆叠场景中准确识别违禁品。针对这一问题，我们提出了一种创新的双视角X射线安检图像分类多尺度交互式特征融合框架。该框架包括三个核心模块：频域交互模块（FDIM）通过傅里叶变换增强频域特征；多尺度跨视图特征增强（MSCFE）利用跨视图注意机制来加强特征交互；卷积注意融合模块（CAFM）通过将通道注意与深度可分离卷积相结合来有效地融合特征。实验结果表明，我们的方法优于多种主干架构中现有的最先进方法，特别是在存在遮挡和对象堆叠的复杂场景中表现出色。]]></description>
      <guid>https://arxiv.org/abs/2502.01710</guid>
      <pubDate>Wed, 05 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MJ-VIDEO：视频生成中的细粒度基准测试和奖励视频偏好</title>
      <link>https://arxiv.org/abs/2502.01719</link>
      <description><![CDATA[arXiv:2502.01719v2 公告类型：新
摘要：视频生成领域的最新进展显著提高了从文本指令合成视频的能力。然而，现有模型仍然面临着指令错位、内容幻觉、安全问题和偏见等关键挑战。为了解决这些限制，我们推出了 MJ-BENCH-VIDEO，这是一个大规模视频偏好基准，旨在从五个关键方面评估视频生成：对齐、安全性、精细度、连贯性和一致性以及偏见和公平性。该基准结合了 28 个细粒度标准，以提供对视频偏好的全面评估。基于这个数据集，我们提出了 MJ-VIDEO，这是一个基于混合专家 (MoE) 的视频奖励模型，旨在提供细粒度的奖励。MJ-VIDEO 可以根据输入的文本-视频对动态选择相关专家来准确判断偏好。这种架构可以实现更精确和适应性更强的偏好判断。通过对 MJ-BENCH-VIDEO 进行广泛的基准测试，我们分析了现有视频奖励模型的局限性，并展示了 MJ-VIDEO 在视频偏好评估方面的卓越性能，在整体和细粒度偏好判断中分别实现了 17.58% 和 15.87% 的提升。此外，在视频生成中引入 MJ-VIDEO 进行偏好调整可增强对齐性能。]]></description>
      <guid>https://arxiv.org/abs/2502.01719</guid>
      <pubDate>Wed, 05 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>生成用于文本到图像定制的多图像合成数据</title>
      <link>https://arxiv.org/abs/2502.01720</link>
      <description><![CDATA[arXiv:2502.01720v1 公告类型：新
摘要：文本到图像模型的定制使用户能够插入自定义概念并在看不见的设置中生成概念。现有方法要么依赖于昂贵的测试时间优化，要么在没有多图像监督的情况下在单图像训练数据集上训练编码器，从而导致图像质量下降。我们提出了一种解决这两个限制的简单方法。我们首先利用现有的文本到图像模型和 3D 数据集来创建一个高质量的合成定制数据集 (SynCD)，该数据集由同一对象在不同光照、背景和姿势下的多张图像组成。然后，我们提出了一种基于共享注意机制的新编码器架构，可以更好地整合输入图像中的细粒度视觉细节。最后，我们提出了一种新的推理技术，通过规范化文本和图像引导向量来缓解推理过程中的过度曝光问题。通过大量实验，我们表明，使用所提出的编码器和推理算法在合成数据集上训练的模型在标准定制基准上优于现有的免调优方法。]]></description>
      <guid>https://arxiv.org/abs/2502.01720</guid>
      <pubDate>Wed, 05 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Sparse VideoGen：利用时空稀疏性加速视频扩散变换器</title>
      <link>https://arxiv.org/abs/2502.01776</link>
      <description><![CDATA[arXiv:2502.01776v1 公告类型：新 
摘要：扩散变换器 (DiT) 主导视频生成，但其高计算成本严重限制了现实世界的适用性，即使在高性能 GPU 上也通常需要数十分钟才能生成几秒钟的视频。这种低效率主要源于 3D 全注意力相对于上下文长度的二次计算复杂度。在本文中，我们提出了一个称为稀疏视频生成 (SVG) 的免训练框架，该框架利用 3D 全注意力固有的稀疏性来提高推理效率。我们发现，根据不同的稀疏模式，注意力头可以动态地分为两组：(1) 空间头，其中只有每个帧内的空间相关标记主导注意力输出，以及 (2) 时间头，其中只有不同帧之间的时间相关标记占主导地位。基于这一洞察，SVG 提出了一种在线分析策略来捕捉动态稀疏模式并预测注意力头的类型。结合新颖的硬件高效张量布局转换和定制的内核实现，SVG 在 CogVideoX-v1.5 和 HunyuanVideo 上分别实现了高达 2.28 倍和 2.33 倍的端到端加速，同时保持了生成质量。]]></description>
      <guid>https://arxiv.org/abs/2502.01776</guid>
      <pubDate>Wed, 05 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AquaticCLIP：用于水下场景分析的视觉语言基础模型</title>
      <link>https://arxiv.org/abs/2502.01785</link>
      <description><![CDATA[arXiv:2502.01785v1 公告类型：新
摘要：保护水生生物多样性对于减轻气候变化的影响至关重要。水生场景理解在帮助海洋科学家进行决策过程中起着关键作用。在本文中，我们介绍了 AquaticCLIP，这是一种专为水生场景理解而量身定制的新型对比语言图像预训练模型。AquaticCLIP 提出了一种新的无监督学习框架，可在水生环境中对齐图像和文本，从而实现分割、分类、检测和物体计数等任务。通过利用我们的大规模水下图像文本配对数据集而无需地面实况注释，我们的模型丰富了水生领域现有的视觉语言模型。为此，我们使用异构资源（包括 YouTube、Netflix、NatGeo 等）构建了一个 200 万个水下图文配对数据集。为了对 AquaticCLIP 进行微调，我们提出了一种提示引导的视觉编码器，它通过可学习的提示逐步聚合补丁特征，而视觉引导机制通过结合视觉上下文来增强语言编码器。该模型通过对比预训练损失进行优化，以对齐视觉和文本模态。AquaticCLIP 在多个水下计算机视觉任务的零样本设置中实现了显着的性能提升，在鲁棒性和可解释性方面均优于现有方法。我们的模型为水下环境中的视觉语言应用树立了新的标杆。AquaticCLIP 的代码和数据集在 GitHub xxx 上公开提供。]]></description>
      <guid>https://arxiv.org/abs/2502.01785</guid>
      <pubDate>Wed, 05 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PolyhedronNet：具有表面属性图的多面体的表示学习</title>
      <link>https://arxiv.org/abs/2502.01814</link>
      <description><![CDATA[arXiv:2502.01814v1 公告类型：新
摘要：无处不在的几何物体可以精确而有效地表示为多面体。将多面体转换为矢量，称为多面体表示学习，对于使用数学和统计工具操纵这些形状以完成分类、聚类和生成等任务至关重要。近年来，这一领域取得了重大进展，但大多数努力都集中在多面体的顶点序列上，而忽略了现实世界多面体物体中至关重要的复杂表面建模。本研究提出了 \textbf{PolyhedronNet}，这是一个专为学习 3D 多面体物体表示而定制的通用框架。我们提出了表面属性图的概念，以无缝地模拟多面体内的顶点、边、面及其几何相互关系。为了有效地学习整个表面属性图的表示，我们首先建议将其分解为局部刚性表示，以有效地学习每个局部区域相对于其余区域的相对位置，而不会丢失几何信息。随后，我们提出 PolyhedronGNN 通过界面内和界面间几何消息传递模块分层聚合局部刚性表示，以获得在保持旋转和平移不变性的同时最大限度减少信息损失的全局表示。我们对四个不同数据集（包括分类和检索任务）进行的实验评估证实了 PolyhedronNet 在捕获 3D 多面体对象的全面和信息丰富的表示方面的有效性。代码和数据可在 {https://github.com/dyu62/3D_polyhedron} 获得。]]></description>
      <guid>https://arxiv.org/abs/2502.01814</guid>
      <pubDate>Wed, 05 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用记忆和残差可变形卷积实现低资源视频超分辨率</title>
      <link>https://arxiv.org/abs/2502.01816</link>
      <description><![CDATA[arXiv:2502.01816v1 公告类型：新
摘要：基于 Transformer 的视频超分辨率 (VSR) 模型近年来树立了新的标杆，但它们大量的计算需求使得大多数模型不适合部署在资源受限的设备上。在 VSR 中，实现模型复杂性和输出质量之间的平衡仍然是一个巨大的挑战。虽然已经引入了轻量级模型来解决这个问题，但它们往往难以提供最先进的性能。我们为 VSR 提出了一种新颖的轻量级、参数高效的深度残差可变形卷积网络。与之前的方法不同，我们的模型通过残差连接提高了特征利用率，并采用可变形卷积进行精确的帧对齐，有效地解决了运动动态问题。此外，我们引入了一个单一的记忆张量来捕获从过去帧中积累的信息并改进跨帧的运动估计。这种设计能够在计算成本和重建质量之间实现有效的平衡。我们的模型仅使用 230 万个参数，在 REDS4 数据集上实现了 0.9175 的最佳 SSIM，在准确率和资源效率方面超越了现有的轻量级和许多重型模型。我们模型的架构见解为流数据的实时 VSR 铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2502.01816</guid>
      <pubDate>Wed, 05 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用基于视觉变换器的空间 GAN 进行纹理图像合成</title>
      <link>https://arxiv.org/abs/2502.01842</link>
      <description><![CDATA[arXiv:2502.01842v1 公告类型：新
摘要：纹理合成是计算机视觉中的一项基本任务，其目标是为从图形到科学模拟的广泛应用生成视觉逼真且结构一致的纹理。虽然传统方法（如平铺和基于块的技术）通常难以处理复杂的纹理，但深度学习的最新进展已经改变了这一领域。在本文中，我们提出了 ViT-SGAN，这是一种新的混合模型，它将视觉变换器 (ViT) 与空间生成对抗网络 (SGAN) 融合在一起，以解决以前方法的局限性。通过将均值方差 (mu、sigma) 和纹理等专门的纹理描述符合并到 ViT 的自注意机制中，我们的模型实现了卓越的纹理合成。这种方法增强了模型捕获复杂空间依赖性的能力，从而提高了纹理质量，优于最先进的模型，尤其是对于规则和不规则纹理。与 FID、IS、SSIM 和 LPIPS 等指标的比较实验表明 ViT-SGAN 有了显著的改进，凸显了其在生成多样化真实纹理方面的效率。]]></description>
      <guid>https://arxiv.org/abs/2502.01842</guid>
      <pubDate>Wed, 05 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>UVGS：使用 UV 映射重新构想非结构化 3D 高斯溅射</title>
      <link>https://arxiv.org/abs/2502.01846</link>
      <description><![CDATA[arXiv:2502.01846v1 公告类型：新
摘要：3D 高斯分层 (3DGS) 在建模 3D 对象和场景方面表现出色。然而，由于 3DGS 具有离散、非结构化和置换不变性，因此生成 3DGS 仍然具有挑战性。在这项工作中，我们提出了一种简单而有效的方法来克服这些挑战。我们利用球面映射将 3DGS 转换为结构化的 2D 表示，称为 UVGS。UVGS 可以被视为多通道图像，其特征维度是高斯属性（例如位置、比例、颜色、不透明度和旋转）的串联。我们进一步发现，可以使用精心设计的多分支网络将这些异构特征压缩到低维（例如 3 通道）共享特征空间中。压缩后的 UVGS 可以视为典型的 RGB 图像。值得注意的是，我们发现使用潜在扩散模型训练的典型 VAE 可以直接推广到这种新的表示，而无需额外的训练。我们新颖的表示方法使得利用基础 2D 模型（例如扩散模型）直接建模 3DGS 变得毫不费力。此外，只需增加 2D UV 分辨率即可容纳更多高斯分布，这使得 UVGS 成为与典型 3D 主干相比可扩展的解决方案。这种方法通过固有利用已经开发的卓越 2D 生成功能，立即解锁了 3DGS 的各种新型生成应用。在我们的实验中，我们展示了基于扩散模型的 3DGS 的各种无条件、条件生成和修复应用，这些应用以前并不简单。]]></description>
      <guid>https://arxiv.org/abs/2502.01846</guid>
      <pubDate>Wed, 05 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于基础模型的苹果成熟度和大小估计，用于选择性采摘</title>
      <link>https://arxiv.org/abs/2502.01850</link>
      <description><![CDATA[arXiv:2502.01850v1 公告类型：新
摘要：采摘是果树产业的一项关键任务，需要大量的体力劳动和高昂的成本，并使工人面临潜在的危险。自动采摘的最新进展提供了一种有希望的解决方案，可以在紧凑的采摘窗口内实现高效、经济且符合人体工程学的采摘。然而，现有的采摘技术通常会不加区分地采摘所有可见和可触及的水果，包括未成熟或尺寸过小的水果。本研究介绍了一种基于基础模型的新型框架，用于高效地估计苹果的成熟度和尺寸。具体来说，我们整理了两个基于 RGBD 的公共富士苹果图像数据集，根据水果颜色和图像拍摄日期整合了成熟度的扩展注释（“成熟”与“未成熟”）。由此产生的综合数据集，富士成熟度大小数据集，包括 4,027 张图像和 16,257 个带有成熟度和尺寸标签的注释苹果。使用 Grounding-DINO（一种基于语言模型的物体检测器），我们实现了强大的苹果检测和成熟度分类，其表现优于其他最先进的模型。此外，我们开发并评估了六种尺寸估算算法，选择误差和变化最小的算法以获得最佳性能。Fuji-Ripeness-Size 数据集以及苹果检测和尺寸估算算法已公开，为未来自动化和选择性采摘研究提供了宝贵的基准。]]></description>
      <guid>https://arxiv.org/abs/2502.01850</guid>
      <pubDate>Wed, 05 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>