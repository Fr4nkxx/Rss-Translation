<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Mon, 23 Dec 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>通过可区分模型合并实现多概念模型免疫</title>
      <link>https://arxiv.org/abs/2412.15320</link>
      <description><![CDATA[arXiv:2412.15320v1 公告类型：新
摘要：模型免疫是一个新兴方向，旨在减轻与开源模型和先进的适应方法相关的潜在滥用风险。这个想法是让发布的模型的权重难以在某些有害应用程序上进行微调，因此得名“免疫”。模型免疫的最新研究集中在单一概念设置上。然而，在现实世界中，模型需要对多个概念进行免疫。为了解决这一差距，我们提出了一种免疫算法，该算法同时学习一组概念上的适应方法的单一“困难初始化”。我们通过合并一个可微分合并层来实现这一点，该层结合了一组适应多个概念的模型权重。在我们的实验中，我们通过将先前工作的重新学习和个性化适应的实验设置推广到多个概念来证明多概念免疫的有效性。]]></description>
      <guid>https://arxiv.org/abs/2412.15320</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>自回归视觉生成的下一个补丁预测</title>
      <link>https://arxiv.org/abs/2412.15321</link>
      <description><![CDATA[arXiv:2412.15321v1 公告类型：新
摘要：基于下一个标记预测 (NTP) 范式构建的自回归模型在开发集成语言和视觉任务的统一框架方面表现出巨大潜力。在这项工作中，我们重新思考了用于自回归图像生成的 NTP，并提出了一种新颖的下一个补丁预测 (NPP) 范式。我们的关键思想是将图像标记分组并聚合为包含高信息密度的补丁标记。使用补丁标记作为较短的输入序列，训练自回归模型来预测下一个补丁，从而显着降低计算成本。我们进一步提出了一种多尺度粗到细的补丁分组策略，利用图像数据的自然层次结构特性。在各种模型（1 亿至 14 亿个参数）上进行的实验表明，下一个补丁预测范式可以将训练成本降低到约 0.6 倍，同时在 ImageNet 基准上将图像生成质量提高高达 1.0 FID 分数。我们强调，我们的方法保留了原始的自回归模型架构，而无需引入额外的可训练参数或专门设计自定义图像标记器，从而确保灵活性和无缝适应各种自回归模型以进行视觉生成。]]></description>
      <guid>https://arxiv.org/abs/2412.15321</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>掌握多模式联合训练，实现高质量视频到音频合成</title>
      <link>https://arxiv.org/abs/2412.15322</link>
      <description><![CDATA[arXiv:2412.15322v1 公告类型：新
摘要：我们建议使用新颖的多模态联合训练框架 MMAudio，在给定视频和可选文本条件的情况下合成高质量同步音频。与仅以（有限）视频数据为条件的单模态训练相比，MMAudio 与更大规模、现成的文本音频数据联合训练，以学习生成语义对齐的高质量音频样本。此外，我们使用条件同步模块改进了视听同步，该模块在帧级别将视频条件与音频潜在条件对齐。使用流匹配目标进行训练后，MMAudio 在音频质量、语义对齐和视听同步方面在公共模型中实现了新的视频到音频最新水平，同时具有较低的推理时间（1.23 秒生成 8 秒剪辑）和仅 157M 个参数。 MMAudio 在文本转音频生成方面也取得了令人惊讶的竞争性表现，表明联合训练不会妨碍单模态性能。代码和演示可在以下网址获取：https://hkchengrex.github.io/MMAudio]]></description>
      <guid>https://arxiv.org/abs/2412.15322</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>探索无人机 (UAV) 物体检测与追踪的机器学习工程</title>
      <link>https://arxiv.org/abs/2412.15347</link>
      <description><![CDATA[arXiv:2412.15347v1 公告类型：新
摘要：随着深度学习方法的进步，自主系统将越来越智能，并包含先进的机器学习算法来执行各种自主操作。其中一项任务涉及感知系统子系统的设计和评估，用于对象检测和跟踪。创建解决该任务的软件的挑战在于发现数据集的需求、数据集的注释、特征的选择、现有算法的集成和改进，同时通过训练和测试评估性能指标。这项研究工作侧重于机器学习管道的开发，强调纳入保证方法并提高自动化程度。在此过程中，通过收集移动物体（例如 Roomba 吸尘器）的视频创建了一个新的数据集，模拟室内环境的搜索和救援 (SAR)。从视频中提取单个帧并使用手动和自动技术的组合进行标记。最初在 YOLOv4 上训练这个带注释的数据集，以提高准确性。在对数据集进行细化后，该模型在第二个 YOLOv4 和 Mask R-CNN 模型上进行训练，该模型部署在 Parrot Mambo 无人机上，以执行实时物体检测和跟踪。实验结果证明了该模型在多次试验中准确检测和跟踪 Roomba 的有效性，平均损失为 0.1942，准确率为 96%。]]></description>
      <guid>https://arxiv.org/abs/2412.15347</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过混合视觉概念进行数据集扩充</title>
      <link>https://arxiv.org/abs/2412.15358</link>
      <description><![CDATA[arXiv:2412.15358v1 公告类型：新
摘要：本文提出了一种通过微调预训练扩散模型来增强数据集的方法。使用具有文本条件的预训练扩散模型生成图像通常会导致真实数据和生成的图像之间的域差异。我们提出了一种微调方法，通过用真实图像和新颖的文本嵌入来调节扩散模型，从而调整扩散模型。我们引入了一种称为混合视觉概念 (MVC) 的独特程序，我们从图像标题中创建新的文本嵌入。MVC 使我们能够生成多种多样但与真实数据相似的多幅图像，从而使我们能够执行有效的数据集增强。我们对提出的数据集增强方法进行了全面的定性和定量评估，展示了生成图像中的粗粒度和细粒度变化。我们的方法在基准分类任务上优于最先进的增强技术。]]></description>
      <guid>https://arxiv.org/abs/2412.15358</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于半监督医学图像分割的不确定性引导交叉注意力集成平均教师</title>
      <link>https://arxiv.org/abs/2412.15380</link>
      <description><![CDATA[arXiv:2412.15380v1 公告类型：新
摘要：这项工作提出了一种新颖的框架，即不确定性引导的交叉注意集成均值教师 (UG-CEMT)，用于在半监督医学图像分割中实现最先进的性能。UG-CEMT 通过将受 Vision Transformers (ViT) 启发的交叉注意集成均值教师框架 (CEMT) 与不确定性引导的一致性正则化和强调不确定性的清晰度感知最小化相结合，充分利用了协同训练和知识提炼的优势。UG-CEMT 通过促进子网络之间的高差异来提高半监督性能，同时保持一致的网络架构和任务设置。实验表明，在视差、领域泛化和医学图像分割性能方面，与均值教师和交叉伪监督等现有方法相比具有显着优势。 UG-CEMT 在多中心前列腺 MRI 和心脏 MRI 数据集上取得了最先进的成果，而这些数据集中的对象分割尤其具有挑战性。我们的结果表明，仅使用 10\% 的标记数据，UG-CEMT 就能接近全监督方法的性能，证明了其在利用未标记数据进行稳健的医学图像分割方面的有效性。代码可在 \url{https://github.com/Meghnak13/UG-CEMT} 上公开获取]]></description>
      <guid>https://arxiv.org/abs/2412.15380</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过自我监督使用最少的标签最大化组织病理学分割</title>
      <link>https://arxiv.org/abs/2412.15389</link>
      <description><![CDATA[arXiv:2412.15389v1 公告类型：新
摘要：组织病理学是对组织样本的显微镜检查，对于疾病的诊断和预后至关重要。准确分割和识别组织病理学图像中的关键区域对于开发自动化解决方案至关重要。然而，像 UNet 这样的最先进的深度学习分割方法需要大量标签，这既昂贵又耗时，尤其是在处理多种染色时。为了缓解这种情况，已经开发了多染色分割方法，例如 MDS1 和 UDAGAN，它们通过仅要求标记一种（源）染色来减少对标签的需求。尽管如此，获取源染色标签仍然具有挑战性，并且当它们不可用时，分割模型会失败。本文表明，通过自监督预训练，包括 SimCLR、BYOL 和一种新方法 HR-CS-CO，即使标签减少了 95%，这些分割方法（UNet、MDS1 和 UDAGAN）的性能也可以保留。值得注意的是，在自监督预训练中，仅使用 5% 的标签，性能下降幅度很小：与各自的完全监督对应物（无预训练，使用 100% 的标签）相比，UNet 下降 5.9%，MDS1 下降 4.5%，UDAGAN 下降 6.2%。代码可从 https://github.com/zeeshannisar/improve_kidney_glomeruli_segmentation 获取 [接受后将公开]。]]></description>
      <guid>https://arxiv.org/abs/2412.15389</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过改进的语义指导学习视觉构图</title>
      <link>https://arxiv.org/abs/2412.15396</link>
      <description><![CDATA[arXiv:2412.15396v1 公告类型：新
摘要：视觉图像不是由单独的物体组成，而是反映了大量流动概念的组成。虽然视觉表征学习取得了巨大进步，但这些进步主要集中在为少数离散物体构建更好的表征，而没有理解这些物体是如何相互作用的。人们可以在通过标题或对比学习学习的表征中观察到这种局限性——学习模型将图像本质上视为一个词袋。一些研究试图通过开发定制的学习架构来解决这一限制，以直接解决组合学习中的缺点。在这项工作中，我们专注于简单且可扩展的方法。特别是，我们证明，通过大幅改进弱标记数据（即标题），我们可以大大提高标准对比学习方法的性能。以前的 CLIP 模型在探索组合学习的具有挑战性的任务上实现了近乎偶然的概率。然而，我们的简单方法大大提高了 CLIP 的性能，并超越了所有定制架构。此外，我们在源自 DOCCI 的相对较新的字幕基准上展示了我们的结果。我们通过一系列消融证明，使用增强数据训练的标准 CLIP 模型可以在图像检索任务中表现出令人印象深刻的性能。]]></description>
      <guid>https://arxiv.org/abs/2412.15396</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SolidGS：整合高斯面元分层以实现稀疏视图表面重建</title>
      <link>https://arxiv.org/abs/2412.15400</link>
      <description><![CDATA[arXiv:2412.15400v1 公告类型：新
摘要：高斯铺层在新型视图合成和多视图图像表面重建方面都取得了令人瞩目的进步。然而，目前的方法仍然难以使用高斯铺层从稀疏视图输入图像中重建高质量的表面。在本文中，我们提出了一种称为 SolidGS 的新方法来解决这个问题。我们观察到，由于高斯函数在几何渲染中的特性，重建的几何图形在多视图之间可能严重不一致。这促使我们通过采用更可靠的核函数来整合所有高斯函数，从而有效地提高表面重建质量。在几何正则化和单目法线估计的额外帮助下，我们的方法在广泛使用的 DTU、Tanks-and-Temples 和 LLFF 数据集上实现了比所有高斯铺层方法和神经场方法更好的稀疏视图表面重建性能。]]></description>
      <guid>https://arxiv.org/abs/2412.15400</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>3D 颜色查找表的高效神经网络编码</title>
      <link>https://arxiv.org/abs/2412.15438</link>
      <description><![CDATA[arXiv:2412.15438v1 公告类型：新
摘要：3D 颜色查找表 (LUT) 通过将输入 RGB 值映射到特定的输出 RGB 值来实现精确的颜色处理。3D LUT 在各种应用中都发挥着重要作用，包括视频编辑、相机内处理、摄影滤镜、计算机图形和显示器的颜色处理。虽然单个 LUT 不会产生高内存开销，但软件和设备可能需要存储数十到数百个 LUT，这些 LUT 可能占用超过 100 MB 的空间。这项工作旨在开发一种神经网络架构，可以在一个紧凑的表示中编码数百个 LUT。为此，我们提出了一个内存占用小于 0.25 MB 的模型，它可以在整个色域内重建 512 个 LUT，并且只有轻微的颜色失真（$\bar{\Delta}E_M$ $\leq$ 2.0）。我们还表明，我们的网络可以对颜色进行加权，从而进一步提高自然图像颜色的质量（$\bar{\Delta}{E}_M$ $\leq$ 1.0）。最后，我们表明，对网络架构进行微小修改可以实现双射编码，从而产生可逆的 LUT，从而实现反向颜色处理。我们的代码可在 https://github.com/vahidzee/ennelut 上找到。]]></description>
      <guid>https://arxiv.org/abs/2412.15438</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LiHi-GS：用于高速公路驾驶场景重建的 LiDAR 监督高斯 Splatting</title>
      <link>https://arxiv.org/abs/2412.15447</link>
      <description><![CDATA[arXiv:2412.15447v1 公告类型：新
摘要：逼真的 3D 场景重建在自动驾驶中起着重要作用，它能够从现有数据集生成新数据，以模拟安全关键场景并扩展训练数据，而无需额外的获取成本。高斯分层 (GS) 通过显式 3D 高斯场景表示实现实时、逼真的渲染，比隐式神经辐射场 (NeRF) 提供更快的处理速度和更直观的场景编辑。虽然广泛的 GS 研究在自动驾驶应用方面取得了有希望的进展，但它们忽略了两个关键方面：首先，现有方法主要关注低速和功能丰富的城市场景，而忽略了高速公路场景在自动驾驶中发挥重要作用的事实。其次，虽然 LiDAR 在自动驾驶平台中很常见，但现有方法主要从图像中学习，仅将 LiDAR 用于初始估计或没有精确的传感器建模，因此错过了利用 LiDAR 提供的丰富深度信息并限制了合成 LiDAR 数据的能力。在本文中，我们提出了一种新颖的 GS 方法，用于动态场景合成和编辑，通过 LiDAR 监督和对 LiDAR 渲染的支持改进了场景重建。与之前主要在城市数据集上测试的研究不同，据我们所知，我们是第一个专注于更具挑战性且与自动驾驶高度相关的高速公路场景的研究，这些场景具有稀疏的传感器视图和单调的背景。]]></description>
      <guid>https://arxiv.org/abs/2412.15447</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>长尾识别的难度感知平衡边际损失</title>
      <link>https://arxiv.org/abs/2412.15477</link>
      <description><![CDATA[arXiv:2412.15477v1 公告类型：新
摘要：当使用严重不平衡的数据进行训练时，深度神经网络通常很难仅使用少量样本准确识别类别。长尾识别领域的先前研究试图使用已知的样本分布来重新平衡有偏差的学习，主要解决类别级别的不同分类难度。然而，这些方法往往忽略了每个类别内的实例难度变化。在本文中，我们提出了一种难度感知平衡边际 (DBM) 损失，它同时考虑了类别不平衡和实例难度。DBM 损失包括两个部分：一个类边际，用于减轻由不平衡的类别频率引起的学习偏差，以及一个实例边际，根据其各自的难度分配给困难正样本。DBM 损失通过为更困难的样本分配更大的边际来提高类别辨别力。我们的方法与现有方法无缝结合，并在各种长尾识别基准中持续提高性能。]]></description>
      <guid>https://arxiv.org/abs/2412.15477</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>迈向稳健的超详细图像字幕：多智能体方法以及事实性和覆盖范围的双重评估指标</title>
      <link>https://arxiv.org/abs/2412.15484</link>
      <description><![CDATA[arXiv:2412.15484v1 公告类型：新
摘要：多模态大型语言模型 (MLLM) 擅长生成高度详细的字幕，但经常会产生幻觉。我们的分析表明，现有的幻觉检测方法在生成详细字幕方面存在困难。我们将其归因于随着序列长度的增加，MLLM 越来越依赖其生成的文本，而不是输入图像。为了解决这个问题，我们提出了一种多智能体方法，利用 LLM-MLLM 协作来纠正给定的字幕。此外，我们引入了一个评估框架和一个基准数据集，以促进对详细字幕的系统分析。我们的实验表明，我们提出的评估方法比现有指标更符合人类对事实性的判断，并且现有的提高 MLLM 事实性的方法可能在超详细图像字幕任务中有所不足。相比之下，我们提出的方法显着提高了字幕的事实准确性，甚至提高了 GPT-4V 生成的字幕。最后，我们通过证明 MLLM 在 VQA 基准测试中的表现可能与其生成详细图像标题的能力不相关，强调了以 VQA 为中心的基准测试的局限性。]]></description>
      <guid>https://arxiv.org/abs/2412.15484</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于外观的非结构化环境下多旋翼无人机自主着陆点识别</title>
      <link>https://arxiv.org/abs/2412.15486</link>
      <description><![CDATA[arXiv:2412.15486v1 公告类型：新
摘要：多旋翼无人机飞行中剩下的一个挑战是在非结构化环境中自主识别可行的着陆点。解决此问题的一种方法是创建轻量级的、基于外观的地形分类器，该分类器可以将无人机的 RGB 图像分割为安全和不安全区域。但是，此类分类器需要图像和掩码的数据集，而创建这些数据集的成本可能过高。我们提出了一种管道来自动生成合成数据集以训练这些分类器，利用现代无人机自动勘测地形的能力以及从此类勘测得出的地形模型中自动计算着陆安全掩码的能力。然后，我们在合成数据集上训练 U-Net，在真实数据上对其进行测试以进行验证，并在我们的无人机平台上实时演示它。]]></description>
      <guid>https://arxiv.org/abs/2412.15486</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GCA-3D：面向 3D 生成器的广义和一致域自适应</title>
      <link>https://arxiv.org/abs/2412.15491</link>
      <description><![CDATA[arXiv:2412.15491v1 公告类型：新
摘要：最近，出现了 3D 生成域自适应，以使预训练的生成器适应其他域，而无需收集大量数据集和相机姿势分布。通常，他们利用大规模预训练的文本到图像扩散模型来合成目标域的图像，然后微调 3D 模型。然而，它们遭受繁琐的数据生成流程的困扰，这不可避免地会在源域和合成数据集之间引入姿势偏差。此外，它们不能推广到支持一次性图像引导的域自适应，这更具挑战性，因为单个图像参考引入了更严重的姿势偏差和额外的身份偏差。为了解决这些问题，我们提出了 GCA-3D，这是一种通用且一致的 3D 域自适应方法，没有复杂的数据生成流程。与以前的流程方法不同，我们引入了多模态深度感知分数蒸馏采样损失，以非对抗方式有效地调整 3D 生成模型。这种多模态损失使 GCA-3D 能够适应文本提示和一次性图像提示。此外，它利用体积渲染模块中的每个实例深度图来缓解过度拟合问题并保留结果的多样性。为了增强姿势和身份一致性，我们进一步提出了分层空间一致性损失，以对齐源域和目标域中生成的图像之间的空间结构。实验表明，GCA-3D 在效率、泛化、姿势准确性和身份一致性方面优于以前的方法。]]></description>
      <guid>https://arxiv.org/abs/2412.15491</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>