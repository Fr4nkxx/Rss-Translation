<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Wed, 02 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>以语言为中心的人类活动识别</title>
      <link>https://arxiv.org/abs/2410.00003</link>
      <description><![CDATA[arXiv:2410.00003v1 公告类型：新
摘要：使用惯性测量单元 (IMU) 传感器进行人体活动识别 (HAR) 对于医疗保健、安全和工业生产中的应用至关重要。然而，活动模式、设备类型和传感器位置的变化会在数据集之间造成分布差距，从而降低 HAR 模型的性能。为了解决这个问题，我们提出了 LanHAR，这是一种利用大型语言模型 (LLM) 为跨数据集 HAR 生成传感器读数和活动标签的语义解释的新系统。这种方法不仅可以缓解跨数据集的异质性，还可以增强对新活动的识别。LanHAR 采用迭代再生方法，使用 LLM 和两阶段训练框架生成高质量的语义解释，该框架将传感器读数和活动标签的语义解释连接起来。这最终导致了适合移动部署的轻量级传感器编码器，使任何传感器读数都可以映射到语义解释空间中。在四个公共数据集上的实验表明，我们的方法在跨数据集 HAR 和新活动识别方面都明显优于最先进的方法。源代码将公开发布。]]></description>
      <guid>https://arxiv.org/abs/2410.00003</guid>
      <pubDate>Wed, 02 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多模式停电预测，实现快速灾难响应和资源分配</title>
      <link>https://arxiv.org/abs/2410.00017</link>
      <description><![CDATA[arXiv:2410.00017v1 公告类型：新
摘要：由于气候变化，极端天气事件越来越常见，带来了重大风险。为了减轻进一步的损害，转向可再生能源势在必行。不幸的是，受影响最严重的代表性不足的社区往往最后才得到基础设施的改善。我们提出了一种新颖的视觉时空框架，用于预测大型飓风前后的夜间灯光 (NTL)、停电严重程度和位置。我们解决方案的核心是视觉时空图神经网络 (VST-GNN)，用于从图像中学习空间和时间连贯性。我们的工作让人们意识到代表性不足的地区迫切需要增强能源基础设施，例如未来的光伏 (PV) 部署。通过确定停电的严重程度和局部性，我们的计划旨在提高政策制定者和社区利益相关者的认识并促使他们采取行动。最终，这项工作旨在赋予能源基础设施脆弱的地区权力，增强高风险社区的恢复力和可靠性。]]></description>
      <guid>https://arxiv.org/abs/2410.00017</guid>
      <pubDate>Wed, 02 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ACE：全能创作者和编辑者，通过扩散变压器遵循指令</title>
      <link>https://arxiv.org/abs/2410.00086</link>
      <description><![CDATA[arXiv:2410.00086v1 公告类型：新
摘要：扩散模型已成为一种强大的生成技术，并被发现适用于各种场景。大多数现有的基础扩散模型主要用于文本引导的视觉生成，不支持多模态条件，而多模态条件对于许多视觉编辑任务至关重要。这种限制使得这些基础扩散模型无法像自然语言处理领域的 GPT-4 一样，在视觉生成领域充当统一模型。在这项工作中，我们提出了 ACE，一个全能的创造者和编辑器，它在广泛的视觉生成任务中实现了与那些专家模型相当的性能。为了实现这一目标，我们首先引入一种称为长上下文条件单元 (LCU) 的统一条件格式，并提出一种以 LCU 为输入的新型基于 Transformer 的扩散模型，旨在跨各种生成和编辑任务进行联合训练。此外，我们提出了一种有效的数据收集方法来解决缺乏可用训练数据的问题。它涉及使用基于合成或基于聚类的管道获取成对图像，并利用经过微调的多模态大型语言模型为这些图像对提供准确的文本指令。为了全面评估我们模型的性能，我们在各种视觉生成任务中建立了手动注释的成对数据基准。大量的实验结果证明了我们的模型在视觉生成领域的优越性。得益于我们模型的一体化功能，我们可以轻松构建一个多模态聊天系统，该系统使用单个模型作为后端来响应任何交互式图像创建请求，从而避免了视觉代理中通常使用的繁琐管道。代码和模型将在项目页面上提供：https://ali-vilab.github.io/ace-page/。]]></description>
      <guid>https://arxiv.org/abs/2410.00086</guid>
      <pubDate>Wed, 02 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CVVLSNet：使用部分联网车辆轨迹数据进行车辆位置和速度估计</title>
      <link>https://arxiv.org/abs/2410.00132</link>
      <description><![CDATA[arXiv:2410.00132v1 公告类型：新 
摘要：实时估计车辆位置和速度对于开发交通管理和控制中的许多有益的交通应用（例如自适应信号控制）至关重要。通信技术的最新进展促进了联网汽车 (CV) 的出现，联网汽车可以与附近的 CV 或基础设施共享交通信息。在连接的早期阶段，只有一部分车辆是 CV。那些非 CV（NC）的位置和速度是无法访问的，必须进行估计才能获得完整的交通信息。为了解决上述问题，本文提出了一种基于 CV 的新型车辆位置和速度估计网络 CVVLSNet，仅使用部分 CV 轨迹数据即可同时估计车辆位置和速度。首先提出了一种道路单元占用率 (RCO) 方法来表示可变的车辆状态信息。只需融合 RCO 表示即可集成时空交互。然后，引入以 Coding-RAte TransformEr (CRATE) 网络为主干的 CVVLSNet 来估计车辆位置和速度。此外，损失函数中还考虑了车辆的物理尺寸约束。大量实验表明，在各种 CV 渗透率、信号时序和容量比下，所提出的方法明显优于现有方法。]]></description>
      <guid>https://arxiv.org/abs/2410.00132</guid>
      <pubDate>Wed, 02 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>EEG 情绪副驾驶：修剪 LLM 以进行情绪 EEG 解释并辅助生成医疗记录</title>
      <link>https://arxiv.org/abs/2410.00166</link>
      <description><![CDATA[arXiv:2410.00166v1 公告类型：新
摘要：在情感计算 (AC) 和脑机接口 (BMI) 领域，通过分析生理和行为信号来辨别个体情绪状态已成为一个关键的研究前沿。虽然基于深度学习的方法在 EEG 情绪识别方面取得了显着进步，特别是在特征提取和模式识别方面，但在实现端到端情绪计算方面仍然存在重大挑战，包括实时处理、个体适应和无缝用户交互。本文介绍了 EEG Emotion Copilot，这是一个利用在本地环境中运行的轻量级大型语言模型 (LLM) 的系统。该系统旨在首先直接从 EEG 信号中识别情绪状态，然后生成个性化的诊断和治疗建议，最后支持电子病历的自动化。所提出的解决方案强调情绪识别的准确性和增强的用户体验，并通过直观的界面促进参与者交互。我们进一步讨论了数据框架的构建、模型修剪、训练和部署策略，旨在提高实时性能和计算效率。我们还致力于解决隐私问题，重点关注合乎道德的数据收集、处理和用户个人信息的保护。通过这些努力，我们旨在推动 AC 在医学领域的应用，为心理健康诊断和治疗提供创新方法。]]></description>
      <guid>https://arxiv.org/abs/2410.00166</guid>
      <pubDate>Wed, 02 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DreamStruct：通过合成数据生成了解幻灯片和用户界面</title>
      <link>https://arxiv.org/abs/2410.00201</link>
      <description><![CDATA[arXiv:2410.00201v1 公告类型：新
摘要：让机器理解幻灯片和用户界面等结构化视觉效果对于让残障人士能够使用它们至关重要。然而，通过计算实现这种理解需要手动收集和注释数据，这既费时又费力。为了克服这一挑战，我们提出了一种使用代码生成生成带有目标标签的合成结构化视觉效果的方法。我们的方法允许人们创建带有内置标签的数据集并使用少量人工注释的示例来训练模型。我们在理解幻灯片和 UI 的三个任务中展示了性能改进：识别视觉元素、描述视觉内容和对视觉内容类型进行分类。]]></description>
      <guid>https://arxiv.org/abs/2410.00201</guid>
      <pubDate>Wed, 02 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAnimals：重新审视动物的人员重新识别，以实现更好的泛化</title>
      <link>https://arxiv.org/abs/2410.00204</link>
      <description><![CDATA[arXiv:2410.00204v1 公告类型：新
摘要：本文解决了动物重新识别的挑战，这是一个新兴领域，与人员重新识别有相似之处，但由于物种、环境和姿势的多样性而呈现出独特的复杂性。为了促进该领域的研究，我们引入了 OpenAnimals，这是一个灵活且可扩展的代码库，专为动物重新识别而设计。我们通过重新审视几种最先进的人员重新识别方法（包括 BoT、AGW、SBS 和 MGN）进行了全面研究，并评估了它们在动物重新识别基准（如 HyenaID、LeopardID、SeaTurtleID 和 WhaleSharkID）上的有效性。我们的研究结果表明，虽然一些技术可以很好地推广，但许多技术却不能，这突显了这两项任务之间的显著差异。为了弥补这一差距，我们提出了 ARBase，这是一个强大的 \textbf{Base} 模型，专门用于 \textbf{动物 \textbf{R}e 识别，它结合了大量实验的见解，并引入了简单而有效的面向动物的设计。实验表明，ARBase 的表现始终优于现有基线，在各种基准测试中都达到了最先进的性能。]]></description>
      <guid>https://arxiv.org/abs/2410.00204</guid>
      <pubDate>Wed, 02 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MM-Conv：虚拟人类的多模态对话数据集</title>
      <link>https://arxiv.org/abs/2410.00253</link>
      <description><![CDATA[arXiv:2410.00253v1 公告类型：新
摘要：在本文中，我们介绍了一种使用 VR 耳机捕获的新型数据集，用于记录物理模拟器 (AI2-THOR) 中参与者之间的对话。我们的主要目标是通过在参考设置中结合丰富的上下文信息来扩展同声手势生成领域。参与者参与各种对话场景，所有这些都基于参考通信任务。该数据集提供了一组丰富的多模式记录，例如动作捕捉、语音、凝视和场景图。这个全面的数据集旨在通过提供多样化且上下文丰富的数据来增强对 3D 场景中手势生成模型的理解和开发。]]></description>
      <guid>https://arxiv.org/abs/2410.00253</guid>
      <pubDate>Wed, 02 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ImmersePro：通过隐式视差学习实现端到端立体视频合成</title>
      <link>https://arxiv.org/abs/2410.00262</link>
      <description><![CDATA[arXiv:2410.00262v1 公告类型：新
摘要：我们引入了 \textit{ImmersePro}，这是一个专门用于将单视图视频转换为立体视频的创新框架。该框架利用时空注意机制，在视频数据上采用一种新颖的双分支架构，包括视差分支和上下文分支。 \textit{ImmersePro} 采用隐式视差引导，无需显式视差图即可从视频序列生成立体对，从而减少与视差估计模型相关的潜在错误。除了技术进步之外，我们还引入了 YouTube-SBS 数据集，这是来自 YouTube 的 423 个立体视频的综合集合。该数据集的规模史无前例，包含超过 700 万个立体对，旨在促进立体视频生成模型的训练和基准测试。我们的实验证明了 \textit{ImmersePro} 在制作高质量立体视频方面的有效性，与现有方法相比有显着改进。与最佳竞争对手立体声单声道相比，我们在数量上将结果提高了 11.76% (L1)、6.39% (SSIM) 和 5.10% (PSNR)。]]></description>
      <guid>https://arxiv.org/abs/2410.00262</guid>
      <pubDate>Wed, 02 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有分层知识增强的程序感知手术视频语言预训练</title>
      <link>https://arxiv.org/abs/2410.00263</link>
      <description><![CDATA[arXiv:2410.00263v1 公告类型：新
摘要：由于知识领域的差距和多模态数据的稀缺性，外科视频语言预训练 (VLP) 面临着独特的挑战。本研究旨在通过解决外科讲座视频中的文本信息丢失问题和外科 VLP 的时空挑战来弥合差距。我们提出了一种分层知识增强方法和一种新颖的程序编码外科知识增强视频语言预训练 (PeskaVLP) 框架来解决这些问题。知识增强使用大型语言模型 (LLM) 来细化和丰富外科概念，从而提供全面的语言监督并降低过度拟合的风险。PeskaVLP 将语言监督与视觉自监督相结合，构建硬负样本并采用基于动态时间规整 (DTW) 的损失函数来有效理解跨模态程序对齐。在多个公共手术场景理解和跨模态检索数据集上进行的大量实验表明，我们提出的方法显着提高了零样本传输性能，并为进一步推进手术场景理解提供了通用的视觉表现。]]></description>
      <guid>https://arxiv.org/abs/2410.00263</guid>
      <pubDate>Wed, 02 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>类别无关的视觉时间场景草图语义分割</title>
      <link>https://arxiv.org/abs/2410.00266</link>
      <description><![CDATA[arXiv:2410.00266v1 公告类型：新
摘要：场景草图语义分割是各种应用的关键任务，包括草图到图像检索和场景理解。现有的草图分割方法将草图视为位图图像，由于从矢量到图像格式的转变，导致笔划之间的时间顺序丢失。此外，这些方法很难从训练数据中缺失的类别中分割出对象。在本文中，我们提出了一种用于场景草​​图语义分割的类无关视觉时间网络 (CAVT)。CAVT 采用类无关对象检测器来检测场景中的单个对象，并通过其后处理模块对实例的笔划进行分组。这是第一种在场景草图中在实例和笔划级别执行分割的方法。此外，缺乏具有实例和笔划级别类注释的徒手场景草图数据集。为了填补这一空白，我们收集了最大的手绘实例和笔画级场景草图数据集 (FrISS)，其中包含 1K 幅场景草图，并涵盖 403 个带有密集注释的对象类。在 FrISS 和其他数据集上进行的大量实验证明了我们的方法比最先进的场景草图分割模型具有更优越的性能。代码和数据集将在接受后公开。]]></description>
      <guid>https://arxiv.org/abs/2410.00266</guid>
      <pubDate>Wed, 02 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>KPCA-CAM：使用核 PCA 实现深度计算机视觉模型的视觉可解释性</title>
      <link>https://arxiv.org/abs/2410.00267</link>
      <description><![CDATA[arXiv:2410.00267v1 公告类型：新
摘要：深度学习模型通常充当黑匣子，无法为其预测提供直接的推理。对于计算机视觉模型尤其如此，它处理像素值的张量以在图像分类和对象检测等任务中生成结果。为了阐明这些模型的推理，类激活图 (CAM) 用于突出显示影响模型输出的显着区域。这项研究介绍了 KPCA-CAM，这是一种旨在通过改进的类激活图增强卷积神经网络 (CNN) 可解释性的技术。KPCA-CAM 利用核技巧的主成分分析 (PCA) 更有效地捕获 CNN 激活中的非线性关系。通过使用核函数将数据映射到高维空间并从该变换后的超平面中提取主成分，KPCA-CAM 提供了底层数据流形的更准确表示。这使得人们能够更深入地了解影响 CNN 决策的特征。在 ILSVRC 数据集上对不同 CNN 模型进行的经验评估表明，与现有 CAM 算法相比，KPCA-CAM 可以生成更精确的激活图，从而更清楚地洞察模型的推理。这项研究推动了 CAM 技术的发展，为研究人员和从业者提供了强大的工具，以更深入地洞察 CNN 决策过程和整体行为。]]></description>
      <guid>https://arxiv.org/abs/2410.00267</guid>
      <pubDate>Wed, 02 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>关于社交媒体图像无监督分类的大型单模和多模模型：以自然对人类的贡献为例</title>
      <link>https://arxiv.org/abs/2410.00275</link>
      <description><![CDATA[arXiv:2410.00275v1 公告类型：新
摘要：社交媒体图像已被证明是了解人类与文化遗产、生物多样性和自然等重要主题之间互动的宝贵信息来源。将这些图像分组为许多没有标签的语义上有意义的集群的任务具有挑战性，因为这些图像的视觉内容具有高度的多样性和复杂性，而且数量庞大。另一方面，大型视觉模型 (LVM)、大型语言模型 (LLM) 和大型视觉语言模型 (LVLM) 的最新进展为探索新的高效和可扩展解决方案提供了重要机会。这项工作提出、分析和比较了基于一个或多个最先进的 LVM、LLM 和 LVLM 的各种方法，用于将社交媒体图像映射到许多预定义的类别中。作为案例研究，我们考虑理解人与自然之间相互作用的问题，也称为自然对人类的贡献或文化生态系统服务 (CES)。我们的实验表明，表现最佳且可提供极具竞争力的结果的方法是在小型标记数据集上经过微调的 LVM DINOv2 和使用简单提示的专有 GPT-4（gpt-4o-mini）等 LVLM 模型。]]></description>
      <guid>https://arxiv.org/abs/2410.00275</guid>
      <pubDate>Wed, 02 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于深度学习的四旋翼无人机检测与跟踪方法性能评估</title>
      <link>https://arxiv.org/abs/2410.00285</link>
      <description><![CDATA[arXiv:2410.00285v1 公告类型：新
摘要：无人驾驶飞行器 (UAV) 在各个领域越来越受欢迎，它提供了许多好处，但也给隐私和安全带来了重大挑战。本文研究了用于检测和跟踪四旋翼无人机的最先进的解决方案，以解决这些问题。对尖端的深度学习模型（特别是 YOLOv5 和 YOLOv8 系列）进行了评估，以准确、快速地识别无人机。此外，还集成了强大的跟踪系统 BoT-SORT 和 Byte Track，以确保即使在具有挑战性的条件下也能进行可靠的监控。我们对 DUT 数据集的测试表明，虽然 YOLOv5 模型在检测精度方面通常优于 YOLOv8，但 YOLOv8 模型在识别不太明显的物体方面表现出色，展示了它们的适应性和先进的功能。此外，BoT-SORT 表现出优于 Byte Track 的性能，在大多数情况下实现了更高的 IoU 和更低的中心误差，表明跟踪更准确、更稳定。
代码：https://github.com/zmanaa/UAV_detection_and_tracking 跟踪演示：https://drive.google.com/file/d/1pe6HC5kQrgTbA2QrjvMN-yjaZyWeAvDT/view?usp=sharing]]></description>
      <guid>https://arxiv.org/abs/2410.00285</guid>
      <pubDate>Wed, 02 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>深入研究短视频参与度预测</title>
      <link>https://arxiv.org/abs/2410.00289</link>
      <description><![CDATA[arXiv:2410.00289v1 公告类型：新 
摘要：理解和建模社交媒体平台上用户生成内容 (UGC) 短视频的受欢迎程度是一项关键挑战，对内容创建者和推荐系统具有广泛的影响。本研究深入探讨了预测用户交互有限的新发布视频的参与度的复杂性。令人惊讶的是，我们的研究结果表明，以前的视频质量评估数据集中的平均意见分数与视频参与度水平没有很强的相关性。为了解决这个问题，我们引入了一个包含来自 Snapchat 的 90,000 个真实 UGC 短视频的庞大数据集。我们提出了两个指标：归一化平均观看百分比 (NAWP) 和参与度持续率 (ECR)，而不是依赖观看次数、平均观看时间或点赞率来描述短视频的参与度。研究了包括视觉内容、背景音乐和文本数据在内的综合多模式特征，以增强参与度预测。利用提出的数据集和两个关键指标，我们的方法证明了其仅从视频内容预测短视频参与度的能力。]]></description>
      <guid>https://arxiv.org/abs/2410.00289</guid>
      <pubDate>Wed, 02 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>