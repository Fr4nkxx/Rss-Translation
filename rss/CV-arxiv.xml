<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Fri, 30 Aug 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>不平衡数据环境下的联合人脸识别元学习</title>
      <link>https://arxiv.org/abs/2408.16003</link>
      <description><![CDATA[arXiv:2408.16003v1 公告类型：新
摘要：围绕人脸图像数据的隐私问题日益严重，需要能够保证用户隐私的新技术。一种声称可以实现更好用户隐私的人脸识别技术是联邦人脸识别 (FRR)，它是联邦学习 (FL) 的一个子领域。然而，由于需要处理的类数量众多，FFR 因数据的异构性而面临挑战。为了解决这个问题，人们在个性化 FL 领域寻求解决方案。这项工作引入了三个基于 CelebA 数据集的新数据分区，每个分区都有不同形式的数据异构性。它还在 FFR 设置中提出了 Hessian-Free 模型不可知元学习 (HF-MAML)。我们表明，HF-MAML 在三个不同的 CelebA 数据分区上的验证测试得分高于当前的 FFR 模型。特别是，验证分数在异构数据分区中的提高最多。为了平衡个性化与有效全局模型的开发，为损失函数引入了一个嵌入正则化项。该项可与 HF-MAML 结合使用，并被证明可以提高全局模型验证性能。最后，这项研究进行了公平性分析，表明 HF-MAML 及其嵌入正则化扩展可以通过降低客户端评估分数的标准差来提高公平性。]]></description>
      <guid>https://arxiv.org/abs/2408.16003</guid>
      <pubDate>Fri, 30 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多世界逆向渲染</title>
      <link>https://arxiv.org/abs/2408.16005</link>
      <description><![CDATA[arXiv:2408.16005v1 公告类型：新
摘要：在基于物理的逆向渲染器中优化表面时，不连续的可见性变化仍然是一个主要瓶颈。许多先前的工作已经提出了复杂的算法和数据结构，以更有效地采样可见性轮廓。
我们的工作提出了另一种解决方案：我们不是在局部区分暂定的表面，而是区分表面的体积扰动。我们将其称为多世界表示，因为它模拟了输入数据集的冲突解释（世界）的非相互作用叠加。每个世界都与其他世界光学隔离，从而产生了一种新的传输定律，将我们的方法与基于指数随机介质的先前工作区分开来。
由此产生的蒙特卡罗算法比以前的方法更简单、更高效。我们证明我们的方法促进了快速收敛，无论是在总迭代次数还是每次迭代的成本方面。]]></description>
      <guid>https://arxiv.org/abs/2408.16005</guid>
      <pubDate>Fri, 30 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有空间记忆的 3D 重建</title>
      <link>https://arxiv.org/abs/2408.16061</link>
      <description><![CDATA[arXiv:2408.16061v1 公告类型：新
摘要：我们提出了 Spann3R，一种从有序或无序图像集合中进行密集 3D 重建的新方法。Spann3R 建立在 DUSt3R 范式之上，使用基于变换器的架构直接从图像中回归点图，而无需任何场景或相机参数的先验知识。与 DUSt3R 不同，DUSt3R 预测每个图像对的点图，每个点图都以局部坐标系表示，而 Spann3R 可以预测以全局坐标系表示的每个图像点图，从而无需基于优化的全局对齐。Spann3R 的关键思想是管理一个外部空间存储器，该存储器学习跟踪所有先前的相关 3D 信息。然后，Spann3R 查询此空间存储器以预测全局坐标系中下一帧的 3D 结构。 Spann3R 利用 DUSt3R 的预训练权重，并在数据集子集上进一步微调，在各种未见过的数据集上表现出竞争性的性能和泛化能力，并可以实时处理有序图像集合。项目页面：\url{https://hengyiwang.github.io/projects/spanner}]]></description>
      <guid>https://arxiv.org/abs/2408.16061</guid>
      <pubDate>Fri, 30 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ChartEye：用于海图信息提取的深度学习框架</title>
      <link>https://arxiv.org/abs/2408.16123</link>
      <description><![CDATA[arXiv:2408.16123v1 公告类型：新
摘要：图表和信息图作为数据可视化手段在各个领域的广泛使用激发了最近对自动图表理解的研究。然而，由于样式变化，从图表图像中提取信息是一个复杂的多任务过程，因此，设计端到端系统具有挑战性。在本研究中，我们提出了一个基于深度学习的框架，为图表信息提取流程中的关键步骤提供了解决方案。所提出的框架利用分层视觉变换器完成图表类型和文本角色分类任务，而 YOLOv7 则用于文本检测。然后使用超分辨率生成对抗网络增强检测到的文本，以改进 OCR 的识别输出。在基准数据集上的实验结果表明，我们提出的框架在每个阶段都取得了优异的性能，图表类型分类的 F1 分数为 0.97，文本角色分类的 F1 分数为 0.91，文本检测的平均精度为 0.95。]]></description>
      <guid>https://arxiv.org/abs/2408.16123</guid>
      <pubDate>Fri, 30 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用骨干基础模型评估没有人口统计数据的胸部 X 光检查的公平性</title>
      <link>https://arxiv.org/abs/2408.16130</link>
      <description><![CDATA[arXiv:2408.16130v1 公告类型：新
摘要：确保不同人群的一致性能并将公平性纳入机器学习模型对于推进医学图像诊断和促进公平医疗保健至关重要。然而，许多数据库不提供受保护的属性或包含不平衡的人口群体表示，这使得评估不同人口统计数据中的模型性能以及应用依赖于这些属性的偏差缓解技术变得复杂。本研究旨在调查使用 Foundation Models 的主干作为嵌入提取器来创建代表受保护属性（例如性别和年龄）的组的有效性。我们建议在偏差缓解的不同阶段使用这些组，包括预处理、处理中和评估。使用数据库分布内和分布外场景，可以确定该方法可以在两个数据库中创建代表性别的组，并将性别属性分布内和分布外的差异减少 4.44% 和 6.16%。然而，该模型在处理年龄属性时缺乏稳健性，这凸显了需要更公平、更稳健的基础模型。这些发现表明，在我们缺乏属性知识的情况下，公平评估可以发挥作用，有助于开发更公平的医疗诊断。]]></description>
      <guid>https://arxiv.org/abs/2408.16130</guid>
      <pubDate>Fri, 30 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>数据有效的泛化是否会加剧基础模型中的偏见？</title>
      <link>https://arxiv.org/abs/2408.16154</link>
      <description><![CDATA[arXiv:2408.16154v1 公告类型：新
摘要：基础模型已成为不同领域中具有标签效率的稳健模型。在医学成像中，由于难以获取标记数据，这些模型有助于促进医学诊断的发展。然而，目前尚不清楚使用大量未标记数据（由于预训练期间存在敏感属性而产生偏差）是否会影响模型的公平性。本研究检查了当基础模型（RetFound）应用于微调巴西多标签眼科数据集（BRSET）时出现的偏差，该数据集与预训练数据集的人群不同。与监督学习相比，模型评估表明，基础模型有可能缩小不同性别和年龄组之间最大 AUC 和最小 AUC 评估之间的差距。然而，在数据高效的泛化中，当数据量减少时，模型会增加偏差。这些发现表明，在数据有限的现实场景中部署基础模型时，应该考虑公平性问题的可能性。]]></description>
      <guid>https://arxiv.org/abs/2408.16154</guid>
      <pubDate>Fri, 30 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>VLM4Bio：用于评估从生物图像中发现特征的预训练视觉语言模型的基准数据集</title>
      <link>https://arxiv.org/abs/2408.16176</link>
      <description><![CDATA[arXiv:2408.16176v1 公告类型：新
摘要：图像正日益成为记录地球生物多样性的货币，为加速生物体生物学领域的科学发现提供了新的机会，尤其是随着大型视觉语言模型 (VLM) 的出现。我们想知道，预先训练的 VLM 是否可以帮助科学家回答一系列与生物学相关的问题，而无需进行任何额外的微调。在本文中，我们使用一个新数据集 VLM4Bio 评估了 12 个最先进 (SOTA) VLM 在生物体生物学领域的有效性，该数据集由 469K 个问答对组成，涉及来自三组生物的 30K 张图像：鱼类、鸟类和蝴蝶，涵盖五项与生物学相关的任务。我们还探讨了应用提示技术和推理幻觉测试对 VLM 性能的影响，为当前 SOTA VLM 使用图像回答生物学相关问题的能力提供了新的见解。运行本文报告的所有分析的代码和数据集可以在 https://github.com/sammarfy/VLM4Bio 找到。]]></description>
      <guid>https://arxiv.org/abs/2408.16176</guid>
      <pubDate>Fri, 30 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>估计跟踪对象组中的动态流特征</title>
      <link>https://arxiv.org/abs/2408.16190</link>
      <description><![CDATA[arXiv:2408.16190v1 公告类型：新
摘要：解释图像序列中捕获的运动对于广泛的计算机视觉应用至关重要。典型的估计方法包括光流 (OF)，它近似场景中瞬间的表观运动，以及多物体跟踪 (MOT)，它跟踪物体随时间的运动。通常，场景中物体的运动由某些底层动态系统控制，可以通过分析物体组的运动来推断。然而，标准的运动分析并非旨在从轨迹数据中直观地了解流动动力学，这使得这种测量在实践中很困难。这项工作的目标是将基于梯度的动态系统分析扩展到现实世界的应用，这些应用的特点是复杂、特征丰富的图像序列和不完美的追踪器。使用深度视觉网络跟踪追踪器轨迹，并使用拉格朗日梯度回归 (LGR) 近似梯度，LGR 是一种旨在从稀疏数据中估计空间梯度的工具。从梯度中可以识别出相干旋转区域和传输障碍等动态特征。所提出的方法实施起来经济实惠，并可以进行高级研究，包括对单个图像序列中两个不同对象类别的运动分析。在标准梯度分析不适用的数据集上给出了该方法的两个示例。]]></description>
      <guid>https://arxiv.org/abs/2408.16190</guid>
      <pubDate>Fri, 30 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DLM-VMTL:用于异构数据视频多任务提示学习的双层映射器</title>
      <link>https://arxiv.org/abs/2408.16195</link>
      <description><![CDATA[arXiv:2408.16195v1 Announce Type: new 
摘要：近年来，视频理解任务的主干网络参数不断增加，甚至达到十亿级。无论是在视频基础模型上微调特定任务，还是对为特定任务设计的模型进行预训练，都会产生很大的开销。如何让这些模型发挥除自身任务以外的其他价值成为一个值得研究的问题。多任务学习（MTL）使视觉任务在联合训练的同时从其他任务中获得丰富的可共享知识。它在图像识别任务尤其是密集预测任务中得到了充分的探索。然而，由于缺乏多标签视频数据，它在视频领域很少使用。本文提出了一种异构数据视频多任务提示学习（VMTL）方法来解决这个问题。与图像域中的不同，提出了一个双层映射器（DLM）来将可共享知识提取到视觉提示中并将其与主要任务的表示对齐。大量实验证明，我们的 DLM-VMTL 在 6 种不同的视频理解任务和 11 个数据集上的表现优于基线。]]></description>
      <guid>https://arxiv.org/abs/2408.16195</guid>
      <pubDate>Fri, 30 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PolarBEVDet：探索鸟瞰视图中多视图 3D 物体检测的极坐标表示</title>
      <link>https://arxiv.org/abs/2408.16200</link>
      <description><![CDATA[arXiv:2408.16200v1 公告类型：新
摘要：最近，基于 LSS 的多视图 3D 物体检测为自动驾驶提供了一种经济且易于部署的解决方案。然而，所有现有的基于 LSS 的方法都将多视图图像特征转换为笛卡尔鸟瞰图 (BEV) 表示，这没有考虑到非均匀图像信息分布，并且几乎不利用视图对称性。在本文中，为了通过常规卷积调整图像信息分布并保持视图对称性，我们提出采用极坐标 BEV 表示来替代笛卡尔 BEV 表示。为了实现这一点，我们精心定制了三个模块：极坐标视图变换器用于生成极坐标 BEV 表示，极坐标时间融合模块用于融合历史极坐标 BEV 特征，以及极坐标检测头用于预测物体的极坐标参数化表示。此外，我们设计了一个 2D 辅助检测头和一个空间注意增强模块，分别用于提高透视图和 BEV 中的特征提取质量。最后，我们将上述改进集成到一种新型多视图 3D 物体检测器 PolarBEVDet 中。在 nuScenes 上的实验表明 PolarBEVDet 实现了卓越的性能。代码可在 https://github.com/Yzichen/PolarBEVDet.git 上找到。]]></description>
      <guid>https://arxiv.org/abs/2408.16200</guid>
      <pubDate>Fri, 30 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Uni-3DAD：基于 GAN-Inversion 的无模型产品通用 3D 异常检测</title>
      <link>https://arxiv.org/abs/2408.16201</link>
      <description><![CDATA[arXiv:2408.16201v1 公告类型：新
摘要：异常检测是制造系统中长期存在的挑战。传统上，异常检测依赖于人工检查员。然而，3D 点云因其对环境因素的鲁棒性和表示几何数据的能力而受到关注。现有的 3D 异常检测方法通常分为两类。一类将扫描的 3D 点云与设计文件进行比较，假设这些文件始终可用。然而，这种假设在许多存在无模型产品的实际应用中经常被违反，例如新鲜农产品（即“饼干”、“土豆”等）、假牙、骨头等。另一类将扫描的 3D 点云的补丁与名为存储库的正常补丁库进行比较。然而，这些方法通常无法检测到不完整的形状，这是一种相当常见的缺陷类型（即不同产品的缺失部分）。主要挑战在于 3D 点云中的缺失区域代表扫描点的缺失。这使得将缺失区域与存储库中现有的点云块进行比较变得不可行。为了解决这两个挑战，我们提出了一个统一的、无监督的 3D 异常检测框架，该框架能够识别无模型产品上的所有类型的缺陷。我们的方法集成了两个检测模块：基于特征的检测模块和基于重建的检测模块。基于特征的检测涵盖几何缺陷，例如凹痕、孔洞和裂纹，而基于重建的方法则检测缺失区域。此外，我们采用一类支持向量机 (OCSVM) 来融合两个模块的检测结果。结果表明：(1) 我们提出的方法在识别不完整形状方面优于最先进的方法，(2) 它在检测所有其他类型的异常方面仍与 SOTA 方法保持相当的性能。]]></description>
      <guid>https://arxiv.org/abs/2408.16201</guid>
      <pubDate>Fri, 30 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>M4CXR：探索多模态大型语言模型在胸部 X 光片解读中的多任务潜力</title>
      <link>https://arxiv.org/abs/2408.16213</link>
      <description><![CDATA[arXiv:2408.16213v1 公告类型：新
摘要：人工智能的快速发展，尤其是大型语言模型 (LLM) 的发展，对包括医疗保健在内的各个领域产生了重大影响。在胸部 X 光 (CXR) 分析中，先前的研究采用了 LLM，但存在局限性：要么未充分利用 LLM 的多任务处理能力，要么缺乏临床准确性。本文介绍了 M4CXR，这是一种旨在增强 CXR 解释的多模态 LLM。该模型在视觉指令跟踪数据集上进行训练，该数据集以对话格式集成了各种特定于任务的数据集。因此，该模型支持多项任务，例如医疗报告生成 (MRG)、视觉基础和视觉问答 (VQA)。M4CXR 通过采用思路链提示策略在 MRG 中实现了最先进的临床准确性，在该策略中，它可以识别 CXR 图像中的发现并随后生成相应的报告。该模型可根据可用输入（例如单图像、多图像和多研究环境）适应各种 MRG 场景。除了 MRG，M4CXR 的视觉基础执行水平可与专业模型相媲美，并且在 VQA 中也表现出色。定量和定性评估都表明 M4CXR 在 MRG、视觉基础和 VQA 方面的多功能性，同时始终保持临床准确性。]]></description>
      <guid>https://arxiv.org/abs/2408.16213</guid>
      <pubDate>Fri, 30 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用大规模预训练模型进行无训练视频时间基础研究</title>
      <link>https://arxiv.org/abs/2408.16219</link>
      <description><![CDATA[arXiv:2408.16219v1 公告类型：新 
摘要：视频时间定位旨在识别未修剪视频中与给定自然语言查询最相关的视频片段。现有的视频时间定位模型依赖特定数据集进行训练，数据收集成本高，但在跨数据集和分布外 (OOD) 设置下表现出较差的泛化能力。在本文中，我们提出了一种无需训练的视频时间定位 (TFVTG) 方法，该方法利用了预先训练的大型模型的能力。一个简单的基线是枚举视频中的提案，并使用预先训练的视觉语言模型 (VLM) 根据视觉语言对齐选择最佳提案。然而，大多数现有的 VLM 都是在图像-文本对或修剪后的视频片段-文本对上训练的，这使得它很难 (1) 掌握关系并区分同一视频中多个事件的时间边界； (2) 理解并对视频中事件的动态转换（从一个事件到另一个事件的转换）保持敏感。为了解决这些问题，我们建议利用大型语言模型 (LLM) 来分析查询文本中包含的多个子事件，并分析这些事件之间的时间顺序和关系。其次，我们将子事件拆分为动态转换和静态状态部分，并提出使用 VLM 的动态和静态评分函数来更好地评估事件和描述之间的相关性。最后，对于每个子事件描述，我们使用 VLM 来定位前 k 个提案，并利用 LLM 提供的子事件之间的顺序和关系来过滤和集成这些提案。我们的方法在 Charades-STA 和 ActivityNet Captions 数据集上的零样本视频时间基础方面取得了最佳性能，无需任何训练，并且在跨数据集和 OOD 设置中表现出更好的泛化能力。]]></description>
      <guid>https://arxiv.org/abs/2408.16219</guid>
      <pubDate>Fri, 30 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLaVA-SG：利用场景图作为视觉语言模型中的视觉语义表达</title>
      <link>https://arxiv.org/abs/2408.16224</link>
      <description><![CDATA[arXiv:2408.16224v1 公告类型：新
摘要：大型视觉语言模型 (VLM) 的最新进展通常采用基于视觉变换器 (ViT) 架构的视觉编码器。ViT 将图像划分为块会导致感知碎片化，从而阻碍 VLM 的视觉理解能力。在本文中，我们提出了一种创新的增强功能来解决这一限制，即在 VLM 中引入场景图表达 (SGE) 模块。该模块提取并结构化地表达图像中的复杂语义信息，从而提高 VLM 的基础感知和理解能力。大量实验表明，集成我们的 SGE 模块可显着提高 VLM 在视觉语言任务中的表现，表明其在保留复杂语义细节和促进更好的视觉理解方面的有效性。代码和数据将可用。]]></description>
      <guid>https://arxiv.org/abs/2408.16224</guid>
      <pubDate>Fri, 30 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 PanoGabor 重新审视 360 度深度估计：新的融合视角</title>
      <link>https://arxiv.org/abs/2408.16227</link>
      <description><![CDATA[arXiv:2408.16227v1 公告类型：新
摘要：单目 360 图像的深度估计对于整个 3D 环境的感知非常重要。然而，360 图像中固有的失真和大视场 (FoV) 对这项任务提出了巨大的挑战。为此，现有的主流解决方案通常引入额外的基于透视的 360 表示（\textit{e.g.}，Cubemap）来实现有效的特征提取。然而，无论引入何种表示，它们最终都需要统一为等距矩形投影 (ERP) 格式以进行后续的深度估计，这不可避免地会重新引入麻烦的失真。在这项工作中，我们提出了一个面向失真感知的 Gabor Fusion 框架 (PGFuse) 来解决上述挑战。首先，我们引入了在频域中分析纹理的 Gabor 滤波器，从而扩展了感受野并增强了深度线索。为了解决重新引入的失真问题，我们设计了一种线性纬度感知失真表示方法来生成定制的、失真感知的 Gabor 滤波器（PanoGabor 滤波器）。此外，我们设计了一个通道和空间单向融合模块 (CS-UFM)，该模块集成了所提出的 PanoGabor 滤波器，以将其他表示统一为 ERP 格式，从而提供有效且无失真的功能。考虑到 Gabor 变换的方向敏感性，我们引入了球面梯度约束来稳定这种敏感性。在三个流行的室内 360 基准上的实验结果表明，所提出的 PGFuse 优于现有的最先进解决方案。代码可在接受后提供。]]></description>
      <guid>https://arxiv.org/abs/2408.16227</guid>
      <pubDate>Fri, 30 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>