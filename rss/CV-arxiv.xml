<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Wed, 18 Sep 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>利用人工智能保护野生动物</title>
      <link>https://arxiv.org/abs/2409.10523</link>
      <description><![CDATA[arXiv:2409.10523v1 公告类型：新
摘要：全球生物多样性的迅速下降需要创新的保护策略。本文研究了人工智能 (AI) 在野生动物保护中的应用，重点介绍了 Conservation AI 平台。Conservation AI 利用机器学习和计算机视觉，使用可见光谱和热红外摄像机检测和分类动物、人类和与偷猎有关的物体。该平台使用卷积神经网络 (CNN) 和 Transformer 架构处理这些数据，以监测物种，包括那些极度濒危的物种。实时检测为时间紧迫的情况（例如偷猎）提供了所需的即时响应，而非实时分析则支持长期野生动物监测和栖息地健康评估。来自欧洲、北美、非洲和东南亚的案例研究凸显了该平台在物种识别、生物多样性监测和偷猎预防方面的成功。论文还讨论了与数据质量、模型准确性和物流约束相关的挑战，同时概述了未来的发展方向，包括技术进步、向新地理区域的扩展以及与当地社区和政策制定者的更深入合作。保护人工智能代表着在应对野生动物保护的紧迫挑战方面迈出了重要一步，提供了一种可扩展且适应性强的解决方案，可在全球范围内实施。]]></description>
      <guid>https://arxiv.org/abs/2409.10523</guid>
      <pubDate>Wed, 18 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从潜在流形到引擎流形：分析 ImageBind 的多模态嵌入空间</title>
      <link>https://arxiv.org/abs/2409.10528</link>
      <description><![CDATA[arXiv:2409.10528v1 公告类型：新
摘要：本研究调查了 ImageBind 为在线汽车零部件列表生成有意义的融合多模态嵌入的能力。我们提出了一种简单的嵌入融合工作流程，旨在捕获图像/文本对的重叠信息，最终将帖子的语义组合成联合嵌入。将此类融合嵌入存储在矢量数据库中后，我们进行了降维实验，并通过聚类和检查最接近每个聚类质心的帖子提供了经验证据来传达联合嵌入的语义质量。此外，我们对 ImageBind 的新兴零样本跨模态检索的初步发现表明，纯音频嵌入可以与语义相似的市场列表相关联，这为未来的研究指明了潜在的途径。]]></description>
      <guid>https://arxiv.org/abs/2409.10528</guid>
      <pubDate>Wed, 18 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉中的道德挑战：确保隐私并减轻公开数据集中的偏见</title>
      <link>https://arxiv.org/abs/2409.10533</link>
      <description><![CDATA[arXiv:2409.10533v2 公告类型：新
摘要：本文旨在阐明创建和部署计算机视觉技术的道德问题，特别是在使用公开可用的数据集时。由于机器学习和人工智能的快速发展，计算机视觉已成为许多行业的重要工具，包括医疗保健、安全系统和贸易。然而，由于对其后果的知情讨论，大量使用通常在未经同意的情况下收集的视觉数据引发了对隐私和偏见的重大担忧。本文还通过分析通常用于训练计算机视觉模型的流行数据集（例如 COCO、LFW、ImageNet、CelebA、PASCAL VOC 等）来研究这些问题。我们提供了一个全面的道德框架，以解决有关保护个人权利、最大限度地减少偏见以及开放性和责任感的挑战。我们的目标是鼓励人工智能的发展，将社会价值观和道德标准考虑在内，以避免任何公众伤害。]]></description>
      <guid>https://arxiv.org/abs/2409.10533</guid>
      <pubDate>Wed, 18 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过对比学习来学习对话中的同声手势表征：内在评价</title>
      <link>https://arxiv.org/abs/2409.10535</link>
      <description><![CDATA[arXiv:2409.10535v1 公告类型：新
摘要：在面对面对话中，同声手势的形式意义关系因上下文因素而异，例如手势所指的内容和说话者的个人特征。这些因素使同声手势表征学习具有挑战性。考虑到手势的多变性和与语音的关系，我们如何学习有意义的手势表征？本文通过采用自监督对比学习技术从骨骼和语音信息中学习手势表征来应对这一挑战。我们提出了一种包括单模态和多模态预训练的方法，以在同现语音中奠定手势表征的基础。对于训练，我们使用一个富含代表性标志性手势的面对面对话数据集。我们通过与人类注释的成对手势相似性进行比较，对学习到的表征进行彻底的内在评估。此外，我们进行诊断探测分析，以评估从学习到的表征中恢复可解释的手势特征的可能性。我们的结果表明，与人类注释的手势相似性呈显著正相关，并表明学习到的表征之间的相似性与对话互动动态相关的良好动机模式一致。此外，我们的研究结果表明，可以从潜在表征中恢复与手势形式有关的几个特征。总体而言，这项研究表明，多模态对比学习是一种很有前途的学习手势表征的方法，这为在更大规模的手势分析研究中使用此类表征打开了大门。]]></description>
      <guid>https://arxiv.org/abs/2409.10535</guid>
      <pubDate>Wed, 18 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>OxML 挑战赛 2023：使用数据增强进行癌症分类</title>
      <link>https://arxiv.org/abs/2409.10544</link>
      <description><![CDATA[arXiv:2409.10544v1 公告类型：新
摘要：癌症是最常见的癌症类型，可表现在身体的各个部位。它分布广泛，可能在身体内的许多位置发展。在医学领域，由于隐私问题，癌症数据通常有限或不可用。此外，即使可用，它也高度不平衡，正类样本稀缺，负类样本丰富。OXML 2023 挑战赛提供了一个小而失衡的数据集，对癌症分类提出了重大挑战。为了解决这些问题，挑战赛的参与者采用了各种方法，依靠预训练模型、预处理技术和少量学习。我们的工作提出了一种结合填充增强和集成的新技术来解决癌症分类挑战。在我们提出的方法中，我们利用五个神经网络的集成并实现填充作为数据增强技术，同时考虑不同的图像大小以增强分类器的性能。使用我们的方法，我们进入前三名并被宣布为获胜者。]]></description>
      <guid>https://arxiv.org/abs/2409.10544</guid>
      <pubDate>Wed, 18 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ResEmoteNet：面部情绪识别中准确率和损失减少的桥梁</title>
      <link>https://arxiv.org/abs/2409.10545</link>
      <description><![CDATA[arXiv:2409.10545v1 公告类型：新
摘要：人脸是一种无声的交流者，通过面部表情表达情感和思想。近年来，随着计算机视觉的进步，面部情绪识别技术取得了重大进展，使机器能够解码面部线索的复杂性。在这项工作中，我们提出了 ResEmoteNet，这是一种用于面部情绪识别的新型深度学习架构，由卷积、挤压激励 (SE) 和残差网络组合设计而成。SE 块的加入有选择地关注人脸的重要特征，增强特征表示并抑制不太相关的特征。这有助于减少损失并提高整体模型性能。我们还将 SE 块与三个残差块集成在一起，有助于通过更深的层学习更复杂的数据表示。我们在三个开源数据库（FER2013、RAF-DB 和 AffectNet）上对 ResEmoteNet 进行了评估，准确率分别达到 79.79%、94.76% 和 72.39%。所提出的网络在这三个数据库中的表现均优于最先进的模型。ResEmoteNet 的源代码可在 https://github.com/ArnabKumarRoy02/ResEmoteNet 上找到。]]></description>
      <guid>https://arxiv.org/abs/2409.10545</guid>
      <pubDate>Wed, 18 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>对自动驾驶视觉深度强化学习中离线训练编码器的检验</title>
      <link>https://arxiv.org/abs/2409.10554</link>
      <description><![CDATA[arXiv:2409.10554v1 公告类型：新
摘要：我们的研究调查了深度强化学习 (DRL) 在复杂的部分可观察马尔可夫决策过程 (POMDP)（例如自动驾驶 (AD)）中面临的挑战，并提出了在这些环境中基于视觉的导航的解决方案。部分可观察性会显著降低 RL 性能，这可以通过增强传感器信息和数据融合来反映更马尔可夫的环境来缓解。然而，这需要一个越来越复杂的感知模块，由于固有的局限性，通过 RL 进行训练非常复杂。随着神经网络架构变得越来越复杂，奖励函数作为误差信号的有效性会降低，因为唯一的监督来源是奖励，而奖励通常是嘈杂的、稀疏的和延迟的。图像中与任务无关的元素，例如天空或某些物体，会带来额外的复杂性。我们的研究采用离线训练的编码器，通过自监督学习利用大型视频数据集来学习可泛化的表示。然后，我们通过 DRL 在这些表示的基础上训练头部网络，以学习在 CARLA AD 模拟器中控制自我车辆。这项研究广泛调查了不同的离线训练编码器的学习方案对 DRL 代理在具有挑战性的 AD 任务中的表现的影响。此外，我们表明，通过观看 BDD100K 驾驶视频学到的特征可以直接迁移到 CARLA 模拟器中，以零样本学习的方式实现车道跟随和防撞。最后，我们探讨了各种架构决策对 RL 网络有效利用迁移表示的影响。因此，在这项工作中，我们介绍并验证了一种获取合适环境表示并将其传输到 RL 网络的最佳方法。]]></description>
      <guid>https://arxiv.org/abs/2409.10554</guid>
      <pubDate>Wed, 18 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>卷积网络作为极小的基础模型：视觉提示和理论视角</title>
      <link>https://arxiv.org/abs/2409.10555</link>
      <description><![CDATA[arXiv:2409.10555v1 公告类型：新 
摘要：与针对特定任务训练的深度神经网络相比，那些在通用数据集（如 ImageNet 分类）上训练的基础深度网络受益于更大规模的数据集、更简单的网络结构和更容易的训练技术。在本文中，我们设计了一个提示模块，它可以对通用深度网络进行少样本适应，以适应新任务。在学习理论的驱动下，我们推导出尽可能简单的提示模块，因为它们在相同的训练误差下具有更好的泛化能力。我们使用视频对象分割案例研究进行实验。我们给出了一个具体的提示模块，即半参数深度森林 (SDForest)，它将相关滤波器、随机森林、图像引导滤波器等几种非参数方法与针对 ImageNet 分类任务训练的深度网络相结合。从学习理论的角度来看，只要实证研究表明，这种简单集成的训练误差可以达到与端到端训练的深度网络相当的结果，所有这些模型的 VC 维度或复杂度都明显较小，因此往往具有更好的泛化能力。我们还提出了一种在视频对象分割设置下分析泛化能力的新方法，以使界限更紧密。在实践中，SDForest 的计算成本极低，即使在 CPU 上也能实现实时。我们在视频对象分割任务上进行了测试，并在 DAVIS2016 和 DAVIS2017 上以纯深度学习方法取得了具有竞争力的性能，而无需任何训练或微调。]]></description>
      <guid>https://arxiv.org/abs/2409.10555</guid>
      <pubDate>Wed, 18 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>现有的道路设计指南是否适合自动驾驶汽车？</title>
      <link>https://arxiv.org/abs/2409.10562</link>
      <description><![CDATA[arXiv:2409.10562v1 公告类型：新
摘要：自动驾驶汽车 (AV) 的出现促使人们研究测试其感知系统的弹性，即确保它们不易做出严重的误判。重要的是，不仅要针对道路上的其他车辆进行测试，还要针对路边的物体进行测试。垃圾桶、广告牌和绿化都是此类物体的例子，它们通常根据为人类视觉系统开发的指导方针放置，但可能与 AV 的需求并不完全一致。然而，现有的测试通常侧重于具有显眼形状/斑块的对抗性物体，考虑到它们不自然的外观和对白盒知识的需求，这些物体最终是不切实际的。在本研究中，我们引入了一种针对 AV 感知系统的黑盒攻击，其目标是通过操纵常见路边物体的位置来创建逼真的对抗场景（即满足道路设计指南），而不诉诸“非自然”的对抗块。具体来说，我们提出了 TrashFuzz，这是一种模糊测试算法，用于查找这些物体的放置位置会导致 AV 产生严重误解的场景——例如误解交通信号灯的颜色——总体目标是导致其违反交通法规。为了确保这些场景的真实性，它们必须满足几条规则，这些规则编码了有关在公共街道上放置物体的监管指南。我们对 Apollo 实施并评估了这些攻击，发现 TrashFuzz 诱导它违反了 24 项不同交通法规中的 15 项。]]></description>
      <guid>https://arxiv.org/abs/2409.10562</guid>
      <pubDate>Wed, 18 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SoccerNet 2024 挑战赛结果</title>
      <link>https://arxiv.org/abs/2409.10587</link>
      <description><![CDATA[arXiv:2409.10587v1 公告类型：新
摘要：SoccerNet 2024 挑战赛是 SoccerNet 团队组织的第四届年度视频理解挑战赛。这些挑战旨在推动足球多个主题的研究，包括广播视频理解、场地理解和球员理解。今年，挑战赛涵盖四个基于视觉的任务。（1）球动作定位，重点是精确定位与球相关的足球动作发生的时间和位置；（2）密集视频字幕，重点是用自然语言和固定时间戳描述广播；（3）多视角犯规识别，这是一项新颖的任务，重点是分析潜在犯规事件的多个视点，以对犯规是否发生进行分类并评估其严重程度；（4）比赛状态重建，另一项新颖的任务，重点是从广播视频重建比赛状态到场地的 2D 顶视图地图上。有关任务、挑战和排行榜的详细信息可以在 https://www.soccer-net.org 上找到，基线和开发工具包可以在 https://github.com/SoccerNet 上找到。]]></description>
      <guid>https://arxiv.org/abs/2409.10587</guid>
      <pubDate>Wed, 18 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过幻觉早期检测优化扩散模型中的资源消耗</title>
      <link>https://arxiv.org/abs/2409.10597</link>
      <description><![CDATA[arXiv:2409.10597v1 公告类型：新
摘要：扩散模型显著推进了生成式人工智能，但在生成多个对象的复杂组合时会遇到困难。由于最终结果在很大程度上取决于初始种子，因此准确确保所需输出可能需要多次迭代生成过程。这种重复不仅浪费时间，而且还会增加能源消耗，这与复杂生成任务中效率和准确性的挑战相呼应。为了解决这个问题，我们引入了 HEaD（幻觉早期检测），这是一种新范式，旨在在扩散过程开始时快速检测错误的生成。HEaD 管道将交叉注意力图与新指标“预测最终图像”相结合，利用生成过程早期可用的信息来预测最终结果。我们证明使用 HEaD 可以节省计算资源并加速生成过程以获得完整的图像，即准确描绘所有请求对象的图像。我们的研究结果表明，HEaD 可以在双对象场景中节省高达 12% 的生成时间，并强调了生成模型中早期检测机制的重要性。]]></description>
      <guid>https://arxiv.org/abs/2409.10597</guid>
      <pubDate>Wed, 18 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>HAVANA：用于加速视频注释的分层随机邻居嵌入</title>
      <link>https://arxiv.org/abs/2409.10641</link>
      <description><![CDATA[arXiv:2409.10641v1 公告类型：新
摘要：视频注释是计算机视觉研究和应用中一项关键且耗时的任务。本文提出了一种新颖的注释流程，该流程使用预提取的特征和降维来加速时间视频注释过程。我们的方法使用分层随机邻域嵌入 (HSNE) 来创建视频特征的多尺度表示，使注释者能够有效地探索和标记大型视频数据集。与传统的线性方法相比，我们在注释工作方面取得了显着的改进，将注释超过 12 小时的视频所需的点击次数减少了 10 倍以上。我们在多个数据集上进行的实验证明了我们的流程在各种场景中的有效性和稳健性。此外，我们研究了不同数据集的 HSNE 参数的最佳配置。我们的工作为在视频理解时代扩大视频注释工作提供了一个有希望的方向。]]></description>
      <guid>https://arxiv.org/abs/2409.10641</guid>
      <pubDate>Wed, 18 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Playground v3：使用 Deep-Fusion 大型语言模型改善文本到图像的对齐</title>
      <link>https://arxiv.org/abs/2409.10695</link>
      <description><![CDATA[arXiv:2409.10695v1 公告类型：新
摘要：我们推出了 Playground v3 (PGv3)，这是我们最新的文本到图像模型，它在多个测试基准中实现了最先进的 (SoTA) 性能，在图形设计能力方面表现出色，并引入了新功能。与依赖预训练语言模型（如 T5 或 CLIP 文本编码器）的传统文本到图像生成模型不同，我们的方法将大型语言模型 (LLM) 与一种新颖的结构完全集成，该结构专门利用仅来自解码器的 LLM 的文本条件。此外，为了提高图像字幕质量，我们开发了一个内部字幕器，能够生成具有不同细节级别的字幕，丰富文本结构的多样性。我们还引入了一个新的基准 CapsBench 来评估详细的图像字幕性能。实验结果表明，PGv3 在文本提示遵守、复杂推理和准确文本渲染方面表现出色。用户偏好研究表明，我们的模型在常见设计应用（如贴纸、海报和徽标设计）中具有超人的图形设计能力。此外，PGv3 还引入了新功能，包括精确的 RGB 颜色控制和强大的多语言理解能力。]]></description>
      <guid>https://arxiv.org/abs/2409.10695</guid>
      <pubDate>Wed, 18 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CoMamba：利用状态空间模型解锁实时协作感知</title>
      <link>https://arxiv.org/abs/2409.10699</link>
      <description><![CDATA[arXiv:2409.10699v1 公告类型：新
摘要：协作感知系统在提高车辆自主性的安全性和效率方面发挥着至关重要的作用。尽管最近的研究强调了车对万物 (V2X) 通信技术在自动驾驶中的有效性，但仍存在一个重大挑战：如何在不断扩展的连接代理网络（例如车辆和基础设施）中有效地集成多个高带宽功能。在本文中，我们介绍了 CoMamba，这是一种新颖的协作 3D 检测框架，旨在利用状态空间模型实现实时车载车辆感知。与之前最先进的基于 Transformer 的模型相比，CoMamba 是一种使用双向状态空间模型的更具可扩展性的 3D 模型，绕过了注意力机制的二次复杂度痛点。通过对 V2X/V2V 数据集的大量实验，CoMamba 与现有方法相比实现了卓越的性能，同时保持了实时处理能力。所提出的框架不仅提高了物体检测的准确性，而且显著减少了处理时间，使其成为智能交通网络中下一代协作感知系统的有前途的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2409.10699</guid>
      <pubDate>Wed, 18 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过记忆进行在线学习：检索增强检测器自适应</title>
      <link>https://arxiv.org/abs/2409.10716</link>
      <description><![CDATA[arXiv:2409.10716v1 公告类型：新
摘要：本文介绍了一种新颖的方法，无需重新训练检测器模型，即可在线将任何现成的物体检测模型适应新领域。受人类快速学习新主题知识（例如记忆）的启发，我们允许检测器在测试时从记忆中查找类似的物体概念。这是通过检索增强分类 (RAC) 模块以及可以灵活更新新领域知识的存储库来实现的。我们尝试了各种现成的开放集检测器和封闭集检测器。仅使用一个很小的存储库（例如每个类别 10 张图像）并且无需训练，我们的在线学习方法在将检测器适应新领域方面可以明显优于基线。]]></description>
      <guid>https://arxiv.org/abs/2409.10716</guid>
      <pubDate>Wed, 18 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>