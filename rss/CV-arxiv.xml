<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Thu, 16 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>SCOT：用于零样本组合检索的自监督对比预训练</title>
      <link>https://arxiv.org/abs/2501.08347</link>
      <description><![CDATA[arXiv:2501.08347v1 公告类型：新
摘要：组合图像检索 (CIR) 是一种多模态学习任务，其中模型将查询图像与用户提供的文本修改相结合以检索目标图像。CIR 可应用于各种领域，包括产品检索（电子商务）和网络搜索。现有方法主要侧重于全监督学习，其中模型在带标签的三元组数据集（例如 FashionIQ 和 CIRR）上进行训练。这带来了两个重大挑战：(i) 整理此类三元组数据集需要大量劳动力；(ii) 模型缺乏对看不见的对象和领域的泛化。在这项工作中，我们提出了 SCOT（自监督组合训练），这是一种新颖的零样本组合预训练策略，它将现有的大型图像文本对数据集与大型语言模型的生成能力相结合，以对比训练嵌入组合网络。具体来说，我们展示了来自大规模对比预训练视觉语言模型的文本嵌入可用作组合预训练期间的代理目标监督，从而取代目标图像嵌入。在零样本设置中，此策略超越了 SOTA 零样本组合检索方法以及 FashionIQ 和 CIRR 等标准基准上的许多全监督方法。]]></description>
      <guid>https://arxiv.org/abs/2501.08347</guid>
      <pubDate>Thu, 16 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于分布外泛化和小样本域自适应的加权平均</title>
      <link>https://arxiv.org/abs/2501.08361</link>
      <description><![CDATA[arXiv:2501.08361v1 公告类型：新
摘要：经验风险最小化 (ERM) 对数据分布的变化不具有鲁棒性。当测试数据的分布与训练数据的分布不同时，该问题称为分布外泛化。最近，已经开发了两种技术来解决计算机视觉中的分布外泛化问题：权重平均 (WA) 和清晰度感知最小化 (SAM)。WA 涉及使用不同的超参数训练多个模型，然后对这些模型的权重进行平均，这可以显著提高分布外泛化性能。SAM 优化神经网络以在平坦区域中找到最小值，这已被证明在分布偏移下表现良好。虽然这些技术取得了很大进展，但仍有改进和进一步探索的空间。在本文中，我们提出通过引入梯度相似性作为损失正则化器来明确增加 WA 中的模型多样性，以进一步提高分布外泛化性能。我们还建议结合 WA 和 SAM 来解决少样本域自适应问题。我们在数字数据集（MNIST、SVHN、USPS、MNIST-M）和其他域自适应数据集（VLCS、PACS）上进行的大量实验表明，结合 WA 和 SAM 可以提高分布外泛化性能，并显著提高少样本域自适应准确率。]]></description>
      <guid>https://arxiv.org/abs/2501.08361</guid>
      <pubDate>Thu, 16 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用二维蒙​​版重建实现三维姿态估计的域自适应</title>
      <link>https://arxiv.org/abs/2501.08408</link>
      <description><![CDATA[arXiv:2501.08408v1 公告类型：新
摘要：随着深度学习的发展和高质量 3D 姿态数据集的出现，基于 RGB 的 3D 姿态估计方法取得了成功。然而，大多数现有方法对于测试分布远离训练数据的图像效果不佳。然而，大多数现有方法对于测试分布远离训练数据的图像效果不佳。这个问题可以通过在训练期间涉及不同的数据来缓解，但是收集具有相应标签（即 3D 姿态）的多样化数据并非易事。在本文中，我们介绍了一种用于 3D 姿态估计的无监督领域自适应框架，该框架通过掩蔽图像建模 (MIM) 框架利用未标记数据和标记数据。进一步提出了以前景为中心的重建和注意力正则化，以提高未标记数据使用的有效性。在人体和手部姿态估计任务中对各种数据集进行了实验，特别是使用跨域场景。我们通过在所有数据集上实现最先进的准确度证明了我们的有效性。]]></description>
      <guid>https://arxiv.org/abs/2501.08408</guid>
      <pubDate>Thu, 16 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>跨模式可转移图像到视频攻击视频质量指标</title>
      <link>https://arxiv.org/abs/2501.08415</link>
      <description><![CDATA[arXiv:2501.08415v1 公告类型：新 
摘要：最近的研究表明，现代图像和视频质量评估 (IQA/VQA) 指标容易受到对抗性攻击。攻击者可以通过预处理来操纵视频，以根据某个指标人为地提高其质量得分，尽管视觉质量并没有实际改善。文献中研究的大多数攻击都是白盒攻击，而 VQA 背景下的黑盒攻击受到的关注较少。此外，一些研究表明，当应用于 VQA 时，为一个模型生成的对抗性示例缺乏到另一个模型的可转移性。在本文中，我们提出了一种跨模态攻击方法 IC2VQA，旨在探索现代 VQA 模型的漏洞。这种方法的动机是观察到图像和视频的低级特征空间是相似的。我们研究对抗性扰动在不同模态之间的可转移性；具体来说，我们分析了在白盒 IQA 模型上生成具有附加 CLIP 模块的对抗性扰动如何有效地针对 VQA 模型。CLIP 模块的添加对于提高可转移性大有裨益，因为 CLIP 模型以有效捕获低级语义而闻名。大量实验表明，IC2VQA 在攻击三种黑盒 VQA 模型时取得了很高的成功率。我们将我们的方法与现有的黑盒攻击策略进行了比较，突出了在相同迭代次数和攻击强度级别内攻击成功率方面的优势。我们相信，所提出的方法将有助于更深入地分析稳健的 VQA 指标。]]></description>
      <guid>https://arxiv.org/abs/2501.08415</guid>
      <pubDate>Thu, 16 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FARE：基于深度学习的雷达人脸识别和分布外检测框架</title>
      <link>https://arxiv.org/abs/2501.08440</link>
      <description><![CDATA[arXiv:2501.08440v1 公告类型：新
摘要：在这项工作中，我们提出了一种使用短程 FMCW 雷达进行人脸识别和分布外 (OOD) 检测的新型管道。所提出的系统利用距离多普勒和微距离多普勒图像。该架构的特点是主路径 (PP) 负责分布内 (ID) 人脸的分类，并辅以专用于 OOD 检测的中间路径 (IP)。网络分两个阶段进行训练：首先，使用三重态损失训练 PP 以优化 ID 人脸分类。在第二阶段，冻结 PP，并专门为 OOD 检测训练 IP（包括简单的线性自动编码器网络）。使用我们用 60 GHz FMCW 雷达生成的数据集，我们的方法实现了 99.30% 的 ID 分类准确率和 96.91% 的 OOD 检测 AUROC。]]></description>
      <guid>https://arxiv.org/abs/2501.08440</guid>
      <pubDate>Thu, 16 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型视觉语言模型中多层视觉特征的指令引导融合</title>
      <link>https://arxiv.org/abs/2501.08443</link>
      <description><![CDATA[arXiv:2501.08443v1 公告类型：新
摘要：大型视觉语言模型 (LVLM) 通过结合预训练的视觉编码器和大型语言模型，在多模态任务中取得了显著的成功。然而，目前的 LVLM 主要依赖于视觉编码器最后几层的特征，而忽略了较浅层中的互补信息。虽然最近的方法已经探索了多层特征，但它们通常与任务无关。我们研究了 18 个基准和 6 个任务类别中不同编码器层的视觉特征的贡献。我们的结果表明，多层特征在不同的任务依赖性下提供了互补的优势，而统一融合的表现并不理想。基于这些发现，我们提出了一种指令引导的视觉聚合器，它根据文本指令动态集成多层特征，而不会增加视觉标记的数量。大量评估表明其性能优异，分析表明中高级特征在语义任务中占主导地位，低级特征在细粒度感知中发挥关键作用。这项工作为 LVLM 中分层视觉特征的自适应使用提供了宝贵的见解，推动了更灵活的多模态系统的发展。]]></description>
      <guid>https://arxiv.org/abs/2501.08443</guid>
      <pubDate>Thu, 16 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Poseidon：基于 ViT 的多帧姿态估计架构，具有自适应帧加权和多尺度特征融合</title>
      <link>https://arxiv.org/abs/2501.08446</link>
      <description><![CDATA[arXiv:2501.08446v1 公告类型：新
摘要：人体姿势估计是计算机视觉中一项重要的任务，涉及检测和定位图像和视频中的人体关节。虽然单帧姿势估计取得了重大进展，但它往往无法捕捉时间动态以理解复杂、连续的运动。我们提出了一种新颖的多帧姿势估计架构 Poseidon，它通过集成时间信息来扩展 ViTPose 模型，以提高准确性和鲁棒性，以解决这些限制。Poseidon 引入了关键创新：(1) 自适应帧加权 (AFW) 机制，可根据帧的相关性动态地对帧进行优先级排序，确保模型专注于最具信息量的数据；(2) 多尺度特征融合 (MSFF) 模块，聚合来自不同主干层的特征以捕获细粒度细节和高级语义；(3) 交叉注意模块，用于在中心帧和上下文帧之间进行有效的信息交换，增强模型的时间连贯性。所提出的架构提高了复杂视频场景下的性能，并提供了适合实际应用的可扩展性和计算效率。我们的方法在 PoseTrack21 和 PoseTrack18 数据集上实现了最佳性能，分别实现了 88.3 和 87.8 的 mAP 得分，优于现有方法。]]></description>
      <guid>https://arxiv.org/abs/2501.08446</guid>
      <pubDate>Thu, 16 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Vchitect-2.0：用于扩展视频扩散模型的并行变压器</title>
      <link>https://arxiv.org/abs/2501.08453</link>
      <description><![CDATA[arXiv:2501.08453v1 公告类型：新摘要：我们提出了 Vchitect-2.0，这是一种并行转换器架构，旨在扩展视频扩散模型，以实现大规模文本到视频的生成。整个 Vchitect-2.0 系统有几个关键设计。（1）通过引入新颖的多模态扩散块，我们的方法实现了文本描述和生成的视频帧之间的一致对齐，同时保持了序列之间的时间一致性。（2）为了克服内存和计算瓶颈，我们提出了一种内存高效的训练框架，该框架结合了混合并行性和其他内存减少技术，从而能够在分布式系统上高效地训练长视频序列。（3）此外，我们增强的数据处理管道确保创建 Vchitect T2V DataVerse，这是一个通过严格的注释和美学评估获得的高质量百万级训练数据集。大量的基准测试表明，Vchitect-2.0 在视频质量、训练效率和可扩展性方面优于现有方法，可以作为高保真视频生成的合适基础。]]></description>
      <guid>https://arxiv.org/abs/2501.08453</guid>
      <pubDate>Thu, 16 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过推理空间和时间事件图实现零样本和可解释的视频描述</title>
      <link>https://arxiv.org/abs/2501.08460</link>
      <description><![CDATA[arXiv:2501.08460v1 公告类型：新
摘要：在当前的机器学习时代，Transformers 已成为计算机视觉和自然语言处理等各种领域的事实上的方法。基于 Transformer 的解决方案是当前最先进的语言生成、图像和视频分类、分割、动作和对象识别等方法的支柱。有趣的是，虽然这些最先进的方法在各自的领域产生了令人印象深刻的结果，但理解视觉和语言之间关系的问题仍然超出了我们的能力范围。在这项工作中，我们以可解释和编程的方式基于空间和时间中的事件提出了视觉和语言之间的共同点，以连接基于学习的视觉和语言最先进的模型，并为用自然语言描述视频的长期问题提供解决方案。我们验证了我们的算法方法能够使用标准指标（例如 Bleu、ROUGE）和现代 LLM-as-a-Jury 方法对从各种数据集收集的视频生成连贯、丰富且相关的文本描述。]]></description>
      <guid>https://arxiv.org/abs/2501.08460</guid>
      <pubDate>Thu, 16 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用随机森林预测电子显微镜中物体检测模型的性能</title>
      <link>https://arxiv.org/abs/2501.08465</link>
      <description><![CDATA[arXiv:2501.08465v1 公告类型：新
摘要：在将对象检测模型应用于新的未标记数据集时，量化预测不确定性对于应用机器学习至关重要。本研究介绍了一种用于估计基于深度学习的对象检测模型在量化透射电子显微镜 (TEM) 图像中的缺陷方面的性能的方法，重点是检测金属合金 TEM 图像中辐照引起的空腔。我们开发了一个随机森林回归模型来预测对象检测 F1 分数，这是一种用于评估准确定位和分类感兴趣对象的能力的统计指标。随机森林模型使用从正在量化不确定性的对象检测模型的预测中提取的特征，从而能够对新的未标记图像进行快速预测。在测试数据上预测训练模型的 F1 的平均绝对误差 (MAE) 为 0.09，$R^2$ 分数为 0.77，表明预测的随机森林回归模型与真实缺陷检测 F1 分数之间存在显着相关性。该方法在三个不同的 TEM 图像数据集（具有不同的成像和材​​料域）中表现出色。我们的方法使用户能够估计缺陷检测和分割模型预测的可靠性，并评估模型对其特定数据集的适用性，从而提供有关可能的域偏移的宝贵信息，以及模型是否需要进行微调或在其他数据上进行训练才能最大限度地满足所需用例的需要。]]></description>
      <guid>https://arxiv.org/abs/2501.08465</guid>
      <pubDate>Thu, 16 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过发现一致的空间区域来检测上下文异常</title>
      <link>https://arxiv.org/abs/2501.08470</link>
      <description><![CDATA[arXiv:2501.08470v1 公告类型：新
摘要：我们描述了一种用于建模空间上下文以实现视频异常检测的方法。主要思想是通过使用高斯混合模型对联合对象属性进行聚类来发现具有相似对象级活动的区域。我们证明，这种简单的方法使用比竞争模型少几个数量级的参数，在具有挑战性的空间上下文相关街景数据集中实现了最先进的性能。作为附带好处，模型学习到的高分辨率发现区域还为人类操作员提供了可解释的常态图，而无需任何预先训练的分割模型。]]></description>
      <guid>https://arxiv.org/abs/2501.08470</guid>
      <pubDate>Thu, 16 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>对人类活动识别的经典、深度和生成模型进行基准测试</title>
      <link>https://arxiv.org/abs/2501.08471</link>
      <description><![CDATA[arXiv:2501.08471v1 公告类型：新
摘要：随着配备传感器的设备和大型数据集的使用日益增多，人类活动识别 (HAR) 变得越来越重要。本文使用 HAR 的五个关键基准数据集 (UCI-HAR、OPPORTUNITY、PAMAP2、WISDM 和 Berkeley MHAD) 评估了三类模型的性能：经典机器学习、深度学习架构和受限玻尔兹曼机 (RBM)。我们评估了各种模型，包括决策树、随机森林、卷积神经网络 (CNN) 和深度信念网络 (DBN)，使用准确度、精确度、召回率和 F1 分数等指标进行全面比较。结果表明，CNN 模型在所有数据集上都表现出色，尤其是在 Berkeley MHAD 上。像随机森林这样的经典模型在较小的数据集上表现良好，但在更大、更复杂的数据上面临挑战。基于 RBM 的模型也显示出显著的潜力，尤其是在特征学习方面。本文提供了详细的比较，以帮助研究人员为 HAR 任务选择最合适的模型。]]></description>
      <guid>https://arxiv.org/abs/2501.08471</guid>
      <pubDate>Thu, 16 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FLAVARS：用于遥感的多模式基础语言和视觉对齐模型</title>
      <link>https://arxiv.org/abs/2501.08490</link>
      <description><![CDATA[arXiv:2501.08490v1 公告类型：新
摘要：遥感图像中充满了物体和上下文视觉信息。最近有一种趋势，即将成对的卫星图像和文本标题结合起来，为下游任务预训练高性能编码器。然而，虽然对比图像文本方法（如 CLIP）能够实现视觉语言对齐和零样本分类能力，但与仅图像预训练（如 MAE）相比，仅视觉的下游性能往往会下降。在本文中，我们提出了 FLAVARS，这是一种结合对比学习和掩蔽建模的优点的预训练方法，以及通过对比位置编码进行地理空间对齐。我们发现，对于仅视觉任务（例如 KNN 分类和语义分割），FLAVARS 的表现明显优于 SkyCLIP 的基线，在 SpaceNet1 上 mIOU 为 +6\%，同时保留了执行零样本分类的能力，这与 MAE 预训练方法不同。]]></description>
      <guid>https://arxiv.org/abs/2501.08490</guid>
      <pubDate>Thu, 16 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SuperSAM：通过结构化修剪和非结构化参数优先级构建 SAM 超级网络</title>
      <link>https://arxiv.org/abs/2501.08504</link>
      <description><![CDATA[arXiv:2501.08504v1 公告类型：新
摘要：神经架构搜索 (NAS) 是一种自动设计高效神经架构的强大方法。与传统的 NAS 方法相比，最近提出的一次性 NAS 方法在执行 NAS 方面被证明更有效。一次性 NAS 的工作原理是生成一个单一的权重共享超网络，该超网络充当子网络的搜索空间（容器）。尽管取得了成就，但设计一次性搜索空间仍然是一项重大挑战。在这项工作中，我们提出了一种基于 Vision Transformer (ViT) 的架构的搜索空间设计策略。具体来说，我们将 Segment Anything 模型 (SAM) 转换为称为 SuperSAM 的权重共享超网络。我们的方法涉及通过分层结构化修剪和参数优先级来自动化搜索空间设计。虽然结构化修剪应用了某些变压器层的概率移除，但参数优先级执行剩余层中的权重重新排序和 MLP 块切片。我们使用三明治规则在多个数据集上训练超级网络。对于部署，我们利用程序自动调谐器来识别搜索空间内的有效子网络，从而增强子网络发现。与原始预训练的 SAM ViT-B 相比，生成的子网络规模小 30-70%，但性能优于预训练模型。我们的工作为 ViT NAS 搜索空间设计引入了一种新的有效方法。]]></description>
      <guid>https://arxiv.org/abs/2501.08504</guid>
      <pubDate>Thu, 16 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Yuan：通过统一网络去除生成图像中的视觉缺陷，实现完美无瑕的美感</title>
      <link>https://arxiv.org/abs/2501.08505</link>
      <description><![CDATA[arXiv:2501.08505v1 公告类型：新
摘要：生成式人工智能在从创意艺术到科学可视化的各个领域都具有变革潜力。然而，人工智能生成的图像的实用性往往受到视觉缺陷的影响，包括解剖学上的不准确、物体放置不当和文本元素放错位置。这些缺陷对实际应用构成了重大挑战。为了克服这些限制，我们引入了 \textit{Yuan}，这是一个新颖的框架，可以自主纠正文本到图像合成中的视觉缺陷。 \textit{Yuan} 对文本提示和分割图像进行独特的条件处理，生成精确的蒙版，无需人工干预即可识别需要细化的区域——这是以前方法中的常见限制。在自动蒙版过程之后，高级修复模块将上下文连贯的内容无缝集成到已识别的区域中，从而保留原始图像和相关文本提示的完整性和保真度。通过对 ImageNet100 和 Stanford Dogs 等公开数据集以及自定义生成的数据集进行大量实验，\textit{Yuan} 在消除视觉缺陷方面表现出色。我们的方法在定量指标（包括 NIQE、BRISQUE 和 PI）中始终取得更高的分数，同时还获得了良好的定性评估。这些结果凸显了 \textit{Yuan} 在不同领域显著提高 AI 生成图像的质量和适用性的潜力。]]></description>
      <guid>https://arxiv.org/abs/2501.08505</guid>
      <pubDate>Thu, 16 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>