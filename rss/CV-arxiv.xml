<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>CS.CV更新arxiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.cv更新arxiv.org e-print存档。</description>
    <lastBuildDate>Tue, 01 Apr 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>守护神：带有原型扩散模型的透明度</title>
      <link>https://arxiv.org/abs/2503.22782</link>
      <description><![CDATA[ARXIV：2503.22782V1公告类型：新 
摘要：基于扩散的生成模型，例如去核扩散概率模型（DDPM），在图像生成方面取得了显着的成功，但是它们的分步denoising过程仍然不透明，留下了无法解释的生成机制的关键方面。为了解决这个问题，我们介绍了\ emph {Patronus}，这是一种受Protopnet启发的可解释扩散模型。 Patronus将原型网络集成到DDPM中，从而使原型提取并在其原型激活载体上的生成过程调节。该设计通过显示学习的原型以及它们如何影响生成过程来增强可解释性。此外，该模型还支持下游任务，例如图像操纵，从而实现更透明和受控的修改。此外，守护神可以通过检测到学到的原型之间的不良相关性来揭示生成过程中的快捷学习学习。值得注意的是，Patronus完全没有任何注释或文本提示。这项工作为通过基于原型的解释性理解和控制扩散模型开辟了新的途径。我们的代码可在\ href {https://github.com/nina-weng/patronus} {https://github.com/nina-weng/patronus}中获得。]]></description>
      <guid>https://arxiv.org/abs/2503.22782</guid>
      <pubDate>Tue, 01 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DitFastAttNV2：多模式扩散变压器的头明智的注意压缩</title>
      <link>https://arxiv.org/abs/2503.22796</link>
      <description><![CDATA[ARXIV：2503.22796V1公告类型：新 
摘要：文本到图像生成模型，尤其是多模式扩散变压器（MMDIT），在生成高质量图像方面表现出了显着的进展。但是，这些模型通常面临着重要的计算瓶颈，尤其是在注意机制上，这阻碍了它们的可扩展性和效率。在本文中，我们介绍了DitFastAttNV2，这是一种旨在加速MMDIT注意力的训练后压缩方法。通过对MMDIT注意模式的深入分析，我们确定了基于DIT的方法的关键差异，并提出了头明确的箭头注意力和缓存机制，以动态调整注意力头，从而有效地弥合了这一差距。我们还设计了一个有效的融合核，以进一步加速。通过利用本地度量方法和优化技术，我们的方法将最佳压缩方案的搜索时间大大减少到只需几分钟，同时保持发电质量。此外，借助自定义的内核，DitFastAttNV2可在2K图像生成上降低注意力失败和1.5倍的端到端速度，而不会损害视觉保真度。]]></description>
      <guid>https://arxiv.org/abs/2503.22796</guid>
      <pubDate>Tue, 01 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GMNET：从频率视图重新审视门控机制</title>
      <link>https://arxiv.org/abs/2503.22841</link>
      <description><![CDATA[ARXIV：2503.22841V1公告类型：新 
摘要：门控机制已成为一种有效的策略，该策略集成了复发性神经网络以外的模型设计，以解决长期依赖问题。在广泛的理解中，它可以在保持计算效率的同时对信息流进行自适应控制。但是，关于门控机制在神经网络中的工作方式缺乏理论分析。在本文中，受{卷积定理}的启发，我们从频率的角度系统地探讨了门控机制对神经网络训练动力学的影响。我们研究了元素的产品与激活功能之间的相互作用，以管理对不同频率组件的响应。利用这些见解，我们提出了一个门控机构网络（GMNET），这是一种轻巧模型，旨在有效利用各种频率组件的信息。它最大程度地减少了现有轻质模型中存在的低频偏差。 GMNET在图像分类任务中的有效性和效率方面取得了令人印象深刻的表现。]]></description>
      <guid>https://arxiv.org/abs/2503.22841</guid>
      <pubDate>Tue, 01 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>3D医学图像分割的基础模型的零拍域概括：一项实验研究</title>
      <link>https://arxiv.org/abs/2503.22862</link>
      <description><![CDATA[ARXIV：2503.22862V1公告类型：新 
摘要：由成像方式和采集协议的变化引起的域移位，限制了医学图像分割中的模型概括。尽管对各种大规模数据进行培训的基础模型（FMS）对零射门的概括有希望，但它们在体积医学数据中的应用仍未得到充满信心。在这项研究中，我们通过进行一项全面的实验研究，其中包括6种医学分割FMS和12个公共数据集，这些研究涵盖了多种方式和解剖学，我们检查了它们的域泛化能力（DG）。我们的发现揭示了通过智能提示技术弥合域间隙的迅速FM的潜力。此外，通过探测零拍DG的多个方面，我们为FMS的可行性提供了宝贵的见解，并确定了有希望的未来研究途径。]]></description>
      <guid>https://arxiv.org/abs/2503.22862</guid>
      <pubDate>Tue, 01 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>视力：手动相互作用的单位形成的手动轨迹</title>
      <link>https://arxiv.org/abs/2503.22869</link>
      <description><![CDATA[ARXIV：2503.22869V1公告类型：新 
摘要：我们介绍了一个新颖的任务，即给出一个对象的单个图像，生成现实和多样的3D手轨迹，该轨迹可以参与手动相互作用场景或本身所示。当人类掌握一个物体时，在我们的脑海中自然形成了适当的轨迹，以将其用于特定任务。手动相互作用轨迹先验可以极大地利用机器人技术，体现AI，增强现实和相关领域的应用。但是，在给定一个对象或手动相互作用图像的情况下，合成现实且适当的手轨迹是一项高度模棱两可的任务，需要正确地识别感兴趣的对象，甚至可能在许多可能的替代方案之间进行正确的交互。为了解决这个具有挑战性的问题，我们提出了观光融合系统，该系统由策划的管道组成，该管道从涉及对象操纵的以中心视频以及基于扩散的条件运动生成模型处理提取的特征中提取手动对象交互细节的视觉特征。我们训练我们的方法给出的视频数据，并带有相应的手轨迹注释，而无需以动作标签的形式进行监督。为了进行评估，我们利用第一人称FPHAB和HOI4D数据集建立了基准，对各种基准进行测试并使用多个指标。我们还介绍了任务模拟器，以执行生成的手轨迹和报告任务成功率作为额外的指标。实验表明，我们的方法比基线生成更合适和更现实的手轨迹，并且在看不见的对象上具有有希望的概括能力。在物理模拟设置中证实了生成的手轨迹的准确性，展示了创建序列的真实性及其在下游用途中的适用性。]]></description>
      <guid>https://arxiv.org/abs/2503.22869</guid>
      <pubDate>Tue, 01 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>海洋碎片前瞻性声纳数据集</title>
      <link>https://arxiv.org/abs/2503.22880</link>
      <description><![CDATA[ARXIV：2503.22880V1公告类型：新 
摘要：Sonar Sensing对于水下机器人技术至关重要，但受AI系统功能的限制，AI系统需要大型培训数据集。缺乏声纳方式的公共数据。本文介绍了海洋碎片前瞻性声纳数据集，具有三种不同的设置（Watertank，Turntable，洪水泛滥的采石场），以增加数据集多样性和多个计算机视觉任务：对象分类，对象检测，语义细分，修补程序匹配，匹配和无人监督的学习。我们为某些任务提供完整的数据集说明，基本分析和初始结果。我们希望研究社区将受益于该数据集，该数据集可在https://doi.org/10.5281/zenodo.15101686上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2503.22880</guid>
      <pubDate>Tue, 01 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>中间表示的成对匹配，以进行细粒度解释性</title>
      <link>https://arxiv.org/abs/2503.22881</link>
      <description><![CDATA[ARXIV：2503.22881V1公告类型：新 
摘要：属于细颗粒类别的图像之间的差异通常是微妙且高度局部的，并且深度学习模型的现有解释性技术通常太分散了，无法提供有用且可解释的解释。我们提出了一种新的解释性方法（PAIR-X），该方法利用中间模型激活和反向传播的相关性得分来生成细粒度高，高度稳定的成对视觉解释。我们将动物和建筑物的重新识别（RE-ID）作为我们方法的主要案例研究，并且在35个公共RE-ID数据集上的一组可解释性基准相比，我们证明了质量改进的结果。在访谈中，动物重新ID专家一致同意，配对X是对现有基准的改进，以进行深层模型解释性，并建议其可视化将直接适用于其工作。我们还为我们的方法提出了一种新颖的定量评估度量，并证明，即使对成对的模型相似性得分相同，对于正确的图像匹配而言，Pair-X可视化对于正确的图像匹配而言似乎更合理。通过提高可解释性，Pair-X可以更好地区分正确和错误的匹配。我们的代码可在以下网址找到：https：//github.com/pairx-explains/pairx]]></description>
      <guid>https://arxiv.org/abs/2503.22881</guid>
      <pubDate>Tue, 01 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>自动组成：使用多模式LLMS组成的姿势检索自动生成姿势过渡描述</title>
      <link>https://arxiv.org/abs/2503.22884</link>
      <description><![CDATA[ARXIV：2503.22884V1公告类型：新 
摘要：组成的姿势检索（CPR）使用户能够通过指定参考姿势和过渡描述来搜索人类姿势，但是该领域的进展受到注释姿势过渡的稀缺性和不一致性的阻碍。现有的CPR数据集依赖于昂贵的人类注释或基于启发式的规则生成，这两者都限制了可扩展性和多样性。在这项工作中，我们介绍了自动化，这是利用多模式大型语言模型（MLLM）自动生成富且结构化的姿势过渡描述的第一个框架。我们的方法通过将过渡到细颗粒的身体部位运动并引入镜像/交换变化来增强注释质量，而环状一致性约束确保向前和逆向过渡之间的逻辑连贯性。为了推进CPR研究，我们构建并发布了两个专用基准AIST-CPR和PoseFixCPR，并补充了具有增强属性的先前数据集。广泛的实验表明，具有自身化的训练检索模型的产生比基于人类的启发式方法的方法优于较高的表现，从而大大降低了注释成本，同时提高了检索质量。我们的工作开创了姿势过渡的自动注释，为未来的CPR研究建立了可扩展的基础。]]></description>
      <guid>https://arxiv.org/abs/2503.22884</guid>
      <pubDate>Tue, 01 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MEDCL：学习一致的解剖学分布，用于涂鸦监督的医学图像细分</title>
      <link>https://arxiv.org/abs/2503.22890</link>
      <description><![CDATA[ARXIV：2503.22890V1公告类型：新 
摘要：策展大规模完全注释的数据集昂贵，费力且繁琐，尤其是对于医学图像。文献中已经提出了几种方法，它们以涂鸦的形式利用弱注释。但是，这些方法需要大量的涂鸦注释，仅应用于常规器官的分割，这些器官通常无法用于长尾分布的疾病物种。在医疗标签具有解剖学分布培训的事实中，我们提出了一个基于涂鸦的基于群集的框架，称为MEDCL，以了解医疗标签的固有解剖结构分布。我们的方法由两个步骤组成：i）将特征与内形内和形象间混合操作混合在一起，ii）进行特征聚类并在局部和全球水平上正规化解剖学分布。结合少量弱监督，拟议的MEDCL能够分割常规器官和具有挑战性的不规则病理。我们基于SAM和UNET骨干实现MEDCL，并评估三个常规结构（MSCMRSEG），多个器官（BTCV）和不规则病理学（Myops）的三个开放数据集（Myops）。结果表明，即使在减少涂鸦监督的情况下，MEDCL也大大优于常规分割方法。我们的代码可在https://github.com/bwgzk/medcl上找到。]]></description>
      <guid>https://arxiv.org/abs/2503.22890</guid>
      <pubDate>Tue, 01 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Socialgen：与语言模型建模多人类社交互动</title>
      <link>https://arxiv.org/abs/2503.22906</link>
      <description><![CDATA[ARXIV：2503.22906V1公告类型：新 
摘要：日常生活中的人类互动本质上是社会的，涉及各种环境中与不同个体的交往。建模这些社交互动对于广泛的现实应用程序至关重要。在本文中，我们介绍了Socialgen，这是第一个统一的运动语言模型，能够在不同数量的个体之间建模相互作用行为，以解决这个关键但具有挑战性的问题。与限于两人互动的先前方法不同，我们提出了一种新颖的社会运动表示形式，该表达支持将任意数量的个体的动作引起，并使他们与语言空间保持一致。这种一致性使该模型能够利用丰富的，验证的语言知识来更好地理解和理论人类的社会行为。为了应对数据稀缺的挑战，我们策划了一个全面的多人类交互数据集（Socialx），并具有文本注释。利用此数据集，我们为多人类交互任务建立了第一个全面的基准。我们的方法在运动语言任务中实现了最新的性能，为多人类交互建模设定了新的标准。]]></description>
      <guid>https://arxiv.org/abs/2503.22906</guid>
      <pubDate>Tue, 01 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>将DeepLabv3+增强到融合空中和卫星图像以进行语义分割</title>
      <link>https://arxiv.org/abs/2503.22909</link>
      <description><![CDATA[ARXIV：2503.22909V1公告类型：新 
摘要：天线和卫星图像本质上是互补的遥感来源，提供了高分辨率的细节以及广泛的空间覆盖范围。但是，将这些来源用于土地覆盖分段引入了几个挑战，促使了各种分割方法的发展。在这些方法中，DeepLabv3+体系结构被认为是单源图像分割领域的一种有希望的方法。但是，尽管对细分的可靠结果可靠，但仍然需要提高其稳健性并提高其性能。这对于多模式图像分割尤其重要，在多模式图像分割中，不同类型的信息的融合至关重要。
  一种有趣的方法涉及通过整合新组件和某些内部过程的修改来增强这种建筑框架。
  在本文中，我们通过引入一个新的转移常规层块来增强DeepLabv3+体系结构，以升级第二个条目将其与高级功能融合在一起。该块旨在扩大和集成卫星图像中的信息，从而通过与空中图像融合来丰富分割过程。
  对于实验，我们将LandCover.AI（来自航空影像的土地覆盖）数据集用于航空图像，以及来自Sentinel 2数据的相应数据集。
  通过两个来源的融合，联合（MIOU）的平均交集达到了84.91％的总MIOU，而没有数据扩大。]]></description>
      <guid>https://arxiv.org/abs/2503.22909</guid>
      <pubDate>Tue, 01 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>不同：通过改变衣服的人的语义提示解开身份特征</title>
      <link>https://arxiv.org/abs/2503.22912</link>
      <description><![CDATA[ARXIV：2503.22912V1公告类型：新 
摘要：改变衣服的人重新识别（CC-REID）旨在认识不同服装方案的人。当前的CC固定方法要么专注于使用其他模态（包括轮廓，姿势和身体网格）进行建模的身体形状，因此可能导致模型忽略其他关键生物特征特征，例如性别，年龄和样式，或者它们通过其他标签通过模型试图忽略或强调的其他标签进行监督，例如服装或个人属性或个人属性或个人属性。但是，这些注释本质上是离散的，不会捕获全面的描述。
  在这项工作中，我们提出了不同的不同：从纠缠表示形式中解开身份特征，这是一种新型的对抗性学习方法，利用文本描述来删除身份特征。认识到图像特征固有地混合了不可分割的信息，Disluccuct介绍了NBDETACH，这是一种通过利用文本描述作为监督的可分离性质，旨在通过特征分开的机制。它将特征空间划分为不同的子空间，并通过梯度逆转层有效地将与身份相关的特征与非生物测定特征分开。我们在4个不同的基准数据集（LTCC，PRCC，Celebreid-Light和CCVID）上评估了不同，以证明其有效性并在所有基准测试中提供最先进的性能。差异始终胜过基线方法，LTCC的TOP-1准确度提高了3.6％，PRCC的3.4％，Celebreid-Light的2.5％，CCVID的1％。我们的代码可以在这里找到。]]></description>
      <guid>https://arxiv.org/abs/2503.22912</guid>
      <pubDate>Tue, 01 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>无监督的特征分离和增强网络，用于一级脸部反欺骗</title>
      <link>https://arxiv.org/abs/2503.22929</link>
      <description><![CDATA[ARXIV：2503.22929V1公告类型：新 
摘要：面部抗散热器（FAS）技术旨在通过区分真实的现场面孔和欺骗性尝试来增强面部身份身份验证的安全性。尽管两级FAS方法有可能过度适应训练攻击以实现更好的性能，但单级FAS方法可以很好地处理看不见的攻击，但对在Livices功能中纠缠的域信息的强大较差。为了解决这个问题，我们提出了一种无监督的特征分离和增强网络（\ textbf {ufdanet}），这是一种单级FAS技术，可通过通过解开的特征增强面部图像来增强通用性。 \ textbf {ufdanet}采用一种新颖的无监督特征分离方法来分开livice和域特征，从而促进了歧视性特征学习。它集成了分布外的耐受性增强方案，以综合看不见的Spoof类的新功能，这些特征偏离了现场类别，从而增强了Livices特征的可用性和可区分性。此外，\ textbf {ufdanet}结合了一个域功能增强例程以综合看不见的域特​​征，从而实现了更好的概括性。广泛的实验表明，所提出的\ textbf {ufdanet}优于先前的一级FAS方法，并且可以实现与最先进的两级FAS方法相当的性能。]]></description>
      <guid>https://arxiv.org/abs/2503.22929</guid>
      <pubDate>Tue, 01 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>双级多视图模糊聚类与指数距离</title>
      <link>https://arxiv.org/abs/2503.22932</link>
      <description><![CDATA[ARXIV：2503.22932V1公告类型：新 
摘要：在这项研究中，我们建议在多视图环境中扩展模糊C均值（FCM）聚类。首先，我们引入了指数多视图FCM（E-MVFCM）。 E-MVFCM是一种集中式MVC，并考虑到热内核系数（H-KC）和权重因子。其次，我们提出了一个指数的双级多视图模糊c均值聚类（EB-MVFCM）。与E-MVFCM不同，EB-MVFCM同时自动计算特征和权重因子。像E-MVFCM一样，EB-MVFCM呈现H-KC的显式形式，以简化在聚类过程中适当的时间$ t $的功能中的热 - 内核$ \ Mathcal {k}（t）$。本研究中使用的所有功能，包括提议算法的工具和功能，将在https://www.github.com/kristinap09/eb-mvfcm上提供。]]></description>
      <guid>https://arxiv.org/abs/2503.22932</guid>
      <pubDate>Tue, 01 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>增强可学习的描述性卷积视觉变压器，以抗刺激性</title>
      <link>https://arxiv.org/abs/2503.22936</link>
      <description><![CDATA[ARXIV：2503.22936V1公告类型：新 
摘要：面部反欺骗（FAS）在很大程度上依赖于识别实时/欺骗性的歧视性特征来对抗面对表现攻击。最近，我们提出了LDCFormer成功地将可学习的描述性卷积（LDC）纳入VIT，以建模FAS局部描述性特征的远程依赖性。在本文中，我们提出了三种新颖的培训策略，以有效地增强LDCFormer的训练，从而在很大程度上提高其特征表征能力。第一种策略是双重注意监督，是为了学习以区域现场/欺骗的指导的精细颗粒livess特征。第二种策略是自我挑战的监督，旨在通过产生具有挑战性的培训数据来增强功能的可区分性。此外，我们通过缩小跨域差距，同时保持实时和欺骗特征之间的过渡关系，从而扩大LDCFormer的域将来能力。广泛的实验表明，在三种新型培训策略的共同监督下，LDCFormer的表现优于先前的方法。]]></description>
      <guid>https://arxiv.org/abs/2503.22936</guid>
      <pubDate>Tue, 01 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>