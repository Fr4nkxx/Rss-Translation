<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>CS.CV更新arxiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.cv更新arxiv.org e-print存档。</description>
    <lastBuildDate>Tue, 18 Mar 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>工业级传感器模拟通过高斯拆卸：可扩展编辑和全堆栈验证的模块化框架</title>
      <link>https://arxiv.org/abs/2503.11731</link>
      <description><![CDATA[ARXIV：2503.11731V1公告类型：新 
摘要：传感器仿真对于自主驾驶系统的可扩展验证至关重要，但现有的神经辐射场（NERF）方法面临着工业工作流程中的适用性和效率挑战。本文介绍了基于高斯的脱落（GS）系统来解决这些挑战：我们首先分解传感器模拟器组件并分析GS比NERF的优势。 Then in practice, we refactor three crucial components through GS, to leverage its explicit scene representation and real-time rendering: (1) choosing the 2D neural Gaussian representation for physics-compliant scene and sensor modeling, (2) proposing a scene editing pipeline to leverage Gaussian primitives library for data augmentation, and (3) coupling a controllable diffusion model for scene expansion and harmonization.我们在支持相机和LIDAR传感器的专有自动驾驶数据集上实施此框架。我们通过消融研究证明，我们的方法可以减少框架的模拟延迟，实现更好的几何和光度一致性，并实现可解释的明确场景编辑和扩展。此外，我们展示了如何将这种基于GS的传感器模拟器与流量和动态模拟器集成，从而使端到端自治算法进行全堆栈测试。我们的工作提供了算法的见解和实践验证，将GS确立为工业级传感器模拟的基石。]]></description>
      <guid>https://arxiv.org/abs/2503.11731</guid>
      <pubDate>Tue, 18 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>安全视觉语言模型通过不安全的权重操纵</title>
      <link>https://arxiv.org/abs/2503.11742</link>
      <description><![CDATA[ARXIV：2503.11742V1公告类型：新 
摘要：视觉模型（VLM）通常继承其大规模培训数据集中存在的偏见和不安全的关联。尽管最近的方法减轻了不安全的行为，但他们的评估集中在模型对不安全输入的安全性上，而忽略了安全性的潜在缺点。在本文中，我们首先通过引入Safeground（一组新的度量标准）来修改安全性评估，以评估不同级别的粒度安全性。有了这个指标，我们发现了一个令人惊讶的基于培训方法的问题：它们使模型在安全输入方面的安全降低。从这一发现，我们采取了不同的方向，并探索是否可以在不训练的情况下更安全，引入不安全的权重操纵（UWM）。 UWM使用安全和不安全实例的校准集比较安全和不安全内容之间的激活，从而确定处理后者的最重要参数。然后通过否定来操纵它们的价值。实验表明，UWM在安全和知识保存之间取得了最佳的权衡，在不安全的查询上始终提高VLM，同时甚至超过了基于培训的安全方法，也要超过培训。]]></description>
      <guid>https://arxiv.org/abs/2503.11742</guid>
      <pubDate>Tue, 18 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使每一步都有效：通过层次kV均衡越狱大型视觉模型</title>
      <link>https://arxiv.org/abs/2503.11750</link>
      <description><![CDATA[ARXIV：2503.11750V1公告类型：新 
摘要：在大型视觉模型（LVLM）的领域中，对抗性越狱攻击是一种红色团队的方法，可确定这些模型的安全漏洞及其相关的防御机制。但是，我们确定了一个关键限制：并非每个对抗性优化步骤都会带来积极的结果，并且在每个步骤中不加区别地接受优化结果可能会降低整体攻击成功率。为了应对这一挑战，我们介绍了HKVE（分层键值均衡），这是一个创新的越狱框架，根据跨不同层的注意力分数的分布，有选择地接受梯度优化结果，以确保每个优化步骤都对攻击有积极贡献。广泛的实验证明了HKVE的显着有效性，在Minigpt4上获得了75.08％的攻击成功率，LLAVA的攻击成功率为85.84％，QWEN-VL的攻击成功率为81.00％，而现有的方法则优于20.43 \％，21.01，21.01 \％和26.43 \％。此外，使每个步骤有效不仅会增加攻击成功率的提高，而且还可以减少迭代次数，从而降低计算成本。警告：本文包含潜在的有害示例数据。]]></description>
      <guid>https://arxiv.org/abs/2503.11750</guid>
      <pubDate>Tue, 18 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从单模式特征学习的角度重新思考多模式对象检测</title>
      <link>https://arxiv.org/abs/2503.11780</link>
      <description><![CDATA[ARXIV：2503.11780V1公告类型：新 
摘要：多模式对象检测（MMOD），由于其对各种复杂环境的适应性更强，已广泛应用于各种应用中。广泛的研究致力于RGB-IR对象检测，主要关注如何整合RGB-IR模式的互补特征。但是，他们忽略了单模式不足的学习问题，即多模式关节学习中的特征提取能力降低。这导致了不合理但普遍的现象 - 融合降解，这阻碍了MMOD模型的性能提高。在本文中，我们将线性探测评估引入了多模式检测器，并从单模式学习的角度重新考虑了多模式对象检测任务。因此，我们构建了一个名为M $^2 $ d-lif的新颖框架，该框架由单模式蒸馏（M $^2 $ d）和局部照明融合（LIF）模块组成。 M $^2 $ d-LIF框架有助于在多模式联合培训期间对单模式的足够学习，并探索轻巧但有效的功能融合方式，以实现出色的对象检测性能。在三个MMOD数据集上进行的广泛实验表明，我们的M $^2 $ d-lif有效地减轻了融合降解现象，并表现出以前的SOTA探测器。]]></description>
      <guid>https://arxiv.org/abs/2503.11780</guid>
      <pubDate>Tue, 18 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用基于超网络的Kolmogorov-Arnold网络进行颜色匹配</title>
      <link>https://arxiv.org/abs/2503.11781</link>
      <description><![CDATA[ARXIV：2503.11781V1公告类型：新 
摘要：我们提出了CMKAN，这是一个用于颜色匹配的多功能框架。给定带有来自源颜色分布的颜色的输入图像，我们的方法有效，准确地绘制了这些颜色，以匹配受监督和无监督的设置中的目标颜色分布。我们的框架利用Kolmogorov-Arnold网络（KANS）的样条能力来建模源和目标分布之间的颜色匹配。具体而言，我们开发了一项超网络，该净值生成空间变化的重量图以控制KAN的非线性花键，从而实现了准确的颜色匹配。作为这项工作的一部分，我们介绍了第一个大规模数据集的配对图像，该数据集由两个不同的摄像机捕获，并评估我们和现有方法在匹配颜色中的功效。我们评估了各种颜色匹配任务的方法，包括：（1）原始映射，其中源颜色分布在一个相机的原始色彩空间中，而目标是另一个相机的原始空间中的目标； （2）原始到SRGB映射，其中源颜色分布位于相机的原始空间中，而目标位于显示器SRGB空间中，从而模拟了相机ISP的颜色渲染； （3）SRGB至SRGB映射，目标是将颜色从源SRGB空间（例如，由源摄像头ISP生产）传输到目标SRGB空间（例如，从其他相机ISP）。结果表明，与其他方法相比，我们的方法平均比现有的方法平均胜过37.3％，同时保持轻量级。代码，数据集和预培训模型可在以下网址找到：https：//github.com/gosha20777/cmkan]]></description>
      <guid>https://arxiv.org/abs/2503.11781</guid>
      <pubDate>Tue, 18 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Eclare：各向异性分辨率增强的有效跨平面学习</title>
      <link>https://arxiv.org/abs/2503.11787</link>
      <description><![CDATA[ARXIV：2503.11787V1公告类型：新 
摘要：在临床成像中，磁共振（MR）图像体积通常被作为2D切片的堆栈获取，允许减少扫描时间，提高信噪比和图像对比2D MR脉冲序列所特有的。虽然这足以进行临床评估，但设计用于3D分析的自动化算法在2D获得的扫描上进行了次优，尤其是那些在切片之间较厚的切片和间隙的扫描。超分辨率（SR）方法旨在解决此问题，但是以前的方法并未解决以下所有问题：切片轮廓形状估计，切片差距，域移位，非整数 /任意UPSMPLING系列。在本文中，我们提出了Eclare（各向异性分辨率增强的有效的跨平面学习），这是一种解决这些因素的自动SR方法。 Eclare估算了从2D获得的多切片MR音量的切片曲线，训练网络以从同一体积学习从低分辨率到高分辨率的平面内贴片的映射，并使用抗偏降低。我们将Eclare与立方B-Spline插值，SMORE和其他当代SR方法进行了比较。我们使用了现实和代表性的模拟，以便可以计算针对地面真理的定量性能，并且Eclare在信号恢复和下游任务中的所有其他方法都优于所有其他方法。在没有地面真理的真实数据上，Eclare也表现出与其他方法相比的定性优势。重要的是，由于Eclare不使用外部训练数据，因此在训练和测试之间无法遭受域的转移。我们的代码是开源的，可在https://www.github.com/sremedios/eclare上找到。]]></description>
      <guid>https://arxiv.org/abs/2503.11787</guid>
      <pubDate>Tue, 18 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>StyleMorpheus：一种基于样式的3D感知形式的面部模型</title>
      <link>https://arxiv.org/abs/2503.11792</link>
      <description><![CDATA[ARXIV：2503.11792V1公告类型：新 
摘要：对于3D面部建模，最近开发的3D感知神经渲染方法能够用任意观看方向渲染具有逼真的脸部图像。但是，对参数可控的3D感知面部模型的训练仍然依赖于实验室收集的大规模数据集。为了解决这个问题，本文介绍了“ Stylemorpheus”，这是第一个基于样式的神经3D可变形面模型（3DMM），该模型在野外图像上训练。它继承了3DMM的脱离可控性（超过面部身份，表达和外观），但无需准确重建的显式3D形状。 Stylemorpheus采用自动编码器结构。编码器旨在学习代表性的分离参数代码空间，而解码器则使用网络不同子模块中的形状和外观相关样式代码来改善分离。此外，我们通过基于样式的生成对抗学习微调解码器，以实现逼真的3D渲染质量。提出的基于样式的设计使Stylemorpheus能够实现最新的3D感知面部重建结果，同时还允许对重建的面部进行分解的控制。我们的模型实现了实时渲染速度，从而可以在虚拟现实应用程序中使用。我们还展示了拟议的基于样式设计在面部编辑应用中的功能，例如样式混合和颜色编辑。项目主页：https：//github.com/ubc-3d-vision-lab/stylemorpheus。]]></description>
      <guid>https://arxiv.org/abs/2503.11792</guid>
      <pubDate>Tue, 18 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>语义剪裁：通过语义引导的视觉选择有效的视觉模型</title>
      <link>https://arxiv.org/abs/2503.11794</link>
      <description><![CDATA[ARXIV：2503.11794V1公告类型：新 
摘要：视觉模型（VLMS）利用对齐的视觉编码器将图像转换为视觉令牌，从而可以通过Backbone大语言模型（LLM）对其进行类似的处理。这种统一的输入范式使VLM能够在视觉范围任务（例如视觉询问）（VQA）等视觉任务中表现出色。为了改善细粒度的视觉推理，视觉建模的最新进步引入了图像裁剪技术，这些技术将所有编码的子图像供应到模型中。但是，这种方法显着增加了视觉令牌的数量，从而导致LLM的效率低下和潜在的干扰。为了解决VLM中图像表示的概括挑战，我们提出了一个轻巧的通用框架，该框架与现有VLM无缝集成，以增强其处理细化细节的能力。我们的方法利用文本语义来识别关键的视觉区域，改善VQA性能，而无需进行VLM的任何重新验证。此外，它将文本信号纳入视觉编码过程中，从而提高效率和有效性。所提出的方法（SEMCLIP）可以在7个基准测试中平均对7B VLM，LLAVA-1.5的视觉理解平均增长了3.3％，尤其是在具有挑战性的详细理解基准V*的情况下，尤其是5.3％。]]></description>
      <guid>https://arxiv.org/abs/2503.11794</guid>
      <pubDate>Tue, 18 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过填充的3D场景布局的人类在循环的本地校正</title>
      <link>https://arxiv.org/abs/2503.11806</link>
      <description><![CDATA[ARXIV：2503.11806V1公告类型：新 
摘要：我们提出了一种新型的人类在循环方法中，以估算3D场景布局，该布局从以自我为中心的角度使用人类反馈。我们通过引入新的本地校正任务来研究这种方法，用户在其中识别本地错误并促使模型自动纠正它们。在SpaceScript的基础上，这是一个用于利用结构性语言的3D场景布局估算的最新框架，我们提出了一种解决此问题的解决方案，将此问题构成“填充”，这是一种自然语言处理研究的任务。我们训练SceneScript的多任务版本，该版本在全球预测上保持性能，同时显着提高了其本地校正能力。我们将其集成到人类的循环系统中，使用户可以通过低摩擦“一键式修复”工作流程进行迭代完善的场景布局估算。我们的系统使最终的精制布局可以与训练分布不同，从而可以更准确地对复杂的布局进行建模。]]></description>
      <guid>https://arxiv.org/abs/2503.11806</guid>
      <pubDate>Tue, 18 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>缓解基于监督机器学习的基于机器学习的糟糕地面真相：带有Sentinel-2图像的多层框架</title>
      <link>https://arxiv.org/abs/2503.11807</link>
      <description><![CDATA[ARXIV：2503.11807V1公告类型：新 
摘要：在农业管理中，精确的基础真理（GT）数据对于基于机器的准确机器学习至关重要。然而，诸如错误标签和不正确的土地识别之类的问题很普遍。我们在利用多时间哨兵2数据来解决这些问题的同时，提出了一个多级GT清洁框架。具体而言，该框架利用为农田生成嵌入，聚集了类似的作物概况以及指示GT错误的异常值的识别。我们用错误的颜色复合（FCC）检查验证了群集，并使用了基于距离的指标来扩展和自动化此验证过程。当对模型进行了清洁和不干净的数据培训时，清洁GT数据的重要性变得显而易见。例如，当我们使用干净的GT数据训练随机森林模型时，我们的F1评分度量达到了70 \％的绝对百分比。这种方法推进了作物分类方法，并有可能在改善贷款承销和农业决策方面进行应用。]]></description>
      <guid>https://arxiv.org/abs/2503.11807</guid>
      <pubDate>Tue, 18 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>迈向地球视觉统一的哥白尼基金会模型</title>
      <link>https://arxiv.org/abs/2503.11849</link>
      <description><![CDATA[ARXIV：2503.11849V1公告类型：新 
摘要：地球观察的进步（EO）基础模型已解锁了大型卫星数据从空间中学习通用表示的潜力，从而使广泛的下游应用程序受益于我们的星球至关重要。但是，大多数现有的努力仍然限于固定光谱传感器，仅专注于地球表面，而忽略了图像之外的宝贵元数据。在这项工作中，我们朝着下一代EO基础模型迈出了三个关键组成部分：1）哥白尼 - 前景，这是一个大规模的预处理数据集，该数据集集成了来自所有主要哥白尼前哨任务的1870万个对齐图像，从地球表面跨越了地面到大气层； 2）Copernicus-FM，一种统一的基础模型，能够使用扩展的动态超网络和灵活的元数据编码来处理任何光谱或非光谱传感器模式； 3）哥白尼板凳，这是一种系统的评估基准，其中15个层次下游任务从预处理到每个前哨任务的专用应用程序。我们的数据集，模型和基准大大提高了EO基础模型的可扩展性，多功能性和多模式适应性，同时还为连接EO，天气和气候研究创造了新的机会。代码，数据集和模型可在https://github.com/zhu-xlab/copernicus-fm上找到。]]></description>
      <guid>https://arxiv.org/abs/2503.11849</guid>
      <pubDate>Tue, 18 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Decalign：分层跨模式对准多耦多模式学习</title>
      <link>https://arxiv.org/abs/2503.11892</link>
      <description><![CDATA[ARXIV：2503.11892V1公告类型：新 
摘要：多模式表示学习旨在捕获多种模式的共享和互补语义信息。但是，各种方式的内在异质性提出了实现有效的跨模式协作和整合的重大挑战。为了解决这个问题，我们介绍了DeCalign，这是一种新型的层次跨模式对齐框架，旨在将多模式表示形式与模态唯一（异质）和模态 - 共鸣（均匀）（同质）特征。为了处理异质性，我们采用了一种原型引导的最佳运输对准策略，利用高斯混合物建模和多界线运输计划，从而减轻分布差异，同时保持模态 - 独特的特征。为了增强同质性，我们通过将潜在分布匹配与最大平均差异正则化来确保跨模态的语义一致性。此外，我们结合了多模式变压器，以增强高级语义特征融合，从而进一步降低了跨模式的不一致。我们对四个广泛使用的多模式基准测试的广泛实验表明，差异始终超过五个指标的现有最新方法。这些结果突出了Decalign在增强出色的跨模式对齐和语义一致性方面的功效，同时保留了模态唯一的特征，这标志着多模式表示学习方案的显着进步。我们的项目页面在https://taco-group.github.io/decalign上，该代码可在https://github.com/taco-group/decalign上找到。]]></description>
      <guid>https://arxiv.org/abs/2503.11892</guid>
      <pubDate>Tue, 18 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>uStyle：通过深度引导特征综合的水体风格转移水下场景的转移</title>
      <link>https://arxiv.org/abs/2503.11893</link>
      <description><![CDATA[ARXIV：2503.11893V1公告类型：新 
摘要：在水下成像和视觉文献中，水体风格转移的概念在很大程度上尚未探索。传统的图像样式转移（STX）方法主要集中于艺术和逼真的融合，通常无法将物体和场景几何保存在诸如水下诸如水下的高散媒体中的图像中。依赖波长的非线性衰减和深度依赖性的反向散射伪像，使学习水下图像STX从未配对的数据中更加复杂。本文介绍了USTYLE，这是第一个数据驱动的学习框架，用于在不需要事先参考图像或场景信息的情况下将水体样式转移到水下图像上。我们提出了一种新颖的深度了解美白和着色转化（DA-WCT）机制，该机制将基于物理的水体合成整合，以确保在保留场景结构的同时，确保感知一致的风格化。为了提高样式转移质量，我们结合了精心设计的损失功能，以指导uSTYLE，以保持色彩鲜艳，轻度，结构完整性和频域特征，以及VGG和CLIP中的高级内容（对比性语言图像预处理）特征空间。通过解决特定领域的挑战，USTYLE为无参考的水下图像STX提供了强大的框架，超过了仅依赖端到端重建损失的最新方法（SOTA）方法。此外，我们介绍了UF7D数据集，这是一个精心策划的高分辨率水下图像集合，涵盖了七种不同的水体样式，建立了一个基准，以支持水下图像STX的未来研究。 USTYLE推理管道和UF7D数据集在以下网址发布：https：//github.com/uf-robopi/ustyle。]]></description>
      <guid>https://arxiv.org/abs/2503.11893</guid>
      <pubDate>Tue, 18 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多任务功能的升级文本对图像扩散模型</title>
      <link>https://arxiv.org/abs/2503.11905</link>
      <description><![CDATA[ARXIV：2503.11905V1公告类型：新 
摘要：近年来，文本对图像的综合见证了显着的进步。已经尝试采用文本图模型来支持多个任务。但是，现有方法通常需要资源密集型的重新训练或其他参数来适应新任务，这使得模型降低了对设备部署的效率。我们提出了多任务升级（MTU），这是一种简单而有效的食谱，扩展了预先训练的文本对图像扩散模型的功能，以支持各种图像到图像生成任务。 MTU用较小的FFN替换了扩散模型中的前馈网络（FFN）层，称为专家，并将它们与动态路由机制结合在一起。据我们所知，MTU是第一种多任务扩散建模方法，它通过减轻参数通货膨胀的问题来无缝将多任务与在设备兼容性融合。我们表明，MTU的性能与几个任务的单任务微调扩散模型相当，包括图像编辑，超分辨率和内部介入，同时保持与单任务微型调整模型相似的延迟和计算负载（GFLOPS）。]]></description>
      <guid>https://arxiv.org/abs/2503.11905</guid>
      <pubDate>Tue, 18 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用深度学习的SAR船分类的调查</title>
      <link>https://arxiv.org/abs/2503.11906</link>
      <description><![CDATA[ARXIV：2503.11906V1公告类型：新 
摘要：深度学习（DL）已成为合成孔径雷达（SAR）船舶分类的强大工具。这项调查全面分析了该域中采用的多种DL技术。我们确定了关键趋势和挑战，强调了整合手工特征，利用公共数据集，数据增强，微调，解释性技术以及促进跨学科合作以提高DL模型性能的重要性。这项调查建立了一种基于DL模型，手工制作的功能使用，SAR属性利用以及微调的影响的相关研究的首个分类法。我们讨论了SAR船分类任务中使用的方法和不同技术的影响。最后，该调查探讨了未来研究的潜在途径，包括解决数据稀缺，探索新颖的DL体系结构，结合了可解释性技术以及建立标准化的绩效指标。通过解决这些挑战并利用DL的进步，研究人员可以为开发更准确，更有效的船舶分类系统做出贡献，最终增强海上监视和相关应用。]]></description>
      <guid>https://arxiv.org/abs/2503.11906</guid>
      <pubDate>Tue, 18 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>