<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Thu, 08 Aug 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>PRTGS：预计算高斯散射辐射传输，实现实时高质量重新照明</title>
      <link>https://arxiv.org/abs/2408.03538</link>
      <description><![CDATA[arXiv:2408.03538v1 公告类型：新
摘要：我们提出了高斯片的预计算辐射传输 (PRTGS)，这是一种用于低频照明环境中高斯片的实时高质量重新照明方法，通过预先计算 3D 高斯片的辐射传输来捕捉软阴影和相互反射。现有研究表明，3D 高斯片 (3DGS) 在动态照明场景中的表现优于神经场的效率。然而，目前基于 3DGS 的重新照明方法仍然难以实时计算动态光的高质量阴影和间接照明，导致渲染结果不切实际。我们通过预先计算阴影等复杂传递函数所需的昂贵传输模拟来解决这个问题，得到的传递函数表示为每个高斯片的密集向量或矩阵集。我们引入了专门针对训练和渲染阶段而定制的独特预计算方法，以及针对 3D 高斯图块的独特光线追踪和间接照明预计算技术，以加快训练速度并计算与环境光相关的精确间接照明。实验分析表明，我们的方法在保持有竞争力的训练时间的同时实现了最先进的视觉质量，并允许在 1080p 分辨率下对动态光和相对复杂的场景进行高质量实时（30+ fps）重新照明。]]></description>
      <guid>https://arxiv.org/abs/2408.03538</guid>
      <pubDate>Fri, 09 Aug 2024 03:14:14 GMT</pubDate>
    </item>
    <item>
      <title>MoExtend：为模态和任务扩展调整新专家</title>
      <link>https://arxiv.org/abs/2408.03511</link>
      <description><![CDATA[arXiv:2408.03511v1 公告类型：新
摘要：大型语言模型 (LLM) 在各种任务中表现出色，但主要在文本数据上进行训练，限制了它们的应用范围。扩展 LLM 功能以包括视觉语言理解至关重要，但从头开始在多模态数据上训练它们具有挑战性且成本高昂。现有的指令调整方法，例如 LLAVA，通常通过完全微调 LLM 连接预训练的 CLIP 视觉编码器和 LLM，以弥合模态差距。然而，完全微调受到灾难性遗忘（即忘记先前的知识）和高昂的训练成本的困扰，尤其是在任务和模态不断增加的时代。为了解决这个问题，我们引入了 MoExtend，这是一个有效的框架，旨在简化混合专家 (MoE) 模型的模态适应和扩展。 MoExtend 将新专家无缝集成到预训练的 MoE 模型中，为他们提供新知识，而无需调整预训练模型（如 MoE 和视觉编码器）。这种方法能够快速适应和扩展到新的模态数据或任务，有效地解决了在 LLM 中容纳新模态的挑战。此外，MoExtend 避免调整预训练模型，从而降低灾难性遗忘的风险。实验结果证明了 MoExtend 在增强 LLM 多模态能力方面的有效性和效率，有助于推动多模态 AI 研究的发展。代码：https://github.com/zhongshsh/MoExtend。]]></description>
      <guid>https://arxiv.org/abs/2408.03511</guid>
      <pubDate>Fri, 09 Aug 2024 03:14:13 GMT</pubDate>
    </item>
    <item>
      <title>利用 LLM 增强自动驾驶中的开放词汇 3D 场景理解</title>
      <link>https://arxiv.org/abs/2408.03516</link>
      <description><![CDATA[arXiv:2408.03516v1 公告类型：新
摘要：本文介绍了一种用于自动驾驶开放词汇 3D 场景理解的新方法，该方法将语言嵌入式 3D 高斯与大型语言模型 (LLM) 相结合以增强推理能力。我们建议利用 LLM 生成上下文相关的规范短语以进行分割和场景解释。我们的方法利用 LLM 的上下文和语义功能来生成一组规范短语，然后将其与嵌入在 3D 高斯中的语言特征进行比较。这种 LLM 引导的方法显着提高了零样本场景理解和感兴趣对象的检测能力，即使在最具挑战性或最不熟悉的环境中也是如此。在 WayveScenes101 数据集上的实验结果表明，我们的方法在开放词汇对象检测和分割的准确性和灵活性方面超越了最先进的方法。这项工作代表了向更智能、情境感知的自动驾驶系统迈出的重大进步，有效地将 3D 场景表示与高级语义理解结合起来。]]></description>
      <guid>https://arxiv.org/abs/2408.03516</guid>
      <pubDate>Fri, 09 Aug 2024 03:14:13 GMT</pubDate>
    </item>
    <item>
      <title>SwinShadow：用于模糊相邻阴影检测的移位窗口</title>
      <link>https://arxiv.org/abs/2408.03521</link>
      <description><![CDATA[arXiv:2408.03521v1 公告类型：新
摘要：阴影检测是许多计算机视觉应用中一项基本且具有挑战性的任务。直观上看，大多数阴影来自物体本身对光线的遮挡，导致物体与其阴影相邻（本文中称为相邻阴影）。在这种情况下，当物体的颜色与阴影的颜色相似时，现有方法难以实现准确检测。为了解决这个问题，我们提出了一种基于 Transformer 的架构 SwinShadow，它充分利用了强大的移位窗口机制来检测相邻阴影。该机制分两步运行。首先，它在单个窗口内应用局部自注意力，使网络能够关注局部细节。随后，它移动注意力窗口以促进窗口间注意力，从而能够捕获更大范围的相邻信息。这些组合步骤显着提高了网络区分阴影和附近物体的能力。整个过程可以分为三个部分：编码器、解码器和特征集成。在编码过程中，我们采用 Swin Transformer 获取分层特征。然后在解码过程中，对于浅层，我们提出了一个深度监督 (DS) 模块来抑制误报并提高阴影特征的表示能力以供后续处理，而对于深层，我们利用双重注意 (DA) 模块在一个阶段整合局部和移位窗口以实现更大的感受野并增强信息的连续性。最后，应用一种新的多级聚合 (MLA) 机制来融合解码后的特征以进行掩码预测。在三个阴影检测基准数据集 SBU、UCF 和 ISTD 上进行的大量实验表明，我们的网络在平衡错误率 (BER) 方面取得了良好的性能。]]></description>
      <guid>https://arxiv.org/abs/2408.03521</guid>
      <pubDate>Fri, 09 Aug 2024 03:14:13 GMT</pubDate>
    </item>
    <item>
      <title>使用 VECTOR 打开 3D 重建误差分析的黑匣子</title>
      <link>https://arxiv.org/abs/2408.03503</link>
      <description><![CDATA[arXiv:2408.03503v1 公告类型：新
摘要：从 2D 图像重建 3D 场景是一项技术挑战，影响从地球和行星科学、太空探索到增强现实和虚拟现实等领域。通常，重建算法首先识别图像之间的共同特征，然后在估计地形形状后最小化重建误差。此捆绑调整 (BA) 步骤围绕单个简化标量值进行优化，该标量值混淆了重建误差的许多可能原因（例如，相机位置和方向的初始估计、光照条件、地形中特征检测的难易程度）。重建误差可能导致不准确的科学推断或危及探索远程环境的航天器。为了应对这一挑战，我们提出了 VECTOR，这是一种视觉分析工具，可改进立体重建 BA 的错误检查。VECTOR 为分析师提供了以前无法获得的特征位置、相机姿势和计算出的 3D 点的可见性。 VECTOR 是与 NASA 喷气推进实验室的毅力号火星探测器和 Ingenuity 火星直升机地形重建团队合作开发的。我们将报告如何使用此工具来调试和改进 Mars 2020 任务的地形重建。]]></description>
      <guid>https://arxiv.org/abs/2408.03503</guid>
      <pubDate>Fri, 09 Aug 2024 03:14:12 GMT</pubDate>
    </item>
    <item>
      <title>使用 SOTA YOLO 深度学习模型检测 GUI 元素</title>
      <link>https://arxiv.org/abs/2408.03507</link>
      <description><![CDATA[arXiv:2408.03507v1 公告类型：新
摘要：图形用户界面 (GUI) 元素检测是从图像和草图自动生成代码、GUI 测试和 GUI 搜索的关键任务。最近的研究利用了老式和现代计算机视觉 (CV) 技术。老式方法利用经典的图像处理算法（例如边缘检测和轮廓检测），而现代方法使用成熟的深度学习解决方案进行一般对象检测任务。然而，GUI 元素检测是对象检测的一个特定领域案例，其中对象重叠更频繁，并且彼此非常接近，而且对象类别的数量要少得多，但与自然图像相比，图像中的对象更多。因此，对比较各种对象检测模型进行的研究可能不适用于 GUI 元素检测。在本研究中，我们评估了四个最近成功的 YOLO 模型在 GUI 元素检测的一般对象检测任务中的性能，并研究了它们在检测各种 GUI 元素方面的准确性性能。]]></description>
      <guid>https://arxiv.org/abs/2408.03507</guid>
      <pubDate>Fri, 09 Aug 2024 03:14:12 GMT</pubDate>
    </item>
    <item>
      <title>遥感领域的人工智能基础模型：一项调查</title>
      <link>https://arxiv.org/abs/2408.03464</link>
      <description><![CDATA[arXiv:2408.03464v1 公告类型：新
摘要：人工智能 (AI) 技术深刻改变了遥感领域，彻底改变了数据收集、处理和分析。传统上依赖于人工解释和特定任务的模型，遥感因基础模型的出现而得到了显着增强——基础模型是大规模、预先训练的 AI 模型，能够以前所未有的准确性和效率执行各种任务。本文对遥感领域的基础模型进行了全面调查，涵盖了 2021 年 6 月至 2024 年 6 月期间发布的模型。我们根据这些模型在计算机视觉和特定领域任务中的应用对其进行分类，从而深入了解它们的架构、预训练数据集和方法。通过详细的性能比较，我们重点介绍了这些基础模型的新兴趋势和取得的重大进步。此外，我们还讨论了技术挑战、实际意义和未来的研究方向，以满足对高质量数据、计算资源和改进的模型泛化的需求。我们的研究还发现，预训练方法，尤其是对比学习和掩蔽自动编码器等自监督学习技术，可显著提高基础模型在遥感任务（如场景分类、物体检测和其他应用）中的性能和稳健性。本调查旨在为研究人员和从业人员提供资源，提供遥感领域基础模型持续开发和应用的进展和有希望的途径全景。]]></description>
      <guid>https://arxiv.org/abs/2408.03464</guid>
      <pubDate>Fri, 09 Aug 2024 03:14:11 GMT</pubDate>
    </item>
    <item>
      <title>FacialPulse：通过时间面部标志实现基于 RNN 的有效抑郁症检测</title>
      <link>https://arxiv.org/abs/2408.03499</link>
      <description><![CDATA[arXiv:2408.03499v1 公告类型：新
摘要：抑郁症是一种普遍存在的心理健康障碍，严重影响个人的生活和幸福感。早期发现和干预对于有效治疗和管理抑郁症至关重要。最近，有许多端到端深度学习方法利用面部表情特征进行自动抑郁症检测。然而，大多数当前方法都忽略了面部表情的时间动态。虽然最近的 3DCNN 方法弥补了这一差距，但由于选择了基于 CNN 的主干和冗余面部特征，它们引入了更多的计算成本。
为了解决上述限制，通过考虑面部表情的时间相关性，我们提出了一种名为 FacialPulse 的新框架，它可以高精度和快速地识别抑郁症。通过利用双向特性并熟练地解决长期依赖性，FacialPulse 中的面部运动建模模块 (FMMM) 被设计为完全捕捉时间特征。由于提出的FMMM具有并行处理能力，并具有缓解梯度消失的门控机制，该模块还可以显著提高训练速度。
此外，为了有效地使用面部特征点替换原始图像以减少信息冗余，设计了面部特征点校准模块（FLCM）来消除面部特征点错误，进一步提高识别准确率。在AVEC2014数据集和MMDA数据集（抑郁症数据集）上进行的大量实验证明了FacialPulse在识别准确率和速度上的优越性，平均MAE（平均绝对误差）与基线相比降低了21％，识别速度与最先进的方法相比提高了100％。代码发布在https://github.com/volatileee/FacialPulse。]]></description>
      <guid>https://arxiv.org/abs/2408.03499</guid>
      <pubDate>Fri, 09 Aug 2024 03:14:11 GMT</pubDate>
    </item>
    <item>
      <title>RRG24 上的 e-Health CSIRO：用于放射学报告生成的熵增强自批评序列训练</title>
      <link>https://arxiv.org/abs/2408.03500</link>
      <description><![CDATA[arXiv:2408.03500v1 公告类型：新
摘要：大规模放射学报告生成共享任务 (RRG24) 旨在加快开发用于解释和报告胸部 X 光 (CXR) 图像的辅助系统。这项任务要求参与者开发模型，使用五个不同的数据集，从患者研究的 CXR 生成放射学报告的发现和印象部分。本文概述了 e-Health CSIRO 团队的方法，该方法在 RRG24 中多次获得第一名。我们方法的核心新颖之处在于在自我批评序列训练中添加了熵正则化，以保持 token 分布中的更高熵。这可以防止过度拟合常用短语并确保在训练期间更广泛地探索词汇，这对于处理 RRG24 数据集中放射学报告的多样性至关重要。我们的模型可在 Hugging Face 上找到 https://huggingface.co/aehrc/cxrmate-rrg24。]]></description>
      <guid>https://arxiv.org/abs/2408.03500</guid>
      <pubDate>Fri, 09 Aug 2024 03:14:11 GMT</pubDate>
    </item>
    <item>
      <title>RayGauss：基于体积高斯的射线投射，实现逼真的新颖视图合成</title>
      <link>https://arxiv.org/abs/2408.03356</link>
      <description><![CDATA[arXiv:2408.03356v1 公告类型：新
摘要：基于可微分体积渲染的方法在新型视图合成方面取得了重大进展。一方面，创新方法已经用局部参数化结构取代了神经辐射场 (NeRF) 网络，从而能够在合理的时间内实现高质量的渲染。另一方面，方法已经使用可微分溅射代替 NeRF 的射线投射，使用高斯核快速优化辐射场，从而可以精细地适应场景。然而，不规则间隔核的可微分射线投射很少被探索，而溅射虽然可以实现快速渲染时间，但容易受到清晰可见的伪影的影响。
我们的工作通过提供发射辐射 c 和密度 {\sigma} 的物理一致公式来弥补这一差距，该公式分解为与球面高斯/谐波相关的高斯函数以实现全频率比色表示。我们还介绍了一种实现不规则分布高斯分布的可微分射线投射的方法，该方法使用逐块集成辐射场并利用 BVH 结构的算法。这使得我们的方法能够精细地适应场景，同时避免溅射伪影。因此，我们实现了与最先进技术相比更出色的渲染质量，同时保持了合理的训练时间，并在 Blender 数据集上实现了 25 FPS 的推理速度。包含视频和代码的项目页面：https://raygauss.github.io/]]></description>
      <guid>https://arxiv.org/abs/2408.03356</guid>
      <pubDate>Fri, 09 Aug 2024 03:14:10 GMT</pubDate>
    </item>
    <item>
      <title>Set2Seq Transformer：学习艺术序列的排列感知集合表示</title>
      <link>https://arxiv.org/abs/2408.03404</link>
      <description><![CDATA[arXiv:2408.03404v1 公告类型：新
摘要：我们提出了 Set2Seq Transformer，这是一种新颖的顺序多实例架构，可以学习对序列的排列感知集表示进行排序。首先，我们说明学习离散时间步长的时间位置感知表示可以极大地改进不考虑时间性并且几乎只专注于视觉内容分析的静态视觉多实例学习方法。我们进一步展示了端到端顺序多实例学习的显著优势，以多模态方式整合了视觉内容和时间信息。作为应用，我们专注于美术分析相关的任务。为此，我们表明我们的 Set2Seq Transformer 可以利用视觉集和时间位置感知表示来建模视觉艺术家的作品，以预测艺术成功。最后，通过使用新数据集 WikiArt-Seq2Rank 和视觉学习排名下游任务进行广泛的定量和定性评估，我们表明我们的 Set2Seq Transformer 捕获了必要的时间信息，从而提高了强静态和顺序多实例学习方法在预测艺术成功方面的性能。]]></description>
      <guid>https://arxiv.org/abs/2408.03404</guid>
      <pubDate>Fri, 09 Aug 2024 03:14:10 GMT</pubDate>
    </item>
    <item>
      <title>混合扩散模型：结合监督和生成预训练，实现分割模型的标签高效微调</title>
      <link>https://arxiv.org/abs/2408.03433</link>
      <description><![CDATA[arXiv:2408.03433v1 公告类型：新
摘要：本文考虑了分割模型的标签高效微调任务：我们假设有一个大型标记数据集可用，并允许在一个域中训练准确的分割模型，并且我们必须在只有少量样本可用的相关域上调整该模型。我们观察到这种调整可以使用两种不同的方法完成：第一种方法，监督预训练，就是使用经典监督学习在第一个域上训练模型，并使用可用的标记样本在第二个域上对其进行微调。第二种方法是使用通用借口任务在第一个域上执行自监督预训练，以获得高质量的表示，然后可以使用这些表示以标签高效的方式在第二个域上训练模型。我们在本文中提出通过引入一种新的借口任务来融合这两种方法，即在第一个域上同时执行图像去噪和掩模预测。我们之所以做出这一选择，是因为我们证明了，使用扩散模型理论，可以将以噪声水平为条件的图像去噪器视为未标记图像分布的生成模型，同样，使用这种新借口任务训练的模型也可以被视为图像和分割蒙版联合分布的生成模型，前提是假设从图像到分割蒙版的映射是确定性的。然后，我们在多个数据集上通过实证研究证明，使用这种方法对预训练的模型进行微调比仅使用监督或无监督预训练对类似模型进行微调可获得更好的结果。]]></description>
      <guid>https://arxiv.org/abs/2408.03433</guid>
      <pubDate>Fri, 09 Aug 2024 03:14:10 GMT</pubDate>
    </item>
    <item>
      <title>使用卷积神经网络从干涉图像重建不规则粗糙颗粒的形状</title>
      <link>https://arxiv.org/abs/2408.03327</link>
      <description><![CDATA[arXiv:2408.03327v1 公告类型：新
摘要：我们开发了一种卷积神经网络 (CNN)，用于从不规则粗糙颗粒的干涉图像中重建其形状。CNN 基于具有残差块模块的 UNET 架构。该数据库是使用在数字微镜设备 (DMD) 上编程并在激光照射下完全已知的伪粒子生成的实验图案构建的。CNN 已使用 AUSTRAL 超级计算机（位于诺曼底的 CRIANN）在 18000 张实验干涉图像的基础上进行了训练。CNN 在中心对称（棒状、十字形、树枝状）和非中心对称（如 T、Y 或 L）粒子的情况下进行了测试。编程粒子的大小和 3D 方向是随机的。CNN 可以很好地重建不同的形状。使用三个视角，可以进一步从三个重建面对粒子进行 3D 重建。]]></description>
      <guid>https://arxiv.org/abs/2408.03327</guid>
      <pubDate>Fri, 09 Aug 2024 03:14:09 GMT</pubDate>
    </item>
    <item>
      <title>InLUT3D：用于点云分析的具有挑战性的真实室内数据集</title>
      <link>https://arxiv.org/abs/2408.03338</link>
      <description><![CDATA[arXiv:2408.03338v1 公告类型：新
摘要：在本文中，我们介绍了 InLUT3D 点云数据集，这是一种旨在推动室内环境场景理解领域的综合资源。该数据集涵盖了罗兹理工大学 W7 教职员楼内的不同空间，其特点是高分辨率激光点云和手动标记。除了数据集之外，我们还提出了指标和基准测试指南，这些对于确保算法评估中的可信和可重复结果至关重要。我们预计，InLUT3D 数据集及其相关基准的引入将催化未来 3D 场景理解的进步，促进方法论的严谨性并激发该领域的新方法。]]></description>
      <guid>https://arxiv.org/abs/2408.03338</guid>
      <pubDate>Fri, 09 Aug 2024 03:14:09 GMT</pubDate>
    </item>
    <item>
      <title>FastEdit：通过语义感知扩散微调实现快速文本引导的单图像编辑</title>
      <link>https://arxiv.org/abs/2408.03355</link>
      <description><![CDATA[arXiv:2408.03355v1 公告类型：新 
摘要：传统的文本引导的单图编辑方法需要两步过程，包括对目标文本嵌入进行超过 1K 次迭代的微调和对生成模型进行另外 1.5K 次迭代。虽然它确保生成的图像与输入图像和目标文本紧密对齐，但此过程通常需要每张图像 7 分钟，由于其时间密集型特性，对实际应用构成了挑战。为了解决这个瓶颈，我们引入了 FastEdit，一种快速的文本引导单图编辑方法，具有语义感知扩散微调，将编辑过程大大加快到仅 17 秒。FastEdit 简化了生成模型的微调阶段，将其从 1.5K 减少到仅 50 次迭代。对于扩散微调，我们根据输入图像和目标文本之间的语义差异采用某些时间步长值。此外，FastEdit 通过使用以特征空间而非文本嵌入空间为条件的图像到图像模型来绕过初始微调步骤。它可以有效地在同一特征空间内对齐目标文本提示和输入图像，并节省大量处理时间。此外，我们将参数高效的微调技术 LoRA 应用于 U-net。借助 LoRA，FastEdit 将模型的可训练参数最小化到原始大小的 0.37\%。同时，我们可以实现可比的编辑结果，同时显着降低计算开销。我们进行了广泛的实验来验证我们方法的编辑性能，并展示了有希望的编辑功能，包括内容添加、风格转换、背景替换和姿势操纵等。]]></description>
      <guid>https://arxiv.org/abs/2408.03355</guid>
      <pubDate>Fri, 09 Aug 2024 03:14:09 GMT</pubDate>
    </item>
    </channel>
</rss>