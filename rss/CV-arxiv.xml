<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Fri, 03 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>LTX-Video：实时视频潜伏扩散</title>
      <link>https://arxiv.org/abs/2501.00103</link>
      <description><![CDATA[arXiv:2501.00103v1 公告类型：新
摘要：我们介绍了 LTX-Video，这是一种基于 Transformer 的潜在扩散模型，它通过无缝集成 Video-VAE 和去噪 Transformer 的职责，采用整体方法生成视频。与将这些组件视为独立组件的现有方法不同，LTX-Video 旨在优化它们的交互以提高效率和质量。其核心是一个精心设计的 Video-VAE，可实现 1:192 的高压缩比，时空缩小为每标记 32 x 32 x 8 像素，这是通过将修补操作从 Transformer 的输入重新定位到 VAE 的输入来实现的。在这个高度压缩的潜在空间中操作使 Transformer 能够有效地执行完全时空自注意力，这对于生成具有时间一致性的高分辨率视频至关重要。然而，高压缩本质上限制了精细细节的表示。为了解决这个问题，我们的 VAE 解码器负责从潜在图像到像素的转换和最终的去噪步骤，直接在像素空间中产生干净的结果。这种方法保留了生成精细细节的能力，而无需承担单独上采样模块的运行时成本。我们的模型支持多种用例，包括文本到视频和图像到视频生成，两种功能同时训练。它实现了比实时更快的生成速度，在 Nvidia H100 GPU 上仅用 2 秒就生成了 5 秒的 24 fps 768x512 分辨率视频，优于所有现有的类似规模的模型。源代码和预训练模型都是公开的，为可访问和可扩展的视频生成设定了新的基准。]]></description>
      <guid>https://arxiv.org/abs/2501.00103</guid>
      <pubDate>Fri, 03 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有预训练表征的文本到图像 GAN</title>
      <link>https://arxiv.org/abs/2501.00116</link>
      <description><![CDATA[arXiv:2501.00116v1 公告类型：新 
摘要：根据给定的文本描述生成所需图像受到了广泛关注。最近，扩散模型和自回归模型已经展示了其出色的表现力，并逐渐取代GAN成为文本到图像合成的首选架构。然而，他们仍然面临一些障碍：推理速度慢，训练成本高昂。为了在复杂场景下实现更强大、更快的文本到图像合成，我们提出了TIGER，一种具有预训练表示的文本到图像GAN。具体来说，我们提出了一个视觉赋能鉴别器和一个高容量生成器。（i）视觉赋能鉴别器吸收了预训练视觉模型的复杂场景理解能力和领域泛化能力，以增强模型性能。与以前的研究不同，我们探索在鉴别器中堆叠多个预训练模型以收集多个不同的表示。（ii）高容量生成器旨在实现有效的文本-图像融合，同时增加模型容量。高容量生成器由多个新颖的高容量融合块 (HFBlock) 组成。HFBlock 包含多个深度融合模块和一个全局融合模块，它们发挥不同的作用来使我们的模型受益。大量实验证明了我们提出的 TIGER 在标准和零样本文本到图像合成任务上的出色性能。在标准文本到图像合成任务中，TIGER 在两个具有挑战性的数据集上实现了最先进的性能，获得了新的 FID 5.48 (COCO) 和 9.38 (CUB)。在零样本文本到图像合成任务中，我们以更少的模型参数、更小的训练数据大小和更快的推理速度实现了可比的性能。此外，在补充材料中进行了更多的实验和分析。]]></description>
      <guid>https://arxiv.org/abs/2501.00116</guid>
      <pubDate>Fri, 03 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PQD：高效扩散模型的训练后量化</title>
      <link>https://arxiv.org/abs/2501.00124</link>
      <description><![CDATA[arXiv:2501.00124v1 公告类型：新
摘要：扩散模型（DM）在合成高保真度和多样性图像方面取得了显著成就。然而，扩散模型的大量计算要求和缓慢的生成速度限制了它们的广泛应用。在本文中，我们提出了一种新的扩散模型后训练量化（PQD），这是一种基于后训练量化的扩散模型时间感知优化框架。该框架通过选择代表性样本并进行时间感知校准来优化推理过程。实验结果表明，我们提出的方法能够将全精度扩散模型直接量化为 8 位或 4 位模型，同时以无训练的方式保持可比的性能，在 ImageNet 上实现了无条件图像生成的少量 FID 变化。我们的方法展示了兼容性，并且首次可以应用于 512x512 文本引导图像生成。]]></description>
      <guid>https://arxiv.org/abs/2501.00124</guid>
      <pubDate>Fri, 03 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从视频中提取知识图谱的检测融合</title>
      <link>https://arxiv.org/abs/2501.00136</link>
      <description><![CDATA[arXiv:2501.00136v1 公告类型：新
摘要：视频理解领域的一项挑战性任务是从视频输入中提取语义内容。大多数现有系统使用语言模型以自然语言句子描述视频，但这有几个主要缺点。这样的系统可能过于依赖语言模型组件，并将其输出基于自然语言文本中的统计规律而不是视频的视觉内容。此外，自然语言注释无法由计算机轻松处理，难以用性能指标评估，也无法轻易翻译成不同的自然语言。在本文中，我们提出了一种使用知识图注释视频的方法，从而避免了这些问题。具体而言，我们为这项任务提出了一个基于深度学习的模型，该模型首先预测个体对，然后预测它们之间的关系。此外，我们提出了模型的扩展，以将背景知识纳入知识图的构建中。]]></description>
      <guid>https://arxiv.org/abs/2501.00136</guid>
      <pubDate>Fri, 03 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>自由像素的极简视觉</title>
      <link>https://arxiv.org/abs/2501.00142</link>
      <description><![CDATA[arXiv:2501.00142v1 公告类型：新
摘要：极简视觉系统使用解决视觉任务所需的最少像素数。传统相机使用大网格方形像素，而极简相机使用可以呈现任意形状的自由形状像素来增加其信息内容。我们表明，极简相机的硬件可以建模为神经网络的第一层，后续层用于推理。针对任何给定任务训练网络都会产生相机自由形状像素的形状，每个像素都使用光电探测器和光学掩模实现。我们设计了用于监控室内空间（8 个像素）、测量室内照明（8 个像素）和估计交通流量（8 个像素）的极简相机。这些系统所展示的性能与像素多几个数量级的传统相机相当。极简视觉有两个主要优点。首先，它自然倾向于保护场景中个人的隐私，因为捕获的信息不足以提取视觉细节。其次，由于简易相机进行的测量次数非常少，我们表明它可以完全自供电，即无需外部电源或电池即可运行。]]></description>
      <guid>https://arxiv.org/abs/2501.00142</guid>
      <pubDate>Fri, 03 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MLLM 作为无需人工标记的图像安全评判者</title>
      <link>https://arxiv.org/abs/2501.00192</link>
      <description><![CDATA[arXiv:2501.00192v1 公告类型：新
摘要：随着在线平台上视觉媒体的兴起，图像内容安全已成为一项重大挑战。同时，在人工智能生成内容 (AIGC) 时代，许多图像生成模型能够生成有害内容，例如包含性或暴力内容的图像。因此，根据既定的安全规则识别此类不安全图像变得至关重要。预先训练的多模态大型语言模型 (MLLM) 在这方面具有潜力，因为它们具有强大的模式识别能力。现有方法通常使用人工标记的数据集对 MLLM 进行微调，但这带来了一系列缺点。首先，依靠人工注释者按照复杂而详细的指南标记数据既昂贵又费力。此外，安全判断系统的用户可能需要频繁更新安全规则，这使得对基于人工的注释进行微调更具挑战性。这就引出了一个研究问题：我们能否通过在零样本设置中使用预定义的安全宪法（一组安全规则）查询 MLLM 来检测不安全的图像？我们的研究表明，简单地查询预先训练的 MLLM 并不能产生令人满意的结果。这种有效性的缺乏源于安全规则的主观性、冗长的宪法的复杂性以及模型中固有的偏见等因素。为了应对这些挑战，我们提出了一种基于 MLLM 的方法，包括客观化安全规则、评估规则和图像之间的相关性、基于去偏的标记概率使用逻辑完整但简化的安全规则前提条件链做出快速判断，并在必要时使用级联的思路链过程进行更深入的推理。实验结果表明，我们的方法对于零样本图像安全性判断任务非常有效。]]></description>
      <guid>https://arxiv.org/abs/2501.00192</guid>
      <pubDate>Fri, 03 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DecoratingFusion：点级与特征级融合的激光雷达-摄像头融合网络</title>
      <link>https://arxiv.org/abs/2501.00220</link>
      <description><![CDATA[arXiv:2501.00220v1 公告类型：新
摘要：激光雷达和摄像头在自动驾驶中起着至关重要的作用，为 3D 检测提供补充信息。最先进的融合方法在特征级别将它们集成在一起，但它们大多依赖于学习到的点云和图像之间的软关联，缺乏可解释性并且忽略了它们之间的硬关联。在本文中，我们将特征级融合与点级融合相结合，使用校准矩阵建立的硬关联来指导对象查询的生成。具体而言，在早期融合阶段，我们使用图像的 2D CNN 特征来修饰点云数据，并使用两个独立的稀疏卷积来提取修饰后的点云特征。在中级融合阶段，我们使用中心热图初始化查询，并将预测的类标签作为辅助信息嵌入到查询中，使初始位置更接近目标的实际中心。在两个流行数据集 KITTI、Waymo 上进行的大量实验证明了 DecoratingFusion 的优越性。]]></description>
      <guid>https://arxiv.org/abs/2501.00220</guid>
      <pubDate>Fri, 03 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>让领域转移成为类增量学习中灾难性遗忘的缓解者</title>
      <link>https://arxiv.org/abs/2501.00237</link>
      <description><![CDATA[arXiv:2501.00237v1 公告类型：新
摘要：在类增量学习 (CIL) 领域，缓解灾难性遗忘问题是一项关键挑战。本文发现了一个违反直觉的观察结果：通过将域转移纳入 CIL 任务，遗忘率显著降低。我们的全面研究表明，加入域转移会导致跨任务的特征分布更清晰地分离，并有助于减少学习过程中的参数干扰。受此观察的启发，我们提出了一种简单而有效的方法来处理 CIL 任务，名为 DisCo。DisCo 引入了一个轻量级原型池，利用对比学习来促进当前任务相对于之前任务的不同特征分布，从而有效地减轻跨任务的干扰。DisCo 可以轻松集成到现有的最先进的类增量学习方法中。实验结果表明，将我们的方法整合到各种 CIL 方法中可实现显著的性能提升，验证了我们的方法通过分离特征表示和减少干扰来增强类增量学习的优势。这些发现表明，DisCo 可以作为未来类增量学习研究的稳健方式。]]></description>
      <guid>https://arxiv.org/abs/2501.00237</guid>
      <pubDate>Fri, 03 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>跨层缓存聚合用于超细粒度图像识别中的 Token 减少</title>
      <link>https://arxiv.org/abs/2501.00243</link>
      <description><![CDATA[arXiv:2501.00243v1 公告类型：新
摘要：超细粒度图像识别 (UFGIR) 是一项具有挑战性的任务，涉及对宏观类别中的图像进行分类。虽然传统的 FGIR 处理对不同物种的分类，但 UFGIR 更进一步，对物种内的子类别（例如植物的栽培品种）进行分类。近年来，基于 Vision Transformer 的主干的使用使方法能够在此任务中获得出色的识别性能，但这在计算方面付出了巨大的代价，特别是因为此任务从合并更高分辨率的图像中受益匪浅。因此，出现了诸如 token 减少之类的技术来降低计算成本。但是，丢弃 token 会导致细粒度类别的重要信息丢失，特别是当 token 保留率降低时。因此，为了抵消使用 token 减少带来的信息丢失，我们提出了一种新颖的跨层聚合分类头和跨层缓存机制，以便在后续位置从先前的层恢复和访问信息。广泛的实验涵盖了 2000 多次运行，涉及各种设置，包括 5 个数据集、9 个主干、7 种 token 减少方法、5 种保留率和 2 种图像大小，证明了所提出的即插即用模块的有效性，并使我们能够通过将保留的 token 减少到高达 10\% 的极低比例来突破 UFGIR 的准确度与成本的界限，同时保持与最先进模型相媲美的准确度。代码可在以下位置获得：\url{https://github.com/arkel23/CLCA}]]></description>
      <guid>https://arxiv.org/abs/2501.00243</guid>
      <pubDate>Fri, 03 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于统一图像生成和理解的双重扩散</title>
      <link>https://arxiv.org/abs/2501.00289</link>
      <description><![CDATA[arXiv:2501.00289v1 公告类型：新
摘要：扩散模型在文本到图像生成方面取得了巨大成功，但在视觉理解任务方面仍然落后，而视觉理解任务主要由自回归视觉语言模型主导。我们提出了一种大规模、完全端到端的扩散模型，用于多模态理解和生成，该模型显着改进了现有的基于扩散的多模态模型，并且是同类模型中第一个支持全套视觉语言建模功能的模型。受多模态扩散变换器 (MM-DiT) 和离散扩散语言建模的最新进展的启发，我们利用跨模态最大似然估计框架，在单个损失函数下同时联合训练图像和文本的条件似然，该损失函数通过扩散变换器的两个分支反向传播。生成的模型非常灵活，能够完成包括图像生成、字幕和视觉问答在内的广泛任务。与最近的统一图像理解和生成模型相比，我们的模型获得了具有竞争力的性能，证明了多模态扩散建模作为自回归下一个标记预测模型的有前途的替代方案的潜力。]]></description>
      <guid>https://arxiv.org/abs/2501.00289</guid>
      <pubDate>Fri, 03 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于改进YOLOv8网络的车辆检测研究</title>
      <link>https://arxiv.org/abs/2501.00300</link>
      <description><![CDATA[arXiv:2501.00300v1 Announce Type: new 
摘要：保证自动驾驶系统安全避障功能的关键在于采用极其精准的车辆识别技术，然而实际道路环境的多变性以及车辆和行人特征的多样性共同构成了提升检测准确率的巨大障碍，对这一目标的实现提出了严峻挑战。针对以上问题，本文提出了一种改进的YOLOv8车辆检测方法。具体而言，以YOLOv8n-seg模型为基础模型，首先使用FasterNet网络替换骨干网络，达到降低计算复杂度和内存的同时提高检测准确率和速度的目的；其次，通过在Neck中加入注意机制CBAM实现特征增强；最后，将损失函数CIoU修改为WIoU，优化检测框定位的同时提高分割准确率。结果表明，改进后的模型对汽车、人物、摩托车的检测准确率分别达到了98.3%、89.1%、88.4%，在Precision等6个指标上与改进前和YOLOv9模型进行了比较。]]></description>
      <guid>https://arxiv.org/abs/2501.00300</guid>
      <pubDate>Fri, 03 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于跨域小样本分割的 SAM-Aware 图形提示推理网络</title>
      <link>https://arxiv.org/abs/2501.00303</link>
      <description><![CDATA[arXiv:2501.00303v1 公告类型：新
摘要：跨域小样本分割 (CD-FSS) 的主要挑战是训练和推理阶段之间的领域差异，这种差异可能存在于输入数据或目标类中。以前的模型很难从有限的训练域样本中学习推广到各种未知域的特征表示。相比之下，大规模视觉模型 SAM 经过来自不同领域和类别的数千万张图像的预训练，具有出色的泛化能力。在这项工作中，我们提出了一个 SAM 感知图提示推理网络 (GPRN)，充分利用 SAM 来指导 CD-FSS 特征表示学习并提高预测准确性。具体而言，我们提出了一个 SAM 感知提示初始化模块 (SPI)，将 SAM 生成的掩码转换为富含高级语义信息的视觉提示。由于 SAM 倾向于将一个对象划分为许多子区域，这可能导致表示相同语义对象的视觉提示具有不一致或碎片化的特征。我们进一步提出了一个图形提示推理 (GPR) 模块，该模块在视觉提示之间构建一个图形来推理它们的相互关系，并使每个视觉提示能够聚合来自类似提示的信息，从而实现全局语义一致性。随后，每个视觉提示将其语义信息嵌入到相应的掩码区域中以协助特征表示学习。为了在测试期间细化分割掩码，我们还设计了一个非参数自适应点选择模块 (APS) 来从查询预测中选择代表性点提示并将它们反馈给 SAM 以细化不准确的分割结果。在四个标准 CD-FSS 数据集上的实验表明，我们的方法建立了新的最先进结果。代码：https://github.com/CVL-hub/GPRN。]]></description>
      <guid>https://arxiv.org/abs/2501.00303</guid>
      <pubDate>Fri, 03 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>时间动力学与逆处理解耦以增强人体运动预测</title>
      <link>https://arxiv.org/abs/2501.00315</link>
      <description><![CDATA[arXiv:2501.00315v1 公告类型：新
摘要：探索历史和未来运动行为之间的桥梁仍然是人体运动预测的核心挑战。虽然大多数现有方法将重建任务作为辅助任务纳入解码器，从而改进了时空依赖关系的建模，但它们忽略了重建和预测任务之间的潜在冲突。在本文中，我们提出了一种新方法：时间解耦解码与逆处理（\textbf{$TD^2IP$}）。我们的方法策略性地分离了重建和预测解码过程，采用不同的解码器将共享的运动特征解码为历史或未来序列。此外，逆处理在时间维度上反转运动信息并将其重新引入模型，利用人体运动行为的双向时间相关性。通过缓解重建和预测任务之间的冲突并增强历史和未来信息的关联，\textbf{$TD^2IP$} 促进了对运动模式的更深入理解。大量实验证明了我们的方法在现有方法中的适应性。]]></description>
      <guid>https://arxiv.org/abs/2501.00315</guid>
      <pubDate>Fri, 03 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于 3D 人体运动预测的时空多子图 GCN</title>
      <link>https://arxiv.org/abs/2501.00317</link>
      <description><![CDATA[arXiv:2501.00317v1 公告类型：新
摘要：人体运动预测 (HMP) 涉及根据历史数据预测未来的人体运动。图卷积网络 (GCN) 因其在捕捉人体运动中关节间关系方面的出色表现而在该领域引起了广泛关注。然而，现有的基于 GCN 的方法往往侧重于时域或空域特征，或者它们结合了时空特征而没有充分利用这两个特征的互补性和交叉依赖性。在本文中，我们提出了时空多子图图卷积网络 (STMS-GCN) 来捕获人体运动中复杂的时空依赖关系。具体而言，我们将时间和空间依赖关系的建模解耦，通过时空信息一致性约束机制实现多尺度的跨域知识转移。此外，我们利用多个子图来提取更丰富的运动信息，并通过同质信息约束机制增强不同子图的学习关联。在标准HMP基准上进行的大量实验证明了我们方法的优越性。]]></description>
      <guid>https://arxiv.org/abs/2501.00317</guid>
      <pubDate>Fri, 03 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过部分级跨模态对应改进基于文本的人物搜索</title>
      <link>https://arxiv.org/abs/2501.00318</link>
      <description><![CDATA[arXiv:2501.00318v1 公告类型：新
摘要：基于文本的人物搜索是查找与作为查询给出的自然语言文本描述最相关的人物图像的任务。这项任务的主要挑战是目标图像和文本查询之间存在很大差距，这使得很难建立对应关系并区分人与人之间的细微差异。为了应对这一挑战，我们引入了一个高效的编码器-解码器模型，该模型提取从粗到细的嵌入向量，这些向量在两种模态之间语义对齐，而无需监督对齐。另一个挑战是学习仅以人员 ID 作为监督来捕获细粒度信息，其中由于缺乏部分级监督，不同个体的相似身体部位被视为不同。为了解决这个问题，我们提出了一种新颖的排名损失，称为基于共性的边际排名损失，它量化了每个身体部位的共性程度并在学习细粒度身体部位细节时反映出来。因此，它使我们的方法能够在三个公共基准上取得最佳记录。]]></description>
      <guid>https://arxiv.org/abs/2501.00318</guid>
      <pubDate>Fri, 03 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>