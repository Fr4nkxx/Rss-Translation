<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Fri, 13 Sep 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>用于半导体电子显微图分析的多模态指令调节小型语言和视觉助手</title>
      <link>https://arxiv.org/abs/2409.07463</link>
      <description><![CDATA[arXiv:2409.07463v1 公告类型：新
摘要：我们提出了一种使用视觉语言指令调整来分析和解释半导体制造中的电子显微镜图像的新框架。该框架采用独特的师生方法，利用预先训练的多模态大型语言模型（如 GPT-4）为零样本视觉问答 (VQA) 和分类任务生成指令跟踪数据，为显微镜图像分析定制较小的多模态模型 (SMM)，从而产生指令调整的语言和视觉助手。我们的框架将知识工程与机器学习相结合，将特定领域的专业知识从较大的多模态模型整合到该专业领域内的较小的多模态模型中，大大减少了对大量人工标记的需求。我们的研究提出了一种安全、经济高效且可定制的方法来分析显微镜图像，解决了在半导体制造中采用专有模型的挑战。]]></description>
      <guid>https://arxiv.org/abs/2409.07463</guid>
      <pubDate>Fri, 13 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>反射式人机协同适应增强文本到图像生成对话系统</title>
      <link>https://arxiv.org/abs/2409.07464</link>
      <description><![CDATA[arXiv:2409.07464v1 公告类型：新
摘要：当今的图像生成系统能够生成逼真的高质量图像。但是，用户提示通常包含歧义，这使得这些系统难以解释用户的潜在意图。因此，机器需要与用户进行多轮交互才能更好地理解用户的意图。通过多次反馈交互使用或学习图像生成模型的不可预测成本阻碍了它们的广泛采用和充分发挥性能潜力，尤其是对于非专家用户而言。在这项研究中，我们旨在增强图像生成系统的用户友好性。为此，我们提出了一种反思性的人机协同适应策略，称为 RHM-CAS。在外部，代理与用户进行有意义的语言交互，以反思和改进生成的图像。在内部，代理尝试根据用户偏好优化策略，确保最终结果与用户偏好紧密一致。针对不同任务的各种实验证明了所提方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2409.07464</guid>
      <pubDate>Fri, 13 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 YOLO NAS 小梯度和超梯度进行室内盲人辅助小物体检测</title>
      <link>https://arxiv.org/abs/2409.07469</link>
      <description><![CDATA[arXiv:2409.07469v1 公告类型：新
摘要：物体检测算法的进步为满足视障人士需求的辅助技术开辟了新途径。本文提出了一种新颖的室内盲人辅助方法，解决了小物体检测的难题。我们提出了一种 YOLO NAS Small 架构技术，这是一种轻量级且高效的物体检测模型，使用超梯度训练框架进行了优化。这种组合可以实时检测小物体，这些小物体对于帮助盲人在室内环境中导航至关重要，例如家具、电器和家居用品。所提出的方法强调低延迟和高准确性，能够及时提供信息丰富的语音指导，以增强用户的空间意识和与周围环境的互动。本文详细介绍了实现、实验结果，并讨论了该系统在为视障人士提供室内辅助的实用解决方案方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2409.07469</guid>
      <pubDate>Fri, 13 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ENACT：基于熵的注意力输入聚类，用于提高对象检测 Transformer 的计算性能</title>
      <link>https://arxiv.org/abs/2409.07541</link>
      <description><![CDATA[arXiv:2409.07541v1 公告类型：新
摘要：在基于视觉的物体检测问题上，Transformers 在精度方面表现出了竞争力。然而，由于注意力权重的二次方大小，它们需要大量的计算资源。在这项工作中，我们建议根据其熵对 Transformer 输入进行聚类。这样做的原因是每个像素的自信息（其总和是熵）在对应于相同对象的像素之间可能相似。聚类减少了作为 Transformer 输入的数据大小，从而减少了训练时间和 GPU 内存使用量，同时保留了有意义的信息以通过网络的其余部分传递。所提出的流程组织在一个名为 ENACT 的模块中，可以插入任何由编码器中的多头自注意力计算组成的 Transformer 架构。我们使用 COCO 物体检测数据集和三个检测 Transformer 进行了广泛的实验。所得结果表明，在所有测试情况下，所需的计算资源持续减少，而检测任务的精度仅略有降低。ENACT 模块的代码将在 https://github.com/GSavathrakis/ENACT 上提供]]></description>
      <guid>https://arxiv.org/abs/2409.07541</guid>
      <pubDate>Fri, 13 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用自蒸馏技术进行无监督点云配准</title>
      <link>https://arxiv.org/abs/2409.07558</link>
      <description><![CDATA[arXiv:2409.07558v1 公告类型：新
摘要：刚性点云配准是一个基本问题，与机器人技术和自动驾驶高度相关。如今，深度学习方法可以训练来匹配一对点云，给定它们之间的转换。然而，由于收集地面真实姿势的成本很高，这种训练往往不可扩展。因此，我们提出了一种自蒸馏方法，以无监督的方式学习点云配准。在这里，每个样本都传递给教师网络，增强视图传递给学生网络。教师包括一个可训练的特征提取器和一个无需学习的鲁棒求解器，如 RANSAC。求解器强制对应关系之间的一致性并优化无监督的内点比，从而无需地面真实标签。我们的方法简化了训练过程，消除了相关方法中对​​初始手工制作特征或连续点云帧的需求。我们表明，我们的方法不仅在 RGB-D 基准 3DMatch 上超越了它们，而且可以很好地推广到汽车雷达，而其他人采用的经典特征则失败了。代码可在 https://github.com/boschresearch/direg 上找到。]]></description>
      <guid>https://arxiv.org/abs/2409.07558</guid>
      <pubDate>Fri, 13 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>EchoDFKD：使用合成数据进行心脏超声分割的无数据知识提取</title>
      <link>https://arxiv.org/abs/2409.07566</link>
      <description><![CDATA[arXiv:2409.07566v1 公告类型：新
摘要：机器学习在心脏医学超声视频（即超声心动图）中的应用最近随着大量公共数据集的出现而受到关注。传统的监督任务（例如射血分数回归）现在正在为更关注数据分布的潜在结构以及生成方法的方法让路。我们提出了一个完全通过知识蒸馏训练的模型，无论是在真实数据还是合成数据上，都涉及检索教师模型建议的掩码。我们在识别舒张末期和收缩末期帧的任务上实现了最先进（SOTA）的值。通过仅在合成数据上训练模型，它可以达到接近在真实数据上训练时的性能的分割能力，并且权重数量显着减少。与现有的 5 种主要方法的比较表明，我们的方法在大多数情况下优于其他方法。我们还提出了一种新的评估方法，它不需要人工注释，而是依赖于大型辅助模型。我们表明，该方法产生的分数与人工注释获得的分数一致。依靠来自大量记录的综合知识，该方法克服了人工注释者标注的某些固有局限性。代码：https://github.com/GregoirePetit/EchoDFKD]]></description>
      <guid>https://arxiv.org/abs/2409.07566</guid>
      <pubDate>Fri, 13 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FaVoR：通过体素渲染实现相机重新定位的功能</title>
      <link>https://arxiv.org/abs/2409.07571</link>
      <description><![CDATA[arXiv:2409.07571v1 公告类型：新
摘要：相机重新定位方法包括从密集图像对齐到从查询图像直接回归相机姿势。其中，稀疏特征匹配是一种高效、通用且通常轻量级的方法，具有众多应用。然而，基于特征的方法通常会在显著的视点和外观变化下挣扎，导致匹配失败和姿势估计不准确。为了克服这一限制，我们提出了一种新方法，利用全局稀疏但局部密集的 2D 特征 3D 表示。通过在一系列帧上跟踪和三角测量地标，我们构建了一个稀疏体素图，该图经过优化以渲染跟踪期间观察到的图像块描述符。给定初始姿势估计，我们首先使用体积渲染从体素中合成描述符，然后执行特征匹配以估计相机姿势。这种方法能够为看不见的视图生成描述符，从而增强对视图变化的鲁棒性。我们在 7-Scenes 和 Cambridge Landmarks 数据集上对我们的方法进行了广泛的评估。我们的结果表明，我们的方法在室内环境中的表现明显优于现有的最先进的特征表示技术，中值翻译误差最高可提高 39%。此外，我们的方法在室外场景中也能产生与其他方法相当的结果，同时保持较低的内存和计算成本。]]></description>
      <guid>https://arxiv.org/abs/2409.07571</guid>
      <pubDate>Fri, 13 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于无监督适应的自掩蔽网络</title>
      <link>https://arxiv.org/abs/2409.07577</link>
      <description><![CDATA[arXiv:2409.07577v1 公告类型：新
摘要：随着十亿参数基础模型的出现，有效的微调对于模型适应下游任务变得越来越重要。然而，特别是在计算机视觉领域，当缺乏对高质量标记数据的访问时，很难获得良好的性能。在这项工作中，我们提出了一种通过学习二进制掩码以自监督方式调整预训练通用模型的方法。这些自监督掩码网络 (SMN) 的存储效率提高了 79 倍，并显著提高了标签效率下游任务的性能。我们在 8 个数据集和 3 个模型架构上验证了学习二进制掩码作为微调方法的实用性，并在 3 个标签效率设置中证明了 SMN 的有效性。]]></description>
      <guid>https://arxiv.org/abs/2409.07577</guid>
      <pubDate>Fri, 13 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用深度循环和卷积神经网络检测视频中的暴力行为</title>
      <link>https://arxiv.org/abs/2409.07581</link>
      <description><![CDATA[arXiv:2409.07581v1 公告类型：新
摘要：近年来，暴力和异常行为检测研究引起了越来越多的关注，这主要是由于全球大城市犯罪率上升。在这项工作中，我们提出了一种用于暴力检测的深度学习架构，它结合了循环神经网络 (RNN) 和二维卷积神经网络 (2D CNN)。除了视频帧，我们还使用使用捕获序列计算的光流。CNN 提取每帧中的空间特征，而 RNN 提取时间特征。使用光流可以对场景中的运动进行编码。所提出的方法达到了与最先进技术相同的水平，有时甚至超越了它们。它在 3 个数据库上进行了验证，取得了良好的效果。]]></description>
      <guid>https://arxiv.org/abs/2409.07581</guid>
      <pubDate>Fri, 13 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>最小化嵌入失真以实现稳健的分布外性能</title>
      <link>https://arxiv.org/abs/2409.07582</link>
      <description><![CDATA[arXiv:2409.07582v1 公告类型：新
摘要：在庞大而多样的数据集上训练的基础模型已展示出在各种零样本任务中跨不同领域和分布进行泛化的卓越能力。我们的工作解决了在通过微调将基础模型适应特定下游任务时保留这些强大泛化能力的挑战。为此，我们引入了一种称为“相似性损失”的新方法，可以将其纳入任何任务的微调过程中。通过最大限度地减少微调嵌入与预训练嵌入之间的失真，我们的方法在特定任务的适应性和保持广泛的泛化能力之间取得了平衡。我们在两个不同的任务上评估了我们的方法：卫星图像上的图像分类和人脸识别，重点关注开放类和领域转移场景以评估分布外 (OOD) 性能。我们证明这种方法显着提高了 OOD 性能，同时保持了强大的分布内 (ID) 性能。]]></description>
      <guid>https://arxiv.org/abs/2409.07582</guid>
      <pubDate>Fri, 13 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于视频中端到端暴力检测的二维双向门控循环单元卷积神经网络</title>
      <link>https://arxiv.org/abs/2409.07588</link>
      <description><![CDATA[arXiv:2409.07588v1 公告类型：新
摘要：视频中的异常行为检测、动作识别、打斗和暴力检测是近年来引起广泛关注的领域。在这项工作中，我们提出了一种结合双向门控循环单元 (BiGRU) 和 2D 卷积神经网络 (CNN) 的架构来检测视频序列中的暴力行为。CNN 用于从每个帧中提取空间特征，而 BiGRU 使用 CNN 从多个帧中提取的特征来提取时间和局部运动特征。所提出的端到端深度学习网络在三个具有不同场景复杂度的公共数据集中进行了测试。所提出的网络实现了高达 98% 的准确率。获得的结果很有希望，并展示了所提出的端到端方法的性能。]]></description>
      <guid>https://arxiv.org/abs/2409.07588</guid>
      <pubDate>Fri, 13 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>令牌图灵机是高效的视觉模型</title>
      <link>https://arxiv.org/abs/2409.07613</link>
      <description><![CDATA[arXiv:2409.07613v1 公告类型：新
摘要：我们提出了视觉令牌图灵机 (ViTTM)，这是一种高效、低延迟、内存增强的视觉转换器 (ViT)。我们的方法建立在神经图灵机和令牌图灵机的基础上，它们被应用于 NLP 和顺序视觉理解任务。ViTTM 专为非顺序计算机视觉任务而设计，例如图像分类和分割。我们的模型创建了两组令牌：进程令牌和内存令牌；进程令牌通过编码器块，并从网络中的每个编码器块的内存令牌中读写，从而允许它们存储和检索内存中的信息。通过确保进程令牌少于内存令牌，我们能够在保持其准确性的同时减少网络的推理时间。在 ImageNet-1K 上，最先进的 ViT-B 的平​​均延迟为 529.5 毫秒，准确率为 81.0%，而我们的 ViTTM-B 快 56%（234.1 毫秒），FLOP 减少了 2.4 倍，准确率为 82.9%。在 ADE20K 语义分割中，ViT-B 以 13.8 帧/秒 (FPS) 实现 45.65mIoU，而我们的 ViTTM-B 模型以 26.8 FPS (+94%) 实现 45.17 mIoU。]]></description>
      <guid>https://arxiv.org/abs/2409.07613</guid>
      <pubDate>Fri, 13 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>行人意图预测中的特征重要性：情境感知综述</title>
      <link>https://arxiv.org/abs/2409.07645</link>
      <description><![CDATA[arXiv:2409.07645v1 公告类型：新
摘要：使用计算机视觉和深度神经网络预测自动驾驶汽车行人过马路意图的最新进展令人鼓舞。然而，DNN 的黑箱性质对理解模型的工作原理以及输入特征如何影响最终预测提出了挑战。这种可解释性的缺乏限制了对模型性能的信任，阻碍了对特征选择、表示和模型优化的明智决策；从而影响了该领域未来研究的有效性。为了解决这个问题，我们引入了上下文感知排列特征重要性 (CAPFI)，这是一种专门用于行人意图预测的新方法。CAPFI 通过利用细分的场景上下文，通过有针对性的改组来减轻特征值的随机性，从而实现对特征重要性的更多可解释性和可靠评估。这旨在减少方差并防止排列过程中重要性得分的偏差估计。我们将行人意图估计 (PIE) 数据集划分为 16 个可比较的上下文集，测量五种不同的神经网络架构在每种上下文中意图预测的基线性能，并使用 CAPFI 评估输入特征的重要性。我们观察到不同上下文特征之间的模型存在细微差异。该研究通过跨上下文置换评估揭示了行人边界框和车辆自速度在预测行人意图方面的关键作用，以及由于速度特征导致的潜在预测偏差。我们提出了一种替代特征表示，通过考虑接近度变化率来呈现动态行人-车辆运动，从而增强输入特征对意图预测的贡献。这些发现强调了上下文特征及其多样性对于开发准确而强大的意图预测模型的重要性。]]></description>
      <guid>https://arxiv.org/abs/2409.07645</guid>
      <pubDate>Fri, 13 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DiffTED：利用基于扩散的同声手势生成一次性音频驱动的 TED 演讲视频</title>
      <link>https://arxiv.org/abs/2409.07649</link>
      <description><![CDATA[arXiv:2409.07649v1 公告类型：新
摘要：音频驱动的说话视频生成已经取得了显着进展，但现有方法通常依赖于视频到视频的转换技术和传统的生成网络（如 GAN），并且它们通常分别生成头部和语音手势，导致输出不太连贯。此外，这些方法产生的手势通常显得过于平滑或柔和，缺乏多样性，许多以手势为中心的方法没有集成说话头部生成。为了解决这些限制，我们引入了 DiffTED，这是一种从单个图像一次性生成音频驱动的 TED 风格说话视频的新方法。具体来说，我们利用扩散模型为薄板样条运动模型生成关键点序列，精确控制头像的动画，同时确保时间连贯和多样化的手势。这种创新方法利用无分类器引导，使手势能够随着音频输入自然流动，而无需依赖预先训练的分类器。实验表明，DiffTED 可以生成具有多种同步讲话手势且时间连贯的谈话视频。]]></description>
      <guid>https://arxiv.org/abs/2409.07649</guid>
      <pubDate>Fri, 13 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基础模型提升低级感知相似性指标</title>
      <link>https://arxiv.org/abs/2409.07650</link>
      <description><![CDATA[arXiv:2409.07650v1 公告类型：新
摘要：对于使用深度学习方法的全参考图像质量评估 (FR-IQA)，失真图像和参考图像之间的感知相似度得分通常计算为从预训练的 CNN 或最近的 Transformer 网络中提取的特征之间的距离测量。通常，这些中间特征需要进一步微调或使用额外的神经网络层进行处理，以使最终的相似度得分与人类判断保持一致。到目前为止，大多数基于基础模型的 IQA 模型主要依赖于最后一层或嵌入来估计质量分数。相比之下，这项工作探索了利用这些基础模型的中间特征的潜力，这些特征在低级感知相似度指标的设计中迄今为止基本上尚未被探索。我们证明中间特征相对更有效。此外，无需任何训练，这些指标就可以通过利用特征之间的距离测量超越传统和最先进的学习指标。]]></description>
      <guid>https://arxiv.org/abs/2409.07650</guid>
      <pubDate>Fri, 13 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>