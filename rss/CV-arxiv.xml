<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>CS.CV更新arxiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.cv更新arxiv.org e-print存档。</description>
    <lastBuildDate>Wed, 26 Mar 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>实时的人类行动识别模型，用于辅助生活</title>
      <link>https://arxiv.org/abs/2503.18957</link>
      <description><![CDATA[ARXIV：2503.18957V1公告类型：新 
摘要：确保在辅助生活环境中老年人和脆弱人群的安全和福祉是一个关键问题。计算机视觉提出了一种通过人类行动识别（HAR）技术来预测视频监控的创新和有力的方法。但是，对具有高性能和效率的人类行为的实时预测是一个挑战。这项研究提出了一种实时的人类行动识别模型，该模型结合了深度学习模型和实时视频预测和警报系统，以预测助长生活中居民的跌倒，惊人和胸痛。选择了NTU RGB+D 60数据集中的六千个RGB视频样本，以创建一个具有四个类别的数据集：下降，惊人，胸痛和正常，正常类包括40个日常活动。转移学习技术用于在GPU服务器上训练四种最先进的HAR模型，即Uniformerv2，TimesFormer，I3D和Slowfast。本文根据班级和宏观性能指标，推理效率，模型复杂性和计算成本介绍了四个模型的结果。提议为开发实时人类行动识别模型而提出的时间表，该模型利用其领先的宏观F1得分（95.33％），召回（95.49％）和精度（95.19％）以及与其他人相比的推理吞吐量明显更高。这项研究提供了见解，以增强老年人的安全和健康，以及在辅助生活环境中患有慢性疾病的人，促进可持续护理，更智能的社区和行业创新。]]></description>
      <guid>https://arxiv.org/abs/2503.18957</guid>
      <pubDate>Wed, 26 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SG-Tailor：Object Inter-opject permonsense关系推理场景图操纵</title>
      <link>https://arxiv.org/abs/2503.18988</link>
      <description><![CDATA[ARXIV：2503.18988V1公告类型：新 
摘要：场景图捕获了对象之间的复杂关系，作为内容生成和操纵的强大先验。然而，无论是添加节点还是修改边缘，合理地操纵场景图 - 仍然是一项具有挑战性且没有触及的任务。诸如将节点添加到图形或有关节点与所有其他关系的关系的推理之类的任务在计算上是棘手的，因为即使是单个边缘修改也可能触发冲突，因为图形中的复杂相互依赖性。为了应对这些挑战，我们介绍了SG-Tailor，这是一种自回归模型，可预测任何两个节点之间无冲突的关系。 SG-Tailor不仅会渗透对象间的关系，包括生成新添加的节点的常识边缘，而且解决了由边缘修改引起的冲突，以产生连贯的，操纵的图形，以实现下游任务。对于节点添加，该模型查询目标节点和图形中的其他节点以预测适当的关系。对于边缘修改，SG-Tailor采用剪裁策略来解决冲突并在全球调整图表。广泛的实验表明，SG-Tailor的表现优于相互竞争的方法，并且可以无缝集成为现场生成和机器人操纵任务的插件模块。]]></description>
      <guid>https://arxiv.org/abs/2503.18988</guid>
      <pubDate>Wed, 26 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用嘈杂的视觉变压器改善食物图像识别</title>
      <link>https://arxiv.org/abs/2503.18997</link>
      <description><![CDATA[ARXIV：2503.18997V1公告类型：新 
摘要：由于食物图像的高可变性和复杂性，食物图像识别是计算机视觉中的一项艰巨任务。在这项研究中，我们研究了嘈杂的视觉变压器（Noisyvit）对改善食品分类性能的潜力。通过将噪声引入学习过程中，Noisyvit降低了任务的复杂性并调整系统的熵，从而提高了模型的准确性。我们在三个基准数据集上微调了Noisyvit：Food2k（2,000个类别，〜1M图像），Food-101（101个类别，〜100K图像）和CNFood-241（241个类别，〜190K图像）。 NOISYVIT的性能是根据最先进的食品识别模型进行评估的。我们的结果表明，NOISYVIT分别达到了95％，99.5％和96.6％的TOP-1精度，分别在Food2k，Food-101和CNFood-241上获得了明显胜过现有方法。这项研究强调了Noisyvit在饮食评估，营养监测和医疗保健应用中的潜力，为未来基于视觉的食物计算的进步铺平了道路。可以在Noisyvit_food上获得用于食品识别的noisyvit的代码。]]></description>
      <guid>https://arxiv.org/abs/2503.18997</guid>
      <pubDate>Wed, 26 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DISETTTALK：通过语义散开扩散模型的跨语性说话面部生成</title>
      <link>https://arxiv.org/abs/2503.19001</link>
      <description><![CDATA[ARXIV：2503.19001V1公告类型：新 
摘要：谈话发电的最新进展已大大改善了面部动画的综合。但是，现有方法面临基本局限性：基于3DMM的方法保持时间一致性，但缺乏细粒度的区域控制，而基于稳定的基于扩散的方法可以实现空间操作，但会遇到时间不一致。这些方法的整合受到不兼容的控制机制和面部表征的语义纠缠的阻碍。本文介绍了DisentTalk，引入了数据驱动的语义分离框架，该框架将3DMM表达参数分解为有意义的子空间，以进行细粒度的面部控制。在此分离表示的基础上，我们开发了一个分层的潜扩散体系结构，该结构在3DMM参数空间中运行，集成了区域感知的注意机制，以确保空间精度和时间连贯性。为了解决高质量中国培训数据的稀缺性，我们介绍了中国高清面部数据集CHDTF。广泛的实验表明，超过多个指标的现有方法，包括唇部同步，表达质量和时间一致性。项目页面：https：//kangweiiliu.github.io/disenttalk。]]></description>
      <guid>https://arxiv.org/abs/2503.19001</guid>
      <pubDate>Wed, 26 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>视频 - 柯尔伯特：文本到视频检索的上下文化的晚互动</title>
      <link>https://arxiv.org/abs/2503.19009</link>
      <description><![CDATA[ARXIV：2503.19009V1公告类型：新 
摘要：在这项工作中，我们解决了文本到视频检索的问题（T2VR）。受到文本文档，文本图像和文本视频检索的成功的启发，我们的方法，视频 - 柯尔伯特引入了一种简单有效的机制，用于查询和视频之间的细粒度相似性评估。视频 - 柯尔伯特建立在3个主要组成部分上：良好的空间和暂时令牌的互动，查询和视觉扩展，以及训练过程中双重sigmoid损失。我们发现，这种互动和训练范式可导致编码视频内容的强大的个人但兼容的表示形式。与其他Bi-编码方法相比，这些表示形式导致公共文本到视频检索基准的性能提高。]]></description>
      <guid>https://arxiv.org/abs/2503.19009</guid>
      <pubDate>Wed, 26 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ROMANTEX：脱钩3D感知的旋转位置嵌入式多发网络，用于纹理合成</title>
      <link>https://arxiv.org/abs/2503.19011</link>
      <description><![CDATA[ARXIV：2503.19011V1公告类型：新 
摘要：现有几何形状的绘画纹理是3D资产生成的关键但劳动密集型的过程。文本对图像（T2I）模型的最新进展导致纹理产生取得了重大进展。大多数现有的研究通过使用图像扩散模型在2D空间中首先生成图像，然后进行纹理烘焙过程以实现UV纹理。但是，由于生成的多视图图像之间的不一致，这些方法通常很难产生高质量的纹理，从而导致接缝和幽灵伪影。相比之下，基于3D的纹理合成方法旨在解决这些不一致，但它们经常忽略2D扩散模型先验，使它们具有挑战性地应用于现实世界中的对象以克服这些局限性，我们建议将基于多的纹理框架与基于多型的3D代表相结合的多型纹理框架，由Indering 3D代表组成，我们的小说3D代表了我们的小说3D。此外，我们将一个脱钩特征纳入多发障碍物中，以增强模型在图像到文本任务中的鲁棒性，从而实现语义上正确的背面视图合成。此外，我们引入了与几何相关的无分类器引导（CFG）机制，以进一步改善几何和图像的比对。定量和定性评估以及全面的用户研究表明，我们的方法实现了最先进的方法，从而带来了质地质量和一致性。]]></description>
      <guid>https://arxiv.org/abs/2503.19011</guid>
      <pubDate>Wed, 26 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>diffv2ir：通过视觉的理解可见到红外扩散模型</title>
      <link>https://arxiv.org/abs/2503.19012</link>
      <description><![CDATA[ARXIV：2503.19012V1公告类型：新 
摘要：翻译可见的边红外图像（V2IR）的任务本质上是具有挑战性的，这是由于三个主要障碍：1）实现语义感知的翻译，2）管理红外图像中不同的波长光谱，以及3）3）稀缺的全面红外数据集。当前的领先方法倾向于将V2IR视为常规的图像到图像综合挑战，通常会忽略这些特定问题。为了解决这个问题，我们介绍了diffv2ir，这是一个包含两个关键要素的图像翻译的新型框架：渐进学习模块（PLM）和视觉理解模块（VLUM）。 PLM具有自适应扩散模型体系结构，该体系结构利用多阶段知识学习到从全范围到目标波长的红外过渡。为了改善V2IR翻译，VLUM结合了统一的视力语言理解。我们还收集了一个大型红外数据集IR-500K，其中包括在各种环境条件下由各种场景和物体编制的500,000个红外图像。通过PLM，VLUM和广泛的IR-500K数据集的组合，DIFFV2IR显着提高了V2IR的性能。实验验证了DiffV2IR在产生高质量翻译，确立其功效和广泛适用性方面的卓越性。代码，数据集和DIFFV2IR模型将在https://github.com/lidongwang-26/diffv2ir上找到。]]></description>
      <guid>https://arxiv.org/abs/2503.19012</guid>
      <pubDate>Wed, 26 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>颜色有条件产生，切成薄片的Wasserstein指导</title>
      <link>https://arxiv.org/abs/2503.19034</link>
      <description><![CDATA[ARXIV：2503.19034V1公告类型：新 
摘要：我们提出了SW-Guidance，这是一种以参考图像的颜色分布为条件的图像产生的无训练方法。虽然可以通过首先从文本提示符创建图像，然后应用色彩样式传输方法来生成具有固定颜色的图像，但这种方法通常会在生成的图像中产生毫无意义的颜色。我们的方法通过修改扩散模型的采样过程来解决此问题，以在生成的图像的颜色分布和参考调色板之间结合可区分的切片1-wassestein距离。我们的方法的表现优于颜色相似性的颜色条件生成的最先进技术，产生的图像不仅与参考颜色相匹配，而且还与原始文本提示符保持语义连贯性。我们的源代码可在https://github.com/alobashev/sw-guidance/上获得。]]></description>
      <guid>https://arxiv.org/abs/2503.19034</guid>
      <pubDate>Wed, 26 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>调制流量</title>
      <link>https://arxiv.org/abs/2503.19062</link>
      <description><![CDATA[ARXIV：2503.19062V1公告类型：新 
摘要：在这项工作中，我们引入了调制流（Modflows），这是一种基于整流流的图像之间色彩传递的新方法。色彩传递的主要目标是调整目标图像的颜色，以匹配参考图像的颜色分布。我们的技术基于最佳运输，并执行色彩传输作为RGB色彩空间内的可逆变换。 MODFLOWS利用了流的射击属性，使我们能够引入一个常见的中间色彩分布并构建一个整流流的数据集。我们在此数据集上训练编码器，以预测新图像的整流模型的权重。在一系列最佳运输计划进行培训之后，我们的方法可以为新的发行对生成计划，而无需进行其他微调。我们还表明，受过训练的编码器提供了仅与其颜色样式相关联的图像嵌入。提出的方法能够处理4K图像并在内容和样式相似性方面实现最新性能。我们的源代码可从https://github.com/maria-larchenko/modflows获得]]></description>
      <guid>https://arxiv.org/abs/2503.19062</guid>
      <pubDate>Wed, 26 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Wikiautogen：迈向多模式Wikipedia风格的文章一代</title>
      <link>https://arxiv.org/abs/2503.19065</link>
      <description><![CDATA[ARXIV：2503.19065V1公告类型：新 
摘要：知识发现和收集是智力密集型任务，传统上需要大量的人类努力来确保高质量的产出。最近的研究探索了通过从Internet检索和合成信息来自动化Wikipedia风格文章生成的多代理框架。但是，这些方法主要集中于仅文本生成，忽略了多模式内容在增强信息性和参与度中的重要性。在这项工作中，我们介绍了Wikiautogen，这是一种新型自动化多模式Wikipedia风格的文章一代的系统。与先前的方法不同，Wikiautogen检索并将相关图像与文本一起集成在一起，从而丰富了生成的内容的深度和视觉吸引力。为了进一步提高事实的准确性和全面性，我们提出了一种多人自我反思机制，该机制从不同的角度评估了内容，以提高可靠性，宽度和相干性等。此外，我们介绍了Wikiseek，我们介绍了构成Wwikipedia文章的基准和图像构造的基于五个挑战的基准，该基准是一种基于互补的构图。实验结果表明，Wikiautogen在我们的Wikiseek基准上的表现优于先前的方法，而Wikiautogen的表现更准确，相干和视觉上富集的Wikipedia风格的文章。我们在https://wikiautogen.github.io/中显示了一些生成的例子。]]></description>
      <guid>https://arxiv.org/abs/2503.19065</guid>
      <pubDate>Wed, 26 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过重新排序来聚类数据</title>
      <link>https://arxiv.org/abs/2503.19067</link>
      <description><![CDATA[ARXIV：2503.19067V1公告类型：新 
摘要：将元素分别分别分别分别分析它们是许多科学领域的标准分析程序。我们在此提出了一种新算法，基于一个简单的想法，即家庭成员看起来彼此相似，并且不像家庭陌生的元素。根据元素之间的距离重新排序数据后，分析将使用易于理解的参数自动执行。明确考虑噪声以处理数据驱动世界的各种问题。我们将算法应用于对生物分子构象，基因序列，细胞，图像和实验条件进行排序。]]></description>
      <guid>https://arxiv.org/abs/2503.19067</guid>
      <pubDate>Wed, 26 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>不确定性意识分解的混合网络</title>
      <link>https://arxiv.org/abs/2503.19096</link>
      <description><![CDATA[ARXIV：2503.19096V1公告类型：新 
摘要：图像识别算法的鲁棒性仍然是一个关键的挑战，因为当前模型通常取决于大量标记的数据。在本文中，我们提出了一种混合方法，将神经网络的适应性与域特异性准算子的可解释性，透明度和鲁棒性相结合。我们的方法将识别分解为多个特定于任务的操作员，这些操作员专注于不同的特征，并得到了针对这些操作员的新型置信度测量的支持。该测量使网络能够优先考虑可靠的功能和噪声的帐户。我们认为我们的设计提高了透明度和鲁棒性，从而提高了性能，尤其是在低数据制度中。交通符号检测的实验结果突出了提出的方法的有效性，尤其是在半监督和无监督的场景中，强调了其对数据约束应用的潜力。]]></description>
      <guid>https://arxiv.org/abs/2503.19096</guid>
      <pubDate>Wed, 26 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用计算机视觉检测的异常检测：班级区别和性能指标的比较分析</title>
      <link>https://arxiv.org/abs/2503.19100</link>
      <description><![CDATA[ARXIV：2503.19100V1公告类型：新 
摘要：本文使用计算机视觉展示了一项有关异常检测的实验研究。该研究重点是班级的区别和绩效评估，将OpenCV与深度学习技术相结合，同时采用基于张量的卷积神经网络进行实时面部识别和分类。该系统有效地区分了三个类别：授权人员（管理员），入侵者和非人类实体。基于MobileNETV2的深度学习模型可用于优化实时性能，从而确保高计算效率而不会损害准确性。广泛的数据集预处理，包括图像增强和归一化，增强了模型的概括功能。我们的分析表明，管理员的分类精度为90.20％，入侵者为98.60％，非人类检测的分类精度为75.80％，同时保持平均处理速率为每秒30帧。该研究利用转移学习，批处理和ADAM优化来实现稳定且稳健的学习，以及对阶级差异策略的比较分析突出了特征提取技术和培训方法的影响。结果表明，高级特征选择和数据增强显着提高了检测性能，尤其是在将人与非人类场景区分开来。作为一项实验研究，这项研究为优化基于深度学习的监视系统提供了重要的见解，以用于高安全性环境，并提高实时异常检测的准确性和效率。]]></description>
      <guid>https://arxiv.org/abs/2503.19100</guid>
      <pubDate>Wed, 26 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>您的VIT是秘密的图像分割模型</title>
      <link>https://arxiv.org/abs/2503.19108</link>
      <description><![CDATA[ARXIV：2503.19108V1公告类型：新 
摘要：视觉变形金刚（VIT）在各种计算机视觉任务中显示出了出色的性能和可扩展性。为了将单尺度VIT应用于图像分割，现有方法采用卷积适配器来生成多尺度功能，像素解码器融合这些功能，以及使用融合功能进行预测的变压器解码器。在本文中，我们表明，鉴于足够大的模型和广泛的预训练，VIT本身可以学到这些特定于任务组件引入的电感偏差。基于这些发现，我们介绍了仅编码的掩码变压器（EOMT），该变压器（EOMT）重新利用了普通的VIT体系结构以进行图像分割。借助大型模型和预训练，EOMT获得了类似于使用特定于任务组件的最新模型的分割精度。同时，EOMT的速度明显快于这些方法，因为它的架构简单性，例如使用VIT-L的速度快4倍。在一系列模型尺寸中，EOMT展示了分割精度和预测速度之间的最佳平衡，这表明计算资源更好地用于缩放VIT本身而不是添加体系结构的复杂性。代码：https：//www.tue-mps.org/eomt/。]]></description>
      <guid>https://arxiv.org/abs/2503.19108</guid>
      <pubDate>Wed, 26 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于无训练的开放式摄影属性检测的组成缓存</title>
      <link>https://arxiv.org/abs/2503.19145</link>
      <description><![CDATA[ARXIV：2503.19145V1公告类型：新 
摘要：属性检测对于许多计算机视觉任务至关重要，因为它使系统能够描述诸如颜色，纹理和材料之类的属性。当前的方法通常依赖于固有限制的劳动密集型注释过程：可以在任意细节级别（例如，颜色与颜色阴影）中描述对象，当未仔细指导注释者时会导致歧义。此外，它们在预定义的一组属性中运行，从而降低了不可预见的下游应用程序的可伸缩性和适应性。我们提出了组成缓存（COMCA），这是一种无训练的开放式vocabulary属性检测方法，它克服了这些约束。 COMCA仅需要目标属性和对象的列表作为输入，通过利用Web尺度数据库和大型语言模型来确定属性 - 对象的兼容性来填充图像的辅助缓存。为了说明属性的组成性质，缓存图像会收到软属性标签。这些基于输入图像和缓存图像之间的相似性在推理时间进行了聚合，从而完善了潜在的视觉模型（VLMS）的预测。重要的是，我们的方法是模型不合时宜的，与各种VLM兼容。公共数据集上的实验表明，COMCA明显优于零射击和基于缓存的基准，与最近的基于培训的方法竞争，证明了经过精心设计的无培训方法可以成功地解决开放式录音率检测。]]></description>
      <guid>https://arxiv.org/abs/2503.19145</guid>
      <pubDate>Wed, 26 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>