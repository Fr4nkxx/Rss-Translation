<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Thu, 25 Jul 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>使用颜色监督扩散模型实现 SAR 到光学图像的转换</title>
      <link>https://arxiv.org/abs/2407.16921</link>
      <description><![CDATA[arXiv:2407.16921v1 公告类型：新
摘要：合成孔径雷达（SAR）具有全天候、高分辨率成像能力，但其复杂的成像机制往往给解释带来挑战。针对这些限制，本文介绍了一种创新的生成模型，旨在将 SAR 图像转换为更易理解的光学图像，从而增强 SAR 图像的可解释性。具体来说，我们的模型主干基于最近的扩散模型，具有强大的生成能力。我们在采样过程中使用 SAR 图像作为条件指导，并集成颜色监督以有效抵消色偏问题。我们在 SEN12 数据集上进行了实验，并使用峰值信噪比、结构相似性和 fr\&#39;echet 起始距离进行了定量评估。结果表明，我们的模型不仅在定量评估方面超越了以前的方法，而且显着提高了生成图像的视觉质量。]]></description>
      <guid>https://arxiv.org/abs/2407.16921</guid>
      <pubDate>Fri, 26 Jul 2024 03:14:40 GMT</pubDate>
    </item>
    <item>
      <title>McGAN：通过将制造规则嵌入条件生成对抗网络来生成可制造的设计</title>
      <link>https://arxiv.org/abs/2407.16943</link>
      <description><![CDATA[arXiv:2407.16943v1 公告类型：新
摘要：生成设计 (GD) 方法旨在自动生成满足功能或美学设计要求的各种设计。然而，迄今为止的研究通常缺乏对生成设计的可制造性的考虑。为此，我们提出了一种新颖的 GD 方法，该方法使用深度神经网络对面向制造设计 (DFM) 规则进行编码，从而修改零件设计以使其可通过给定的制造工艺进行制造。具体而言，提出了一种三步方法：首先，使用实例分割方法 Mask R-CNN 将零件设计分解为子区域。其次，条件生成对抗神经网络 (cGAN) Pix2Pix 将不可制造的分解子区域转换为可制造的子区域。随后将设计的转换子区域重新整合为统一的可制造设计。这三个步骤，即 Mask-RCNN、Pix2Pix 和重新集成，构成了所提出的可制造条件 GAN (McGAN) 框架的基础。实验结果表明，McGAN 可以转换现有的不可制造设计，以自动生成相应的可制造对应物，以高效且稳健的方式实现指定的制造规则。通过注塑工艺的二维设计案例研究证明了 McGAN 的有效性。]]></description>
      <guid>https://arxiv.org/abs/2407.16943</guid>
      <pubDate>Fri, 26 Jul 2024 03:14:40 GMT</pubDate>
    </item>
    <item>
      <title>SINDER：修复 DINOv2 的奇异缺陷</title>
      <link>https://arxiv.org/abs/2407.16826</link>
      <description><![CDATA[arXiv:2407.16826v1 公告类型：新
摘要：在大规模数据集上训练的 Vision Transformer 模型虽然有效，但通常会在它们提取的补丁标记中表现出伪影。虽然可以通过使用额外的分类标记重新训练整个模型来缓解此类缺陷，但这些标记存在的根本原因仍不清楚。在本文中，我们对这一现象进行了彻底的研究，结合理论分析和经验观察。我们的研究结果表明，这些伪影源自预训练网络本身，具体源于网络权重的前导左奇异向量。此外，为了减轻这些缺陷，我们提出了一种新颖的微调平滑正则化，仅使用小数据集即可纠正结构缺陷，从而避免完全重新训练的需要。我们在各种下游任务上验证了我们的方法，包括无监督分割、分类、监督分割和深度估计，证明了其在提高模型性能方面的有效性。代码和检查点可在https://github.com/haoqiwang/sinder上找到。]]></description>
      <guid>https://arxiv.org/abs/2407.16826</guid>
      <pubDate>Fri, 26 Jul 2024 03:14:39 GMT</pubDate>
    </item>
    <item>
      <title>用于天气状况分类和灾害预测的多层次分级框架</title>
      <link>https://arxiv.org/abs/2407.16834</link>
      <description><![CDATA[arXiv:2407.16834v1 公告类型：新
摘要：本文提出了一种用于天气状况分类和灾害预测的多层次分层框架。近年来，数据的重要性显著增加，文本、数字、图像、音频和视频等各种类型都发挥着关键作用。其中，图像占可用数据的很大一部分。该应用在各种用途上都显示出良好的前景，尤其是与交通管理、造林和天气预报的决策支持系统相结合时。它在传统天气预报不太准确的情况下特别有用，例如确保自动驾驶汽车在危险天气下安全运行。虽然以前的研究用较少的类别来研究这个主题，但本文重点关注 11 种特定类型的天气图像。目标是创建一个模型，该模型可以在对大量图像进行训练后准确预测天气状况。准确性在现实生活中对于防止事故至关重要，因此它是本文的首要任务。这项工作为未来天气预报的应用奠定了基础，特别是在人类专业知识缺乏或可能存在偏见的情况下。该框架能够将图像分为 11 种天气类别：露水、霜冻、冰雹、雾凇、雪、冰雹、雨、闪电、彩虹和沙尘暴，提供实时天气信息，准确率为 0.9329。所提出的框架满足了对准确天气分类和灾害预测日益增长的需求，为该领域的各种应用提供了强大的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2407.16834</guid>
      <pubDate>Fri, 26 Jul 2024 03:14:39 GMT</pubDate>
    </item>
    <item>
      <title>CompBench：多模态法学硕士 (LLM) 的比较推理基准</title>
      <link>https://arxiv.org/abs/2407.16837</link>
      <description><![CDATA[arXiv:2407.16837v1 公告类型：新
摘要：比较物体、场景或情况的能力对于日常生活中有效的决策和解决问题至关重要。例如，比较苹果的新鲜度可以帮助我们在购物时做出更好的选择，而比较沙发设计有助于优化我们生活空间的美感。尽管比较能力很重要，但在人工智能 (AGI) 中，这种能力在很大程度上尚未得到探索。在本文中，我们介绍了 CompBench，这是一个旨在评估多模态大型语言模型 (MLLM) 比较推理能力的基准。CompBench 通过面向视觉的问题挖掘和配对图像，涵盖八个相对比较维度：视觉属性、存在、状态、情感、时间性、空间性、数量和质量。我们使用来自不同视觉数据集的元数据和 CLIP 相似度分数整理了大约 40K 个图像对的集合。这些图像对涵盖了广泛的视觉领域，包括动物、时尚、运动以及室外和室内场景。这些问题经过精心设计，可以辨别两幅图像之间的相对特征，并由人工注释者标注以确保准确性和相关性。我们使用 CompBench 评估最近的 MLLM，包括 GPT-4V(ision)、Gemini-Pro 和 LLaVA-1.6。我们的结果揭示了它们在比较能力方面的明显缺陷。我们相信 CompBench 不仅揭示了这些局限性，还为未来增强 MLLM 的比较能力奠定了坚实的基础。]]></description>
      <guid>https://arxiv.org/abs/2407.16837</guid>
      <pubDate>Fri, 26 Jul 2024 03:14:39 GMT</pubDate>
    </item>
    <item>
      <title>PathwayBench：评估从多城市图像推断出的行人通道网络的可路由性</title>
      <link>https://arxiv.org/abs/2407.16875</link>
      <description><![CDATA[arXiv:2407.16875v1 公告类型：新
摘要：支持城市地区行人流动的应用需要完整且可路由的建筑环境图形表示。包括航空图像在内的全球可用信息为构建这些路径网络提供了可扩展的来源，但相关的学习问题具有挑战性：相对于道路网络路径，行人网络路径更窄、更频繁断开、在较小的区域内通常在视觉和物质上发生变化，并且它们的边界被车道入侵、小巷、有标记或无标记的道路交叉口打破。现有的提取行人通道网络图的算法评估不一致，并且往往忽略可路由性，因此很难评估移动应用的实用性：即使所有路径段都可用，不连续性也可能极大地任意改变行人所走的整体路径。在本文中，我们描述了行人通道图提取问题的第一个标准基准，包括配备手动审查的地面实况注释的最大可用数据集（覆盖 8 个城市各区域的 3,000 平方公里土地面积），以及以可路由性和下游效用为中心的一系列评估指标。通过将数据按单个交叉点的规模划分为多边形，我们计算局部可路由性作为全局可路由性的有效代理。我们考虑了多边形级可路由性的多个度量，并将预测度量与地面实况进行比较以构建评估指标。使用这些指标，我们表明该基准可以揭示现有方法的优势和劣势，这些优势和劣势被先前工作中使用的单区域数据集上的简单边数指标所隐藏，代表了计算机视觉和机器学习中一个具有挑战性、影响深远的问题。]]></description>
      <guid>https://arxiv.org/abs/2407.16875</guid>
      <pubDate>Fri, 26 Jul 2024 03:14:39 GMT</pubDate>
    </item>
    <item>
      <title>范围视图 3D 物体检测中最重要的事</title>
      <link>https://arxiv.org/abs/2407.16789</link>
      <description><![CDATA[arXiv:2407.16789v1 公告类型：新
摘要：基于激光雷达的感知管道依靠 3D 物体检测模型来解释复杂场景。虽然激光雷达存在多种表示形式，但距离视图很诱人，因为它无损编码了整个激光雷达传感器输出。在这项工作中，我们在不使用过去距离视图文献中提出的多种技术的情况下，实现了距离视图 3D 物体检测模型中最先进的技术。我们在两个具有显著不同属性的现代数据集上探索距离视图 3D 物体检测：Argoverse 2 和 Waymo Open。​​我们的调查揭示了关键见解：（1）输入特征维数显着影响整体性能，（2）令人惊讶的是，与更复杂的基于 IoU 的损失相比，采用基于 3D 空间接近度的分类损失效果一样好甚至更好，（3）通过简单的范围子采样技术解决非均匀激光雷达密度优于现有的多分辨率、距离条件网络。我们的实验表明，无需使用近期范围视图文献中提出的技术即可实现最佳性能。结合上述发现，我们建立了一种新的最佳范围视图 3D 物体检测模型 - 在 Waymo Open 数据集上将 AP 提高 2.2%，同时保持 10 Hz 的运行时间。我们在 Argoverse 2 数据集上建立了第一个范围视图模型，其性能优于基于体素的强大基线。所有模型都是多类且开源的。代码可在 https://github.com/benjaminrwilson/range-view-3d-detection 上找到。]]></description>
      <guid>https://arxiv.org/abs/2407.16789</guid>
      <pubDate>Fri, 26 Jul 2024 03:14:38 GMT</pubDate>
    </item>
    <item>
      <title>从带有噪声标签的长尾数据中进行分布感知稳健学习</title>
      <link>https://arxiv.org/abs/2407.16802</link>
      <description><![CDATA[arXiv:2407.16802v1 公告类型：新
摘要：深度神经网络利用大型、注释良好的数据集在各个领域取得了显着进步。然而，现实世界的数据通常表现出长尾分布和标签噪声，从而显著降低泛化性能。最近针对这些问题的研究集中在噪声样本选择方法上，该方法基于每个目标类中的高置信度样本来估计每个类的质心。这些方法的性能有限，因为它们仅使用每个类中的训练样本进行类质心估计，使得质心的质量容易受到长尾分布和噪声标签的影响。在本研究中，我们提出了一个强大的训练框架，称为分布感知样本选择和对比学习 (DaSC)。具体而言，DaSC 引入了分布感知类质心估计 (DaCC) 来生成增强的类质心。DaCC 对所有样本的特征进行加权平均，权重根据模型预测确定。此外，我们提出了一种置信度感知对比学习策略，以获得平衡且稳健的表示。训练样本分为高置信度样本和低置信度样本。然后，我们的方法使用高置信度样本应用半监督平衡对比损失 (SBCL)，利用可靠的标签信息来减轻类别偏差。对于低置信度样本，我们的方法计算 Mixup 增强实例判别损失 (MIDL)，以自监督的方式改进其表示。我们在 CIFAR 和现实世界噪声标签数据集上的实验结果表明，与以前的方法相比，所提出的 DaSC 具有更优异的性能。]]></description>
      <guid>https://arxiv.org/abs/2407.16802</guid>
      <pubDate>Fri, 26 Jul 2024 03:14:38 GMT</pubDate>
    </item>
    <item>
      <title>零样本人体动作识别的融合与跨模态迁移</title>
      <link>https://arxiv.org/abs/2407.16803</link>
      <description><![CDATA[arXiv:2407.16803v1 公告类型：新
摘要：尽管生活在一个多感官的世界里，但大多数人工智能模型仅限于对人类运动和行为的文本和视觉解释。惯性测量单元 (IMU) 提供了一个显著的信号来理解人体运动；然而，由于它们的不可解释性和数据稀缺性，它们很难使用。我们研究了一种在视觉和惯性模态之间传递知识的方法，该方法使用为人类动作识别 (HAR) 设计的信息联合表示空间的结构。我们将得到的融合和跨模态转移 (FACT) 方法应用于一种新颖的设置，其中模型在训练期间无法访问标记的 IMU 数据，并且能够在测试期间仅使用 IMU 数据执行 HAR。在广泛的 RGB-IMU 数据集上进行的大量实验表明，FACT 在零样本跨模态转移方面明显优于现有方法。]]></description>
      <guid>https://arxiv.org/abs/2407.16803</guid>
      <pubDate>Fri, 26 Jul 2024 03:14:38 GMT</pubDate>
    </item>
    <item>
      <title>使用临床知识图谱和数据驱动量化的 AI 增强型黑色素瘤检测 7 点清单</title>
      <link>https://arxiv.org/abs/2407.16822</link>
      <description><![CDATA[arXiv:2407.16822v1 公告类型：新
摘要：7 点检查表 (7PCL) 广泛应用于皮肤镜检查，以识别需要紧急医疗救治的恶性黑色素瘤病变。它为七个属性分配分值：主要属性每个值 2 分，次要属性每个值 1 分。总分 3 分或更高时，需要进一步评估，通常包括活检。然而，当前方法的一个重大限制是对属性的统一权重，这会导致不精确并忽略它们的相互联系。先前的深度学习研究将每个属性的预测视为与预测黑色素瘤同等重要，这未能认识到属性对黑色素瘤的临床意义。为了解决这些限制，我们引入了一种集成两个创新元素的新型诊断方法：基于临床知识的拓扑图 (CKTG) 和具有数据驱动加权标准的梯度诊断策略 (GD-DDW)。 CKTG 将 7PCL 属性与诊断信息相结合，揭示了内部和外部关联。通过采用自适应接受域和加权边缘，我们建立了黑色素瘤相关特征之间的联系。同时，GD-DDW 模拟了皮肤科医生的诊断过程，他们首先观察与黑色素瘤相关的视觉特征，然后做出预测。我们的模型对同一病变使用两种成像方式，确保全面获取特征。我们的方法在预测恶性黑色素瘤及其特征方面表现出色，平均 AUC 值达到 85%。这在 EDRA 数据集上得到了验证，EDRA 数据集是 7 点检查表算法最大的公开数据集。具体而言，集成加权系统可以为临床医生提供有价值的数据驱动基准以供评估。]]></description>
      <guid>https://arxiv.org/abs/2407.16822</guid>
      <pubDate>Fri, 26 Jul 2024 03:14:38 GMT</pubDate>
    </item>
    <item>
      <title>跨监督、无监督和半监督学习范式的动物动作分割算法研究</title>
      <link>https://arxiv.org/abs/2407.16727</link>
      <description><![CDATA[arXiv:2407.16727v1 公告类型：新
摘要：行为视频的动作分割是将每个帧标记为属于一个或多个离散类别的过程，是许多研究动物行为的研究的重要组成部分。存在多种算法来自动解析离散动物行为，包括监督、无监督和半监督学习范式。这些算法（包括基于树的模型、深度神经网络和图形模型）在结构和数据假设上存在很大差异。使用四个涵盖多个物种（苍蝇、老鼠和人类）的数据集，我们系统地研究了这些不同算法的输出如何与手动注释的感兴趣行为保持一致。在此过程中，我们引入了一个半监督动作分割模型，该模型弥合了监督深度神经网络和无监督图形模型之间的差距。我们发现，在观察中添加时间信息的完全监督时间卷积网络在所有数据集的监督指标上表现最佳。]]></description>
      <guid>https://arxiv.org/abs/2407.16727</guid>
      <pubDate>Fri, 26 Jul 2024 03:14:37 GMT</pubDate>
    </item>
    <item>
      <title>VisMin：视觉最小变化理解</title>
      <link>https://arxiv.org/abs/2407.16772</link>
      <description><![CDATA[arXiv:2407.16772v1 公告类型：新
摘要：对对象、属性和对象间关系的细粒度理解对于视觉语言模型 (VLM) 至关重要。现有基准主要侧重于评估 VLM 在给定图像的情况下区分两个非常相似的 \textit{captions} 的能力。在本文中，我们引入了一个新的、具有挑战性的基准，称为 \textbf{Vis}ual \textbf{Min}imal-Change Understanding (VisMin)，它要求模型在给定两幅图像和两个标题的情况下预测正确的图像-标题匹配。图像对和标题对包含最小变化，即，每次只有一个方面发生变化，这些方面包括：\textit{对象}、\textit{属性}、\textit{计数} 和 \textit{空间关系}。这些变化测试模型对对象、属性（例如颜色、材质、形状）、计数和对象间空间关系的理解。我们使用大型语言模型和扩散模型构建了一个自动化框架，然后由人工注释者进行严格的 4 步验证过程。实证实验表明，当前的 VLM 在理解空间关系和计数能力方面表现出明显不足。我们还生成了一个大规模训练数据集来微调 CLIP 和 Idefics2，结果显示跨基准的细粒度理解和 CLIP 的一般图像文本对齐方面有显著改进。我们在 \url{https://vismin.net/} 发布所有资源，包括基准、训练数据和微调模型检查点。]]></description>
      <guid>https://arxiv.org/abs/2407.16772</guid>
      <pubDate>Fri, 26 Jul 2024 03:14:37 GMT</pubDate>
    </item>
    <item>
      <title>用于盲人和视力低下人士导航的关键物体识别的数据集</title>
      <link>https://arxiv.org/abs/2407.16777</link>
      <description><![CDATA[arXiv:2407.16777v1 公告类型：新
摘要：本文介绍了一个用于改进实时物体识别系统的数据集，以帮助盲人和低视力 (BLV) 人士完成导航任务。该数据集包括 21 个 BLV 人士在户外空间导航的视频，以及通过焦点小组研究完善的 90 个对 BLV 导航至关重要的物体的分类法。我们还为由 21 个视频创建的 31 个视频片段中的 90 个物体提供了物体标签。更深入的分析表明，用于训练计算机视觉模型的大多数当代数据集仅包含我们数据集中分类法的一小部分。对我们的数据集上最先进的计算机视觉模型的初步评估突出了准确检测与 BLV 导航相关的关键物体的缺点，强调了对专门数据集的需求。我们公开我们的数据集，为开发更具包容性的 BLV 个人导航系统提供宝贵的资源。]]></description>
      <guid>https://arxiv.org/abs/2407.16777</guid>
      <pubDate>Fri, 26 Jul 2024 03:14:37 GMT</pubDate>
    </item>
    <item>
      <title>用于异常行为检测的遮挡感知 3D 运动解释</title>
      <link>https://arxiv.org/abs/2407.16788</link>
      <description><![CDATA[arXiv:2407.16788v1 公告类型：新
摘要：基于 3D 姿势估计异常姿势在人体姿势分析中至关重要，但它也带来了挑战，尤其是在从有遮挡的单目数据集重建 3D 人体姿势时。准确的重建可以恢复 3D 运动，有助于提取分析异常行为所需的语义细节。然而，大多数现有方法依赖于预定义的关键点作为估计遮挡关节坐标的基础，而数据质量的变化对这些模型的性能产生了不利影响。在本文中，我们提出了 OAD2D，它基于从单目视频重建网格顶点和人体关节的 3D 坐标来区分运动异常。OAD2D 使用光流来捕获视频流中的运动先验信息，丰富了有关遮挡人体运动的信息并确保姿势的时空对齐。此外，我们通过将异常姿势估计与运动到文本 (M2T) 模型相结合来重新制定异常姿势估计，其中采用 VQVAE 来量化运动特征。这种方法将运动标记映射到文本标记，从而可以对运动进行语义上可解释的分析，并通过语言模型增强异常姿势检测的泛化。我们的方法证明了异常行为检测对严重和自我遮挡的鲁棒性，因为它在全局坐标中重建人体运动轨迹以有效缓解遮挡问题。我们的方法使用 Human3.6M、3DPW 和 NTU RGB+D 数据集进行了验证，在 NTU RGB+D 数据集上实现了 0.94 的高 $F_1-$Score，用于医疗状况检测。我们将发布所有代码和数据。]]></description>
      <guid>https://arxiv.org/abs/2407.16788</guid>
      <pubDate>Fri, 26 Jul 2024 03:14:37 GMT</pubDate>
    </item>
    <item>
      <title>通过分层上下文描述进行类别可扩展的分布外检测</title>
      <link>https://arxiv.org/abs/2407.16725</link>
      <description><![CDATA[arXiv:2407.16725v1 公告类型：新
摘要：OOD 检测的关键有两个方面：广义的特征表示和精确的类别描述。最近，CLIP 等视觉语言模型在这两个问题上都取得了重大进展，但由于缺乏未见类别，构建精确的类别描述仍处于起步阶段。这项工作引入了两个分层上下文，即感知上下文和虚假上下文，通过自动提示调整来仔细描述精确的类别边界。具体而言，感知上下文感知当前分类任务的类别间差异（例如，猫与苹果），而虚假上下文进一步识别每个类别的虚假（相似但完全不同）OOD 样本（例如，猫与豹，苹果与桃子）。这两个上下文分层构建了对某一类别的精确描述，即首先将样本粗略地归类到预测的类别中，然后精细地识别它是真正的 ID 样本还是真正的 OOD。此外，视觉语言框架中对这些类别的精确描述提出了一种新颖的应用：CATegory-EXtensible OOD 检测 (CATEX)。只需合并在不同子任务设置下学习到的分层上下文，就可以有效地扩展可识别类别集。并且进行了大量的实验来证明 CATEX 的有效性、鲁棒性和类别可扩展性。例如，CATEX 在具有挑战性的 ImageNet-1K 数据集上通过几种协议持续以大幅度领先于竞争对手。此外，我们还提供了有关如何有效地扩展视觉语言模型中的即时工程以识别数千个对象类别以及如何结合大型语言模型（如 GPT-3）来增强零样本应用的新见解。代码将很快公开。]]></description>
      <guid>https://arxiv.org/abs/2407.16725</guid>
      <pubDate>Fri, 26 Jul 2024 03:14:36 GMT</pubDate>
    </item>
    </channel>
</rss>