<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Thu, 18 Jul 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>深入研究使用图像分类对自监督预训练进行基准测试</title>
      <link>https://arxiv.org/abs/2407.12210</link>
      <description><![CDATA[arXiv:2407.12210v2 公告类型：新
摘要：自监督学习 (SSL) 是一种机器学习方法，其中数据本身提供监督，无需外部标签。通过解决借口任务，模型被迫学习数据结构或上下文。使用 SSL，模型可以从丰富且廉价的未标记数据中学习，从而显着降低在标签昂贵或无法访问的情况下训练模型的成本。在计算机视觉中，SSL 被广泛用作预训练，然后是下游任务，例如监督迁移、较小标记数据集上的少量学习和/或无监督聚类。不幸的是，在所有可能的下游任务上评估 SSL 方法并客观衡量学习表示的质量是不可行的。相反，使用域内评估协议来评估 SSL 方法，例如微调、线性探测和 k-最近邻 (kNN)。然而，人们尚不清楚这些评估协议如何在不同条件下（例如数据集、度量和模型架构）评估预训练模型对不同下游任务的表示质量。我们研究了基于分类的 SSL 评估协议如何关联，以及它们如何预测不同数据集类型上的下游性能。我们的研究包括 11 个常见的图像数据集和 26 个模型，这些模型使用不同的 SSL 方法进行预训练或具有不同的模型主干。我们发现，域内线性/kNN 探测协议平均而言是域外性能的最佳一般预测因子。我们进一步研究了批量标准化的重要性，并评估了不同类型的数据集域转移的稳健相关性。我们对判别式和生成式自监督方法之间关系的假设提出了质疑，发现它们的大多数性能差异可以通过模型主干的变化来解释。]]></description>
      <guid>https://arxiv.org/abs/2407.12210</guid>
      <pubDate>Fri, 19 Jul 2024 02:23:01 GMT</pubDate>
    </item>
    <item>
      <title>VideoClusterNet：视频的自监督和自适应聚类</title>
      <link>https://arxiv.org/abs/2407.12214</link>
      <description><![CDATA[arXiv:2407.12214v1 公告类型：新
摘要：随着数字媒体内容制作的兴起，分析电影和电视剧集以准确定位主要角色的需求变得越来越重要。具体来说，视频人脸聚类旨在将检测到的视频人脸轨迹与共同的面部身份组合在一起。这个问题非常具有挑战性，因为给定面部在视频帧中的姿势、表情、外观和光照变化范围很大。通用的预训练人脸识别 (ID) 模型无法很好地适应视频制作领域，因为它具有高动态范围内容和独特的电影风格。此外，传统的聚类算法依赖于需要跨数据集单独调整的超参数。在本文中，我们提出了一种新颖的视频人脸聚类方法，该方法学习以完全自监督的方式将通用人脸 ID 模型适应新的视频人脸轨迹。我们还提出了一种无参数聚类算法，该算法能够自动适应任何输入视频的微调模型的嵌入空间。由于缺乏全面的电影人脸聚类基准，我们还推出了首创的电影数据集：MovieFaceCluster。我们的数据集由电影行业专业人士精心挑选，包含极具挑战性的面部识别场景。实验表明，我们的方法在我们的基准数据集上处理困难的主流电影场景时非常有效，在传统电视剧数据集上的表现也达到了最高水平。]]></description>
      <guid>https://arxiv.org/abs/2407.12214</guid>
      <pubDate>Fri, 19 Jul 2024 02:23:01 GMT</pubDate>
    </item>
    <item>
      <title>您只需要 Beta 采样：使用逐步谱分析的扩散模型高效图像生成策略</title>
      <link>https://arxiv.org/abs/2407.12173</link>
      <description><![CDATA[arXiv:2407.12173v1 公告类型：新
摘要：生成扩散模型已成为高质量图像合成的强大工具，但其迭代性质需要大量计算资源。本文提出了一种基于扩散过程图像光谱分析的有效时间步长采样方法，旨在优化去噪过程。我们引入了一种类似 Beta 分布的采样技术，而不是传统的基于均匀分布的时间步长采样，该技术优先考虑过程早期和后期的关键步骤。我们的假设是某些步骤在图像内容中表现出显着变化，而其他步骤的贡献最小。我们使用傅里叶变换来验证我们的方法，以测量每个步骤的频率响应变化，揭示早期的大量低频变化和后期的高频调整。使用 ADM 和稳定扩散的实验表明，我们的 Beta 采样方法始终优于均匀采样，获得更好的 FID 和 IS 分数，并且相对于 AutoDiffusion 等最先进的方法具有竞争力的效率。这项工作通过将计算资源集中在最具影响力的步骤上，为提高扩散模型效率提供了一个实用的框架，并具有进一步优化和更广泛应用的潜力。]]></description>
      <guid>https://arxiv.org/abs/2407.12173</guid>
      <pubDate>Fri, 19 Jul 2024 02:23:00 GMT</pubDate>
    </item>
    <item>
      <title>CroMo-Mixup：增强跨模型表征以实现持续自监督学习</title>
      <link>https://arxiv.org/abs/2407.12188</link>
      <description><![CDATA[arXiv:2407.12188v1 公告类型：新 
摘要：持续自监督学习（CSSL）在未标记数据上按顺序学习一系列任务。持续学习的两个主要挑战是灾难性遗忘和任务混淆。虽然已经研究了 CSSL 问题以解决灾难性遗忘挑战，但在解决任务混淆方面所做的工作很少。在这项工作中，我们通过大量实验表明，自监督学习（SSL）可以使 CSSL 更容易受到任务混淆问题的影响，特别是在类别增量学习的多样性较低的设置中，因为属于不同任务的不同类别不会同时进行训练。受这一挑战的启发，我们提出了一个新颖的跨模型特征混合（CroMo-Mixup）框架，通过两个关键组件解决这个问题：1）跨任务数据混合，它将样本混合到任务之间以增强负样本多样性； 2) 跨模型特征混合，学习从混合样本的当前和旧模型获得的嵌入与原始图像之间的相似性，促进跨任务类对比学习和旧知识检索。我们评估了 CroMo-Mixup 在不同的类增量学习设置下在三个数据集 CIFAR10、CIFAR100 和 tinyImageNet 上改进所有任务的任务 ID 预测和平均线性准确度的有效性。我们在四个最先进的 SSL 目标上验证了 CroMo-Mixup 的兼容性。代码可在 \url{https://github.com/ErumMushtaq/CroMo-Mixup} 获得。]]></description>
      <guid>https://arxiv.org/abs/2407.12188</guid>
      <pubDate>Fri, 19 Jul 2024 02:23:00 GMT</pubDate>
    </item>
    <item>
      <title>NeuSurfEmb：无需 CAD 模型的基于密集对应的 6D 物体姿态估计完整流程</title>
      <link>https://arxiv.org/abs/2407.12207</link>
      <description><![CDATA[arXiv:2407.12207v1 公告类型：新
摘要：最先进的 6D 物体姿势估计方法假设 CAD 模型可用，并要求用户手动设置基于物理的渲染 (PBR) 管道以生成合成训练数据。这两个因素都限制了这些方法在现实世界场景中的应用。在这项工作中，我们提出了一种不需要 CAD 模型的管道，允许训练最先进的姿势估计器，只需要一小组真实图像作为输入。我们的方法基于 NeuS2 对象表示，我们通过基于结构运动 (SfM) 和对象无关分割的半自动化程序进行学习。我们利用 NeuS2 的新颖视图合成能力和简单的剪切粘贴增强来自动生成逼真的物体渲染，我们用它来训练基于对应的 SurfEmb 姿势估计器。我们在 LINEMOD-Occlusion 数据集上评估了我们的方法，广泛研究了其各个组件的影响，并展示了与基于 CAD 模型和 PBR 数据的方法相比具有竞争力的性能。我们还展示了我们的管道在自收集的真实世界物体上的易用性和有效性，表明我们的方法优于最先进的无 CAD 模型方法，具有更好的准确性和对轻微遮挡的鲁棒性。为了让机器人社区受益于该系统，我们将在 https://www.github.com/ethz-asl/neusurfemb 上公开发布它。]]></description>
      <guid>https://arxiv.org/abs/2407.12207</guid>
      <pubDate>Fri, 19 Jul 2024 02:23:00 GMT</pubDate>
    </item>
    <item>
      <title>FoodMem：近实时且精确的食物视频分割</title>
      <link>https://arxiv.org/abs/2407.12121</link>
      <description><![CDATA[arXiv:2407.12121v1 公告类型：新
摘要：食品分割（包括视频中的食品分割）对于解决现实世界的健康、农业和食品生物技术问题至关重要。当前的局限性导致营养分析不准确、作物管理效率低下和食品加工不理想，影响粮食安全和公共卫生。改进分割技术可以增强饮食评估、农业生产力和食品生产过程。本研究介绍了一种强大的框架的开发，用于使用最少的硬件资源对视频中的食品进行高质量、近乎实时的分割和跟踪。我们提出了 FoodMem，这是一种新颖的框架，旨在从 360 度无界场景的视频序列中分割食物。FoodMem 可以在视频序列中持续生成食物部分的蒙版，克服了现有语义分割模型的局限性，例如视频处理环境中的闪烁和推理速度过快。为了解决这些问题，FoodMem 利用了一个两阶段解决方案：一个变换器分割阶段，用于创建初始分割蒙版；一个基于内存的跟踪阶段，用于监控复杂场景中的食物蒙版。我们的框架优于当前最先进的食物分割模型，在各种条件下（例如摄像机角度、照明、反射、场景复杂性和食物多样性）均表现出色。这可以减少分割噪声、消除伪影并完成缺失的片段。在这里，我们还引入了一个新的带注释的食物数据集，其中包含以前基准测试中不存在的具有挑战性的场景。在 Nutrition5k 和 Vegetables &amp; Fruits 数据集上进行的大量实验表明，FoodMem 在食物视频分割中将最先进的平均精度提高了 2.5%，平均速度提高了 58 倍。]]></description>
      <guid>https://arxiv.org/abs/2407.12121</guid>
      <pubDate>Fri, 19 Jul 2024 02:22:59 GMT</pubDate>
    </item>
    <item>
      <title>开放手术中关节手术器械的单目姿态估计</title>
      <link>https://arxiv.org/abs/2407.12138</link>
      <description><![CDATA[arXiv:2407.12138v1 公告类型：新
摘要：这项工作提出了一种用于开放手术中手术器械单目 6D 姿势估计的新方法，解决了诸如物体关节、对称性、遮挡和缺乏带注释的真实世界数据等挑战。该方法利用合成数据生成和领域自适应技术来克服这些障碍。所提出的方法包括三个主要部分：（1）使用带有关节装配和基于物理的渲染的手术器械 3D 建模生成合成数据；（2）结合物体检测与姿势估计和混合几何融合策略的定制姿势估计框架；（3）利用合成和真实未注释数据的训练策略，使用自动生成的伪标签对真实视频数据进行领域自适应。对开放手术视频进行的评估证明了所提出方法的良好性能和现实世界适用性，凸显了其集成到医学增强现实和机器人系统中的潜力。该方法无需对真实手术数据进行大量手动注释。]]></description>
      <guid>https://arxiv.org/abs/2407.12138</guid>
      <pubDate>Fri, 19 Jul 2024 02:22:59 GMT</pubDate>
    </item>
    <item>
      <title>通过基于偏好的强化学习实现主题驱动的文本到图像生成</title>
      <link>https://arxiv.org/abs/2407.12164</link>
      <description><![CDATA[arXiv:2407.12164v1 公告类型：新
摘要：文本到图像生成模型最近引起了相当大的兴趣，它能够从文本提示中合成高质量的图像。然而，这些模型通常缺乏从给定的参考图像生成特定主题或在不同条件下合成新颖再现的能力。DreamBooth 和主题驱动的文本到图像 (SuTI) 等方法在这一领域取得了重大进展。然而，这两种方法主要侧重于增强与参考图像的相似性，并且需要昂贵的设置，往往忽略了高效训练的需要和避免过度拟合参考图像。在这项工作中，我们提出了 $\lambda$-Harmonic 奖励函数，它提供了可靠的奖励信号并允许提前停止以加快训练速度和有效正则化。通过结合 Bradley-Terry 偏好模型，$\lambda$-Harmonic 奖励函数还为主题驱动的生成任务提供了偏好标签。我们提出了奖励偏好优化 (RPO)，它提供了更简单的设置（只需要 DreamBooth 使用的 $3\%$ 的负样本）和更少的梯度步骤进行微调。与大多数现有方法不同，我们的方法不需要训练文本编码器或优化文本嵌入，而是通过仅微调 U-Net 组件来实现文本-图像对齐。从经验上看，$\lambda$-Harmonic 被证明是一种可靠的方法，可用于主题驱动的生成任务中的模型选择。基于偏好标签和来自 $\lambda$-Harmonic 奖励函数的早期停止验证，我们的算法在 DreamBench 上获得了最先进的 CLIP-I 得分 0.833 和 CLIP-T 得分 0.314。]]></description>
      <guid>https://arxiv.org/abs/2407.12164</guid>
      <pubDate>Fri, 19 Jul 2024 02:22:59 GMT</pubDate>
    </item>
    <item>
      <title>一次性忘记个人身份</title>
      <link>https://arxiv.org/abs/2407.12069</link>
      <description><![CDATA[arXiv:2407.12069v1 公告类型：新
摘要：机器学习 (MU) 旨在从模型中删除数据，就好像在训练期间从未见过它们一样。就此而言，现有的机器学习方法假设可以完全或部分访问训练数据，但由于隐私法规，这种访问可能会随着时间的推移而受到限制。但是，没有设置或基准来探究 MU 方法在这种情况下的有效性，即当训练数据缺失时。为了填补这一空白，我们提出了一项新任务，我们称之为个人身份一次性学习 (O-UPI)，该任务在无法访问训练数据时评估机器学习模型。具体而言，我们专注于身份机器学习案例，这很重要，因为当前法规要求在训练后删除数据。为了应对数据缺失，我们希望用户提供一张肖像照片来执行机器学习。为了评估 O-UPI 中的方法，我们对具有不同机器学习集大小的 CelebA 和 CelebA-HQ 数据集上的遗忘进行了基准测试。我们在这个具有挑战性的基准上测试了适用的方法，并提出了一种有效的方法，即通过元学习从单个图像中忘记身份。我们的研究结果表明，现有方法在数据可用性有限时会遇到困难，当提供的样本与训练时使用的数据存在差异时，困难会更大。我们将在接受后发布代码和基准。]]></description>
      <guid>https://arxiv.org/abs/2407.12069</guid>
      <pubDate>Fri, 19 Jul 2024 02:22:58 GMT</pubDate>
    </item>
    <item>
      <title>关系表示蒸馏</title>
      <link>https://arxiv.org/abs/2407.12073</link>
      <description><![CDATA[arXiv:2407.12073v1 公告类型：新
摘要：知识蒸馏 (KD) 是一种将知识从大型、训练有素的教师模型转移到较小、更高效的学生模型的有效方法。尽管取得了成功，但 KD 的主要挑战之一是确保复杂知识的有效转移，同时保持学生的计算效率。与以前应用对比目标促进明确负面实例的工作不同，我们引入了关系表示蒸馏 (RRD)。我们的方法利用成对的相似性来探索和加强教师和学生模型之间的关系。受自我监督学习原理的启发，它使用了一种宽松的对比损失，专注于相似性而不是精确复制。该方法在大型内存缓冲区中对齐教师样本的输出分布，从而无需严格的负面实例区分即可提高学生模型的稳健性和性能。我们的方法在 CIFAR-100 上表现出色，优于传统的 KD 技术并超越了 13 种最先进的方法。它还成功转移到其他数据集，如 Tiny ImageNet 和 STL-10。代码将很快公开。]]></description>
      <guid>https://arxiv.org/abs/2407.12073</guid>
      <pubDate>Fri, 19 Jul 2024 02:22:58 GMT</pubDate>
    </item>
    <item>
      <title>集成查询感知分割和交叉注意以实现稳健的 VQA</title>
      <link>https://arxiv.org/abs/2407.12055</link>
      <description><![CDATA[arXiv:2407.12055v1 公告类型：新
摘要：本文介绍了一种使用 LVLM 进行 VizWiz-VQA 的方法，该方法具有可训练的交叉注意和 LoRA 微调。我们在以下条件下训练模型：1）使用原始图像进行训练。2）使用 CLIPSeg 进行增强图像训练以突出显示或对比原始图像。3）通过整合 Vision Transformer (ViT) 的输出特征和原始图像的 CLIPSeg 特征进行训练。然后，我们根据 Levenshtein 距离对结果进行集成，以增强对最终答案的预测。在实验中，我们展示并分析了所提出方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2407.12055</guid>
      <pubDate>Fri, 19 Jul 2024 02:22:57 GMT</pubDate>
    </item>
    <item>
      <title>在不受约束的视频中临时固定教学图</title>
      <link>https://arxiv.org/abs/2407.12066</link>
      <description><![CDATA[arXiv:2407.12066v1 公告类型：新
摘要：我们研究了同时定位视频中以教学图形式呈现的一系列查询这一具有挑战性的问题。这不仅需要了解单个查询，还需要了解它们的相互关系。然而，大多数现有方法都侧重于一次确定一个查询，而忽略了查询之间的固有结构，例如一般的互斥性和时间顺序。因此，不同步骤图的预测时间跨度可能会有很大的重叠或违反时间顺序，从而损害准确性。在本文中，我们通过同时确定一系列步骤图来解决此问题。具体而言，我们提出了复合查询，它通过详尽地配对步骤图的视觉内容特征和固定数量的可学习位置嵌入来构建。我们的见解是，携带不同内容特征的复合查询之间的自注意力相互抑制以减少预测中的时间跨度重叠，而交叉注意力通过内容和位置联合引导来纠正时间错位。我们证明了我们的方法在 IAW 数据集上对于基础步骤图的有效性以及在 YouCook2 基准上对于基础自然语言查询的有效性，在同时基础多个查询的同时明显优于现有方法。]]></description>
      <guid>https://arxiv.org/abs/2407.12066</guid>
      <pubDate>Fri, 19 Jul 2024 02:22:57 GMT</pubDate>
    </item>
    <item>
      <title>MaskVD：区域掩蔽，实现高效的视频对象检测</title>
      <link>https://arxiv.org/abs/2407.12067</link>
      <description><![CDATA[arXiv:2407.12067v1 公告类型：新
摘要：视频任务计算量大，因此在实时应用中部署时带来挑战，特别是对于需要最先进的 Vision Transformers (ViT) 的任务。一些研究工作试图通过利用视频的大部分在帧之间几乎没有变化这一事实来解决这一挑战，从而导致基于帧的视频处理中的冗余计算。特别是，一些工作利用了帧之间的像素或语义差异，然而，这会产生有限的延迟优势，同时显着增加内存开销。相比之下，本文提出了一种屏蔽视频帧中区域的策略，该策略利用图像中的语义信息和帧之间的时间相关性来显着减少 FLOP 和延迟，而性能与基线模型相比几乎没有损失。特别是，我们证明通过利用从前几帧中提取的特征，ViT 主干直接受益于区域屏蔽，跳过高达 80% 的输入区域，将 FLOP 和延迟提高 3.14 倍和 1.5 倍。与最先进的方法 (SOTA) 相比，我们将内存和延迟分别降低了 2.3 倍和 1.14 倍，同时保持了类似的检测性能。此外，我们的方法在卷积神经网络 (CNN) 上展示了良好的结果，并使用专门的计算内核将延迟时间缩短了 1.3 倍。]]></description>
      <guid>https://arxiv.org/abs/2407.12067</guid>
      <pubDate>Fri, 19 Jul 2024 02:22:57 GMT</pubDate>
    </item>
    <item>
      <title>使用多模态基础模型和聚类来改善风格模糊损失</title>
      <link>https://arxiv.org/abs/2407.12009</link>
      <description><![CDATA[arXiv:2407.12009v1 公告类型：新
摘要：教导文本到图像模型具有创造性需要使用风格模糊性损失，这需要预先训练的分类器。在这项工作中，我们探索了一种用于近似创造力的风格模糊性训练目标的新形式，它不需要训练分类器甚至不需要标记的数据集。然后，我们训练一个扩散模型来最大化风格模糊性，从而为扩散模型注入创造力，并发现我们的新方法改进了传统方法，基于人类判断的自动化指标，同时仍保持了创造力和新颖性。]]></description>
      <guid>https://arxiv.org/abs/2407.12009</guid>
      <pubDate>Fri, 19 Jul 2024 02:22:56 GMT</pubDate>
    </item>
    <item>
      <title>YOLOv10、YOLOv9 和 YOLOv8 在复杂果园环境下检测和计数小果实的综合性能评估</title>
      <link>https://arxiv.org/abs/2407.12040</link>
      <description><![CDATA[arXiv:2407.12040v2 公告类型：新
摘要：本研究对 YOLOv8、YOLOv9 和 YOLOv10 物体检测算法的所有配置在商业果园中检测果实（绿色果实）的性能进行了广泛的评估。此外，本研究使用 iPhone 和机器视觉传感器对 5 种不同的苹果品种（Scifresh、Scilate、Honeycrisp、Cosmic crisp 和 Golden delicious）进行了实地果实计数并验证了该算法。这项对总共 17 种不同配置（YOLOv8 为 5 种，YOLOv9 为 6 种，YOLOv10 为 6 种）的全面调查表明，YOLOv9 在 mAP@50 方面优于 YOLOv10 和 YOLOv8，而 YOLOv10x 在准确率和召回率方面优于所有测试的 17 种配置。具体来说，YOLOv9 Gelan-e 实现了最高的 mAP@50 0.935，优于 YOLOv10n 的 0.921 和 YOLOv8s 的 0.924。在精度方面，YOLOv10x 的精度最高，为 0.908，与其他测试配置相比，其物体识别精度更高（例如 YOLOv9 Gelan-c 的精度为 0.903，YOLOv8m 的精度为 0.897）。在召回率方面，YOLOv10s 的召回率是同类产品中最高的（0.872），而 YOLOv9 Gelan m 的召回率是 YOLOv9 配置中最好的（0.899），YOLOv8n 的召回率是 YOLOv8 配置中最好的（0.883）。同时，YOLOv10 的三种配置：YOLOv10b、YOLOv10l 和 YOLOv10x 的后处理速度都达到了 1.5 毫秒，超过了 YOLOv9 和 YOLOv8 系列中的所有其他配置。其中，YOLOv9 Gelan-e 的召回率达到了 1.9毫秒，而 YOLOv8m 则达到了 2.1 毫秒。此外，YOLOv8n 在所有测试配置中表现出最高的推理速度，实现了 4.1 毫秒的处理时间，而 YOLOv9 Gelan-t 和 YOLOv10n 也表现出相对较慢的推理速度，分别为 9.3 毫秒和 5.5 毫秒。]]></description>
      <guid>https://arxiv.org/abs/2407.12040</guid>
      <pubDate>Fri, 19 Jul 2024 02:22:56 GMT</pubDate>
    </item>
    </channel>
</rss>