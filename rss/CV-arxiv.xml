<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>CS.CV更新arxiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.cv更新arxiv.org e-print存档。</description>
    <lastBuildDate>Mon, 10 Mar 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>Llave：具有硬度加权对比度学习的大语言和视觉嵌入模型</title>
      <link>https://arxiv.org/abs/2503.04812</link>
      <description><![CDATA[ARXIV：2503.04812V1公告类型：新 
摘要：通用多模式嵌入模型在诸如交织的图像文本检索，多模式抹布和多模式聚类等任务中起着至关重要的作用。但是，我们的经验结果表明，经过标准Infonce损失训练的现有基于LMM的嵌入模型在正面和负面对之间的相似性分布中表现出高度重叠，这使得有效区分硬性负面对具有挑战性。为了解决这个问题，我们提出了一个简单而有效的框架，该框架可以根据其歧视性难度动态地改善嵌入模型的负面对象。在此框架内，我们训练一系列名为Llave的模型，并在MMEB基准测试中对其进行评估，该基准涵盖了4个元任务和36个数据集。实验结果表明，Llave建立了更强大的基准，可以实现最先进的（SOTA）性能，同时表现出强大的可扩展性和效率。具体而言，LLAVE-2B超过了先前的SOTA 7B型号，而LLAVE-7B则取得了6.2分的进一步提高。尽管Llave受到图像文本数据的培训，但它可以以零射击方式推广到文本视频检索任务并实现强大的性能，从而证明了其具有显着的转移到其他嵌入任务的潜力。]]></description>
      <guid>https://arxiv.org/abs/2503.04812</guid>
      <pubDate>Mon, 10 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>无形字符串：揭示潜在的舞者与舞者与图神经网络的相互作用</title>
      <link>https://arxiv.org/abs/2503.04816</link>
      <description><![CDATA[ARXIV：2503.04816V1公告类型：新 
摘要：在二重奏中跳舞通常需要对自己的伴侣提高调整：他们在太空中的方向，他们的动力以及他们对您施加的力量。在合作环境中工作的舞蹈艺术家在他们的动作与伴侣之间的关系中可能具有深刻的理解，但是典型的舞蹈文档未能捕捉到这些多样而微妙的关系。我们与有兴趣加深他们对合作伙伴的理解的舞蹈艺术家紧密合作，我们利用图形神经网络（GNN）来突出和解释两位舞者共享的复杂联系。使用视频到3D-Pose提取管道，我们从当代舞蹈二重奏的精选视频中提取3D动作，应用专门的预处理以改善重建，并训练GNN来预测舞者之间的加权连接。通过可视化和解释这两个搬家之间的预测关系，我们证明了基于图的方法构建二重奏协作动力学的替代模型的潜力。最后，我们提供了一些示例策略，以使用这些见解来告知生成和共同创造的工作室实践。]]></description>
      <guid>https://arxiv.org/abs/2503.04816</guid>
      <pubDate>Mon, 10 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DA-STGCN：基于时空特征提取的4D轨迹预测</title>
      <link>https://arxiv.org/abs/2503.04823</link>
      <description><![CDATA[ARXIV：2503.04823V1公告类型：新 
摘要：空中交通管理系统中四维（4D）轨迹预测的重要性正在上升。诸如冲突检测和解决方案，飞机异常监测以及拥挤飞行道路的管理等关键操作越来越依赖这一基础技术，强调了对智能解决方案的紧急需求。机场航站楼和拥挤的空域的动态是复杂的且不断变化的。但是，当前的方法论不能充分说明飞机之间的相互作用。为了应对这些挑战，我们提出了DA-STGCN，这是一种创新的时空图卷积网络，该网络集成了双重注意机制。我们的模型通过自我注意的方法重建了邻接矩阵，增强了节点相关性的捕获，并采用了图形注意力，从而对蒸发时空特征进行了图形，从而产生了预测轨迹的概率分布。这个新颖的邻接矩阵通过自我发项机制重建，在整个网络的训练过程中都进行了动态优化，与传统算法相比，节点间关系的反射更加细微。该模型的性能在两个ADS-B数据集上进行了验证，一个在机场终端区域附近，另一个在密集的空域中进行了验证。实验结果表明，比当前的4D轨迹预测方法有了显着改善，平均位移误差（ADE）和最终位移误差（FDE）分别减少了20％和30％。通过消融实验证实，双重意见模块的掺入可显着增强节点相关性的提取。]]></description>
      <guid>https://arxiv.org/abs/2503.04823</guid>
      <pubDate>Mon, 10 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Stickmotion：通过画贴纸来产生3D人类动作</title>
      <link>https://arxiv.org/abs/2503.04829</link>
      <description><![CDATA[ARXIV：2503.04829V1公告类型：新 
摘要：将文本描述转化为人类动作的文本到动作生成，在准确捕获简单文本输入的详细用户想象的动作方面一直在挑战。本文介绍了Stickmotion，这是一个为多条件方案设计的高效基于扩散的网络，该网络分别基于传统文本和我们提出的Stickman条件，分别为全球和本地控制这些动作生成所需的动作。我们从三个角度解决了用户友好的Stickman引入的挑战：1）数据生成。我们开发了一种算法来自动在不同的数据集格式上生成手绘棍棒。 2）多条件融合。我们提出了一个多条件模块，该模块集成到扩散过程中并获得所有可能条件组合的输出，从而降低了计算复杂性并增强了与自我发场模块的常规方法相比，可以增强Stickmotion的性能。 3）动态监督。我们使Stickmotion能够对Stickman在输出序列中的位置进行少量调整，从而通过我们提出的动态监督策略产生更自然的运动。通过定量实验和用户研究，素描Stickmen可以节省用户约51.5％的时间产生与他们的想象力一致的动作。我们的代码，演示和相关数据将被发布，以促进科学界的进一步研究和验证。]]></description>
      <guid>https://arxiv.org/abs/2503.04829</guid>
      <pubDate>Mon, 10 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>RD有效的FPGA部署学习图像压缩：知识蒸馏和混合量化</title>
      <link>https://arxiv.org/abs/2503.04832</link>
      <description><![CDATA[ARXIV：2503.04832V1公告类型：新 
摘要：可学习的图像压缩（LIC）表明有潜力超过RD效率的标准化视频编解码器，从而促使研究对硬件友好实现。大多数现有的LIC硬件实施方法优先考虑RD效率的延迟，并通过对硬件设计空间进行广泛的探索。我们提出了一种新颖的设计范式，其中调整特定硬件平台设计的负担朝向模型尺寸，而不会损害RD效率。首先，我们设计了一个框架，用于从参考教师中提取更精益的学生LIC模型：通过调整单个模型超参数，我们可以在没有复杂的硬件设计探索的情况下达到不同硬件平台的约束。其次，我们提出了对广义分裂归一化（GDN）激活的硬件友好实现，该实现甚至在参数量化后都保留了RD效率。第三，我们设计了一个管道的FPGA配置，该配置通过利用并行处理和优化资源分配来充分利用可用的FPGA资源。我们使用最先进的LIC模型进行的实验表明，我们在RD效率方面非常接近原始模型，均优于所有现有的FPGA实现。]]></description>
      <guid>https://arxiv.org/abs/2503.04832</guid>
      <pubDate>Mon, 10 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>针对越狱攻击的多模式大语言模型的对抗训练</title>
      <link>https://arxiv.org/abs/2503.04833</link>
      <description><![CDATA[ARXIV：2503.04833V1公告类型：新 
摘要：多模式的大语言模型（MLLM）在跨模式理解和生成任务方面取得了显着步骤。但是，它们仍然容易受到越狱攻击的影响，在这些袭击中，精心制作的扰动绕过安全护栏并引起有害产出。在本文中，我们介绍了第一个针对MLLM训练阶段的越狱训练（AT）范式。将传统扩展到该领域提出了两个关键挑战：有效地调整大量参数并确保对多种方式的攻击稳健性。为了应对这些挑战，我们引入了针对对抗训练（PROEAT）的投影层，这是一个框架的端到端。 Proeat结合了一个基于投影仪的对抗训练架构，该体系结构有效地处理大型参数，同时通过将对抗性训练集中在轻量级投影仪层而不是整个模型来维持计算可行性。此外，我们设计了一种动态重量调整机制，该机制可以根据任务要求优化损失函数的重量分配，从而简化调谐过程。为了提高防御性能，我们提出了跨视觉和文本方式的联合优化策略，以确保对源自两种方式的越狱攻击的强烈抵抗。对三个主流MLLM的五种主要越狱攻击方法进行的广泛实验证明了我们方法的有效性。 Proeat实现了最先进的防御性能，在文本和图像方式中，平均差距 +34％，而清洁准确性仅降低了1％。此外，对现实世界中体现的智能系统的评估突出了我们框架的实际适用性，为开发更安全和可靠的多模式系统铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2503.04833</guid>
      <pubDate>Mon, 10 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>将数据集蒸馏到神经领域</title>
      <link>https://arxiv.org/abs/2503.04835</link>
      <description><![CDATA[ARXIV：2503.04835V1公告类型：新 
摘要：使用大规模数据集对于培训高性能深度学习模型至关重要，但它也带有实质性的计算和存储成本。为了克服这些挑战，通过将大规模数据集压缩为较小的合成数据集，该数据集蒸馏已成为一种有希望的解决方案，该数据集保留了培训所需的基本信息。本文提出了一个用于数据集蒸馏的新型参数化框架，将数据集蒸发到神经场（DDIF），该数据集利用神经场来存储大型数据集的必要信息。由于神经场的独特性质（将坐标为输入和输出数量）有效地保留了信息并轻松生成各种数据形状。从理论上讲，当使用单个合成实例的预算相同时，DDIF比以前的文献表现出更大的表现力。通过广泛的实验，我们证明了DDIF在几个基准数据集上实现了卓越的性能，扩展了图像域，以包括视频，音频和3D Voxel。我们在https://github.com/aailab-kaist/ddif上发布代码。]]></description>
      <guid>https://arxiv.org/abs/2503.04835</guid>
      <pubDate>Mon, 10 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FEDPALM：一个通用联合学习框架，用于封闭式和开放式棕榈线验证</title>
      <link>https://arxiv.org/abs/2503.04837</link>
      <description><![CDATA[ARXIV：2503.04837V1公告类型：新 
摘要：当前的深度学习（DL）基于大型数据集的集中培训依赖于集中式培训，由于生物识别数据的敏感和不变的性质，这引起了严重的隐私问题。联合学习〜（FL）是一种隐私性的分布式学习范式，通过实现无需数据共享的协作模型培训提供了令人信服的替代方案。但是，基于FL的掌刻验证面临着关键的挑战，包括来自不同身份的数据异质性以及缺乏标准化评估基准。本文通过建立用于基于FL的棕榈印刷验证的综合基准来解决这些差距，该验证明确定义和评估了两个实际情况：封闭设置和开放式验证。我们提出了FedPalm，这是一个统一的FL框架，可以平衡局部适应性与全球概括。每个客户培训针对本地数据量身定制的个性化纹理专家，并为提取广义功能的共享全球纹理专家提供协作。为了进一步提高验证性能，我们引入了一个质地专家交互模块，该模块会在专家之间动态纹理特征，以产生精致的侧面纹理特征。可学习的参数用于建模原始特征和侧面特征之间的关系，从而促进跨文本特征交互并改善特征歧视。广泛的实验验证了FEDPALM的有效性，证明了两种情况下的稳健性能，并为推进基于FL的Palmprint验证研究提供了有希望的基础。]]></description>
      <guid>https://arxiv.org/abs/2503.04837</guid>
      <pubDate>Mon, 10 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>合并的物理和事件摄像机模拟器用于滑移检测</title>
      <link>https://arxiv.org/abs/2503.04838</link>
      <description><![CDATA[ARXIV：2503.04838V1公告类型：新 
摘要：机器人操纵是工业制造等领域的常见任务。检测物体何时从机器人的掌握中滑落，对于安全可靠的操作至关重要。登记像素级亮度在高时空分辨率（称为``事件&#39;&#39;）时会变化的事件摄像机在安装在机器人的最终效应器上时提供了优雅的功能：由于它们仅检测相对于其观点的运动，因此没有适当握住的对象会产生任何事件，而滑滑的对象会立即触发它们。为了研究此功能，代表性数据集至关重要，无论是用于分析方法还是用于培训机器学习模型。在现实世界情景和手动数据收集以及数据标签的其他设置上，大多数对基于事件的数据进行滑移检测的研究大多数是进行了基于事件数据的。这可能会导致数据收集所需的时间显着增加，场景设置缺乏灵活性以及实验重复的高度复杂性。本文提出了一个模拟管道，用于使用机器人臂中描述的摄像头配置生成滑移数据，并通过初始数据驱动的实验证明了其有效性。一旦建立模拟器，使用模拟器就有可能减少数据收集的时间，提供随时更改设置的能力，简化重复的过程，并生成任意大型数据集。通过视觉检查和人工神经网络（ANN）创建并验证了两个不同的数据集。视觉检查确认了逼真的框架生成和准确的滑移建模，而对此数据进行训练的三个ANN可实现高验证精度，并在单独的测试集中证明了良好的概括能力，以及对现实世界数据的初始适用性。项目页面：https：//github.com/tub-rip/event_slip]]></description>
      <guid>https://arxiv.org/abs/2503.04838</guid>
      <pubDate>Mon, 10 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在大型视觉语言模型中推进多模式的内在学习，并通过任务感知演示</title>
      <link>https://arxiv.org/abs/2503.04839</link>
      <description><![CDATA[ARXIV：2503.04839V1公告类型：新 
摘要：多模式的内在学习（ICL）已成为大型视觉模型（LVLM）的关键能力，这是由于其规模和适用性的增加而驱动。尽管有希望，由于图像文本输入的固有复杂性以及ICL性能对输入配置的高灵敏度，因此在多模式设置中有效ICL仍然具有挑战性。在这项工作中，我们阐明了多模式ICL的核心机制，将任务映射确定为配置强大的内在示范（ICD）序列的关键因素。在这些见解的基础上，我们提出了\ textit {Saber}，这是一种配备了任务吸引注意力的轻巧但功能强大的仅解码器的变压器，它以自动性的方式智能选择并从演示库中安排ICD。该设计实现了细粒的特征提取和跨模式推理，迭代精炼的任务映射以生成高质量的ICD序列。通过涵盖五个LVLM和9个基准数据集的广泛实验，Saber不仅表现出强烈的经验性能，而且还对任务语义如何与多模式ICD相互作用提供了更深入的了解。我们的发现突出了原则性ICD序列配置和开放新途径的重要性，以增强多模式ICL在各种真实世界的情况下。]]></description>
      <guid>https://arxiv.org/abs/2503.04839</guid>
      <pubDate>Mon, 10 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Zaugnet用于生物成像中的Z-Slice增强</title>
      <link>https://arxiv.org/abs/2503.04843</link>
      <description><![CDATA[ARXIV：2503.04843V1公告类型：新 
摘要：三维生物学显微镜显着提高了我们对复杂生物结构的理解。但是，由于显微镜技术，样品特性或光毒性引起的局限性通常会导致z分解差，阻碍了准确的细胞测量结果。在这里，我们介绍了Zaugnet，这是一种快速，准确且自学的深度学习方法，用于增强生物图像中的Z分解。通过在连续切片之间执行非线性插值，Zaugnet在每次迭代中有效地分辨率加倍。在几种显微镜模式和生物学对象上，它在大多数指标上的表现都优于竞争方法。我们的方法利用生成的对抗网络（GAN）结构结合了知识蒸馏，以最大程度地提高预测速度而不会损害准确性。我们还开发了Zaugnet+，这是一个扩展版本，可以在任意距离下连续插值，这使其对于具有非均匀切片间距的数据集特别有用。 Zaugnet和Zaugnet+均提供用于大规模3D成像的高性能，可扩展的Z-Slice增强解决方案。它们可作为Pytorch的开源框架，并带有直观的Colab笔记本接口，可轻松访问科学界。]]></description>
      <guid>https://arxiv.org/abs/2503.04843</guid>
      <pubDate>Mon, 10 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Causal3D：从视觉数据中学习因果学习的综合基准</title>
      <link>https://arxiv.org/abs/2503.04852</link>
      <description><![CDATA[ARXIV：2503.04852V1公告类型：新 
摘要：真正的智能取决于发现和利用隐藏因果关系的能力。尽管AI和计算机视觉（CV）取得了重大进展，但仍缺乏评估模型能力从复杂的视觉数据中推断潜在因果关系的基准。在本文中，我们介绍了\ textsc {\ textbf {causal3d}}，这是一种新颖而全面的基准，将结构化数据（表）与相应的视觉表示（图像）集成在一起以评估因果推理。 Causal3D在系统的框架内设计，包括19个3D-Scene数据集捕获各种因果关系，观点和背景，从而在各种复杂性的场景中进行评估。我们评估了多种最先进的方法，包括经典因果发现，因果关系学习和大/视觉模型（LLMS/VLMS）。我们的实验表明，随着因果结构在没有先验知识的情况下变得更加复杂，绩效大大下降，突出了即使在复杂的因果场景中面临的高级方法所面临的挑战。 Causal3D是推进简历中的因果推理并促进关键领域中值得信赖的AI的重要资源。]]></description>
      <guid>https://arxiv.org/abs/2503.04852</guid>
      <pubDate>Mon, 10 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>形状：通过迭代产生整体赢家的自我改进的视觉偏好对齐</title>
      <link>https://arxiv.org/abs/2503.04858</link>
      <description><![CDATA[ARXIV：2503.04858V1公告类型：新 
摘要：大型视觉语言模型（LVLMS）越来越依赖于偏好对齐来确保可靠性，这通过偏好微调对``图像 - 冠军 - 获胜者文本-Loser Text&#39;&#39;构成的偏好数据来引导模型行为。但是，现有的方法通常会遭受与人类宣传的偏好数据相关的多样性和高成本的损失，从而阻碍LVLM完全实现其预期的一致性能力。我们提出\ ProjectName，这是一个自制的框架，能够将已经丰富的有监督的文本图像对转换为整体偏好三胞胎，以更有效，更便宜的LVLM对齐，从而消除了对人类偏好注释的需求。我们的方法促进了通过迭代自我完善逐步增强对齐能力的LVLM。关键的设计基本原理是设计偏好三胞胎，其中获胜者文本在整体上始终提高整体性，并优于质量的失败者响应，从而推动模型``通过偏好的微调来努力``努力&#39;for of最大的一线&#39;&#39;&#39;。对于每个给定的文本图像对，Shape都会引入多个视觉增强，并将它们与摘要的文本配对，以作为赢家响应，同时将原始文本指定为失败者响应。 \ textBf {12}在包括llava和deepseek-vl在内的各种模型体系结构和大小的基准测试中进行的实验表明，在MMVET（全面评估）上实现+11.3 \％的实现，在MMVET上实现+11.3 \％，而MMBENCH上的+1.4 \％（一般VQA）， +8.0.0.0 \％On +8.8 on Pressely（\ pers）（\ pers）（\ 8.8 on PRESEL）（\ 8.0 c）（\ 8.4在7B型号中。值得注意的是，定性分析证实了对视觉细节的关注，并与人类对整体描述的偏好更好地对齐。]]></description>
      <guid>https://arxiv.org/abs/2503.04858</guid>
      <pubDate>Mon, 10 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>可穿戴传感器的端到端人姿势重建6G扩展现实系统</title>
      <link>https://arxiv.org/abs/2503.04860</link>
      <description><![CDATA[ARXIV：2503.04860V1公告类型：新 
摘要：完整的3D人姿势重建是未来第六代（6G）网络中扩展现实（XR）应用程序的关键推动者，支持游戏，虚拟会议和远程协作中的沉浸式互动。但是，由于通道障碍，位误差和量化效果，实现无线网络上准确的姿势重建仍然具有挑战性。现有方法通常假设在室内设置中无错误的传输，从而将其适用性限制在现实世界中。为了应对这些挑战，我们提出了一个基于深度学习的新型框架，用于对正交频划分多路复用（OFDM）系统的重建。该框架引入了两个阶段的深度学习接收器：第一阶段共同估算了无线通道和DM符号的解码，第二阶段将接收到的传感器信号映射到完整的3D身体姿势。仿真结果表明，与采用单独的信号检测步骤（即最小二乘通道估计和线性最小平方误差平等）相比，提出的神经接收器降低了位错误率（BER），从而获得5 dB间隙，以$ 10^{ -  4} $ BER的差距获得5 dB的间隙。此外，我们的经验发现表明，8位量化足以进行精确的姿势重建，对于重建的传感器信号，达到了$ 5 \ times10^{ -  4} $的平均平方误差，与基线相比，重建的人类姿势将关节角误差减少了37 \％。]]></description>
      <guid>https://arxiv.org/abs/2503.04860</guid>
      <pubDate>Mon, 10 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于高精度变压器的视觉伺服伺服，用于对齐微型物体的人形机器人</title>
      <link>https://arxiv.org/abs/2503.04862</link>
      <description><![CDATA[ARXIV：2503.04862V1公告类型：新 
摘要：对于现实世界中的人形机器人，高精度的微小物体对准仍然是一个普遍且关键的挑战。为了解决这个问题，本文提出了一个基于视觉的框架，用于精确估计和控制人形机器人的手持式工具与目标对象之间的相对位置，例如，螺丝刀尖端和螺丝头插槽。通过在机器人的头部和躯干摄像机的头部角度融合图像，提出的基于变压器的视觉伺服方法可以有效地纠正手持式工具的位置误差，尤其是在近距离距离。 M4-M8螺钉上的实验表明，平均收敛误差为0.8-1.3 mm，成功率为93 \％-100 \％。通过比较分析，结果验证了高精度小对象对齐的这种能力，可以通过距离估计变压器体系结构以及本文提出的多听头机制来实现。]]></description>
      <guid>https://arxiv.org/abs/2503.04862</guid>
      <pubDate>Mon, 10 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>