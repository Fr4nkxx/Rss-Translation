<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Tue, 10 Dec 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>TagFog：用于视觉分布不均检测的文本锚点引导和假异常值生成</title>
      <link>https://arxiv.org/abs/2412.05292</link>
      <description><![CDATA[arXiv:2412.05292v1 公告类型：新
摘要：分布外 (OOD) 检测在许多实际应用中至关重要。然而，智能模型通常仅在分布内 (ID) 数据上进行训练，导致在将 OOD 数据错误分类为 ID 类时过于自信。在本研究中，我们提出了一个新的学习框架，该框架利用基于 Jigsaw 的简单假 OOD 数据和来自 ChatGPT ID 知识描述的丰富语义嵌入（“锚点”）来帮助指导图像编码器的训练。该学习框架可以灵活地与现有的 OOD 检测事后方法相结合，并且对多个 OOD 检测基准的大量实证评估表明，ID 知识和假 OOD 知识的丰富文本表示可以很好地帮助训练用于 OOD 检测的视觉编码器。借助该学习框架，在所有基准上都实现了新的最先进性能。代码可在 \url{https://github.com/Cverchen/TagFog} 获得。]]></description>
      <guid>https://arxiv.org/abs/2412.05292</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FodFoM：基础模型的虚假异常数据可创建更强大的视觉分布不均检测器</title>
      <link>https://arxiv.org/abs/2412.05293</link>
      <description><![CDATA[arXiv:2412.05293v1 公告类型：新
摘要：在开放世界应用中部署机器学习模型时，分布外 (OOD) 检测至关重要。OOD 检测的核心挑战是减轻模型对 OOD 数据的过度自信。虽然最近使用辅助异常数据集或合成异常特征的方法已经显示出良好的 OOD 检测性能，但由于数据收集成本高或假设简化，它们受到限制。在本文中，我们提出了一种新颖的 OOD 检测框架 FodFoM，它创新地结合了多个基础模型来生成两种具有挑战性的假异常图像用于分类器训练。
第一种类型基于 BLIP-2 的图像字幕功能、CLIP 的视觉语言知识和 Stable Diffusion 的图像生成能力。联合利用这些基础模型构建与分布内 (ID) 图像在语义上相似但不同的假异常图像。对于第二种类型，利用 GroundingDINO 的对象检测能力，通过模糊 ID 图像中的前景 ID 对象来帮助构建纯背景图像。所提出的框架可以灵活地与多种现有的 OOD 检测方法相结合。
大量的实证评估表明，借助构建的假图像，图像分类器可以更准确地区分真实的 OOD 图像和 ID 图像。
在多个基准测试中实现了新的最先进的 OOD 检测性能。代码可在 \url{https://github.com/Cverchen/ACMMM2024-FodFoM} 获得。]]></description>
      <guid>https://arxiv.org/abs/2412.05293</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>文本到图像模型在高级风格转换应用中的作用：以 DALL-E 3 为例</title>
      <link>https://arxiv.org/abs/2412.05325</link>
      <description><![CDATA[arXiv:2412.05325v1 公告类型：新 
摘要：虽然 DALL-E 3 因其能够根据文本描述生成富有创意的复杂图像而广受欢迎，但它在风格转换领域的应用仍未得到充分探索。该项目研究了 DALL-E 3 与传统神经风格转换技术的集成，以评估生成的风格图像对最终输出质量的影响。DALL-E 3 用于根据提供的描述生成风格图像，并将其与 Magenta 任意图像风格化模型相结合。通过结构相似性指数测量 (SSIM) 和峰值信噪比 (PSNR) 等指标以及处理时间评估来评估这种集成。研究结果表明，DALL-E 3 显著提高了风格化图像的多样性和艺术品质。虽然这种改进会略微增加风格转换时间，但数据表明这种权衡是值得的，因为 DALL-E 3 的总体处理时间比传统方法快约 2.5 秒，使其成为一种高效且视觉上更优越的选择。]]></description>
      <guid>https://arxiv.org/abs/2412.05325</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于动态场景分析、物体检测和运动跟踪的深度学习和混合方法</title>
      <link>https://arxiv.org/abs/2412.05331</link>
      <description><![CDATA[arXiv:2412.05331v1 公告类型：新
摘要：该项目旨在开发一个强大的视频监控系统，该系统可以根据对活动的检测将视频分割成更小的片段。例如，它使用闭路电视录像来记录主要事件（例如一个人或一个小偷的出现），以便优化存储并更容易进行数字搜索。它利用对象检测和跟踪方面的最新技术，包括 YOLO、SSD 和 Faster R-CNN 等卷积神经网络 (CNN)，以及循环神经网络 (RNN) 和长短期记忆网络 (LSTM)，以实现高精度检测和捕获时间依赖性。该方法结合了通过高斯混合模型 (GMM) 进行自适应背景建模和 Lucas-Kanade 等光流方法来检测运动。多尺度和上下文分析用于改进不同物体大小和环境的检测。混合运动分割策略结合了统计和深度学习模型来管理复杂的运动，而实时处理的优化则确保了高效的计算。采用卡尔曼滤波器和连体网络等跟踪方法，即使在遮挡的情况下也能保持平滑的跟踪。通过多尺度和上下文分析，可以改进多种场景中各种大小物体的检测。结果表明，在检测和跟踪物体方面具有很高的精确度和召回率，由于实时优化和光照不变特性，处理时间和准确性显著提高。这项研究的影响在于它有可能通过可靠、高效的物体检测和跟踪来改变视频监控，降低存储要求并增强安全性。]]></description>
      <guid>https://arxiv.org/abs/2412.05331</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ACT-Bench：面向自动驾驶的动作可控世界模型</title>
      <link>https://arxiv.org/abs/2412.05337</link>
      <description><![CDATA[arXiv:2412.05337v1 公告类型：新
摘要：世界模型已成为自动驾驶的有前途的神经模拟器，有可能补充稀缺的现实世界数据并实现闭环评估。然而，当前的研究主要基于视觉真实感或下游任务性能来评估这些模型，而对特定动作指令的保真度关注有限——这是生成有针对性的模拟场景的关键属性。虽然一些研究解决了动作保真度问题，但它们的评估依赖于闭源机制，限制了可重复性。为了解决这一差距，我们开发了一个开放获取的评估框架 ACT-Bench，用于量化动作保真度，以及一个基线世界模型 Terra。我们的基准测试框架包括一个大型数据集，将来自 nuScenes 的短上下文视频与相应的未来轨迹数据配对，这为生成未来视频帧提供了条件输入，并能够评估已执行动作的动作保真度。此外，Terra 在多个大型轨迹注释数据集上进行训练，以增强动作保真度。利用这个框架，我们证明了最先进的模型不能完全遵循给定的指令，而 Terra 实现了更高的动作保真度。我们的基准框架的所有组件都将公开，以支持未来的研究。]]></description>
      <guid>https://arxiv.org/abs/2412.05337</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于生成模型的融合改进红外图像的少样本语义分割</title>
      <link>https://arxiv.org/abs/2412.05341</link>
      <description><![CDATA[arXiv:2412.05341v1 公告类型：新
摘要：红外 (IR) 成像通常用于各种场景，包括自动驾驶、消防安全和防御应用。因此，对此类图像进行语义分割非常有趣。然而，这项任务面临着一些挑战，包括数据稀缺、与自然图像相比对比度和输入通道数不同，以及在某些情况下（例如国防应用）数据库中未表示的类的出现。小样本分割 (FSS) 通过使用一些标记的支持样本对查询图像进行分割，提供了一个克服这些问题的框架。然而，现有的 IR 图像 FSS 模型需要成对的可见 RGB 图像，这是一个主要限制，因为在某些应用中获取这种成对的数据是困难的或不可能的。在这项工作中，我们使用生成建模和融合技术为 IR 图像的 FSS 开发了新的策略。为此，我们建议合成辅助数据以提供额外的通道信息来补充 IR 图像中有限的对比度，以及 IR 数据合成以进行数据增强。在这里，前者帮助 FSS 模型更好地捕捉支持集和查询集之间的关系，而后者解决数据稀缺问题。最后，为了进一步改进前者，我们提出了一种新颖的融合集成模块来整合两种不同的模式。我们的方法在不同的 IR 数据集上进行了评估，并改进了最先进的 (SOTA) FSS 模型。]]></description>
      <guid>https://arxiv.org/abs/2412.05341</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>自动动态图像分析用于三维颗粒尺寸和形状分类</title>
      <link>https://arxiv.org/abs/2412.05347</link>
      <description><![CDATA[arXiv:2412.05347v1 公告类型：新
摘要：我们介绍了 OCULAR，这是一种用于细颗粒三维动态图像分析的创新硬件和软件解决方案。目前最先进的动态图像分析仪器主要限于二维成像。然而，大量文献表明，依赖单一二维投影进行粒子表征会导致许多应用中的不准确性。现有的三维成像技术，如计算机断层扫描、激光扫描和正射影像，仅限于静态物体。这些方法通常不具有统计代表性，并且具有大量的后处理要求，以及对专门的成像和计算资源的需求。OCULAR 通过使用同步光学摄像机阵列提供对连续粒子流进行成像的经济高效的解决方案来解决这些挑战。通过重建粒子的三维表面来实现粒子形状表征。本文详细介绍了 OCULAR 方法，评估了其可重复性，并将其结果与 X 射线微型计算机断层扫描进行了比较，强调了其高效可靠的粒子分析潜力。]]></description>
      <guid>https://arxiv.org/abs/2412.05347</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过量化共享特征表示来预测基于转移的攻击的成功率</title>
      <link>https://arxiv.org/abs/2412.05351</link>
      <description><![CDATA[arXiv:2412.05351v1 公告类型：新
摘要：人们已经付出了很多努力来解释和提高基于转移的攻击 (TBA) 对黑盒计算机视觉模型的成功率。这项工作通过识别目标模型中是否存在易受攻击的特征，首次尝试先验预测攻击成功率。Chen 和 Liu (2024) 最近的研究提出了流形攻击模型，这是一个统一的框架，提出成功的 TBA 存在于一个共同的流形空间中。我们的工作通过一种新方法实验性地测试了共同的流形空间假设：首先，将在 ImageNet 上训练的替代和目标特征提取器的特征向量投影到同一个低维流形上；其次，量化流形上任何观察到的结构相似性；最后，将这些观察到的相似性与 TBA 的成功联系起来。我们发现共享特征表示与 TBA 成功率的提高有中等相关性（\r{ho}= 0.56）。该方法可用于预测攻击是否会在不了解模型权重、训练、架构或攻击细节的情况下进行转移。结果证实了不同大小和复杂度的两个特征提取器之间存在共享特征表示，并证明了来自不同目标域的数据集作为解释黑盒特征表示的测试信号的实用性。]]></description>
      <guid>https://arxiv.org/abs/2412.05351</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MotionShop：混合评分指导的视频扩散模型中的零镜头运动迁移</title>
      <link>https://arxiv.org/abs/2412.05355</link>
      <description><![CDATA[arXiv:2412.05355v1 公告类型：新
摘要：在这项工作中，我们通过混合分数指导 (MSG) 提出了扩散变压器中的第一种运动传递方法，MSG 是一种理论基础的扩散模型运动传递框架。我们的主要理论贡献在于重新制定条件分数以分解扩散模型中的运动分数和内容分数。通过将运动传递公式化为势能的混合，MSG 自然地保留了场景构图并实现了创造性的场景转换，同时保持了传递运动模式的完整性。这种新颖的采样直接在预先训练的视频扩散模型上运行，无需额外的训练或微调。通过大量实验，MSG 展示了对各种场景的成功处理，包括单个对象、多个对象和跨对象运动传递以及复杂的相机运动传递。此外，我们推出了 MotionBench，这是第一个运动传递数据集，由 200 个源视频和 1000 个传递的运动组成，涵盖单/多对象传递和复杂的相机运动。]]></description>
      <guid>https://arxiv.org/abs/2412.05355</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DIFEM：基于关键点交互的视频暴力识别特征提取模块</title>
      <link>https://arxiv.org/abs/2412.05386</link>
      <description><![CDATA[arXiv:2412.05386v1 公告类型：新
摘要：监控视频中的暴力检测是确保公共安全的关键任务。因此，对高效、轻量级的暴力行为自动检测系统的需求日益增加。在这项工作中，我们提出了一种有效的方法，利用人体骨骼关键点来捕捉暴力的固有属性，例如特定关节的快速运动及其紧密接近。我们方法的核心是我们新颖的动态交互特征提取模块 (DIFEM)，它可以捕捉速度和关节交叉等特征，有效地捕捉暴力行为的动态。利用我们的 DIFEM 提取的特征，我们使用各种分类算法，如随机森林、决策树、AdaBoost 和 k-最近邻。与采用深度学习技术的现有最先进 (SOTA) 方法相比，我们的方法的参数开销要小得多。我们对三个标准暴力识别数据集进行了广泛的实验，结果显示这三个数据集的性能都很好。我们提出的方法超越了几种 SOTA 暴力识别方法。]]></description>
      <guid>https://arxiv.org/abs/2412.05386</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于 YOLOv5 的航空影像应急响应目标检测</title>
      <link>https://arxiv.org/abs/2412.05394</link>
      <description><![CDATA[arXiv:2412.05394v1 公告类型：新
摘要：本文介绍了一种使用 YOLOv5 模型在航拍图像中进行物体检测的稳健方法。我们专注于识别关键物体，例如救护车、车祸、警车、拖车、消防车、翻车和着火车辆。通过利用自定义数据集，我们概述了从数据收集和注释到模型训练和评估的完整流程。我们的结果表明，YOLOv5 有效地平衡了速度和准确性，使其适用于实时应急响应应用。这项工作解决了航拍图像中的关键挑战，包括小物体检测和复杂背景，并为未来自动应急响应系统的研究提供了见解。]]></description>
      <guid>https://arxiv.org/abs/2412.05394</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>交换路径网络以实现稳健的人员搜索预训练</title>
      <link>https://arxiv.org/abs/2412.05433</link>
      <description><![CDATA[arXiv:2412.05433v1 公告类型：新
摘要：在人物搜索中，我们在一组图库场景中检测并对查询人物图像的匹配进行排序。大多数人物搜索模型都使用特征提取主干，然后使用单独的头进行检测和重新识别。虽然视觉主干的预训练方法已经很成熟，但之前尚未研究过为人物搜索任务预训练附加模块。在这项工作中，我们提出了第一个端到端人物搜索预训练框架。我们的框架将人物搜索分为以对象为中心和以查询为中心的方法，并且我们表明以查询为中心的框架对标签噪声具有鲁棒性，并且仅使用弱标记的人物边界框即可进行训练。此外，我们提供了一种称为交换路径网络 (SPNet) 的新模型，它实现了以查询为中心和以对象为中心的训练目标，并且可以在使用相同权重的情况下在两者之间切换。使用 SPNet，我们展示了以查询为中心的预训练，然后以对象为中心的微调，在标准 PRW 和 CUHK-SYSU 人物搜索基准上取得了最佳结果，CUHK-SYSU 上的 mAP 为 96.4%，PRW 上的 mAP 为 61.2%。此外，我们展示了我们的方法对于人物搜索预训练比最近的仅限主干的预训练替代方案更有效、更高效、更稳健。]]></description>
      <guid>https://arxiv.org/abs/2412.05433</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>UniScene：以占用为中心的统一驾驶场景生成</title>
      <link>https://arxiv.org/abs/2412.05435</link>
      <description><![CDATA[arXiv:2412.05435v1 公告类型：新
摘要：生成高保真、可控和带注释的训练数据对于自动驾驶至关重要。现有方法通常直接从粗略的场景布局生成单一数据形式，这不仅无法输出各种下游任务所需的丰富数据形式，而且难以对直接布局到数据的分布进行建模。在本文中，我们介绍了UniScene，这是第一个用于生成驾驶场景中三种关键数据形式（语义占用、视频和激光雷达）的统一框架。UniScene采用渐进式生成过程，将复杂的场景生成任务分解为两个分层步骤：（a）首先从定制的场景布局生成语义占用作为富含语义和几何信息的元场景表示，然后（b）以占用为条件，分别生成视频和激光雷达数据，采用基于高斯的联合渲染和先验引导的稀疏建模两种新颖的传输策略。这种以占用为中心的方法减轻了生成负担，尤其是对于复杂的场景，同时为后续生成阶段提供了详细的中间表示。大量实验表明，UniScene 在占用、视频和 LiDAR 生成方面的表现优于之前的 SOTA，这也确实有利于下游驾驶任务。]]></description>
      <guid>https://arxiv.org/abs/2412.05435</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CigTime：通过逆向运动编辑生成纠正指令</title>
      <link>https://arxiv.org/abs/2412.05460</link>
      <description><![CDATA[arXiv:2412.05460v1 公告类型：新
摘要：将自然语言与人类动作联系起来的模型的最新进展已显示出基于指导性文本的动作生成和编辑的巨大前景。受体育教练和运动技能学习应用的启发，我们研究了逆问题：利用动作编辑和生成模型生成纠正性指导性文本。我们引入了一种新方法，该方法根据用户的当前动作（源）和期望的动作（目标），生成文本指令以指导用户实现目标动作。我们利用大型语言模型来生成纠正文本，并利用现有的动作生成和编辑框架来编译三元组数据集（源动作、目标动作和纠正文本）。利用这些数据，我们提出了一种用于生成纠正指令的新动作语言模型。我们在各种应用中展示了定性和定量结果，这些结果在很大程度上改进了基线。我们的方法证明了其在教学场景中的有效性，提供了基于文本的指导来纠正和提高用户的表现。]]></description>
      <guid>https://arxiv.org/abs/2412.05460</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>COOOL：超越标签的挑战 自动驾驶的新基准</title>
      <link>https://arxiv.org/abs/2412.05462</link>
      <description><![CDATA[arXiv:2412.05462v1 公告类型：新
摘要：随着计算机视觉社区快速开发和改进自动驾驶系统算法，更安全、更高效的自动驾驶交通的目标越来越容易实现。然而，现在是 2024 年，我们仍然没有完全自动驾驶的汽车。剩下的核心挑战之一在于解决新颖性问题，自动驾驶系统仍然难以处理开放道路上以前从未见过的情况。通过我们的标签外挑战 (COOOL) 基准，我们引入了一个用于危险检测的新型数据集，提供适用于各种任务的多功能评估指标，包括异常检测、开放集识别、开放词汇和领域自适应等新颖性相邻领域。COOOL 包含 200 多个行车记录仪视频集，由人工标注员标注以识别感兴趣的物体和潜在的驾驶危险。它包括各种各样的危险和干扰物体。由于数据集的大小和数据的复杂性，COOOL 仅作为评估基准。]]></description>
      <guid>https://arxiv.org/abs/2412.05462</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>