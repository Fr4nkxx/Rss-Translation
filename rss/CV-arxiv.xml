<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Wed, 04 Dec 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>使用 MobileNetV2 模型通过背景去除进行数据增强，以实现苹果叶片病害分类</title>
      <link>https://arxiv.org/abs/2412.01854</link>
      <description><![CDATA[arXiv:2412.01854v1 公告类型：新
摘要：深度学习技术推动的计算机视觉进步越来越多地应用于精准农业，以自动检测和分类植物疾病。植物疾病的症状通常出现在叶子上。现有数据集中的叶子图像是在受控条件下或在田间收集的。以前的大多数研究都集中于使用在受控实验室环境中捕获的图像来识别叶子疾病，通常可以实现高性能。然而，旨在检测和分类田间图像中的叶子疾病的方法通常表现出较低的性能。本研究的目的是评估数据增强方法（涉及从叶子图像中去除复杂背景）对在现实条件下捕获的图像中苹果叶子疾病分类性能的影响。为了实现这一目标，对轻量级预训练的 MobileNetV2 深度学习模型进行了微调，随后用于评估使用背景去除图像扩展训练数据集对分类性能的影响。实验结果表明，这种增强策略提高了分类准确性。具体来说，使用 Adam 优化器，所提出的方法在植物病理学数据库上实现了 98.71% 的分类准确率，提高了约 3%，并且优于最先进的方法。这证明了背景去除作为一种数据增强技术在提高现实条件下疾病分类模型的稳健性方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2412.01854</guid>
      <pubDate>Wed, 04 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从想象中进行规划：视觉和语言导航的情景模拟和情景记忆</title>
      <link>https://arxiv.org/abs/2412.01857</link>
      <description><![CDATA[arXiv:2412.01857v1 公告类型：新
摘要：人类利用情景模拟和情景记忆的能力来驾驭陌生的环境。开发类似于情景模拟和情景记忆的基于想象的记忆可以增强具身代理对环境和物体之间复杂关系的理解。然而，现有的视觉和语言导航 (VLN) 代理无法执行上述机制。我们提出了一种新颖的架构来帮助代理构建一个循环想象记忆系统。具体来说，代理可以在导航过程中保持现实-想象混合全局记忆，并通过想象机制和导航动作扩展记忆图。相应地，我们设计了一系列预训练任务来帮助代理获得细粒度的想象能力。我们的代理将最先进 (SoTA) 的成功率 (SR) 提高了 7%，同时为未来场景想象高保真 RGB 表示。]]></description>
      <guid>https://arxiv.org/abs/2412.01857</guid>
      <pubDate>Wed, 04 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>BAFPN：双向对齐特征以提高定位精度</title>
      <link>https://arxiv.org/abs/2412.01859</link>
      <description><![CDATA[arXiv:2412.01859v1 公告类型：新
摘要：当前最先进的视觉模型通常利用特征金字塔来提取多尺度信息，其中特征金字塔网络（FPN）是最广泛使用的经典架构之一。然而，传统的 FPN 及其变体（例如 AUGFPN、PAFPN）未能完全解决全局尺度上的空间错位问题，导致在高精度物体定位中性能不佳。在本文中，我们提出了一种新颖的双向对齐特征金字塔网络（BAFPN），它在自下而上的信息传播阶段通过空间特征对齐模块（SPAM）全局对齐未对齐的特征。随后，它通过自上而下的阶段的细粒度语义对齐模块（SEAM）进一步减轻跨尺度特征融合引起的混叠效应。在 DOTAv1.5 数据集上，BAFPN 分别将基线模型的 AP75、AP50 和 mAP 提高了 1.68%、1.45% 和 1.34%。此外，BAFPN 在应用于其他各种高级检测器时也表现出显著的性能提升。]]></description>
      <guid>https://arxiv.org/abs/2412.01859</guid>
      <pubDate>Wed, 04 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 ArcFace 成对辨别 AffectNet 表情</title>
      <link>https://arxiv.org/abs/2412.01860</link>
      <description><![CDATA[arXiv:2412.01860v1 公告类型：新
摘要：本研究迈出了通过面部情绪识别 (FER) 教计算机识别人类情绪的初步步骤。迁移学习使用 ResNeXt、EfficientNet 模型和最初在面部验证任务上训练的 ArcFace 模型，利用 AffectNet 数据库（一组带有相应情绪注释的人脸图像）。研究结果强调了一致域迁移学习的价值、不平衡数据集在学习面部情绪模式方面带来的挑战，以及成对学习在解决类别不平衡方面以提高模型在 FER 任务上的性能的有效性。]]></description>
      <guid>https://arxiv.org/abs/2412.01860</guid>
      <pubDate>Wed, 04 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>理解大规模视觉数据集中的偏差</title>
      <link>https://arxiv.org/abs/2412.01876</link>
      <description><![CDATA[arXiv:2412.01876v1 公告类型：新
摘要：最近的一项研究表明，大规模视觉数据集存在很大偏差：现代神经网络可以很容易地对它们进行分类。然而，这些数据集中偏差的具体形式仍不清楚。在本研究中，我们提出了一个框架来识别区分这些数据集的独特视觉属性。我们的方法应用各种转换来从数据集中提取语义、结构、边界、颜色和频率信息，并评估每种类型的信息在多大程度上反映了它们的偏差。我们进一步通过对象级分析分解它们的语义偏差，并利用自然语言方法生成每个数据集特征的详细、开放式描述。我们的工作旨在帮助研究人员了解现有大规模预训练数据集中的偏差，并在未来构建更多样化和更具代表性的数据集。我们的项目页面和代码可在 http://boyazeng.github.io/understand_bias 上找到。]]></description>
      <guid>https://arxiv.org/abs/2412.01876</guid>
      <pubDate>Wed, 04 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PROFIT：用于多任务学习的近端微调优化器</title>
      <link>https://arxiv.org/abs/2412.01930</link>
      <description><![CDATA[arXiv:2412.01930v1 公告类型：新
摘要：微调预训练模型在计算机视觉和机器人技术中变得非常有价值。最近的微调方法侧重于通过使用较小的学习率或冻结的主干来提高效率而不是准确性。为了将焦点重新集中在模型准确性上，我们提出了 PROFIT，这是首批专门为在新任务或数据集上逐步微调收敛模型而设计的优化器之一。与由于随机初始化而做出最少假设的传统优化器（如 SGD 或 Adam）不同，PROFIT 利用收敛模型的结构来规范优化过程，从而获得更好的结果。通过采用简单的时间梯度正交化过程，PROFIT 在各种任务中的表现优于传统的微调方法：图像分类、表示学习和大规模运动预测。此外，PROFIT 封装在优化器逻辑中，使其能够以最少的工程工作量轻松集成到任何训练管道中。随着微调和增量训练变得越来越普遍，像 PROFIT 这样的新型微调优化器可以推动进步，减少对从头开始进行昂贵的模型训练的依赖。]]></description>
      <guid>https://arxiv.org/abs/2412.01930</guid>
      <pubDate>Wed, 04 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>平面高斯溅射</title>
      <link>https://arxiv.org/abs/2412.01931</link>
      <description><![CDATA[arXiv:2412.01931v1 公告类型：新
摘要：本文介绍了平面高斯分层 (PGS)，这是一种新颖的神经渲染方法，可直接从多个 RGB 图像中学习 3D 几何并解析场景的 3D 平面。PGS 利用高斯基元对场景进行建模，并采用分层高斯混合方法对其进行分组。相似的高斯在树形高斯混合中逐渐概率合并，以识别不同的 3D 平面实例并形成整体 3D 场景几何。为了实现分组，高斯基元包含其他参数，例如通过从一般 2D 分割模型和表面法线中提取 2D 掩模得到的平面描述符。实验表明，所提出的 PGS 在 3D 平面重建中实现了最先进的性能，而无需 3D 平面标签或深度监督。与现有的通用性有限且在领域转移下表现挣扎的监督方法相比，PGS 得益于其神经渲染和场景特定的优化机制，能够在跨数据集上保持其性能，同时也比现有的基于优化的方法快得多。]]></description>
      <guid>https://arxiv.org/abs/2412.01931</guid>
      <pubDate>Wed, 04 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 Transformer 进行稳健语义分割的全局平均特征增强</title>
      <link>https://arxiv.org/abs/2412.01941</link>
      <description><![CDATA[arXiv:2412.01941v1 公告类型：新
摘要：对分布外数据的鲁棒性对于部署现代神经网络至关重要。最近，用于语义分割的 Vision Transformers（例如 SegFormer）已表现出对影响采集设备的模糊或噪声等视觉损坏的令人印象深刻的鲁棒性。在本文中，我们提出了通道智能特征增强（CWFA），这是一种简单而有效的特征增强技术，可提高 Vision Transformers 在语义分割中的鲁棒性。CWFA 在训练期间以最小的计算开销对每个编码器应用全局估计的扰动。对 Cityscapes 和 ADE20K 进行了广泛的评估，使用三种最先进的 Vision Transformer 架构：SegFormer、Swin Transformer 和 Twins，表明 CWFA 增强模型显着提高了鲁棒性，而不会影响干净数据的性能。例如，在 Cityscapes 上，与未增强的 SegFormer-B1 相比，CWFA 增强的 SegFormer-B1 模型在脉冲噪声上实现了高达 27.7% 的 mIoU 稳健性增益。此外，CWFA 增强的 SegFormer-B5 实现了新的最先进的 84.3% 保留率，比最近发布的 FAN+STL 提高了 0.7%。]]></description>
      <guid>https://arxiv.org/abs/2412.01941</guid>
      <pubDate>Wed, 04 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用变压器网络增强卫星图像时间序列中的作物分割</title>
      <link>https://arxiv.org/abs/2412.01944</link>
      <description><![CDATA[arXiv:2412.01944v1 公告类型：新
摘要：最近的研究表明，卷积神经网络 (CNN) 在卫星图像时间序列 (SITS) 的裁剪分割中取得了令人印象深刻的效果。然而，Transformer 网络在各种视觉任务中的出现提出了一个问题，即它们是否也能胜过 CNN。本文介绍了基于 Transformer 的 Swin UNETR 模型的修订版本，专门适用于 SITS 的裁剪分割。所提出的模型取得了显着的进步，在慕尼黑数据集上的验证准确率为 96.14%，测试准确率为 95.26%，超过了之前的最佳验证结果 93.55% 和测试结果 92.94%。此外，该模型在伦巴第数据集上的性能与 UNet3D 相当，优于 FPN 和 DeepLabV3。这项研究的实验表明，该模型很可能实现与 CNN 相当甚至更高的准确率，同时所需的训练时间却要少得多。这些发现凸显了基于 Transformer 的架构在 SITS 中用于作物分割的潜力，为遥感应用开辟了新途径。]]></description>
      <guid>https://arxiv.org/abs/2412.01944</guid>
      <pubDate>Wed, 04 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过变形再训练增强深度学习模型的鲁棒性</title>
      <link>https://arxiv.org/abs/2412.01958</link>
      <description><![CDATA[arXiv:2412.01958v1 公告类型：新
摘要：本文评估了使用变质关系来增强机器学习模型的稳健性和实际性能。我们提出了一个变质再训练框架，它将变质关系应用于数据，并在迭代和自适应多循环过程中利用半监督学习算法。该框架集成了多种半监督再训练算法，包括 FixMatch、FlexMatch、MixMatch 和 FullMatch，以自动对具有指定配置的模型进行再训练、评估和测试。为了评估这种方法的有效性，我们使用各种图像处理模型（包括预训练和非预训练的模型）对 CIFAR-10、CIFAR-100 和 MNIST 数据集进行了实验。我们的结果证明了变质再训练可以显着提高模型稳健性的潜力，因为我们在结果中显示，每个模型的稳健性指标平均增加了 17%。]]></description>
      <guid>https://arxiv.org/abs/2412.01958</guid>
      <pubDate>Wed, 04 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 YOLOv8、YOLOv9、YOLOv10 和 YOLOv11 进行逐像素 ROI 选择以进行车辆检测的智能停车</title>
      <link>https://arxiv.org/abs/2412.01983</link>
      <description><![CDATA[arXiv:2412.01983v1 公告类型：新
摘要：城市化进程的加快和城市车辆数量的增长凸显了对高效停车管理系统的需求。传统的智能停车解决方案通常依靠传感器或摄像头进行占用检测，但每种方法都有其局限性。深度学习的最新进展引入了新的 YOLO 模型（YOLOv8、YOLOv9、YOLOv10 和 YOLOv11），但这些模型尚未在智能停车系统的背景下得到广泛评估，尤其是与感兴趣区域 (ROI) 选择相结合进行物体检测时。现有方法仍然依赖于固定的多边形 ROI 选择或简单的基于像素的修改，这限制了灵活性和精度。这项工作引入了一种集成物联网、边缘计算和深度学习概念的新方法，使用最新的 YOLO 模型进行车辆检测。通过探索边缘和云计算，发现边缘设备上的推理时间范围为 1 到 92 秒，具体取决于硬件和模型版本。此外，还提出了一种新的逐像素后处理 ROI 选择方法，用于准确识别感兴趣的区域以计算停车场图像中的车辆数。所提出的系统在 3,484 张图像的自定义数据集上实现了 99.68% 的平衡准确率，提供了一种经济高效的智能停车解决方案，可确保精确的车辆检测，同时保护数据隐私]]></description>
      <guid>https://arxiv.org/abs/2412.01983</guid>
      <pubDate>Wed, 04 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>HybridMQA：探索几何纹理相互作用以进行彩色网格质量评估</title>
      <link>https://arxiv.org/abs/2412.01986</link>
      <description><![CDATA[arXiv:2412.01986v1 公告类型：新
摘要：网格质量评估 (MQA) 模型在各种应用中的网格操作系统的设计、优化和评估中起着关键作用。当前的 MQA 模型，无论是使用拓扑感知特征的基于模型的方法，还是基于投影的渲染 2D 投影方法，通常都无法捕捉纹理和 3D 几何之间的复杂交互。我们推出了 HybridMQA，这是一种首创的混合全参考彩色 MQA 框架，它集成了基于模型和基于投影的方法，可捕捉纹理信息和 3D 结构之间的复杂交互，以丰富质量表示。我们的方法采用图形学习来提取详细的 3D 表示，然后使用新颖的特征渲染过程将其投影到 2D，该过程将它们与彩色投影精确对齐。这使得能够通过交叉注意探索几何纹理交互，从而产生全面的网格质量表示。大量实验证明了 HybridMQA 在不同数据集上的卓越性能，凸显了其有效利用几何纹理相互作用来彻底了解网格质量的能力。我们的实现将公开发布。]]></description>
      <guid>https://arxiv.org/abs/2412.01986</guid>
      <pubDate>Wed, 04 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ShowHowTo：生成场景调节的分步可视化说明</title>
      <link>https://arxiv.org/abs/2412.01987</link>
      <description><![CDATA[arXiv:2412.01987v1 公告类型：新
摘要：这项工作的目标是在给定提供场景上下文和文本指令序列的输入图像的情况下，以图像序列的形式生成分步视觉指令。这是一个具有挑战性的问题，因为它需要生成多步骤图像序列以实现复杂目标，同时以特定环境为基础。部分挑战源于缺乏针对此问题的大规模训练数据。因此，这项工作的贡献有三方面。首先，我们引入了一种自动方法，用于从教学视频中收集大量分步视觉指令训练数据。我们将这种方法应用于一百万个视频，并创建了一个包含 0.6M 个图像文本对序列的大规模高质量数据集。其次，我们开发并训练 ShowHowTo，这是一个视频扩散模型，能够生成与提供的输入图像一致的分步视觉指令。第三，我们在三个准确度维度（步骤、场景和任务）上评估生成的图像序列，并表明我们的模型在所有维度上都取得了最佳结果。我们的代码、数据集和经过训练的模型都是公开的。]]></description>
      <guid>https://arxiv.org/abs/2412.01987</guid>
      <pubDate>Wed, 04 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>揭示自监督语音表征对帕金森病诊断的可解释性</title>
      <link>https://arxiv.org/abs/2412.02006</link>
      <description><![CDATA[arXiv:2412.02006v1 公告类型：新
摘要：病理语音分析的最新研究越来越多地依赖于强大的自监督语音表示，从而取得了令人鼓舞的结果。然而，这些嵌入的复杂、黑箱性质以及对其可解释性的有限研究严重限制了它们在临床诊断中的应用。为了解决这一差距，我们提出了一个新颖的可解释框架，专门用于支持帕金森病 (PD) 诊断。通过为嵌入和时间级分析设计简单而有效的交叉注意机制，所提出的框架从两个不同但互补的角度提供了可解释性。在五个成熟的 PD 检测语音基准上的实验结果表明，该框架能够在自监督表示中识别有意义的语音模式，以完成广泛的评估任务。细粒度时间分析进一步强调了其增强深度学习病理语音模型可解释性的潜力，为开发更透明、更可靠、更适用于临床的计算机辅助诊断系统铺平了道路。此外，在分类准确性方面，我们的方法取得了与最先进方法相媲美的结果，同时在应用于自发语音生成时也表现出跨语言场景的稳健性。]]></description>
      <guid>https://arxiv.org/abs/2412.02006</guid>
      <pubDate>Wed, 04 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>NitroFusion：通过动态对抗训练实现高保真单步扩散</title>
      <link>https://arxiv.org/abs/2412.02030</link>
      <description><![CDATA[arXiv:2412.02030v1 公告类型：新
摘要：我们介绍了 NitroFusion，这是一种与单步扩散完全不同的方法，可通过动态对抗框架实现高质量生成。虽然单步方法具有显著的速度优势，但与多步方法相比，它们通常会遭受质量下降的困扰。就像艺术评论家小组通过专注于构图、色彩和技巧等不同方面提供全面反馈一样，我们的方法维护了大量专门的鉴别器头，它们共同指导生成过程。每个鉴别器组都会在不同的噪声水平下发展特定质量方面的专业知识，从而提供多样化的反馈，从而实现高保真的单步生成。我们的框架结合了：（i）动态鉴别器池和专门的鉴别器组以提高生成质量，（ii）战略刷新机制以防止鉴别器过度拟合，以及（iii）用于多尺度质量评估的全局-局部鉴别器头，以及用于平衡生成的无条件/条件训练。此外，我们的框架通过自下而上的细化独特地支持灵活部署，允许用户在同一模型中动态选择 1-4 个去噪步骤，以实现直接的质量和速度权衡。通过全面的实验，我们证明 NitroFusion 在多个评估指标上的表现明显优于现有的单步方法，尤其是在保留精细细节和全局一致性方面表现出色。]]></description>
      <guid>https://arxiv.org/abs/2412.02030</guid>
      <pubDate>Wed, 04 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>