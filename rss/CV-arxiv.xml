<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>CS.CV更新arxiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.cv更新arxiv.org e-print存档。</description>
    <lastBuildDate>Mon, 10 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>从2D视频中预测基于行为的VR生物识别技术的3D运动</title>
      <link>https://arxiv.org/abs/2502.04361</link>
      <description><![CDATA[ARXIV：2502.04361V1公告类型：新 
摘要：使用传统证书（例如PIN，密码或多因素身份验证）等领域中的关键VR应用，例如医疗保健，教育和金融，如果恶意人员获得用户凭据或用户，则有可能受到损害的机会将他们的资格交给盟友。最近，出现了许多关于用户身份验证的方法，这些方法在VR中的用户交互过程中使用了VR头部安装的显示（HMD）和手持控制器的动作，以表示用户的行为作为VR生物特征识别签名。基于行为的方法的基本局限性之一是，针对HMD和控制器的当前的设备跟踪缺乏执行全身关节表达的跟踪的能力，丢失了用户发音封装的关键签名数据。在本文中，我们提出了一种使用2D主体接头的方法，即使用外部2D摄像头从参与者的右侧获取的肩膀，肘部，腕，臀部，膝盖和脚踝。我们的方法使用基于变压器的深神经网络，使用VR设备未跟踪的身体关节的2D数据来预测正确的控制器的过去和将来3D轨道，从而提供了增强身份验证3D知识的好处。我们的方法提供的最小误差率（EER）为0.025，在以前的工作中，使用单单元3D轨迹作为输入的最大eer下降为0.040。]]></description>
      <guid>https://arxiv.org/abs/2502.04361</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>机上索拉：为移动设备启用基于扩散的文本到视频生成</title>
      <link>https://arxiv.org/abs/2502.04363</link>
      <description><![CDATA[ARXIV：2502.04363V1公告类型：新 
摘要：我们提出了Device Sora，这是第一个开创性的解决方案，用于基于扩散的基于设备的文本到视频生成，可在智能手机级设备上有效运行。在开放式的基础上，开发项目SORA应用了三种新型技术，以应对基于扩散的文本到视频生成在计算和内存限制的移动设备上的挑战。首先，线性比例LEAP（LPL）通过有效的基于LEAP的方法来减少视频扩散所需的过度降解步骤。其次，时间维令牌合并（TDTM）通过沿时间维度合并连续令牌来最大程度地减少注意力层中的密集令牌处理。第三，与动态加载（CI-DL）的同时推断将大型模型动态分配到较小的块中，并将它们加载到内存中以进行并发模型推理，从而有效地解决了有限的设备内存的挑战。我们在iPhone 15 Pro上实现了device Sora，实验评估表明，它能够在设备上生成高质量的视频，与在高端GPU上运行的开放式SORA生产的视频相媲美。这些结果表明，在资源约束的移动设备上，启用了device Sora可以在资源受限的移动设备上产生高效，高质量的视频生成，确保用户隐私，降低对云基础架构的依赖以及降低相关成本。我们将拟议的智商索拉（Sora）视为使最先进的生成技术民主化的重要第一步，从而在商品移动设备和嵌入式设备上实现了视频生成功能。代码实现可在GitHub存储库中公开获得：https：//github.com/eai-lab/on-device-sora。]]></description>
      <guid>https://arxiv.org/abs/2502.04363</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>丢失了编辑？ AIGC出处的$ \ lambda $ -compass</title>
      <link>https://arxiv.org/abs/2502.04364</link>
      <description><![CDATA[ARXIV：2502.04364V1公告类型：新 
摘要：扩散模型的最新进展推动了文本指导的图像编辑工具的增长，从而实现了合成内容的精确和迭代修改。但是，随着这些工具变得越来越易于​​访问，它们还引入了滥用的重大风险，强调了对可靠归因方法的关键需求，以确保内容真实性和可追溯性。尽管这种工具具有创造力，但它们对归因构成了重大挑战，尤其是在可以分层以模糊图像起源的对抗环境中。我们提出了LambDatracer，这是一种新型潜在空间归因方法，可牢固地识别和区分真实的输出与受操纵的输出，而无需对生成或编辑管道进行任何修改。通过自适应校准重建损失，LambDatracer在各种迭代编辑过程中仍然有效，无论是通过文本指导的编辑工具（例如ConscessPix2Pix和ControlNet）自动化，还是使用Adobe Photoshop等编辑软件手动执行。广泛的实验表明，我们的方法在区分恶意编辑的图像方面始终优于基线方法，从而提供了一种实用的解决方案来保护公开，快速发展的AI生态系统中所有权，创造力和信誉。]]></description>
      <guid>https://arxiv.org/abs/2502.04364</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于AI的热视频分析隐私保护医疗保健：检测出生时间的案例研究</title>
      <link>https://arxiv.org/abs/2502.04365</link>
      <description><![CDATA[ARXIV：2502.04365V1公告类型：新 
摘要：大约10％的新生儿需要一些帮助才能开始呼吸和5％的适当通风。至关重要的是，出生后尽快开始干预措施。因此，准确的出生时间文档（TOB）对于记录和改善新生儿复苏表现至关重要。但是，当前的临床实践依赖于TOB的手动记录，通常具有微小的精度。在这项研究中，我们提出了一种基于AI驱动的，基于视频的系统，用于使用热成像进行自动TOB检测，旨在通过避免使用可识别的视觉数据来保护医疗保健提供者和母亲的隐私。在性能评估期间，我们的方法达到了91.4％的精度和97.4％的召回，以检测热视频片段中的TOB。此外，与手动注释相比，我们的系统在96％的测试用例中成功识别了96％的测试用例，绝对中值偏差为1秒。该方法提供了一种可靠的解决方案，可改善TOB文档并增强新生儿复苏结果。]]></description>
      <guid>https://arxiv.org/abs/2502.04365</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>HSI：任意风格转移的整体样式注射器</title>
      <link>https://arxiv.org/abs/2502.04369</link>
      <description><![CDATA[ARXIV：2502.04369V1公告类型：新 
摘要：基于注意力的任意风格转移方法最近由于其令人印象深刻的综合样式细节而引起了重大关注。但是，注意机制内的点匹配可能会过度关注本地模式，从而忽略了样式图像的显着全局特征。此外，在处理大图像时，注意机制的二次复杂性将带来较高的计算负载。为了减轻上述问题，我们提出了整体风格喷油器（HSI），这是一种新型的注意力转换模块，以提供目标风格的艺术表达。具体而言，HSI仅基于全球样式表示形式执行风格化，这更符合样式转移的特征，以避免在风格化的图像中生成本地不和谐的模式。此外，我们提出了HSI内部的双重关系学习机制，以通过利用内容和样式的语义相似性来动态渲染图像，从而确保风格化的图像保留原始内容并改善样式的保真度。请注意，所提出的HSI达到了线性计算复杂性，因为它通过元素乘法而不是矩阵乘法建立了特征映射。定性和定量结果表明，我们的方法在有效性和效率方面都优于最先进的方法。]]></description>
      <guid>https://arxiv.org/abs/2502.04369</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MapFusion：用于多模式地图构造的新型BEV特征融合网络</title>
      <link>https://arxiv.org/abs/2502.04377</link>
      <description><![CDATA[ARXIV：2502.04377V1公告类型：新 
摘要：地图构造任务在提供自主驾驶系统必不可少的精确且全面的静态环境信息方面起着至关重要的作用。主传感器包括相机和LIDAR，基于成本效果的考虑因素，配置在仅相机，仅激光镜或相机范围的融合之间变化。尽管基于融合的方法通常会表现最佳，但现有方法通常会忽略模式相互作用并依赖于简单的融合策略，这些策略遭受了未对准和信息丢失问题的困扰。为了解决这些问题，我们提出了MapFusion，这是一种新型的多模式鸟类视图（BEV）用于地图结构的特征融合方法。具体而言，为了解决摄像机和激光镜头BEV功能之间的语义不对对准问题，我们介绍了跨模式相互作用变换（CIT）模块，从而可以通过自我注意事项机制在两个BEV特征空间之间进行相互作用，并增强功能表示。此外，我们提出了一个有效的双动力融合（DDF）模块，以适应不同方式从不同模式中选择有价值的信息，这可以充分利用不同方式之间的固有信息。此外，MapFusion设计为简单且插件，易于集成到现有管道中。我们在两个地图构造任务上评估MAPFusion，包括高清（HD）地图和BEV地图分段，以显示其多功能性和有效性。与最先进的方法相比，MapFusion分别在Nuscenes数据集上的HD MAP构造和BEV MAP分割任务上实现了3.6％和6.2％的绝对改进，这表明了我们方法的优势。]]></description>
      <guid>https://arxiv.org/abs/2502.04377</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Dillema：多模式增强的扩散和大型语言模型</title>
      <link>https://arxiv.org/abs/2502.04378</link>
      <description><![CDATA[ARXIV：2502.04378V1公告类型：新 
摘要：确保深度学习模型的鲁棒性需要全面和多样化的测试。现有的方法通常基于简单的数据增强技术或生成对抗网络，在产生现实和多样化的测试用例方面受到限制。为了解决这些局限性，我们提出了一个新的框架，用于测试视觉神经网络，该框架利用大型语言模型和控制条件的扩散模型生成合成的高保真测试案例。我们的方法首先使用字幕模型将图像转换为详细的文本描述，从而允许语言模型识别图像的可修改方面并生成反事实描述。然后，这些描述用于通过文本对图像扩散过程产生新的测试图像，该过程保留空间一致性并保持场景的关键要素。我们使用两个数据集证明了我们方法的有效性：Imagenet1k用于图像分类和自主驾驶中语义分割的变化。结果表明，我们的方法可以产生重要的测试案例，以揭示弱点并通过靶向重新培训提高模型的鲁棒性。我们使用机械Turk进行了人类评估以验证生成的图像。参与者的回应证实，在选民之间达成了高度同意，我们的方法会产生有效和现实的图像。]]></description>
      <guid>https://arxiv.org/abs/2502.04378</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型可以捕获视频游戏的参与吗？</title>
      <link>https://arxiv.org/abs/2502.04379</link>
      <description><![CDATA[ARXIV：2502.04379V1公告类型：新 
摘要：在观察视频时，现成的预处理大语模型（LLM）可以成功地检测人类影响吗？为了第一次解决这个问题，我们全面评估了流行的LLMS注释和成功地预测视频的连续注释的能力，并以多模式方式提示一系列文本和视频帧。特别是在本文中，我们测试了LLMS在80分钟内从Gamevibe Corpus的20个第一人称射击游戏中的注释视频游戏镜头中正确标记游戏中参与度更改的能力。我们进行了2,400多个实验，以研究LLM体系结构，模型大小，输入方式，提示策略和地面真相处理方法对参与预测的影响。我们的发现表明，尽管LLMS正确地声称在多个领域声称具有类似人类的表现，但它们通常落后于捕获人类提供的连续体验注释。我们研究了相对较差的总体表现的一些根本原因，突出了LLM超过预期的情况，并为通过LLMS进一步探索自动情绪标签的路线图。]]></description>
      <guid>https://arxiv.org/abs/2502.04379</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Texlidar：全景激光雷达数据的自动化文本理解</title>
      <link>https://arxiv.org/abs/2502.04385</link>
      <description><![CDATA[ARXIV：2502.04385V1公告类型：新 
摘要：将LIDAR数据与文本（例如LidarClip）连接起来的努力主要集中在将3D点云嵌入剪辑文本图像空间中。但是，这些方法取决于3D点云，这些云在编码效率和神经网络处理方面面临挑战。随着驱动器OS1等高级激光雷达传感器的出现，除3D点云外，还产生了固定的分辨率深度，信号和环境全景2D图像，基于LIDAR的任务出现了新的机会。在这项工作中，我们提出了一种替代方法，通过利用OS1传感器生成的2D图像而不是3D点云将LIDAR数据与文本联系起来。在零拍设置中使用佛罗伦萨2大型模型，我们执行图像字幕和对象检测。我们的实验表明，与现有方法相比，佛罗伦萨2在对象检测任务中产生更丰富的字幕，并在对象检测任务中实现卓越的性能。通过将高级LIDAR传感器数据与大型预训练模型相结合，我们的方法为挑战检测方案提供了强大而准确的解决方案，包括需要高准确性和鲁棒性的实时应用程序。]]></description>
      <guid>https://arxiv.org/abs/2502.04385</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>迈向公平医疗AI：3D CT基金会嵌入的对抗性偏见</title>
      <link>https://arxiv.org/abs/2502.04386</link>
      <description><![CDATA[ARXIV：2502.04386V1公告类型：新 
摘要：自我监督的学习通过从大规模未标记的数据集中提取有效且可推广的特征来彻底改变了医学成像。最近，自我监督的基础模型已扩展到三维（3D）计算机断层扫描（CT）数据，生成具有1408个功能的紧凑，信息丰富的嵌入，可在下游任务上实现最先进的性能出血检测和肺癌风险预测。但是，这些嵌入已被证明可以编码人口统计信息，例如年龄，性别和种族，这对临床应用的公平构成了重大风险。
  在这项工作中，我们提出了一个基于变体自动编码器（VAE）的对抗性偏见框架，以将这些嵌入到一个新的潜在空间中，其中不再编码人口统计信息，同时保持关键下游任务的性能。我们在NLST肺癌筛查数据集上验证了我们的方法，表明伪造的嵌入有效地消除了多个编码的人口统计学信息，并提高了公平性，而不会在1年间和2年间隔内损害肺癌风险的预测准确性。此外，我们的方法可确保嵌入对抗性偏置攻击方面的牢固性。这些结果突出了对抗性偏见技术的潜力，以确保自我监管的3D CT嵌入临床应用中的公平性和公平性，为他们在公正的医疗决策中更广泛采用铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2502.04386</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>迈向生成AI的公平和健壮的面部解析：一种多目标方法</title>
      <link>https://arxiv.org/abs/2502.04391</link>
      <description><![CDATA[ARXIV：2502.04391V1公告类型：新 
摘要：面部解析是计算机视觉中的一项基本任务，可以实现诸如身份验证，面部编辑和可控图像合成之类的应用程序。但是，现有的面部解析模型通常缺乏公平和鲁棒性，从而导致人口组群体之间的分割和障碍，噪声和域移动下的错误。这些局限性影响下游面部合成，其中分割偏差会降低生成模型输出。我们提出了一个多目标学习框架，该框架优化了面部解析的准确性，公平性和鲁棒性。我们的方法引入了基于同质损失的功能，该功能会在训练过程中动态调整这些目标的重要性。为了评估其影响，我们比较了基于GAN的面部合成管道（Pix2PixHD）中的多目标和单目标U-NET模型。我们的结果表明，公平意识和稳健的分割可改善面部产生的光真相和一致性。此外，我们使用ControlNet（用于基于扩散的合成的结构化条件模型）进行初步实验，以探索分割质量如何影响引导图像产生。我们的发现表明，多目标面对解析提高了人口统计学的一致性和鲁棒性，从而导致了高质量的基于GAN的合成。]]></description>
      <guid>https://arxiv.org/abs/2502.04391</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>UNICP：一个统一的缓存和修剪框架，用于有效的视频生成</title>
      <link>https://arxiv.org/abs/2502.04393</link>
      <description><![CDATA[ARXIV：2502.04393V1公告类型：新 
摘要：视频生成中的扩散变压器（DIT）Excel，但由于注意力的二次复杂性而遇到了重大的计算挑战。值得注意的是，相邻扩散步骤之间的注意力差异遵循U形模式。当前的方法通过缓存注意力障碍来利用这一属性，但是，它们仍然在突然的错误峰值和较大的差异中挣扎。为了解决这些问题，我们建议UNICP一个统一的缓存和修剪框架，以进行有效的视频生成。 UNICP通过通过时间和空间维度来优化。错误了解动态缓存窗口（EDCW）：在各种时间步中，动态调整不同块的缓存窗口大小，以适应突然的错误更改。基于PCA的切片（PCA）和动态重量移位（DWS）：PCAS修剪冗余注意力组件，而DWS通过在修剪和cached输出之间启用动态切换来集成缓存和修剪。通过调整缓存窗口并修剪冗余组件，UNICP提高了计算效率并保持视频详细信息的保真度。实验结果表明，UNICP在性能和效率方面均优于现有方法。]]></description>
      <guid>https://arxiv.org/abs/2502.04393</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>时间vlm：探索增强时间序列的多模式视觉模型预测</title>
      <link>https://arxiv.org/abs/2502.04395</link>
      <description><![CDATA[ARXIV：2502.04395V1公告类型：新 
摘要：时间序列预测的最新进步已经探索了具有文本或视觉方式的增强模型，以提高准确性。尽管文本提供了上下文理解，但通常缺乏细粒度的时间细节。相反，视觉捕获了复杂的时间模式，但缺乏语义上下文，从而限制了这些方式的互补潜力。为了解决这个问题，我们提出了Time-VLM，这是一种新型的多模式框架，利用预先训练的视觉语言模型（VLMS）来桥接时间，视觉和文本方式，以增强预测。我们的框架包括三个关键组成部分：（1）检索授权的学习者，该学习者通过内存库交互来提取丰富的时间特征； （2）一个具有远见的学习者，将时间序列编码为信息图像； （3）文本启动的学习者，该学习者生成上下文文本描述。这些组件与冷冻预训练的VLMS合作生成多模式嵌入，然后将其与时间特征融合以进行最终预测。跨不同数据集的广泛实验表明，Time-VLM可以实现卓越的性能，尤其是在几次射击和零拍摄的情况下，从而为多模式时间序列序列建立了新的方向。]]></description>
      <guid>https://arxiv.org/abs/2502.04395</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>仅解码器的LLM是扩散模型的更好控制器</title>
      <link>https://arxiv.org/abs/2502.04412</link>
      <description><![CDATA[ARXIV：2502.04412V1公告类型：新 
摘要：随着扩散模型的出现，最近已经实现了文本到图像生成的突破性进步。这些模型具有非凡的能力，可以根据文本提示产生高度艺术和错综复杂的图像。但是，获得所需的生成成果通常需要重复的操纵文本提示的试验，就像在魔术镜上铸造法术一样，其原因是当前图像生成模型中固有的语义理解能力有限。具体而言，现有的扩散模型用预先训练的编码器结构编码文本提示输入，该结构通常在有限数量的图像符号对上进行训练。基于仅解码器结构的最先进的大语言模型（LLM）显示出强大的语义理解能力，因为它们的体系结构更适合在非常大规模的未标记数据上培训。在这项工作中，我们建议通过从大型语言模型中借用语义理解的强度来增强文本形象扩散模型，并设计一个简单而有效的适配器，以允许扩散模型与仅解码器的结构兼容。同时，我们还提供了支持各种体系结构（例如，仅编码，编码器编码器和仅解码器）的支持理论分析，并进行了广泛的经验评估以验证其有效性。实验结果表明，在文本到图像生成的质量和可靠性方面，使用适配器模块的增强模型优于最终模型。]]></description>
      <guid>https://arxiv.org/abs/2502.04412</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Terraq：卫星图像档案中的时空提问</title>
      <link>https://arxiv.org/abs/2502.04415</link>
      <description><![CDATA[ARXIV：2502.04415V1公告类型：新 
摘要：Terraq是用于卫星图像档案的时空提问引擎。这是一种自然语言处理系统，旨在处理满足某些标准的卫星图像的请求。这些请求可以参考来自专业知识库（例如Emilia-Romagna地区）的图像元数据和实体。有了它，用户可以提出诸如“给我一百张法国港口河流的图像，雪覆盖范围不到20％，云覆盖率超过10％”，从而使地球观察数据更容易访问，并与当前的数字助手景观。]]></description>
      <guid>https://arxiv.org/abs/2502.04415</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>