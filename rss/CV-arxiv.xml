<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>CS.CV更新arxiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.cv更新arxiv.org e-print存档。</description>
    <lastBuildDate>Tue, 25 Mar 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>AI驱动的缝合专业知识端到端评估的自动化</title>
      <link>https://arxiv.org/abs/2503.17391</link>
      <description><![CDATA[ARXIV：2503.17391V1公告类型：新 
摘要：我们提出了一种基于AI的方法来自动化缝合专业知识的端到端评估，这是一种缝合技能评估工具，该工具全面定义了围绕相关子技能的标准。1虽然Ease提供了与Suthular的技能评估相关的颗粒技能评估，以提供对受训者的相关评估，以便对他们的能力进行客观评估，并通过可衡量的是人类的洞察力，并进行了详尽的评估。基于AI的方法通过在模型推理期间以最少的资源启用实时得分预测来解决这一问题。这使得可能会对外科医生/学员进行实时反馈，从而有可能加速缝合任务的学习过程，并减轻手术过程中的关键错误，从而改善患者的结果。在这项研究中，我们专注于以下在3个缝合阶段的7个舒适域：1）针头处理：重新定位的数量，针保持深度，针保持比率和针头保持角度； 2）针线驾驶：驾驶平滑度和腕部旋转； 3）提取针头：腕部旋转。]]></description>
      <guid>https://arxiv.org/abs/2503.17391</guid>
      <pubDate>Tue, 25 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>IREF-VLA：在3D场景中使用不完美语言进行交互式参考接地的基准</title>
      <link>https://arxiv.org/abs/2503.17406</link>
      <description><![CDATA[ARXIV：2503.17406V1公告类型：新 
摘要：随着大型语言模型，视觉模型和其他一般基础模型的最新兴起，多模式，多任务机器人技术的潜力越来越大，可以在自然语言输入的情况下在不同的环境中运行。一种这样的应用程序是使用自然语言说明的室内导航。但是，尽管最近进展了，但由于需要3D空间推理和语义理解，因此这个问题仍然具有挑战性。此外，所使用的语言可能不完美或与场景未对准，从而使任务变得更加复杂。为了应对这一挑战，我们策划了一个基准数据集IREF-VLA，以在具有不完美的参考文献的3D场景中进行交互式的参考视觉和语言引导的动作。 IREF-VLA是用于参考接地任务的最大的现实世界数据集，由现有数据集的11.5K扫描3D室，760万启发式的语义关系和470万参考语句组成。我们的数据集还包含语义对象和房间注释，场景图，可通道的自由空间注释，并用语言具有不完美或歧义的语句进行增强。我们通过使用最先进的模型评估数据集以获取性能基线，并开发图形搜索基线来验证数据集的普遍性，以使用场景刻画知识来证明性能和替代方案的生成。借助此基准，我们旨在为3D场景的理解提供资源，从而有助于发展稳健的交互式导航系统。数据集和所有源代码均在https://github.com/haochenz11/iref-vla上公开发布。]]></description>
      <guid>https://arxiv.org/abs/2503.17406</guid>
      <pubDate>Tue, 25 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过视觉语言模型（VLM）增强随后的视频检索</title>
      <link>https://arxiv.org/abs/2503.17415</link>
      <description><![CDATA[ARXIV：2503.17415V1公告类型：新 
摘要：视频内容的快速增长需要有效而精确的检索系统。虽然视觉模型（VLM）在表示学习方面表现出色，但他们经常在适应性，时间敏感的视频检索中挣扎。本文介绍了一个新颖的框架，将矢量相似性搜索与基于图的数据结构相结合。通过利用VLM嵌入来进行初始检索和建模视频段之间的上下文关系，我们的方法可以实现自适应查询的细化并提高检索准确性。实验证明了其精度，可伸缩性和鲁棒性，为在动态环境中的交互式视频检索提供了有效的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2503.17415</guid>
      <pubDate>Tue, 25 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于功能的双视觉特征提取模型，用于复合多模式情绪识别</title>
      <link>https://arxiv.org/abs/2503.17453</link>
      <description><![CDATA[ARXIV：2503.17453V1公告类型：新 
摘要：本文介绍了我们对野外竞争（ABAW）竞争的第八次情感行为分析的结果。多种情感识别（ER）在情感计算和人类计算机相互作用中具有重要的应用。但是，在现实世界中，复合情绪识别面临着更多的不确定性和模态冲突问题。对于复合表达（CE）识别挑战，本文提出了一种多模式情感识别方法，该方法融合了视觉变压器（VIT）和残留网络（RESNET）的特征。我们在C-EXPR-DB和MELD数据集上进行了实验。结果表明，在具有复杂的视觉和音频提示的情况下（例如C-EXPR-DB），融合Vit和Resnet功能的模型表现出卓越的性能。我们的代码在https://github.com/mygithub-ax/8th_abaw上可用。]]></description>
      <guid>https://arxiv.org/abs/2503.17453</guid>
      <pubDate>Tue, 25 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>高效率WIENER滤波器基于MPEG G-PCC的云质量增强</title>
      <link>https://arxiv.org/abs/2503.17467</link>
      <description><![CDATA[ARXIV：2503.17467V1公告类型：新 
摘要：点云直接通过大量点记录场景或对象的属性，在虚拟现实和沉浸式通信等各种应用中广泛使用。但是，由于数据量和非结构化的几何形状，对点云的有效压缩至关重要。近年来，Moving Picture专家组正在为静态和动态点云建立基于几何的点云压缩（G-PCC）标准。尽管G-PCC的损耗压缩可以达到非常高的压缩比，但重建质量相对较低，尤其是在低比特率下。为了减轻此问题，我们提出了一个高效率的Wiener滤波器，该过滤器可以集成到G-PCC的编码器和解码器管道中，以提高重建质量以及动态点云的速率延伸性能。具体而言，我们首先提出了一个基本的Wiener滤波器，然后通过引入LUMA组件的系数遗传和基于方差的点分类来改进它。此外，为了减少在使用Wiener滤波器期间最近邻居搜索的复杂性，我们还提出了一种基于Morton代码的快速快速邻居搜索算法，以有效地计算过滤器系数。实验结果表明，所提出的方法可以分别在Luma，Chroma CB和Chroma CR组件分别实现-6.1％，-6.1％，-7.3％和-8.0％的平均BJ {\ o} ntegaard Delta速率，分别在无损失几何形式的配置中，与最新的g-pcc sepry nepry Geentersing（I. I.候选人2）消耗负担得起的计算复杂性。]]></description>
      <guid>https://arxiv.org/abs/2503.17467</guid>
      <pubDate>Tue, 25 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>带有上下文感知视频小管的时空学习超声视频分析</title>
      <link>https://arxiv.org/abs/2503.17475</link>
      <description><![CDATA[ARXIV：2503.17475V1公告类型：新 
摘要：基于视频的成像方式的计算机辅助病理检测算法必须通过整合多个帧的发现来准确地解释复杂的时空信息。当前的最新方法是通过对视频子卷（Tubelets）进行分类来运行的，但是它们通常仅通过专注于检测ROI中的本地区域而失去全球空间环境。在这里，我们为基于小管的对象检测和视频分类提出了一个轻巧的框架，该框架既保留全局空间上下文又保留良好的时空特征。为了解决全球上下文的损失，我们将小管位置，大小和信心嵌入到分类器的输入中。此外，我们还使用了预训练的检测模型中的ROI分配特征图，利用学习的特征表示来增加接受场并降低计算复杂性。我们的方法是有效的，时空小管分类器仅包含0.40万参数。我们采用我们的方法来检测和分类超声视频中的肺合并和胸腔积液。来自828名患者的14,804个视频的五倍跨验证显示，我们的方法表现优于先前的基于小管的方法，并且适用于实时工作流程。]]></description>
      <guid>https://arxiv.org/abs/2503.17475</guid>
      <pubDate>Tue, 25 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>原型：使用3D高斯原型的高效且高质量的渲染</title>
      <link>https://arxiv.org/abs/2503.17486</link>
      <description><![CDATA[ARXIV：2503.17486V1公告类型：新 
摘要：3D高斯脱落（3DGS）在新型视图合成中取得了长足的进步，但受到所需的高斯原始数量的限制，对轻量级设备的部署构成了挑战。最近的方法通过压缩致密的高斯人的存储大小来解决此问题，但无法保留呈现质量和效率。为了克服这些局限性，我们建议原型学习高斯原型以代表高斯原始物，从而大大减少了总高斯量而不牺牲视觉质量。我们的方法直接使用高斯原型来实现有效的渲染并利用所得的重建损失来指导原型学习。为了进一步优化训练期间的记忆效率，我们将结构 - 动作（SFM）点结合在一起，作为高斯基原始人的锚点。高斯原型通过K-均值的聚类来得出，并且锚点和原型均已共同优化。我们对现实世界和合成数据集进行的实验证明，我们的表现优于现有方法，实现了高斯数量的大幅减少，并在维持甚至增强渲染保真度的同时，可以实现高渲染速度。]]></description>
      <guid>https://arxiv.org/abs/2503.17486</guid>
      <pubDate>Tue, 25 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ProdeHaze：促使扩散模型朝着忠实的形象降临</title>
      <link>https://arxiv.org/abs/2503.17488</link>
      <description><![CDATA[ARXIV：2503.17488V1公告类型：新 
摘要：使用大规模预处理扩散模型进行图像去悬式的最新方法提高了感知质量，但通常会遭受幻觉问题的困扰，从而使原始图像产生不忠的脱掩图。为了减轻这种情况，我们提出了ProdeHaze，该框架采用了内部图像先验来指导在验证模型中编码的外部先验。我们介绍了两种类型的\ textIt {选择性}内部先验，它们促使该模型专注于关键图像区域：潜在空间中的结构启用修复器，强调结构丰富的区域，以及在解码过程中的雾化自我校正的炼油机，以使较清晰的输入区域和输出之间的分配相一致。对现实世界数据集的广泛实验表明，ProdeHaze获得高保真性会导致图像飞行，尤其是在减少色彩变化方面。我们的代码在https://github.com/tianwenzhou/prodehaze上。]]></description>
      <guid>https://arxiv.org/abs/2503.17488</guid>
      <pubDate>Tue, 25 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用多模式分析的模因相似性和情感检测</title>
      <link>https://arxiv.org/abs/2503.17493</link>
      <description><![CDATA[ARXIV：2503.17493V1公告类型：新 
摘要：互联网模因是在线文化，融合图像和文本的核心要素。尽管大量研究集中在模因的视觉或文本组成部分上，但对其相互作用的关注很少。这个差距提出了一个关键问题：哪些方法可以有效地比较模因和他们引起的情绪？我们的研究采用了多模式方法论方法，分析了模因的视觉和文本元素。具体而言，我们执行了一个多模式剪辑（对比性语言图像预训练）模型，以根据文本和视觉内容嵌入进行类似的模因进行分组，从而实现了跨模态的强大相似性评估。使用Reddit Meme数据集和Memotion数据集，我们提取低级视觉特征和高级语义功能来识别类似的模因对。为了验证这些自动化的相似性评估，我们对50名参与者进行了一项用户研究，要求他们提供有关模因相似性及其情绪反应的回答。实验结果与人类判断的比较表明，一致性为67.23 \％，这表明计算方法与人类的感知很好地保持一致。此外，我们使用Distilbert模型实施了基于文本的分类器，将模因分类为六种基本情绪之一。结果表明，愤怒和喜悦是模因中的主要情绪，动机模因引起了更强的情感反应。这项研究有助于研究多模式模因，从而增强了基于语言的和视觉方法，以分析和改善在线视觉交流和用户体验。此外，它为在线平台中提供更好的内容审核策略提供了见解。]]></description>
      <guid>https://arxiv.org/abs/2503.17493</guid>
      <pubDate>Tue, 25 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>您只查看一次（任何时间）：分析和优化对象检测的早期外观</title>
      <link>https://arxiv.org/abs/2503.17497</link>
      <description><![CDATA[ARXIV：2503.17497V1公告类型：新 
摘要：我们介绍Anytimeyolo，这是Yolo体系结构的变体系列，可以随时进行对象检测。我们的Any TimeYolo网络允许中断推断，即，它们在任何时间点提供了预测，这是对安全至关重要的实时应用所需的属性。
  我们提出结构化的探索，以修改Yolo架构，从而使早期终止以获得中间结果。我们专注于通过可用终止点的高粒度提供细粒度的控制。首先，我们将任何时间模型形式化为特殊的预测模型，这些模型随时提供预测。然后，我们讨论了Yolo体系结构的一种新颖的转换变体，该变体改变了体系结构，以使更好的早期预测和更大的自由度在处理阶段。最后，我们提出了两种优化算法，鉴于任何时间模型，可以使用这些算法来确定最佳的退出执行顺序和早期外观的最佳子集，以在低资源环境中选择用于部署。我们评估了设计选择的任何时间性能和权衡取舍，并为此目的提出了新的质量指标。特别是，我们还讨论了目前使其部署昂贵的任何时间推理的关键挑战。]]></description>
      <guid>https://arxiv.org/abs/2503.17497</guid>
      <pubDate>Tue, 25 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于事件的交叉数据集（EBCD）</title>
      <link>https://arxiv.org/abs/2503.17499</link>
      <description><![CDATA[ARXIV：2503.17499V1公告类型：新 
摘要：基于事件的视觉通过捕获异步强度变化而不是静态框架来彻底改变传统图像感测，从而实现超快的时间分辨率，稀疏数据编码和增强的运动感知。尽管该范式具有显着优势，但常规的基于事件的数据集施加固定阈值约束以确定像素激活，从而严重限制了对现实世界环境波动的适应性。较低的阈值保留了更精细的细节，但会引入普遍的噪声，而较高的阈值以牺牲关键对象信息为代价抑制无关激活。为了减轻这些限制，我们介绍了基于事件的交叉数据集（EBCD），这是一个针对动态室外环境中的行人和车辆检测量身定制的综合数据集，其中包含了一个多阈值框架以完善事件表示。通过在十个不同的阈值水平（4、8、12、16、20、30、40、50、60和75）下捕获基于事件的图像，该数据集可有助于对对象检测性能在不同条件的稀疏和噪声抑制条件下进行广泛的评估。我们基准基准的最先进的检测体系结构，包括Yolov4，Yolov7，EditiveDet-B0，Mobilenet-V1和定向梯度的直方图（HOG） - 对阈值选择对检测性能的细微影响进行实验。通过提供系统的阈值变化方法，我们预见EBCD可以对基于事件的对象检测进行更适应性的评估，从而使多样化的神经形态视觉与现实场景动态保持一致。我们将数据集展示为公开可用于推动低扩展，高效率神经形态成像的进一步进步：https：//ieee-dataport.org/documents/event/event--crossing-crossing-crossing-dataset-ebcd]]></description>
      <guid>https://arxiv.org/abs/2503.17499</guid>
      <pubDate>Tue, 25 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>我们是否应该为对比的密集预测任务进行对比学习中的解码器吗？</title>
      <link>https://arxiv.org/abs/2503.17526</link>
      <description><![CDATA[ARXIV：2503.17526V1公告类型：新 
摘要：自我监管设置中的对比度学习主要集中于预训练编码器，而解码器通常是在下游密集的预测任务中分别引入和培训的。但是，这种常规的方法忽略了共同预先培训编码器和解码器的潜在好处。在本文中，我们提出了Decon：一种框架 - 不足的适应性，以转换仅包含编码的自我监督学习（SSL）对比方法，以将可以以对比方式进行预训练的有效的Encoder-Decoder框架。我们首先更新现有的体系结构，以容纳解码器及其各自的对比损失。然后，我们引入了加权编码器对比度损失，并通过非竞争目标有助于预训练联合编码器架构。我们适应了针对密集预测任务量身定制的两个已建立的对比度SSL框架，实现了可可对象检测和实例细分的新最先进的结果，并匹配Pascal VOC语义分段的最新性能。我们表明，我们的方法允许预先训练解码器并增强编码器的表示能力及其在密集的预测任务中的性能。这种好处在预训练和微调之间存在着异质解码器架构，并且持续存在于室外，有限的DATA场景中。]]></description>
      <guid>https://arxiv.org/abs/2503.17526</guid>
      <pubDate>Tue, 25 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FMDCONV：通过速度准确权衡取舍的快速多发动态卷积</title>
      <link>https://arxiv.org/abs/2503.17530</link>
      <description><![CDATA[ARXIV：2503.17530V1公告类型：新 
摘要：空间卷积对于构建深层卷积神经网络（CNN）的基础是视觉识别的基础。尽管动态卷积通过自适应结合静态内核来增强模型精度，但它会产生大量的计算开销，从而限制了其在资源受限环境（例如联合边缘计算）中的部署。为了解决这个问题，我们提出了快速的多发动态卷积（FMDCONV），该卷积（FMDCONV）集成了输入注意力，温度下降的内核注意力和输出注意力以优化速度准确性的权衡。 FMDCONV通过选择性增强功能提取及其较低的复杂性来实现准确性和效率之间的更好平衡。此外，我们介绍了两个新颖的定量指标，即逆效率评分和比率校准得分，以系统地评估这一权衡。关于CIFAR-10，CIFAR-100和ImageNet的广泛实验表明，与先前的多次多项式卷积方法相比，RESNET-18的RESNET-18上的计算成本最高为49.8 \％，RESNET-50上的RESNET-50降低了42.2 \％。这些优势使FMDCONV非常适合现实世界中的资源受限应用程序。]]></description>
      <guid>https://arxiv.org/abs/2503.17530</guid>
      <pubDate>Tue, 25 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Dermdiff：减轻皮肤病学诊断种族偏见的生成扩散模型</title>
      <link>https://arxiv.org/abs/2503.17536</link>
      <description><![CDATA[ARXIV：2503.17536V1公告类型：新 
摘要：皮肤疾病，例如皮肤癌，是一个重大的公共卫生问题，早期诊断对于有效治疗至关重要。人工智能（AI）算法有可能协助良性良性与恶性皮肤病变并提高诊断准确性。但是，现有的用于皮肤疾病诊断的AI模型经常在有限和有偏见的数据集上开发和测试，从而导致某些肤色的性能差。为了解决这个问题，我们提出了一种名为Dermdiff的新型生成模型，该模型可以生成多样化和代表性的皮肤镜图像数据，以进行皮肤病诊断。 Dermdiff利用文本提示和多模式图像文本学习，改善了高度不平衡的数据集中代表性不足的组（患者，疾病等）的表示。我们广泛的实验在高保真度和多样性方面展示了Dermdiff的有效性。此外，下游评估表明，Dermdiff在减轻种族偏见以进行皮肤病学诊断方面的潜力。我们的代码可从https://github.com/munia03/dermdiff获得]]></description>
      <guid>https://arxiv.org/abs/2503.17536</guid>
      <pubDate>Tue, 25 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>生成，快速和缓慢：可扩展的并行视频生成，并使用视频接口网络</title>
      <link>https://arxiv.org/abs/2503.17539</link>
      <description><![CDATA[ARXIV：2503.17539V1公告类型：新 
摘要：扩散变压器（DITS）可以生成简短的影片视频，但直接训练和抽样视频在整个视频中都充分关注仍然在计算上具有挑战性。替代方法将长视频分解为短视频段的顺序生成，需要多个采样链迭代和专门的一致性模块。为了克服这些挑战，我们引入了一个名为“视频接口网络”（VIN）的新范式，该范式以抽象模块的形式增强，以实现视频块的平行推断。在每个扩散步骤中，VIN从局部块的嘈杂输入和编码表示的噪声中编码全局语义，而指导在并行地块块中的指导点。 VIN和DIT的耦合是在denoising目标的端到端学习的。此外，VIN体系结构维护固定尺寸的编码令牌，该代币通过单个交叉注意步骤编码输入。从输入中解开编码令牌，使VIN可以扩展到长时间的视频并学习基本语义。在VBENCH上进行的实验表明，VIN超过了基于基于块的基于块的方法，可以保留背景一致性和主题连贯性。然后，我们通过光流分析表明，我们的方法达到了最先进的运动平滑度，而使用的拖鞋比全身少25-40％。最后，人类评估者在用户研究中对我们方法的整体视频质量和时间一致性进行了有利评估。]]></description>
      <guid>https://arxiv.org/abs/2503.17539</guid>
      <pubDate>Tue, 25 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>